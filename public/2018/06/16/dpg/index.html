<!DOCTYPE html>
<html lang="kr">

<!-- Head tag -->
<head>

    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!--Description-->
    
        <meta name="description" content="RLKorea 블로그">
    

    <!--Author-->
    
        <meta name="author" content="Bomi Yu">
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="Deterministic Policy Gradient Algorithms"/>
    

    <!--Open Graph Description-->
    
        <meta property="og:description" content="RLKorea 블로그" />
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content="RLKorea"/>

    <!--Type page-->
    
        <meta property="og:type" content="article" />
    

    <!--Page Cover-->
    

        <meta name="twitter:card" content="summary" />
    

    <!-- Title -->
    
    <title>Deterministic Policy Gradient Algorithms - RLKorea</title>

    <!-- Bootstrap Core CSS -->
    <link href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" rel="stylesheet"/>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/style.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.5.0/styles/monokai-sublime.min.css">
    <link rel="stylesheet" href="/css/customizing.css">

    <!-- Custom Fonts -->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="//fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css">
    <link href="//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    <script src="//oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="//oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Gallery -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.css" type="text/css" rel="stylesheet" />

    <!-- Google Analytics -->
    


    <!-- favicon -->
    
    <link rel="icon" href="/img/favicon.ico"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    
	
</head>


<body>

    <!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- Menu -->
    <!-- Navigation -->
<div class="custom-nav">
	<div class="menu-icon">
		<button>
			<span class="bar"></span>
			<span class="bar"></span>
			<span class="bar"></span>			
		</button>
	</div>
	<a class="logo" href="/">RLKorea</a>

</div>	

<div class="left-side on" id="bs-example-navbar-collapse-1">
	<button class="close">
		<span class="bar"></span>
		<span class="bar"></span>
	</button>
	<div class="intro">
		<a href="/"><img src="/img/logo.png" alt="내사진"></a>
		<p><i class="fa fa-facebook"></i><a href="https://www.facebook.com/groups/ReinforcementLearningKR/"><strong> RLKorea</strong></a><br>
			RLKorea 블로그</p>
	</div>
	<ul>
		<!--<li><a href="/2018/05/16/RLKorea_intro/">RLKorea소개</a></li> -->
		<li class="more_btn">
			<p class="clearfix">
			    <span class="left">
			        프로젝트
			    </span>
			    <i class="fa fa-sort-down right"></i>
			</p>
			<ul>
				<li><a href="/tags/피지여행/">피지여행</a></li>
				<li><a href="/tags/알파오목/">알파오목</a></li>
				<li><a href="/tags/홈네비/">홈네비</a></li>
				<li><a href="/tags/DQN 뽀개기/">DQN 뽀개기</a></li>
				<li><a href="/tags/광산깨기/">광산깨기</a></li>
			</ul>
		</li>
		<li class="more_btn"><a href="#">논문리뷰</a></li>
		<li class="more_btn"><a href="#">강화학습 관련 컨텐츠</a></li>
		<li class="more_btn"><a href="#">강화학습 관련 Q&A</a></li>
		<!-- 
			<li>
				<a href="/">
					
						Home
					
				</a>
			</li>
		
			<li>
				<a href="/archives">
					
						Archives
					
				</a>
			</li>
		
			<li>
				<a href="/tags">
					
						Tags
					
				</a>
			</li>
		
			<li>
				<a href="/categories">
					
						Categories
					
				</a>
			</li>
		
			<li>
				<a href="https://bomee88.github.io/">
					
						<i class="fa fa-github fa-stack-2x"></i>
					
				</a>
			</li>
		 -->
	</ul>
</div>



    <!-- Main Content -->
    <!-- Page Header -->
<!-- Set your background image for this header in your post front-matter: cover -->

<header class="intro-header post">
	<div class="container">
		<div class="row">
			<div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
				<div class="post-heading">
					<h1>Deterministic Policy Gradient Algorithms</h1>
					
					<h2 class="post-subheading">
						피지여행 2번째 논문
					</h2>
					
					<span class="meta">
						<!-- Date and Author -->
						
							Posted by 이웅원 on
						
						
							2018-06-16
						
					</span>
				</div>
			</div>
		</div>
	</div>
</header>

<!-- Post Content -->
<article>
	<div class="container">
		<div class="row">

			<!-- Tags and categories -->
		   
				<div class="col-lg-5 col-lg-offset-2 col-md-6 col-md-offset-1 post-tags">
					
						


<a href="/tags/프로젝트/">#프로젝트</a> <a href="/tags/피지여행/">#피지여행</a>


					
				</div>
				<div class="col-lg-3 col-md-4 post-categories">
					
						

<a href="/categories/프로젝트/">프로젝트</a>

					
				</div>
			

			<!-- Gallery -->
			

			<!-- Post Main Content -->
			<div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
				<p>Authors: David Silver<sup>1)</sup>, Guy Lever<sup>2)</sup>, Nicloas Heess<sup>1)</sup>, Thomas Degris<sup>1)</sup>, Daan Wierstra<sup>1)</sup>, Martin Riedmiller<sup>1)</sup><br>Affiliation: 1) Google Deepmind, 2) University College London (UCL)<br>Proceeding: International Conference on Machine Learning (ICML) 2014</p>
<ul>
<li><a href="#deterministic-policy-gradient-algorithms">Deterministic Policy Gradient Algorithms</a><ul>
<li><a href="#summary">Summary</a></li>
<li><a href="#background">Background</a><ul>
<li><a href="#performance-objective-function">Performance objective function</a></li>
<li><a href="#spg-theorem">SPG Theorem</a></li>
<li><a href="#stochastic-actor-critic-algorithms">Stochastic Actor-Critic Algorithms</a></li>
<li><a href="#off-policy-actor-critic">Off-policy Actor-Critic</a></li>
</ul>
</li>
<li><a href="#gradient-of-deterministic-policies">Gradient of Deterministic Policies</a><ul>
<li><a href="#regulariy-conditions">Regulariy Conditions</a></li>
<li><a href="#deterministic-policy-gradient-theorem">Deterministic Policy Gradient Theorem</a></li>
<li><a href="#dpg-형태에-대한-informal-intuition">DPG 형태에 대한 informal intuition</a></li>
<li><a href="#dpg-는-spg-의-limiting-case-임">DPG 는 SPG 의 limiting case 임</a></li>
</ul>
</li>
<li><a href="#deterministic-actor-critic-algorithms">Deterministic Actor-Critic Algorithms</a></li>
<li><a href="#experiments">Experiments</a><ul>
<li><a href="#continuous-bandit">Continuous Bandit</a></li>
<li><a href="#continuous-reinforcement-learning">Continuous Reinforcement Learning</a></li>
<li><a href="#octopus-arm">Octopus Arm</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<!-- /code_chunk_output -->
<p><br></p>
<hr>
<h2 id="1-Summary"><a href="#1-Summary" class="headerlink" title="1. Summary"></a>1. Summary</h2><ul>
<li>Deterministic Policy Gradient (DPG) Theorem 제안함 [<a href="#deterministic-policy-gradient-theorem">Theorem 1</a>]<br>  1) DPG는 존재하며,<br>  2) DPG는 Expected gradient of the action-value function의 형태를 띈다.</li>
<li>Policy variance가 0에 수렴할 경우, DPG는 Stochastic Policy Gradient(SPG)와 동일해짐을 보임 [<a href="#dpg-는-spg-의-limiting-case-임">Theorem 2</a>]<ul>
<li>Theorem 2 로 인해 기존 Policy Gradient (PG) 와 관련된 기법들을 DPG 에 적용할 수 있게 됨<ul>
<li>예. Sutton PG, natural gradients, actor-critic, episodic/batch methods</li>
</ul>
</li>
</ul>
</li>
<li>적절한 exploration 을 위해 model-free, off-policy Actor-Critic algorithm 을 제안함<ul>
<li>Action-value function approximator 사용으로 인해 policy gradient가 bias되는 것을 방지하기 위해 compatibility conditions을 제공 [<a href="##deterministic-actor-critic-algorithms">Theorem 3</a>]</li>
</ul>
</li>
<li>DPG 는 SPG 보다 성능이 좋음<ul>
<li>특히 high dimensional action spaces 을 가지는 tasks에서의 성능 향상이 큼<ul>
<li>SPG의 policy gradient는 state와 action spaces 모두에 대해서, DPG의 policy gradient는  state spaces에 대해서만 평균을 취함</li>
<li>결과적으로, action spaces의 dimension이 커질수록 data efficiency가 높은 DPG의 학습이 더 잘 이뤄지게 됨</li>
<li>무한정 학습을 시키게 되면, SPG도 최적으로 수렴할 것으로 예상되기에 위 성능 비교는 일정 iteration 내로 한정함</li>
</ul>
</li>
<li>기존 기법들에 비해 computation 양이 많지 않음<ul>
<li>Computation 은 action dimensionality 와 policy parameters 수에 비례함<br><br></li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><h3 id="2-1-Performance-objective-function"><a href="#2-1-Performance-objective-function" class="headerlink" title="2.1 Performance objective function"></a>2.1 Performance objective function</h3><p>$$<br>\begin{align}<br>J(\pi_{\theta}) &amp;= \int_{S}\rho^{\pi}(s)\int_{A}\pi_{\theta}(s,a)r(s,a)dads = E_{s \sim \rho^{\pi}, a \sim \pi_{\theta}}[r(s,a)]<br>\end{align}<br>$$</p>
<h3 id="2-2-SPG-Theorem"><a href="#2-2-SPG-Theorem" class="headerlink" title="2.2 SPG Theorem"></a>2.2 SPG Theorem</h3><ul>
<li>State distribution $ \rho^{\pi}(s) $ 은 policy parameters에 영향을 받지만, policy gradient 를 계산할 때는 state distribution 의 gradient 를 고려할 필요가 없다.</li>
<li>$$\begin{eqnarray}\nabla_{\theta}J(\pi_{\theta}) &amp;=&amp; \int_{S}\rho^{\pi}(s)\int_{A}\nabla_{\theta}\pi_{\theta}(a|s)Q^{\pi}(s,a)dads \nonumber \ &amp;=&amp; E_{s \sim \rho^{\pi}, a \sim \pi_{\theta}}[\nabla_{\theta}log\pi_{\theta}(a|s)Q^{\pi}(s,a)]<br>\end{eqnarray}$$</li>
</ul>
<h3 id="2-3-Stochastic-Actor-Critic-Algorithms"><a href="#2-3-Stochastic-Actor-Critic-Algorithms" class="headerlink" title="2.3 Stochastic Actor-Critic Algorithms"></a>2.3 Stochastic Actor-Critic Algorithms</h3><ul>
<li>Actor 와 Critic 이 번갈아가면서 동작하며 stochastic policy 를 최적화하는 기법</li>
<li>Actor: $ Q^{\pi}(s,a) $ 를 근사한 $ Q^w(s,a) $ 를 이용해 stochastic policy gradient 를 ascent 하는 방향으로 policy parameter $ \theta $ 를 업데이트함으로써 stochastic policy 를 발전시킴<ul>
<li>$ \nabla_{\theta}J(\pi_{\theta}) = E_{s \sim \rho^{\pi}, a \sim \pi_{\theta}}[\nabla_{\theta}log\pi_{\theta}(a|s)Q^{w}(s,a)] $</li>
</ul>
</li>
<li>Critic: SARSA 나 Q-learning 같은 Temporal-difference (TD) learning 을 이용해 action-value function의 parameter, $ w $ 를 업데이트함으로써 $ Q^w(s,a) $ 가 $ Q^{\pi}(s,a) $ 과 유사해지도록 함</li>
<li>실제 값인 $ Q^{\pi}(s,a) $ 대신 이를 근사한 $ Q^w(s,a) $ 를 사용하게 되면, 일반적으로 bias 가 발생하게 된다. 하지만, compatible condition 에 부합하는 $ Q^w(s,a) $ 를 사용하게 되면, bias 가 발생하지 않는다.</li>
</ul>
<h3 id="2-4-Off-policy-Actor-Critic"><a href="#2-4-Off-policy-Actor-Critic" class="headerlink" title="2.4 Off-policy Actor-Critic"></a>2.4 Off-policy Actor-Critic</h3><ul>
<li>Distinct behavior policy $ \beta(a|s) ( \neq \pi_{\theta}(a|s) ) $ 로부터 샘플링된 trajectories 를 이용한 Actor-Critic</li>
<li>Performance objective function<ul>
<li>$\begin{eqnarray}<br>  J_{\beta}(\pi_{\theta})<br>  &amp;=&amp; \int_{S}\rho^{\beta}(s)V^{\pi}(s)ds \nonumber \\<br>  &amp;=&amp; \int_{S}\int_{A}\rho^{\beta}(s)\pi_{\theta}(a|s)Q^{\pi}(s,a)dads<br>  \end{eqnarray} $</li>
</ul>
</li>
<li>off-policy policy gradient<ul>
<li>$ \begin{eqnarray}<br>  \nabla_{\theta}J_{\beta}(\pi_{\theta}) &amp;\approx&amp; \int_{S}\int_{A}\rho^{\beta}(s)\nabla_{\theta}\pi_{\theta}(a|s)Q^{\pi}(s,a)dads \nonumber \end{eqnarray} $<br>$=E_{s \sim \rho^{\beta}, a \sim \beta}[\frac{\pi_{\theta}(a|s)}{\beta_{\theta}(a|s)}\nabla_{\theta}log\pi_{\theta}(a|s)Q^{\pi}(s,a)]$</li>
<li>off-policy policy gradient 식에서의 물결 표시는 [Degris, 2012b] 논문에 근거함<ul>
<li>[Degris, 2012b] “Linear off-policy actor-critic,” ICML 2012</li>
<li>Exact off-policy policy gradient 와 이를 approximate 한 policy gradient 는 아래와 같음. (빨간색 상자에 있는 항목을 삭제함으로써 근사함)<ul>
<li><img src="https://www.dropbox.com/s/xzpv3okc139c1fs/Screenshot%202018-06-16%2017.48.51.png?dl=1" width="500px"></li>
</ul>
</li>
<li>[Degris, 2012b] Theorem 1 에 의해 policy parameter 가 approximated policy gradient ( $\nabla_{u}𝑄^{\pi,\gamma}(𝑠,𝑎)$ term 제거)에 따라 업데이트되어도 policy 는 improve 가 됨이 보장되기에 exact off-policy policy gradient 대신 approximated off-policy policy gradient 를 사용해도 괜찮음.<ul>
<li><img src="https://www.dropbox.com/s/mk13931r4scjngo/Screenshot%202018-06-16%2017.49.24.png?dl=1" width="500px"></li>
</ul>
</li>
</ul>
</li>
<li>off-policy policy gradient 식에서 $ \frac{\pi_{\theta}(a|s)}{\beta_{\theta}(a|s)} $ 는 importance sampling ratio 임<ul>
<li>off-policy actor-critic 에서는 $ \beta $ 에 의해 샘플링된 trajectory 를 이용해서 stochastic policy $ \pi $ 를 예측하는 것이기 때문에 imnportance sampling 이 필요함<br><br></li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="3-Gradient-of-Deterministic-Policies"><a href="#3-Gradient-of-Deterministic-Policies" class="headerlink" title="3. Gradient of Deterministic Policies"></a>3. Gradient of Deterministic Policies</h2><h3 id="3-1-Regulariy-Conditions"><a href="#3-1-Regulariy-Conditions" class="headerlink" title="3.1 Regulariy Conditions"></a>3.1 Regulariy Conditions</h3><ul>
<li>어떠한 이론이 성립하기 위한 전제 조건</li>
<li>Regularity conditions A.1<ul>
<li>$ p(s’|s,a), \nabla_{a}p(s’|s,a), \mu_{\theta}(s), \nabla_{\theta}\mu_{\theta}(s), r(s,a), \nabla_{a}r(s,a), p_{1}(s) $ are continuous in all parameters and variables $ s, a, s’ $ and $ x $.</li>
</ul>
</li>
<li>regularity conditions A.2<ul>
<li>There exists a $ b $ and $ L $ such that $ \sup_{s}p_{1}(s) &lt; b $, $ \sup_{a,s,s’}p(s’|s,a) &lt; b $, $ \sup_{a,s}r(s,a) &lt; b $, $ \sup_{a,s,s’}|\nabla_{a}p(s’|s,a)| &lt; L $, and $ \sup_{a,s}|\nabla_{a}r(s,a)| &lt; L $.</li>
</ul>
</li>
</ul>
<h3 id="3-2-Deterministic-Policy-Gradient-Theorem"><a href="#3-2-Deterministic-Policy-Gradient-Theorem" class="headerlink" title="3.2 Deterministic Policy Gradient Theorem"></a>3.2 Deterministic Policy Gradient Theorem</h3><ul>
<li>Deterministic policy<ul>
<li>$ \mu_{\theta} : S \to A $ with parameter vector $ \theta \in \rm I ! R^n $</li>
</ul>
</li>
<li>Probability distribution<ul>
<li>$ p(s \to s’, t, \mu) $</li>
</ul>
</li>
<li>Discounted state distribution<ul>
<li>$ \rho^{\mu}(s) $</li>
</ul>
</li>
<li>Performance objective</li>
</ul>
<p>$$<br>J(\mu_{\theta}) = E[r^{\gamma}_{1} | \mu]<br>$$</p>
<p>$$<br>= \int_{S}\rho^{\mu}(s)r(s,\mu_{\theta}(s))ds<br>= E_{s \sim \rho^{\mu}}[r(s,\mu_{\theta}(s))]<br>$$</p>
<ul>
<li><p>DPG Theorem</p>
<ul>
<li><p>MDP 가 A.1 만족한다면, 아래 식(9)이 성립함<br>$\nabla_{\theta}J(\mu_{\theta}) = \int_{S}\rho^{\mu}(s)\nabla_{\theta}\mu_{\theta}(s)\nabla_{a}Q^{\mu}(s,a)\vert_{a=\mu_{\theta}(s)}ds \nonumber$<br>$= E_{s \sim \rho^{\mu}}[\nabla_{\theta}\mu_{\theta}(s)\nabla_{a}Q^{\mu}(s,a)\vert_{a=\mu_{\theta}(s)}]   \nonumber $ (9)</p>
</li>
<li><p>DPG는 State space 에 대해서만 평균을 취하면 되기에, State와 Action space 모두에 대해 평균을 취해야 하는 <a href="#spg-theorem">SPG</a>에 비해 data efficiency가 좋다. 즉, 더 적은 양의 데이터로도 학습이 잘 이뤄지게 된다.</p>
</li>
</ul>
</li>
</ul>
<h3 id="3-3-DPG-형태에-대한-informal-intuition"><a href="#3-3-DPG-형태에-대한-informal-intuition" class="headerlink" title="3.3 DPG 형태에 대한 informal intuition"></a>3.3 DPG 형태에 대한 informal intuition</h3><ul>
<li>Generalized policy iteration<ul>
<li>정책 평가와 정책 발전을 한 번 씩 번갈아 가면서 실행하는 정책 iteration<ul>
<li>위와 같이 해도 정책 평가에서 예측한 가치함수가 최적 가치함수에 수렴함</li>
</ul>
</li>
</ul>
</li>
<li>정책 평가<ul>
<li>action-value function $ Q^{\pi}(s,a) $ or $ Q^{\mu}(s,a) $ 을 estimate 하는 것</li>
</ul>
</li>
<li>정책 발전<ul>
<li>위 estimated action-value function 에 따라 정책을 update 하는 것</li>
<li>주로 action-value function 에 대한 greedy maximisation 사용함<ul>
<li>$ \mu^{k+1}(s) = \arg\max\limits_{a}Q^{\mu^{k}}(s,a) $</li>
<li>greedy 정책 발전은 매 단계마다 global maximization을 해야하는데, 이는 continuous action spaces 에서 계산량이 급격히 늘어나게 됨.</li>
</ul>
</li>
<li>그렇기에 policy gradient 방법이 나옴<ul>
<li>policy 를 $ \theta $ 에 대해서 parameterize 함</li>
<li>매 단계마다 global maximisation 수행하는 대신, 방문하는 state $ s $ 마다 policy parameter 를 action-value function $ Q $ 의 $ \theta $ 에 대한 gradient $ \nabla_{\theta}Q^{\mu^{k}}(s,\mu_{\theta}(s)) $ 방향으로 proportional 하게 update 함</li>
<li>하지만 각 state 는 다른 방향을 제시할 수 있기에, state distribution $ \rho^{\mu}(s) $ 에 대한 기대값을 취해 policy parameter 를 update 할 수도 있음<ul>
<li>$ \theta^{k+1} = \theta^{k} + \alpha \rm I!E_{s \sim \rho^{\mu^{k}}} [\nabla_{\theta}Q^{\mu^{k}}(s,\mu_{\theta}(s))] $</li>
</ul>
</li>
<li>이는 chain-rule 에 따라 아래와 같이 분리될 수 있음<ul>
<li>$ \theta^{k+1} = \theta^{k} + \alpha \rm I!E_{s \sim \rho^{\mu^{k}}} [\nabla_{\theta}\mu_{\theta}(s)\nabla_{a}Q^{\mu^{k}}(s,a)\vert_{a=\mu_{\theta}(s)}] $ (7)</li>
<li>chain rule: $ \frac{\partial Q}{\partial \theta} = \frac{\partial a}{\partial \theta} \frac{\partial Q}{\partial a} $</li>
</ul>
</li>
<li>하지만 state distribution $ \rho^{\mu} $ 은 정책에 dependent 함<ul>
<li>정책이 바꾸게 되면, 바뀐 정책에 따라 방문하게 되는 state 가 변하게 되기 때문에 state distribution이 변하게 됨</li>
</ul>
</li>
<li>그렇기에 정책 update 시 state distribution에 대한 gradient 를 고려하지 않는데 정책 발전이 이뤄진다는 것은 직관적으로 와닿지 않을 수 있음</li>
<li>deterministic policy gradient theorem 은 state distribution에 대한 gradient 계산없이 위 식(7) 대로만 update 해도 performance objective 의 gradient 를 정확하게 따름을 의미한다.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="3-4-DPG-는-SPG-의-limiting-case-임"><a href="#3-4-DPG-는-SPG-의-limiting-case-임" class="headerlink" title="3.4 DPG 는 SPG 의 limiting case 임"></a>3.4 DPG 는 SPG 의 limiting case 임</h3><ul>
<li>stochastic policy parameterization<ul>
<li>$ \pi_{\mu_{\theta},\sigma} $ by a deterministic policy $ \mu_{\theta} : S \to A $ and a variance parameter $ \sigma $</li>
<li>$ \sigma = 0 $ 이면, $ \pi_{\mu_{\theta},\sigma} \equiv \mu_{\theta} $</li>
</ul>
</li>
<li>Theorem 2. Policy 의 variance 가 0 에 수렴하면, 즉, $ \sigma \to 0 $, stochastic policy gradient 와 deterministic policy gradient 는 동일해짐<ul>
<li>조건 : stochastic policy $ \pi_{\mu_{\theta},\sigma} = \nu_{\sigma}(\mu_{\theta}(s),a) $<ul>
<li>$ \sigma $ 는 variance</li>
<li>$ \nu_{\sigma}(\mu_{\theta}(s),a) $ 는 conditions B.1 만족</li>
<li>MDP 는 conditions A.1 및 A.2 만족</li>
</ul>
</li>
<li>결과 :<ul>
<li>$ \lim\limits_{\sigma\downarrow0}\nabla_{\theta}J(\pi_{\mu_{\theta},\sigma}) = \nabla_{\theta}J(\mu_{\theta})  $<ul>
<li>좌변은 standard stochastic gradient 이며, 우변은 deterministic gradient.</li>
</ul>
</li>
</ul>
</li>
<li>의미 :<ul>
<li>deterministic policy gradient 는 stochastic policy gradient 의 특수 case 임</li>
<li>기존 유명한 policy gradients 기법들에 deterministic policy gradients 를 적용할 수 있음<ul>
<li>기존 기법들 예: compatible function approximation (Sutton, 1999), natural gradients (Kakade, 2001), actor-critic (Bhatnagar, 2007) or episodic/batch methods (Peters, 2005)<br><br></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="4-Deterministic-Actor-Critic-Algorithms"><a href="#4-Deterministic-Actor-Critic-Algorithms" class="headerlink" title="4. Deterministic Actor-Critic Algorithms"></a>4. Deterministic Actor-Critic Algorithms</h2><ol>
<li>살사 critic 을 이용한 on-policy actor-critic<ul>
<li>단점<ul>
<li>deterministic policy 에 의해 행동하면 exploration 이 잘 되지 않기에, sub-optimal 에 빠지기 쉬움</li>
</ul>
</li>
<li>목적<ul>
<li>교훈/정보제공</li>
<li>환경에서 충분한 noise 를 제공하여 exploration을 시킬 수 있다면, deterministic policy 를 사용한다고 하여도 좋은 학습 결과를 얻을 수도 있음$<ul>
<li>예. 바람이 agent의 행동에 영향(noise)을 줌</li>
</ul>
</li>
</ul>
</li>
<li>Remind: 살사(SARSA) update rule<ul>
<li>$ Q(s_{t},a_{t}) \leftarrow Q(s_{t},a_{t}) + \alpha(r_{t} + \gamma Q(s_{t+1},a_{t+1}) - Q(s_{t},a_{t})) $</li>
</ul>
</li>
<li>Algorithm<ul>
<li>Critic 은 MSE 를 $ \bf minimize $ 하는 방향, 즉, action-value function 을 stochastic gradient $ \bf descent $ 방법으로 update 함<ul>
<li>$ MSE = [Q^{\mu}(s,a) - Q^{w}(s,a)]^2 $<ul>
<li>critic 은 실제 $ Q^{\mu}(s,a) $ 대신 미분 가능한 $ Q^{w}(s,a) $ 로 대체하여 action-value function 을 estimate 하며, 이 둘 간 Mean Square Error 를 최소화하는 것이 목표</li>
</ul>
</li>
<li>$ \nabla_{w}MSE \approx -2 * [r + \gamma Q^{w}(s’,a’) - Q^{w}(s,a)]\nabla_{w}Q^{w}(s,a)  $<ul>
<li>$ \nabla_{w}MSE = -2 * [Q^{\mu}(s,a) - Q^{w}(s,a)]\nabla_{w}Q^{w}(s,a)  $</li>
<li>$ Q^{\mu}(s,a) $ 를 $ r + \gamma Q^{w}(s’,a’) $ 로 대체<ul>
<li>$ Q^{\mu}(s,a) = r + \gamma Q^{\mu}(s’,a’) $</li>
</ul>
</li>
</ul>
</li>
<li>$ w_{t+1} = w_{t} + \alpha_{w}\delta_{t}\nabla_{w}Q^{w}(s_{t},a_{t}) $<ul>
<li>$w_{t+1} = w_{t} - \alpha_{w}\nabla_{w}MSE  \nonumber$<br>$ \approx w_{t} - \alpha_{w} <em> (-2 </em> [r + \gamma Q^{w}(s’,a’) - Q^{w}(s,a)] \nabla_{w}Q^{w}(s,a)$</li>
<li>$ \delta_{t} = r_{t} + \gamma Q^{w}(s_{t+1},a_{t+1}) - Q^{w}(s_{t},a_{t}) $</li>
</ul>
</li>
</ul>
</li>
<li>Actor 는 식(9)에 따라 보상이 $ \bf maximize $ 되는 방향, 즉, deterministic policy 를 stochastic gradient $ \bf ascent $ 방법으로 update함<ul>
<li>$ \theta_{t+1} = \theta_{t} + \alpha_{\theta} \nabla_{\theta}\mu_{\theta}(s_{t})\nabla_{a}Q^{w}(s_{t},a_{t})\vert_{a=\mu_{\theta}(s)} $</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Q-learning 을 이용한 off-policy actor-critic<ul>
<li>stochastic behavior policy $ \beta(a|s) $ 에 의해 생성된 trajectories 로부터 deterministic target policy $ \mu_{\theta}(s) $ 를 학습하는 off-policy actor-critic</li>
<li>performance objective<ul>
<li>$ J_{\beta}(\mu_{\theta}) = \int_{S}\rho^{\beta}(s)V^{\mu}(s)ds \nonumber \\$<br>$= \int_{S}\rho^{\beta}(s)Q^{\mu}(s,\mu_{\theta}(s))ds \nonumber \\$<br>$= E_{s \sim \rho^{\beta}}[Q^{\mu}(s,\mu_{\theta}(s))]$</li>
</ul>
</li>
<li>off-policy deterministic policy gradient<ul>
<li>$ \nabla_{\theta}J_{\beta}(\mu_{\theta}) = E_{s \sim \rho^{\beta}}[\nabla_{\theta}\mu_{\theta}(s)\nabla_{a}Q^{\mu}(s,a)\vert_{a=\mu_{\theta}(s)}] $<ul>
<li>논문에는 아래와 같이 나와있는데, 물결 표시 부분은 오류로 판단됨.</li>
<li>$ \begin{eqnarray}<br>  \nabla_{\theta}J_{\beta}(\mu_{\theta}) &amp;\approx&amp; \int_{S}\rho^{\beta}(s)\nabla_{\theta}\mu_{\theta}(a|s)Q^{\mu}(s,a)ds \nonumber \<br>  &amp;=&amp; E_{s \sim \rho^{\beta}}[\nabla_{\theta}\mu_{\theta}(s)\nabla_{a}Q^{\mu}(s,a)\vert_{a=\mu_{\theta}(s)}]<br>  \end{eqnarray} $</li>
<li>근거: Action이 deterministic 하기에 stochastic 경우와는 다르게 performance objective 에서 action 에 대해 평균을 구할 필요가 없음. 그렇기에, 곱의 미분이 있을 필요가 없고, [Degris, 2012b] 에서처럼 곱의 미분을 통해 생기는 action-value function 에 대한 gradient term 를 생략할 필요가 사라짐. <a href="#off-policy-actor-critic">참고</a></li>
</ul>
</li>
</ul>
</li>
<li>Remind: 큐러닝(Q-learning) update rule<ul>
<li>$ Q(s_{t},a_{t}) \leftarrow Q(s_{t},a_{t}) + \alpha(r_{t} + \gamma \max\limits_{a}Q(s_{t+1},a) - Q(s_{t},a_{t})) $</li>
</ul>
</li>
<li>algorithm: OPDAC (Off-Policy Deterministic Actor-Critic)<ul>
<li>살사를 이용한 on-policy deterministic actor-critic 과 아래 부분을 제외하고는 같음<ul>
<li>target policy 는 $ \beta(a|s) $ 에 의해 생성된 trajectories 를 통해 학습함</li>
<li>업데이트 목표 부분에 실제 행동 값 $ a_{t+1} $ 이 아니라 정책으로부터 나온 행동 값 $ \mu_{\theta}(s_{t+1}) $ 사용함<ul>
<li>$ \mu_{\theta}(s_{t+1}) $ 는 가장 높은 Q 값을 가지는 행동. 즉, Q-learning.</li>
</ul>
</li>
</ul>
</li>
<li>$ \delta_{t} = r_{t} + \gamma Q^{w}(s_{t+1},\mu_{\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $</li>
<li>$ w_{t+1} = w_{t} + \alpha_{w}\delta_{t}\nabla_{w}Q^{w}(s_{t},a_{t}) $</li>
<li>$ \theta_{t+1} = \theta_{t} + \alpha_{\theta} \nabla_{\theta}\mu_{\theta}(s_{t})\nabla_{a}Q^{w}(s_{t},a_{t})\vert_{a=\mu_{\theta}(s)} $</li>
</ul>
</li>
<li>Stochastic off-policy actor-critic 은 대개 actor 와 critic 모두 importance sampling을 필요로 하지만, deterministic policy gradient 에선 importance sampling이 필요없음<ul>
<li>Actor 는 deterministic 이기에 sampling 자체가 필요없음<ul>
<li>Stochastic policy 인 경우, Actor 에서 importance sampling이 필요한 이유는 상태 $ s $ 에서의 가치 함수 값 $ V^{\pi}(s) $ 을 estimate 하기 위해 $ \pi $ 가 아니라 $ \beta $ 에 따라 sampling을 한 후, 평균을 내기 때문임.</li>
<li>하지만 Deterministic policy 인 경우, 상태 $ s $ 에서의 가치 함수 값 $ V^{\pi}(s) = Q^{\pi}(s,\mu_{\theta}) $ 즉, action 이 상태 s 에 대해 deterministic 이기에 sampling 을 통해 estimate 할 필요가 없고, 따라서 importance sampling도 필요없어짐.</li>
<li>stochastic vs. deterministic performance objective<ul>
<li>stochastic : $ J_{\beta}(\mu_{\theta}) = \int_{S}\int_{A}\rho^{\beta}(s)\pi_{\theta}(a|s)Q^{\pi}(s,a)dads $</li>
<li>deterministic : $ J_{\beta}(\mu_{\theta}) = \int_{S}\rho^{\beta}(s)Q^{\mu}(s,\mu_{\theta}(s))ds $</li>
</ul>
</li>
</ul>
</li>
<li>Critic 이 사용하는 Q-learning 은 importance sampling이 필요없는 off policy 알고리즘임.<ul>
<li>Q-learning 도 업데이트 목표를 특정 분포에서 샘플링을 통해 estimate 하는 것이 아니라 Q 함수를 최대화하는 action을 선택하는 것이기에 위 actor 에서의 deterministic 경우와 비슷하게 볼 수 있음</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>compatible function approximation 및 gradient temporal-difference learning 을 이용한 actor-critic<ul>
<li>위 살사/Q-learning 기반 on/off-policy 는 아래와 같은 문제가 존재<ul>
<li>function approximator 에 의한 bias<ul>
<li>일반적으로 $ Q^{\mu}(s,a) $ 를 $ Q^{w}(s,a) $ 로 대체하여 deterministic policy gradient 를 구하면, 그 gradient 는 ascent 하는 방향이 아닐 수도 있음</li>
</ul>
</li>
<li>off-policy learning 에 의한 instabilities</li>
</ul>
</li>
<li>그래서 stochastic 처럼 $ \nabla_{a}Q^{\mu}(s,a) $ 를 $ \nabla_{a}Q^{w}(s,a) $ 로 대체해도 deterministic policy gradient 에 영향을 미치지 않을 compatible function approximator $ Q^{w}(s,a) $ 를 찾아야 함</li>
<li>Theorem 3. 아래 두 조건을 만족하면, $ Q^{w}(s,a) $ 는 deterministic policy $ \mu_{\theta}(s) $ 와 compatible 함. 즉, $ \nabla_{\theta}J_{\beta}(\mu_{\theta}) = E_{s \sim \rho^{\beta}}[\nabla_{\theta}\mu_{\theta}(s)\nabla_{a}Q^{w}(s,a)\vert_{a=\mu_{\theta}(s)}] $<ul>
<li>$ \nabla_{a}Q^{w}(s,a)\vert_{a=\mu_{\theta}(s)} = \nabla_{\theta}\mu_{\theta}(s)^{\top}w $</li>
<li>$ w $ 는 $ MSE(\theta, w) = E[\epsilon(s;\theta,w)^{\top}\epsilon(s;\theta,w)] $ 를 최소화함<ul>
<li>$ \epsilon(s;\theta,w) = \nabla_{a}Q^{w}(s,a)\vert_{a=\mu_{\theta}(s)} - \nabla_{a}Q^{\mu}(s,a)\vert_{a=\mu_{\theta}(s)}  $</li>
</ul>
</li>
</ul>
</li>
<li>Theorem 3 은 on-policy 뿐만 아니라 off-policy 에도 적용 가능함</li>
<li>$ Q^{w}(s,a) = (a-\mu_{\theta}(s))^{\top}\nabla_{\theta}\mu_{\theta}(s)^{\top} w + V^{v}(s) $<ul>
<li>어떠한 deterministic policy 에 대해서도 위 형태와 같은 compatible function approximator 가 존재함.</li>
<li>앞의 term 은 advantage 를, 뒤의 term 은 value 로 볼 수 있음</li>
</ul>
</li>
<li>$ Q^{w}(s,a) = \phi(s,a)^{\top} w + v^{\top}\phi(s) $<ul>
<li>정의 : $ \phi(s,a) \overset{\underset{\mathrm{def}}{}}{=} \nabla_{\theta}\mu_{\theta}(s)(a-\mu_{\theta}(s)) $</li>
<li>일례 : $ V^{v}(s) = v^{\top}\phi(s) $</li>
<li>Theorem 3 만족 여부<ul>
<li>첫 번째 조건 만족.</li>
<li>두 번째 조건은 대강 만족.<ul>
<li>$ \nabla_{a}Q^{\mu}(s,a) $ 에 대한 unbiased sample 을 획득하기는 매우 어렵기에, 일반적인 정책 평가 방법들로 $ w $ 학습.</li>
<li>이 정책 평가 방법들을 이용하면 $ Q^{w}(s,a) \approx Q^{\mu}(s,a) $ 인 reasonable solution을 찾을 수 있기에 대강 $ \nabla_{a}Q^{w}(s,a) \approx \nabla_{a}Q^{\mu}(s,a) $ 이 될 것.</li>
</ul>
</li>
</ul>
</li>
<li>action-value function 에 대한 linear function approximator 는 큰 값을 가지는 actions 에 대해선 diverge할 수 있어 global 하게 action-values 예측하기에는 좋지 않지만, local critic 에 사용할 때는 매우 유용하다.<ul>
<li>즉, 절대값이 아니라 작은 변화량을 다루는 gradient method 경우엔 $ A^{w}(s,\mu_{\theta}(s)+\delta) = \delta^{\top}\nabla_{\theta}\mu_{\theta}(s)^{\top}w $ 로, diverge 하지 않고, 값을 얻을 수 있음</li>
</ul>
</li>
</ul>
</li>
<li>COPDAC-Q algorithm (Compatible Off-Policy Deterministic Actor-Critic Q-learning critic)<ul>
<li>Critic: 실제 action-value function 에 대한 linear function approximator 인 $ Q^{w}(s,a) = \phi(s,a)^{\top}w $ 를 estimate<ul>
<li>$ \phi(s,a) = a^{\top}\nabla_{\theta}\mu_{\theta} $</li>
<li>Behavior policy $ \beta(a|s) $ 로부터 얻은 samples를 이용하여 Q-learning 이나 gradient Q-learning 과 같은 off-policy algorithm 으로 학습 가능함</li>
</ul>
</li>
<li>Actor: estimated action-value function 에 대한 gradient 를 $ \nabla_{\theta}\mu_{\theta}(s)^{\top}w $ 로 치환 후, 정책을 업데이트함</li>
<li>$ \delta_{t} = r_{t} + \gamma Q^{w}(s_{t+1},\mu_{\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $</li>
<li>$ w_{t+1} = w_{t} + \alpha_{w}\delta_{t}\phi(s_{t},a_{t}) $</li>
<li>$ \theta_{t+1} = \theta_{t} + \alpha_{\theta} \nabla_{\theta}\mu_{\theta}(s_{t})(\nabla_{\theta}\mu_{\theta}(s_{t})^{\top} w_{t}) $</li>
</ul>
</li>
<li>off-policy Q-learning은 linear function approximation을 이용하면 diverge 할 수도 있음<ul>
<li>$ \mu_{\theta}(s_{t+1}) $ 이 diverge 할 수도 있기 때문으로 판단됨.</li>
<li>그렇기에 simple Q-learning 대신 다른 기법이 필요함.</li>
</ul>
</li>
<li>그렇기에 critic 에 gradient Q-learning 사용한 COPDAC-GQ (Gradient Q-learning critic) algorithm 제안<ul>
<li>gradient temporal-difference learning 에 기반한 기법들은 true gradient descent algorithm 이며, converge가 보장됨. (Sutton, 2009)<ul>
<li>기본 아이디어는 stochastic gradient descent 로 Mean-squared projected Bellman error (MSPBE) 를 최소화하는 것</li>
<li>critic 이 actor 보다 빠른 time-scale 로 update 되도록 step size 들을 잘 조절하면, critic 은 MSPBE 를 최소화하는 parameters 로 converge 하게 됨</li>
<li>critic 에 gradient temporal-difference learning 의 일종인 gradient Q-learning 사용한 논문 (Maei, 2010)</li>
</ul>
</li>
</ul>
</li>
<li>COPDAC-GQ algorithm<ul>
<li>$ \delta_{t} = r_{t} + \gamma Q^{w}(s_{t+1},\mu_{\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $</li>
<li>$ \theta_{t+1} = \theta_{t} + \alpha_{\theta} \nabla_{\theta}\mu_{\theta}(s_{t})(\nabla_{\theta}\mu_{\theta}(s_{t})^{\top} w_{t}) $</li>
<li>$ w_{t+1} = w_{t} + \alpha_{w}\delta_{t}\phi(s_{t},a_{t}) - \alpha_{w}\gamma\phi(s_{t+1}, \mu_{\theta}(s_{t+1}))(\phi(s_{t},a_{t})^{\top} u_{t}) $</li>
<li>$ v_{t+1} = v_{t} + \alpha_{v}\delta_{t}\phi(s_{t}) - \alpha_{v}\gamma\phi(s_{t+1})(\phi(s_{t},a_{t})^{\top} u_{t}) $</li>
<li>$ u_{t+1} = u_{t} + \alpha_{u}(\delta_{t}-\phi(s_{t}, a_{t})^{\top} u_{t})\phi(s_{t}, a_{t}) $</li>
</ul>
</li>
<li>stochastic actor-critic 과 같이 매 time-step 마다 update 시 필요한 계산의 복잡도는 $ O(mn) $<ul>
<li>m 은 action dimensions, n 은 number of policy parameters</li>
</ul>
</li>
<li>Natural policy gradient 를 이용해 deterministic policies 를 찾을 수 있음<ul>
<li>$ M(\theta)^{-1}\nabla_{\theta}J(\mu_{\theta}) $ 는 any metric $ M(\theta) $ 에 대한 our performance objective (식(14)) 의 steepest ascent direction 임 (Toussaint, 2012)</li>
<li>Natural gradient 는 Fisher information metric $ M_{\pi}(\theta) $ 에 대한 steepest ascent direction 임<ul>
<li>$ M_{\pi}(\theta) = E_{s \sim \rho^{\pi}, a \sim \pi_{\theta}}[\nabla_{\theta}\log\pi_{\theta}(a|s)\nabla_{\theta}\log\pi_{\theta}(a|s)^{\top}] $</li>
<li>Fisher information metric 은 policy reparameterization 에 대해 불변임 (Bagnell, 2003)</li>
</ul>
</li>
<li>deterministic policies 에 대해 metric 으로 $ M_{\mu}(\theta) = E_{s \sim \rho^{\mu}}[\nabla_{\theta}\mu_{\theta}(s)\nabla_{\theta}\mu_{\theta}(s)^{\top}] $ 을 사용.<ul>
<li>이는 variance 가 0 인 policy 에 대한 Fisher information metric 으로 볼 수 있음</li>
<li>$ \frac{\nabla_{\theta}\pi_{\theta}(a\vert s)}{\pi_{\theta}(a\vert s)}$ 에서 policy variance 가 0 이면, 특정 s 에 대한 $ \pi_{\theta}(a|s)$ 만 1 이 되고, 나머지는 0 임</li>
</ul>
</li>
<li>deterministic policy gradient theorem 과 compatible function approximation 을 결합하면 $ \nabla_{\theta}J(\mu_{\theta}) = E_{s \sim \rho^{\mu}}[\nabla_{\theta}\mu_{\theta}(s)\nabla_{\theta}\mu_{\theta}(s)^{\top}w] $ 이 됨<ul>
<li>$ \nabla_{\theta}J(\mu_{\theta}) = E_{s \sim \rho^{\mu}}[\nabla_{\theta}\mu_{\theta}(s)\nabla_{a}Q^{\mu}(s,a)\vert_{a=\mu_{\theta}(s)}] $</li>
<li>$ \nabla_{a}Q^{\mu}(s,a)\vert_{a=\mu_{\theta}(s)} \approx \nabla_{a}Q^{w}(s,a)\vert_{a=\mu_{\theta}(s)} = \nabla_{\theta}\mu_{\theta}(s)^{\top}w $</li>
</ul>
</li>
<li>그렇기에 steepest ascent direction 은 $ M_{\mu}(\theta)^{-1}\nabla_{\theta}J_{\beta}(\mu_{\theta}) = w $ 이 됨<ul>
<li>$ E_{s \sim \rho^{\mu}}[\nabla_{\theta}\mu_{\theta}(s)\nabla_{\theta}\mu_{\theta}(s)^{\top}]^{-1}E_{s \sim \rho^{\mu}}[\nabla_{\theta}\mu_{\theta}(s)\nabla_{\theta}\mu_{\theta}(s)^{\top}w] = w $</li>
</ul>
</li>
<li>이 알고리즘은 COPDAC-Q 혹은 COPDAC-GQ 에서 $ \theta_{t+1} = \theta_{t} + \alpha_{\theta} \nabla_{\theta}\mu_{\theta}(s_{t})(\nabla_{\theta}\mu_{\theta}(s_{t})^{\top} w_{t}) $ 식을 $ \theta_{t+1} = \theta_{t} + \alpha_{\theta}w_{t} $ 로 바꿔주기만 하면 됨</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="Continuous-Bandit"><a href="#Continuous-Bandit" class="headerlink" title="Continuous Bandit"></a>Continuous Bandit</h3><ul>
<li>Stochastic Actor-Critic (SAC)과 COPDAC 간 성능 비교 수행<ul>
<li>Action dimension이 커질수록 성능 차이가 심함</li>
<li>빠르게 수렴하는 것을 통해 DPG의 data efficiency가 SPG에 비해 좋다는 것을 확인할 수 있지만, 반면, time-step이 증가할수록 SAC와 COPDAC 간 성능 차이가 줄어드는 것을 통해 성능 차이가 심하다는 것은 일정 time step 내에서만 해당하는 것이라고 유추해볼 수 있음</li>
<li><img src="https://www.dropbox.com/s/hrkyq0s2f24z66r/Screenshot%202018-06-16%2017.47.38.png?dl=1"></li>
</ul>
</li>
</ul>
<h3 id="Continuous-Reinforcement-Learning"><a href="#Continuous-Reinforcement-Learning" class="headerlink" title="Continuous Reinforcement Learning"></a>Continuous Reinforcement Learning</h3><ul>
<li>COPDAC-Q가 SAC와 off-policy stochastic actor-critic(OffPAC-TD) 간 성능 비교 수행<ul>
<li>COPDAC-Q의 성능이 약간 더 좋음</li>
<li>COPDAC-Q의 학습이 더 빨리 이뤄짐</li>
<li><img src="https://www.dropbox.com/s/qdca4augapmzsxi/Screenshot%202018-06-16%2017.47.07.png?dl=1"><h3 id="Octopus-Arm"><a href="#Octopus-Arm" class="headerlink" title="Octopus Arm"></a>Octopus Arm</h3></li>
</ul>
</li>
<li>목표: 6 segments octopus arm (20 action dimensions &amp; 50 state dimensions)을 control하여 target을 맞추는 것<ul>
<li>COPDAC-Q 사용 시, action space dimension이 큰 octopus arm을 잘 control하여 target을 맞춤</li>
<li><img src="https://www.dropbox.com/s/xrxb0a52wntekld/Screenshot%202018-06-16%2017.46.28.png?dl=1" width="600px"></li>
<li>기존 기법들은 action spaces 혹은 action 과 state spaces 둘 다 작은 경우들에 대해서만 실험했다고 하며, 비교하고 있지 않음.<ul>
<li>기존 기법들이 6 segments octopus arm에서 동작을 잘 안 했을 것 같긴한데, 그래도 실험해서 결과를 보여주지…왜 안 했을까?</li>
</ul>
</li>
<li>8 segment arm 동영상이 저자 홈페이지에 있다고 하는데, 안 보임.</li>
</ul>
</li>
<li>[참고] Octopus Arm 이란?<ul>
<li><a href="https://www.youtube.com/watch?v=AxeeHif0euY" target="_blank" rel="noopener">OctopusArm Youtube Link</a></li>
<li><img src="https://www.dropbox.com/s/950ycj06sudakjx/Screenshot%202018-06-16%2017.45.52.png?dl=1"></li>
</ul>
</li>
</ul>


				
			</div>

			<!-- Comments -->
			
				<div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
					


				</div>
			
		</div>
	</div>
</article>

<!-- S 디스쿼스 주석처리 웅원이가 꺼달라그럼 2018.05.10 보미 -->
<div id="disqus_thread"></div>
<script>
	

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
// (function() { // DON'T EDIT BELOW THIS LINE
// var d = document, s = d.createElement('script');
// s.src = 'https://bomee88.disqus.com/embed.js';
// s.setAttribute('data-timestamp', +new Date());
// (d.head || d.body).appendChild(s);
// })();
</script>
<!-- <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript> -->

<!-- E 디스쿼스 추가함 2018.04.09 보미-->

    <!-- Footer -->
    <!-- Footer -->
<footer>
	<div class="container">
		<div class="row">
			<div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
				<ul class="list-inline text-center">
					

					

					

					

					
						<li>
							<a href="mailto:bomee88@naver.com" target="_blank">
								<span class="fa-stack fa-lg">
									<i class="fa fa-circle fa-stack-2x"></i>
									<i class="fa fa-envelope-o fa-stack-1x fa-inverse"></i>
								</span>
							</a>
						</li>
					

					
				</ul>
				<p class="copyright text-muted">&copy; 2018 Bomi Yu</p>
			</div>
		</div>
	</div>
</footer>


    <!-- After footer scripts -->
    
<!-- jQuery -->
<script src="//code.jquery.com/jquery-2.1.4.min.js"></script>
<script src="/js/customizing.js"></script>

<!-- Bootstrap -->
<script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>

<!-- Highlight -->
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.5.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<!-- Gallery -->
<script src="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.js" type="text/javascript" charset="utf-8"></script>

<!-- Disqus Comments //디스쿼스 새로운 js삽입 2018.04.09 보미-->
<script id="dsq-count-scr" src="//bomee88.disqus.com/count.js" async></script>

<!-- // mathJax 추가 2018.05.29 보미 -->
<!-- // mathJax 수정 2018.06.15 웅원 -->
<!--
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
-->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$']]
    }
  });
</script>

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML">
</script>
                          
<!-- //기존 디스쿼스 구동안됨으로 주석처리 2018.04.09 보미
 --><!-- hexo-inject:begin --><!-- hexo-inject:end -->




</body>

</html>