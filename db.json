{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/img/Exp_OctopusArm.png","path":"img/Exp_OctopusArm.png","modified":0,"renderable":0},{"_id":"source/img/Exp_ContinuousBandit.png","path":"img/Exp_ContinuousBandit.png","modified":0,"renderable":0},{"_id":"source/img/Ref_Degris2012b_Theorem1.png","path":"img/Ref_Degris2012b_Theorem1.png","modified":0,"renderable":0},{"_id":"source/img/Exp_OctopusArm_Ref.png","path":"img/Exp_OctopusArm_Ref.png","modified":0,"renderable":0},{"_id":"source/img/Exp_ContinuousRL.png","path":"img/Exp_ContinuousRL.png","modified":0,"renderable":0},{"_id":"source/img/Ref_Degris2012b_offpolicygradient.png","path":"img/Ref_Degris2012b_offpolicygradient.png","modified":0,"renderable":0},{"_id":"themes/clean-blog/source/css/article.styl","path":"css/article.styl","modified":0,"renderable":1},{"_id":"themes/clean-blog/source/css/base.styl","path":"css/base.styl","modified":0,"renderable":1},{"_id":"themes/clean-blog/source/css/mixins.styl","path":"css/mixins.styl","modified":0,"renderable":1},{"_id":"themes/clean-blog/source/css/style.styl","path":"css/style.styl","modified":0,"renderable":1},{"_id":"themes/clean-blog/source/css/variables.styl","path":"css/variables.styl","modified":0,"renderable":1},{"_id":"themes/clean-blog/source/img/about-bg.jpg","path":"img/about-bg.jpg","modified":0,"renderable":1},{"_id":"themes/clean-blog/source/img/favicon.ico","path":"img/favicon.ico","modified":0,"renderable":1},{"_id":"themes/clean-blog/source/img/home-bg.jpg","path":"img/home-bg.jpg","modified":0,"renderable":1},{"_id":"themes/clean-blog/source/img/contact-bg.jpg","path":"img/contact-bg.jpg","modified":0,"renderable":1},{"_id":"source/img/1.jpg","path":"img/1.jpg","modified":0,"renderable":0},{"_id":"source/img/mixure_policy.png","path":"img/mixure_policy.png","modified":0,"renderable":0},{"_id":"source/img/tvd.png","path":"img/tvd.png","modified":0,"renderable":0},{"_id":"source/img/importance_sampling.png","path":"img/importance_sampling.png","modified":0,"renderable":0},{"_id":"source/img/heuristic_approx.png","path":"img/heuristic_approx.png","modified":0,"renderable":0},{"_id":"source/img/kld.png","path":"img/kld.png","modified":0,"renderable":0},{"_id":"source/img/sample-based.png","path":"img/sample-based.png","modified":0,"renderable":0},{"_id":"source/img/single.png","path":"img/single.png","modified":0,"renderable":0},{"_id":"source/img/surrogate.png","path":"img/surrogate.png","modified":0,"renderable":0},{"_id":"source/img/vine1.png","path":"img/vine1.png","modified":0,"renderable":0},{"_id":"source/img/vine2.png","path":"img/vine2.png","modified":0,"renderable":0},{"_id":"source/img/state_visitation_change.png","path":"img/state_visitation_change.png","modified":0,"renderable":0},{"_id":"source/img/policy_change.png","path":"img/policy_change.png","modified":0,"renderable":0}],"Cache":[{"_id":"source/.DS_Store","hash":"cc7d1323fdf1fb465d97d2547c601caa064dcf9a","modified":1531911524266},{"_id":"themes/clean-blog/.DS_Store","hash":"18b40c0a074fb756ef76fd437c21c2205a08a585","modified":1529045736390},{"_id":"themes/clean-blog/LICENSE","hash":"8726b416df4f067cff579e859f05c4b594b8be09","modified":1529043522273},{"_id":"themes/clean-blog/README.md","hash":"861dd2f959ab75d121226f4f3e2f61f4bc95fddb","modified":1529043522273},{"_id":"themes/clean-blog/_config.yml","hash":"c0ee3fcf1841410d07402e8d6e50e8847f31ae4b","modified":1529047469272},{"_id":"source/_posts/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1529228886819},{"_id":"source/_posts/sutton-pg.md","hash":"b4da05ee9d6ad49c718914ec457bc3f5e6e55fa7","modified":1532416237219},{"_id":"source/_posts/피지여행-소개.md","hash":"9ee06938369d11302335856c2728bee3f367eafe","modified":1532002582289},{"_id":"source/_posts/2018-06-15-npg.md","hash":"ad0e79383db0552d94078ca181c5e7c764b81f57","modified":1529150062936},{"_id":"source/_posts/dpg.md","hash":"91f84948996f7018f2a36801c34b3e70e2d9fb51","modified":1532416237218},{"_id":"source/img/Exp_OctopusArm.png","hash":"f0ae59d0c2a6600ef961fd0b2ea8903dcfdbd4d9","modified":1529117349098},{"_id":"source/img/Exp_ContinuousBandit.png","hash":"72eddde3b296070075706688038f1267bfd04d31","modified":1529113799292},{"_id":"source/img/Ref_Degris2012b_Theorem1.png","hash":"154f42239538c3dd0508e815014d84685e18f89b","modified":1529106775328},{"_id":"themes/clean-blog/languages/default.yml","hash":"97326c9e6518d9f379778178b3b8f9a58434725d","modified":1529043522274},{"_id":"themes/clean-blog/languages/en.yml","hash":"97326c9e6518d9f379778178b3b8f9a58434725d","modified":1529043522274},{"_id":"themes/clean-blog/languages/de.yml","hash":"424a9c1e6ab69334d7873f6574da02ca960aa572","modified":1529043522273},{"_id":"themes/clean-blog/languages/no.yml","hash":"8ca475a3b4f8efe6603030f0013aae39668230e1","modified":1529043522274},{"_id":"themes/clean-blog/languages/es.yml","hash":"cb4eeca0ed3768a77e0cd216300f2b2549628b1b","modified":1529043522274},{"_id":"themes/clean-blog/languages/fr.yml","hash":"e9e6f7cb362ebb7997f11027498a2748fe3bac95","modified":1529043522274},{"_id":"themes/clean-blog/languages/pt.yml","hash":"1d0c3689eb32fe13f37f8f6f303af7624ebfbaf0","modified":1529043522275},{"_id":"themes/clean-blog/languages/pl.yml","hash":"de7eb5850ae65ba7638e907c805fea90617a988c","modified":1529043522274},{"_id":"themes/clean-blog/languages/ru.yml","hash":"42df7afeb7a35dc46d272b7f4fb880a9d9ebcaa5","modified":1529043522275},{"_id":"themes/clean-blog/languages/zh-TW.yml","hash":"9acac6cc4f8002c3fa53ff69fb8cf66c915bd016","modified":1529043522275},{"_id":"themes/clean-blog/languages/zh-CN.yml","hash":"7bfcb0b8e97d7e5edcfca8ab26d55d9da2573c1c","modified":1529043522275},{"_id":"themes/clean-blog/layout/index.ejs","hash":"b9ab94682f4d4860b8cb8443d1f48d4649e67d6f","modified":1533607217043},{"_id":"themes/clean-blog/layout/archive.ejs","hash":"f2ef73afc3d275333329bb30b9369b82e119da76","modified":1529043522279},{"_id":"themes/clean-blog/layout/layout.ejs","hash":"da2f9018047924ddaf376aee5996c7ddc06cebc1","modified":1529043522279},{"_id":"themes/clean-blog/layout/page.ejs","hash":"591af587e1aae962950de7e79bd25c1f060c69ac","modified":1529043522279},{"_id":"themes/clean-blog/source/.DS_Store","hash":"84f35e390633eadc3c78584a28f5f5f8ce7f43a5","modified":1529045731387},{"_id":"themes/clean-blog/layout/post.ejs","hash":"38382e9bbeb6b8d2eafbd53fff2984111f524c1a","modified":1529043522279},{"_id":"source/img/Exp_OctopusArm_Ref.png","hash":"ebf69a16fc09e447442808f8cc46d770bed0cf10","modified":1529116470075},{"_id":"source/img/Exp_ContinuousRL.png","hash":"85b9ebfb164631804d2fb6be0164e49cd1db746b","modified":1529115804602},{"_id":"source/img/Ref_Degris2012b_offpolicygradient.png","hash":"46f0f17392ff40927579899fd10bdaf6bf562ebd","modified":1529106760142},{"_id":"themes/clean-blog/layout/_partial/article-categories.ejs","hash":"5a0bf5a20f670621d8013c9b9d7976b45c8aa80f","modified":1529043522276},{"_id":"themes/clean-blog/layout/_partial/article-archive.ejs","hash":"3d8d98c6545b8332a6d6ed4f8b00327df03ea945","modified":1529043522276},{"_id":"themes/clean-blog/layout/_partial/after-footer.ejs","hash":"a6ad079ded70024d35264fae798ae73bdbcb0ae6","modified":1529149982621},{"_id":"themes/clean-blog/layout/_partial/article-full.ejs","hash":"6cf24bd7785d57cb7198b3f1ed4fa6a86c84a502","modified":1529043522276},{"_id":"themes/clean-blog/layout/_partial/article-index.ejs","hash":"e433df4e245e2d4c628052c6e59966563542d94d","modified":1529043522276},{"_id":"themes/clean-blog/layout/_partial/article-tags.ejs","hash":"6136434be09056c1466149cecb3cc2e80d107999","modified":1529043522277},{"_id":"themes/clean-blog/layout/_partial/comments.ejs","hash":"3fedb75436439d1d6979b7e4d20d48a593e12be4","modified":1529043522277},{"_id":"themes/clean-blog/layout/_partial/footer.ejs","hash":"a92f5168c006193c3d964fd293ad3c38aae69419","modified":1529043522277},{"_id":"themes/clean-blog/layout/_partial/gallery.ejs","hash":"21e4f28909f4a79ff7d9f10bdfef6a8cb11632bf","modified":1529043522277},{"_id":"themes/clean-blog/layout/_partial/google-analytics.ejs","hash":"4e6e8de9becea5a1636a4dcadcf7a10c06e2426e","modified":1529043522277},{"_id":"themes/clean-blog/layout/_partial/head.ejs","hash":"f8ddbced1627704ab35993e8fc6d6e34cc6f2ba9","modified":1529043522278},{"_id":"themes/clean-blog/layout/_partial/menu_origin.ejs","hash":"cfc30e6b1ef9487cff3ce594d403d1e7c4d9cdf4","modified":1529043522278},{"_id":"themes/clean-blog/layout/_partial/pagination.ejs","hash":"557d6bb069a1d48af49ae912994653f44b32a570","modified":1529043522278},{"_id":"themes/clean-blog/layout/_partial/menu.ejs","hash":"f556ee53f976e8ff1348c9d343a51c6b842d47b4","modified":1533607217042},{"_id":"themes/clean-blog/layout/_partial/tag-category-index.ejs","hash":"10cdc1b7866999c714a666557c150d2c79c1fba9","modified":1529043522278},{"_id":"themes/clean-blog/source/css/article.styl","hash":"f5294d7a3d6127fcb287de3ff0c12aebb1766c7b","modified":1529043522280},{"_id":"themes/clean-blog/source/css/base.styl","hash":"29b54c63060bd2d7f5c501d403d9db5a552ad10c","modified":1529043522280},{"_id":"themes/clean-blog/source/css/mixins.styl","hash":"14264bf86b4e3194a3156447f7b7bce2fd0db5bd","modified":1529043522280},{"_id":"themes/clean-blog/source/css/style.styl","hash":"c40dc495a41007d21c59f342ee42b2d31d7b5ff4","modified":1529043522280},{"_id":"themes/clean-blog/source/css/variables.styl","hash":"cd82df5ca8dfbcfec12d833f01adfac00878e835","modified":1529043522280},{"_id":"themes/clean-blog/source/img/about-bg.jpg","hash":"d39126a6456f2bac0169d1779304725f179c9900","modified":1529043522281},{"_id":"themes/clean-blog/source/img/favicon.ico","hash":"3412e0d657aa5a6cfbbfcf4ef398572c24035565","modified":1529045668405},{"_id":"themes/clean-blog/source/img/home-bg.jpg","hash":"990f6f9dd0ecb5348bfcc47305553d58c0d8f326","modified":1529043522283},{"_id":"themes/clean-blog/source/img/contact-bg.jpg","hash":"6af63305c923899017e727b5ca968a2703bc08cf","modified":1529043522282},{"_id":"public/index.html","hash":"b9ae24f8a309b0254ea55364af286ad9c9e3f67d","modified":1536408346387},{"_id":"public/archives/index.html","hash":"3a7e8b3ad7a80c0b5a99bd0b47468bbc46661d8e","modified":1536408346387},{"_id":"public/archives/2018/index.html","hash":"0c591492481d4ffb42551b98883d367c57c6a47b","modified":1536408346387},{"_id":"public/archives/2018/06/index.html","hash":"32a8c023c413d211b893f317118b1a56e62f3c3f","modified":1536408346387},{"_id":"public/categories/프로젝트/index.html","hash":"930803e6749f8773bf74f603e3c168219bf30feb","modified":1536408346387},{"_id":"public/tags/프로젝트/index.html","hash":"544e0b7fbecbcb7b1f9b5719f7613e17f33504a8","modified":1536408346387},{"_id":"public/tags/피지여행/index.html","hash":"b9e460af25fc1702f13f8a6dfd505e8d85a2aec6","modified":1536408346387},{"_id":"public/2018/06/16/dpg/index.html","hash":"7bc394c8fcb2dca3d19acc45af754700fb5389e7","modified":1529230754824},{"_id":"public/2018/06/15/sutton-pg/index.html","hash":"2a84a7e4ea8168f22bebf2b1150355e14e9efea1","modified":1529230754824},{"_id":"public/2018/06/14/2018-06-15-npg/index.html","hash":"63865f511c586719f341629a55a4354cf9e39371","modified":1529230754824},{"_id":"public/2018/06/17/피지여행-소개/index.html","hash":"4444d94a395bd894dc96f4c1968ca4d457493924","modified":1529230754823},{"_id":"source/img/Screen Shot 2018-07-10 at 3.41.06 PM.png","hash":"c20c1e539d1b1fe5801de009e693492158ac961f","modified":1531204874000},{"_id":"source/_posts/ddpg.md","hash":"33a85a803d4a37f1577690ff3f4929a4ce1fd036","modified":1532002649265},{"_id":"source/_posts/npg.md","hash":"4819bfbb7dcdf3e8857423df860f53278e7eabe2","modified":1532002657422},{"_id":"source/_posts/gae.md","hash":"c9775d889c17384b0089e3f77d328fc821749a4a","modified":1532492285530},{"_id":"source/_posts/pg-travel-guide.md","hash":"996c58fbc529b7d2affc527d6fa0a2fd2d2e8368","modified":1532340141420},{"_id":"source/images/Exp_ContinuousBandit.png","hash":"72eddde3b296070075706688038f1267bfd04d31","modified":1531290478254},{"_id":"source/images/Exp_OctopusArm.png","hash":"f0ae59d0c2a6600ef961fd0b2ea8903dcfdbd4d9","modified":1531290478256},{"_id":"source/images/Ref_Degris2012b_Theorem1.png","hash":"154f42239538c3dd0508e815014d84685e18f89b","modified":1531290478257},{"_id":"source/images/figure.png","hash":"c20c1e539d1b1fe5801de009e693492158ac961f","modified":1531204874000},{"_id":"source/images/Exp_ContinuousRL.png","hash":"85b9ebfb164631804d2fb6be0164e49cd1db746b","modified":1531290478255},{"_id":"source/images/Exp_OctopusArm_Ref.png","hash":"ebf69a16fc09e447442808f8cc46d770bed0cf10","modified":1531290478256},{"_id":"source/images/Ref_Degris2012b_offpolicygradient.png","hash":"46f0f17392ff40927579899fd10bdaf6bf562ebd","modified":1531290478257},{"_id":"source/img/figure.png","hash":"c20c1e539d1b1fe5801de009e693492158ac961f","modified":1531204874000},{"_id":"source/img/1.jpg","hash":"179e50805da05b9a2417192bff85de448f5de998","modified":1531306513731},{"_id":"public/2018/07/11/gae/index.html","hash":"27ec6523b1572a0fe8e9bd806926005578ec9067","modified":1531306829478},{"_id":"public/2018/07/11/pg-travel-guide/index.html","hash":"5a90be92767db785e6e33ef1b213cceb48882f71","modified":1531306829479},{"_id":"public/2018/07/10/sutton-pg/index.html","hash":"709c482d1075fdee66a7cc080add172ff3892017","modified":1531306829479},{"_id":"public/archives/2018/07/index.html","hash":"9713c99afe7263b1f69f4bc88c757bda9ea22a4b","modified":1531306829480},{"_id":"public/2018/06/23/ddpg/index.html","hash":"c8278826e2c5a127debc6847b215143da26c7a0d","modified":1531306829481},{"_id":"public/2018/06/14/npg/index.html","hash":"63865f511c586719f341629a55a4354cf9e39371","modified":1531306829481},{"_id":"public/img/figure.png","hash":"c20c1e539d1b1fe5801de009e693492158ac961f","modified":1531306829481},{"_id":"public/img/1.jpg","hash":"179e50805da05b9a2417192bff85de448f5de998","modified":1531306829481},{"_id":"source/img/figure10.jpg","hash":"5ac3e77e51f235f1ff01913f60b39b0d4c15c702","modified":1531380515987},{"_id":"source/img/figure2.jpg","hash":"be090f33c5c52aac5affb1a5141b7bb59df95feb","modified":1531378668469},{"_id":"source/img/figure4.jpg","hash":"4f3a5da99d582cfaeef680bcad5ffe04b9f77b2e","modified":1531378999629},{"_id":"source/img/figure6.jpg","hash":"dc10abad67ca9b7dc1e1f2f5a50979b597cff957","modified":1531379789001},{"_id":"source/img/figure9.jpg","hash":"a5b762eedea6af2e0f842f6c8171cf8600d0f066","modified":1531380074106},{"_id":"source/img/figure5.jpg","hash":"6ff807873a00811aebdf9dbd3d7a2780ddac3719","modified":1531379639148},{"_id":"source/img/figure3.jpg","hash":"b48ff63551254da2f6ef90124ca63435e3768ed7","modified":1531378674752},{"_id":"source/img/figure1.jpg","hash":"9e0b12cbbedcc3e1d4ac20552fcbad8bb3e8071a","modified":1531378413225},{"_id":"source/img/figure8.jpg","hash":"74a82b06007bcbe9ea602275785247bd250c4341","modified":1531380066337},{"_id":"source/img/figure7.jpg","hash":"a72367c5b094841966473e3d3be983413b486933","modified":1531380059406},{"_id":"source/_posts/trpo.md","hash":"c02f14c872160d74043a1a87ad839cc56295e3a6","modified":1532416237220},{"_id":"source/img/mixure_policy.png","hash":"a29de731fa31bb01bfe2d32714d2c330b62ebf1c","modified":1531555460393},{"_id":"source/img/tvd.png","hash":"48832e9dfdc030c0eec69f4eb3a6df6f79e443b2","modified":1531555460408},{"_id":"source/img/importance_sampling.png","hash":"557cb910e6cbaa52570749c971f30a9ac99f5b90","modified":1531555460389},{"_id":"source/img/heuristic_approx.png","hash":"5f2474685413fe5220cabe92f8efd55541c19654","modified":1531555460387},{"_id":"source/img/kld.png","hash":"ec5b1eebc409fbe8b5a2a1d6ab8cdbb921b39d42","modified":1531555460392},{"_id":"source/img/sample-based.png","hash":"c4b79bd8d64eaa2ce36fb0a3ee0981b5455cce30","modified":1531555460399},{"_id":"source/img/single.png","hash":"52a63dd842c7448ebf20fa53d5485bd0364250b9","modified":1531555460401},{"_id":"source/img/surrogate.png","hash":"a0f501e64f45b6edde6282688416c513a27b6dff","modified":1531555460407},{"_id":"source/img/vine1.png","hash":"acea47e2739665b82e467660d2f1e382ef0307b6","modified":1531555460410},{"_id":"source/img/vine2.png","hash":"80103e41a869da947827726b5719bb30a33de414","modified":1531555460413},{"_id":"source/img/state_visitation_change.png","hash":"81c9b7879e5cb69e65519415b5ba72dbf09c19ee","modified":1531555460404},{"_id":"source/img/policy_change.png","hash":"e1fd4af3c5e8236d14eed042b9eec7358479626d","modified":1531555460397},{"_id":"source/img/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1531628462724},{"_id":"source/_posts/ppo.md","hash":"7960c037fc9761d79601ad3c695bafc41dce2f90","modified":1532441900323},{"_id":"source/_posts/1_pg-travel-guide.md","hash":"b025ab35163cbbbcc0570787cee003ec9be63946","modified":1532836865645},{"_id":"source/_posts/3_dpg.md","hash":"91f84948996f7018f2a36801c34b3e70e2d9fb51","modified":1532652398291},{"_id":"source/_posts/2_sutton-pg.md","hash":"0ad2f73eb230765ba2ff09be4e6b2f649b0b45b3","modified":1532652398292},{"_id":"source/_posts/0_피지여행-소개.md","hash":"9ee06938369d11302335856c2728bee3f367eafe","modified":1532652398293},{"_id":"source/_posts/4_ddpg.md","hash":"5f8dcf6e391150268ac85079b29b0f89d1afd07b","modified":1532857630463},{"_id":"source/_posts/5_npg.md","hash":"f2605826d2d5fb06830b14e4d1ee6ba3956f25db","modified":1532652398262},{"_id":"source/_posts/7_gae.md","hash":"fffb1271d63d496c3350421a6fa5b1af8ddc6b17","modified":1532836213338},{"_id":"source/_posts/6_trpo.md","hash":"86bc1df352a469f8f1f5ab01765df52f1d483692","modified":1532652398289},{"_id":"source/_posts/8_ppo.md","hash":"a33bc1b79927b61849d35920ac2201c2ecfcf868","modified":1533046426051},{"_id":"source/_posts/0_pg-travel-guide.md","hash":"28bb35ec01b701024c2eca384055695bf3bcc417","modified":1536404420604},{"_id":"source/_posts/3_ddpg.md","hash":"fed9fe3b79c5e473ac2698689baf3e0a9a5847d3","modified":1536404420606},{"_id":"source/_posts/1_sutton-pg.md","hash":"a6ed2e7fafa110aaa13df23a51a80ca0a52d09c6","modified":1536404420605},{"_id":"source/_posts/2_dpg.md","hash":"68f871f041ba8a2731df6171f62fc31dbd64fc2e","modified":1536404420605},{"_id":"source/_posts/4_npg.md","hash":"a5101a88b0a19f1512e3de331aff6881ae276fd4","modified":1536404420606},{"_id":"source/_posts/5_trpo.md","hash":"e2c5bfdd360e93197dd690cc3473bf39d6d5aa18","modified":1536404420610},{"_id":"source/_posts/6_gae.md","hash":"51204f82f035fd719961dbb8c4676b61e6fc0076","modified":1536404420611},{"_id":"source/_posts/7_ppo.md","hash":"1dfb53de06b84e30fad7f97ff68c1e3cbb35dd0a","modified":1536404420612},{"_id":"source/_posts/8_implement.md","hash":"cbddf31d02a4acaf31a85236753dc945bb2b356e","modified":1536406495269},{"_id":"public/archives/2018/08/index.html","hash":"a9cd12a2331251d0bce0d8ae697474504bb43300","modified":1536408346387},{"_id":"public/2018/08/23/8_implement/index.html","hash":"961459a89f95f0b09216450f7f0450e7e1a6961e","modified":1536408346387},{"_id":"public/2018/06/29/0_pg-travel-guide/index.html","hash":"5a042b093ff19b21c8a7dfd54489683865ef2cf3","modified":1536408346388},{"_id":"public/2018/06/28/1_sutton-pg/index.html","hash":"13e5b28b18a98f69563444165b44407c3e723aec","modified":1536408346388},{"_id":"public/2018/06/27/2_dpg/index.html","hash":"6a70d723574f151259972fc719729c13377af520","modified":1536408346388},{"_id":"public/2018/06/26/3_ddpg/index.html","hash":"79669bf310266403a5ef4aa65f9f2c32247dbe37","modified":1536408346388},{"_id":"public/2018/06/25/4_npg/index.html","hash":"e656a788942190f6d61c9683bdd6296a479f753b","modified":1536408346388},{"_id":"public/2018/06/24/5_trpo/index.html","hash":"127437bdc70d74b2e75938b0f71eb3d64e9519d6","modified":1536408346388},{"_id":"public/2018/06/23/6_gae/index.html","hash":"8c5fd96fb3e8013cd97e14d62ba6b2df0d1ef1da","modified":1536408346388},{"_id":"public/2018/06/22/7_ppo/index.html","hash":"1dd93573f4fd2eac58182a499210e6f32f4f5840","modified":1536408346388}],"Category":[{"name":"프로젝트","_id":"cjiinnj6o0003j28aoi038xjq"}],"Data":[],"Page":[],"Post":[{"title":"Deep Determinstic Policy Gradient (DDPG)","date":"2018-06-26T02:20:45.000Z","author":"양혁렬, 이동민, 차금강","subtitle":"피지여행 3번째 논문","_content":"\n<center > <img src=\"https://www.dropbox.com/s/zv8rk0uf87ipiaj/Screenshot%202018-06-23%2010.01.48.png?dl=1\" width=\"600\"> </center>\n\n논문 저자 : Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver & Daan Wierstra\n논문 링크 : https://arxiv.org/pdf/1509.02971.pdf\nProceeding : International Conference on Learning Representations (ICLR) 2016\n정리 : 양혁렬, 이동민, 차금강\n\n---\n\n# 1. 들어가며...\n\n<br>\n## 1.1 Success & Limination of DQN\n\n-  Success\n    - sensor로부터 나오는 전처리를 거친 input 대신에 raw pixel input을 사용합니다. 이렇게 함으로써 High dimensional observation space 문제를 풀어냅니다.\n- Limitation\n    - discrete & low dimensional action space만 다룰 수 있습니다. Continuous action space를 다루기 위해서는 매 스텝 이를 위한 iterative optimization process를 거쳐야 합니다.\n\n<br>\n## 1.2 Problems of discritization\n\n<center> <img src=\"https://www.dropbox.com/s/nulhzxs8bak2fn6/Screenshot%202018-06-23%2012.22.20.png?dl=1\" width=\"450px\"> </center>\n\n- 만약 7개의 관절을 가진 로봇 팔이 있다면, 가장 간단한 discretization은 각 관절을 다음과 같이 $a_{i}\\in \\\\{ -k, 0, k \\\\}$ 3개의 값을 가지도록 하는 것입니다.\n- 그렇다면 $3 \\times 3 \\times 3 \\times 3 \\times 3 \\times 3 \\times 3 = 3^{7} = 2187$ 가지의 dimension을 가진 action space가 만들어집니다. 이와같이 Discretization을 하면 action space가 exponential하게 늘어납니다.\n- 충분히 큰 action space 임에도 discretization으로 인한 정보의 손실이 있을 수 있고, 섬세한 Control을 할 수 없습니다.\n\n<br>\n## 1.3 New approach for continuous control\n\n- Model-free, Off-policy, Actor-critic algorithm을 제안합니다.\n- Deep Deterministic Policy(이하 DPG)를 기반으로 합니다.\n- Actor-Critic approach와 DQN의 성공적이었던 부분을 합칩니다.\n    - Replay buffer : 샘플들 사이의 상관관계를 줄여줍니다.\n    - target Q Network : Update 동안 target을 안정적으로 만듭니다.\n\n<br><br>\n\n# 2.Background\n\n<br>\n## 2.1 Notation\n\n- Observation : $x_{t}$\n- Action : $a_t \\in {\\rm IR}^N $\n- Reward : $r_t$\n- Discount factor : $\\gamma$\n- Environment : $E$\n- Policy : $\\pi : S \\rightarrow P(A)  $\n- Transition dynamics : $p(s_{t+1} \\vert s_t, a_t) $\n- Reward function : $r(s_t, a_t)$\n- Return : $ \\sum_{ i=t }^{ T } \\gamma^{(i-t)}r(s_i, a_i) $\n- Discounted state visitation distribution for a policy : $\\rho^\\pi $\n\n<br>\n## 2.2 Bellman Equation\n\n- 상태 $s_t$에서 행동 $a_t$를 취했을 때 Expected return은 다음과 같습니다.\n\n$$Q^{\\pi}(s_t, a_t)={\\rm E}_{r_{i \\geqq t},s_{i \\geqq t} \\backsim E, a_{i \\geqq t} \\backsim \\pi } [R_{t} \\vert s_t, a_t  ]$$\n\n- 벨만 방정식을 사용하여 위의 식을 변형합니다.\n\n$$ Q^{\\pi}(s_t, a_t)={\\rm E}_{r_{t},s_{t} \\backsim E } \\[r(s_t,a_t)+\\gamma {\\rm E}_{a_{t+1} \\backsim \\pi } \\[ Q^{\\pi}(s_{t+1}, a_{t+1}) \\] \\]$$\n\n- Determinsitc policy를 가정합니다.\n\n$$Q^{\\mu}(s_t, a_t)={\\rm E}_{r_{t},s_{t} \\backsim E } \\[r(s_t,a_t)+\\gamma Q^{\\mu}(s_{t+1}, \\mu (s_{t+1})) \\]$$\n\n- 위의 수식에 대한 추가설명\n    - 두 번째 수식에서 위의 수식으로 내려오면서 policy가 determinstic하기 때문에 policy에 dependent한 Expectation이 빠진 것을 알 수 있습니다.\n    - Deterministic policy를 가정하기 전의 수식에서는 $a_{t+1}$을 골랐던 순간의 policy로 Q에 대한 Expection을 원래 구해야하기 때문에 off-policy가 아니지만, Determinsitic policy를 가정한다면 update 할 당시의 policy로 $a_{t+1}$를 구할 수 있기 때문에 off-policy가 됩니다.\n\n- Q learning\n$$L(\\theta^{Q}) = {\\rm E}_{s_t \\backsim \\rho^\\beta , a_t \\backsim \\beta , r_t \\backsim E} [(Q(s_t, a_t \\vert \\theta^Q)-y_t)^2]$$ \n\n- 위의 수식에 대한 추가설명\n    - $\\beta$는 behavior policy를 의미합니다.\n    - $ y_t = r(s_t, a_t) + \\gamma Q^{\\mu}(s_{t+1},\\mu(s_{t+1}))$  \n    - $\\mu(s) = argmax_{a}Q(s,a)$\n        - Q learning은 위와 같이 $argmax$라는 deterministic policy를 사용하기 때문에 off policy로 사용할 수 있습니다. \n\n<br>\n## 2.3 DPG\n\n$$\\nabla_{\\theta^\\mu} J \\approx  {\\rm E}_{s_t \\backsim \\rho^\\beta} [ \\nabla_{\\theta^\\mu} Q(s, a \\vert \\theta ^ Q) \\vert_{s=s_t, a=\\mu(s_t)}] = {\\rm E}_{s_t \\backsim \\rho^\\beta} [ \\nabla_{a} Q(s, a \\vert \\theta ^ Q) \\vert_{s=s_t, a=\\mu(s_t)} \\nabla_{\\theta^\\mu} \\mu(s \\vert Q^{\\mu})\\vert_{s=s_t}]$$\n\n- 위의 수식은 피지여행 DPG 글 4-2.Q-learning을 이용한 off-policy actor-critic에서 이미 정리 한 바 있습니다. [DPG](http://localhost:4000/2018/06/27/2_dpg/)를 참고해주세요.\n\n<br><br>\n\n# 3.Algorithm\n\nContinous control을 위한 새로운 알고리즘을 제안합니다. 제안하는 알고리즘의 특징은 다음과 같습니다.\n\n- Replay buffer를 사용합니다.\n- \"soft\" target update를 사용합니다.\n- 각 차원의 scale이 다른 low dimension vector로 부터 학습할 때 Batch Normalization을 사용합니다. \n- 탐험을 위해 action에 Noise를 추가합니다.\n\n<br>\n## 3.1 Replay buffer\n\n<center> <img src=\"https://www.dropbox.com/s/lc61b8nas1clqme/Screenshot%202018-06-23%2016.32.53.png?dl=1\" width=\"700px\"> </center>\n\n- 큰 State space를 학습하고 일반화 하기위해서는 Neural Network와 같은 non-linear approximator가 필수적이지만 수렴한다는 보장이 없습니다.\n- NFQCA에서는 수렴의 안정성을 위해서 batch learning을 도입합니다. 하지만 NFQCA에서는 업데이트시에 policy를 reset하지 않습니다.\n- DDPG는 DQN에서 사용된 Replay buffer를 사용하여 online batch update를 가능하게 합니다.\n\n<br>\n## 3.2 Soft target update\n\n$$ \\theta^{Q^{'}} \\leftarrow \\tau \\theta^{Q} + (1-\\tau) \\theta^{Q^{'}}$$\n\n$$ \\theta^{\\mu^{'}} \\leftarrow \\tau \\theta^{\\mu} + (1-\\tau) \\theta^{\\mu^{'}}$$\n\n- DQN에서는 일정 주기마다 origin network의 weight를 target network로 직접 복사해서 사용합니다.\n- DDPG에서는 exponential moving average(지수이동평균) 식으로 대체합니다.\n- soft update가 DQN에서 사용했던 방식에 비해 어떤 장점이 있는지는 명확하게 설명되어있지 않지만 stochatic gradient descent와 같이 급격하게 학습이 진행되는 것을 막기 위해 사용하는 것 같습니다.\n\n<br>\n## 3.3 Batch Normalization\n\n<center> <img src=\"https://www.dropbox.com/s/1erxzrgk69x04j8/Screenshot%202018-06-23%2016.51.56.png?dl=1\" width=\"500px\"> </center>\n\n- 서로 scale이 다른 feature를 state로 사용할 때에 Neural Net이 일반화에서 어려움을 겪습니다.\n    - 이걸 해결하기 위해서는 원래 직접 스케일을 조정해주었습니다.\n\n- 하지만 각 layer의 Input을 Unit Gaussian이 되도록 강제하는 BatchNormalization을 사용하여 이 문제를 해결합니다.\n\n<br>\n## 3.4 Noise Process\n\nDDPG 에서는 Exploration을 위해서 output으로 나온 행동에 노이즈를 추가해줍니다.\n\nORNSTEIN UHLENBECK PROCESS(OU)\n\n$$dx_t = \\theta (\\mu - x_t) dt + \\sigma dW_t$$\n\n- OU Process는 평균으로 회귀하는 random process입니다.\n- $\\theta$는 얼마나 빨리 평균으로 회귀할 지를 나타내는 파라미터이며 $\\mu$는 평균을 의미합니다.\n- $\\sigma$는 process의 변동성을 의미하며 $W_t$는 Wiener process를 의미합니다.\n- 따라서 이전의 noise들과 temporally correlated입니다.\n- 위와 같은 temporally correlated noise process를 사용하는 이유는 physical control과 같은 관성이 있는 환경에서 학습 시킬 때 보다 효과적이기 때문입니다.\n\n<br>\n## 3.5 Diagram & Pseudocode \n\n- DDPG의 학습 과정을 간단히 도식화 해본 다이어그램입니다.\n\n<img src=\"https://www.dropbox.com/s/0ffb2c9irctjx2n/Screenshot%202018-06-23%2017.58.57.png?dl=1\"> \n\n- DDPG의 알고리즘 수도코드입니다. 모든 항목을 위에서 설명했으니 순서대로 보시면 이해에 도움이 될것입니다.\n\n<center> <img src=\"https://www.dropbox.com/s/fd0nj7goixfnd6z/Screenshot%202018-06-23%2018.02.13.png?dl=1\" width=\"700px\"> </center>\n\n<br><br>\n\n# 4. Results\n\n<br>\n## 4.1 Variants of DPG\n\n<center> <img src=\"https://www.dropbox.com/s/k8q6tih85lgow5l/Screenshot%202018-06-23%2018.08.05.png?dl=1\" width=\"800px\"> </center>\n\n- original DPG에 batchnorm만 추가(연한 회색), target network만 추가(진한 회색), 둘 다 추가(초록), pixel로만 학습(파랑). Target network가 성능을 가장 좌지우지합니다.\n\n<br>\n## 4.2 Q estimation of DDPG\n\n<center> <img src=\"https://www.dropbox.com/s/vvcfoni0lqrisst/Screenshot%202018-06-23%2018.10.23.png?dl=1\" width=\"650px\"> </center>\n\n- DQN은 Q value를 Over-estimate하는 경향이 있었지만, DDPG는 simple task에 대해서는 잘한다. 복잡한 문제에 대해서는 estimation을 잘 못했지만, 여전히 좋은 Policy를 찾았습니다.\n\n<br>\n## 4.3 Performance Comparison\n\n<center> <img src=\"https://www.dropbox.com/s/u8ibmz9q4kfxh6l/Screenshot%202018-06-23%2018.11.40.png?dl=1\" width=\"800px\"> </center>\n\n- Score는 naive policy를 0, ILQG (planning algorithm)의 mean score를 1점으로 놓았을 때의 점수 Torcs 환경에 대해서만 raw reward를 score로 사용합니다.\n\n<br><br>\n\n# 5. Implementation Details\n\n<br>\n## 5.1 Hyper parameters\n\n- Optimizer : Adam\n    - actor lr : 0.0001, critic lr : 0.001\n- Weight decay(L2) for critic(Q) : 0.001\n- Discount factor : $\\gamma = 0.99 $\n- Soft target updates : $\\tau = 0.001 $\n- Size of replay buffer : 1,000,000\n- Orstein Uhlenbeck Process : $\\theta = 0.15$, $\\sigma = 0.2$\n\n<br>\n## 5.2 Etc.\n\n- Final output layer of actor : tanh (행동의 최소 최대를 맞춰주기 위해서)\n- low-dimentional 문제에서 네트워크는 2개의 hidden layer (1st layer 400 units, 2nd layer 300 units)를 가집니다.\n- 이미지를 통해서 학습시킬 때 : 3 convolutional layers (no pooling) with 32 filters at each layer.\n- actor와 critic 각각의 final layer(weight, bias 모두)는 다음 범위의 uniform distribution에서 샘플링합니다. [ - 0.003, 0.003], [ - 0.0003 , 0.0003]. 이렇게 하는 이유는 가장 처음의 policy와 value의 output이 0에 가깝게 나오도록 하기 위합니다.\n\n<br><br>\n\n# 6.Conclusion\n\n- 이 연구는 최근 딥러닝의 발전과 강화학습을 엮은 것으로 Continuous action space를 가지는 문제를 robust하게 풀어냅니다.\n- non-linear function approximators을 쓰는 것은 수렴을 보장하지 않지만, 여러 환경에 대해서 특별한 조작 없이 안정적으로 수렴하는 것을 실험으로 보여냅니다.\n- Atari 도메인에서 DQN보다 상당히 적은 step만에 수렴하는 것을 실험을 통해서 알아냅니다.\n- model-free 알고리즘은 좋은 solution을 찾기 위해서는 많은 sample을 필요로 한다는 한계가 있지만 더 큰 시스템에서는 이러한 한계를 물리칠 정도로 중요한 역할을 하게 될 것이라고 합니다.\n\n<br><br>\n\n# 처음으로\n\n## [PG Travel Guide](https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/)\n\n<br>\n\n# 이전으로\n\n## [DPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/27/2_dpg/)\n\n<br>\n\n# 다음으로\n\n## [NPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/25/4_npg/)","source":"_posts/3_ddpg.md","raw":"---\ntitle: Deep Determinstic Policy Gradient (DDPG)\ndate: 2018-06-26 11:20:45\ntags: [\"프로젝트\", \"피지여행\"]\ncategories: 프로젝트\nauthor: 양혁렬, 이동민, 차금강\nsubtitle: 피지여행 3번째 논문\n---\n\n<center > <img src=\"https://www.dropbox.com/s/zv8rk0uf87ipiaj/Screenshot%202018-06-23%2010.01.48.png?dl=1\" width=\"600\"> </center>\n\n논문 저자 : Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver & Daan Wierstra\n논문 링크 : https://arxiv.org/pdf/1509.02971.pdf\nProceeding : International Conference on Learning Representations (ICLR) 2016\n정리 : 양혁렬, 이동민, 차금강\n\n---\n\n# 1. 들어가며...\n\n<br>\n## 1.1 Success & Limination of DQN\n\n-  Success\n    - sensor로부터 나오는 전처리를 거친 input 대신에 raw pixel input을 사용합니다. 이렇게 함으로써 High dimensional observation space 문제를 풀어냅니다.\n- Limitation\n    - discrete & low dimensional action space만 다룰 수 있습니다. Continuous action space를 다루기 위해서는 매 스텝 이를 위한 iterative optimization process를 거쳐야 합니다.\n\n<br>\n## 1.2 Problems of discritization\n\n<center> <img src=\"https://www.dropbox.com/s/nulhzxs8bak2fn6/Screenshot%202018-06-23%2012.22.20.png?dl=1\" width=\"450px\"> </center>\n\n- 만약 7개의 관절을 가진 로봇 팔이 있다면, 가장 간단한 discretization은 각 관절을 다음과 같이 $a_{i}\\in \\\\{ -k, 0, k \\\\}$ 3개의 값을 가지도록 하는 것입니다.\n- 그렇다면 $3 \\times 3 \\times 3 \\times 3 \\times 3 \\times 3 \\times 3 = 3^{7} = 2187$ 가지의 dimension을 가진 action space가 만들어집니다. 이와같이 Discretization을 하면 action space가 exponential하게 늘어납니다.\n- 충분히 큰 action space 임에도 discretization으로 인한 정보의 손실이 있을 수 있고, 섬세한 Control을 할 수 없습니다.\n\n<br>\n## 1.3 New approach for continuous control\n\n- Model-free, Off-policy, Actor-critic algorithm을 제안합니다.\n- Deep Deterministic Policy(이하 DPG)를 기반으로 합니다.\n- Actor-Critic approach와 DQN의 성공적이었던 부분을 합칩니다.\n    - Replay buffer : 샘플들 사이의 상관관계를 줄여줍니다.\n    - target Q Network : Update 동안 target을 안정적으로 만듭니다.\n\n<br><br>\n\n# 2.Background\n\n<br>\n## 2.1 Notation\n\n- Observation : $x_{t}$\n- Action : $a_t \\in {\\rm IR}^N $\n- Reward : $r_t$\n- Discount factor : $\\gamma$\n- Environment : $E$\n- Policy : $\\pi : S \\rightarrow P(A)  $\n- Transition dynamics : $p(s_{t+1} \\vert s_t, a_t) $\n- Reward function : $r(s_t, a_t)$\n- Return : $ \\sum_{ i=t }^{ T } \\gamma^{(i-t)}r(s_i, a_i) $\n- Discounted state visitation distribution for a policy : $\\rho^\\pi $\n\n<br>\n## 2.2 Bellman Equation\n\n- 상태 $s_t$에서 행동 $a_t$를 취했을 때 Expected return은 다음과 같습니다.\n\n$$Q^{\\pi}(s_t, a_t)={\\rm E}_{r_{i \\geqq t},s_{i \\geqq t} \\backsim E, a_{i \\geqq t} \\backsim \\pi } [R_{t} \\vert s_t, a_t  ]$$\n\n- 벨만 방정식을 사용하여 위의 식을 변형합니다.\n\n$$ Q^{\\pi}(s_t, a_t)={\\rm E}_{r_{t},s_{t} \\backsim E } \\[r(s_t,a_t)+\\gamma {\\rm E}_{a_{t+1} \\backsim \\pi } \\[ Q^{\\pi}(s_{t+1}, a_{t+1}) \\] \\]$$\n\n- Determinsitc policy를 가정합니다.\n\n$$Q^{\\mu}(s_t, a_t)={\\rm E}_{r_{t},s_{t} \\backsim E } \\[r(s_t,a_t)+\\gamma Q^{\\mu}(s_{t+1}, \\mu (s_{t+1})) \\]$$\n\n- 위의 수식에 대한 추가설명\n    - 두 번째 수식에서 위의 수식으로 내려오면서 policy가 determinstic하기 때문에 policy에 dependent한 Expectation이 빠진 것을 알 수 있습니다.\n    - Deterministic policy를 가정하기 전의 수식에서는 $a_{t+1}$을 골랐던 순간의 policy로 Q에 대한 Expection을 원래 구해야하기 때문에 off-policy가 아니지만, Determinsitic policy를 가정한다면 update 할 당시의 policy로 $a_{t+1}$를 구할 수 있기 때문에 off-policy가 됩니다.\n\n- Q learning\n$$L(\\theta^{Q}) = {\\rm E}_{s_t \\backsim \\rho^\\beta , a_t \\backsim \\beta , r_t \\backsim E} [(Q(s_t, a_t \\vert \\theta^Q)-y_t)^2]$$ \n\n- 위의 수식에 대한 추가설명\n    - $\\beta$는 behavior policy를 의미합니다.\n    - $ y_t = r(s_t, a_t) + \\gamma Q^{\\mu}(s_{t+1},\\mu(s_{t+1}))$  \n    - $\\mu(s) = argmax_{a}Q(s,a)$\n        - Q learning은 위와 같이 $argmax$라는 deterministic policy를 사용하기 때문에 off policy로 사용할 수 있습니다. \n\n<br>\n## 2.3 DPG\n\n$$\\nabla_{\\theta^\\mu} J \\approx  {\\rm E}_{s_t \\backsim \\rho^\\beta} [ \\nabla_{\\theta^\\mu} Q(s, a \\vert \\theta ^ Q) \\vert_{s=s_t, a=\\mu(s_t)}] = {\\rm E}_{s_t \\backsim \\rho^\\beta} [ \\nabla_{a} Q(s, a \\vert \\theta ^ Q) \\vert_{s=s_t, a=\\mu(s_t)} \\nabla_{\\theta^\\mu} \\mu(s \\vert Q^{\\mu})\\vert_{s=s_t}]$$\n\n- 위의 수식은 피지여행 DPG 글 4-2.Q-learning을 이용한 off-policy actor-critic에서 이미 정리 한 바 있습니다. [DPG](http://localhost:4000/2018/06/27/2_dpg/)를 참고해주세요.\n\n<br><br>\n\n# 3.Algorithm\n\nContinous control을 위한 새로운 알고리즘을 제안합니다. 제안하는 알고리즘의 특징은 다음과 같습니다.\n\n- Replay buffer를 사용합니다.\n- \"soft\" target update를 사용합니다.\n- 각 차원의 scale이 다른 low dimension vector로 부터 학습할 때 Batch Normalization을 사용합니다. \n- 탐험을 위해 action에 Noise를 추가합니다.\n\n<br>\n## 3.1 Replay buffer\n\n<center> <img src=\"https://www.dropbox.com/s/lc61b8nas1clqme/Screenshot%202018-06-23%2016.32.53.png?dl=1\" width=\"700px\"> </center>\n\n- 큰 State space를 학습하고 일반화 하기위해서는 Neural Network와 같은 non-linear approximator가 필수적이지만 수렴한다는 보장이 없습니다.\n- NFQCA에서는 수렴의 안정성을 위해서 batch learning을 도입합니다. 하지만 NFQCA에서는 업데이트시에 policy를 reset하지 않습니다.\n- DDPG는 DQN에서 사용된 Replay buffer를 사용하여 online batch update를 가능하게 합니다.\n\n<br>\n## 3.2 Soft target update\n\n$$ \\theta^{Q^{'}} \\leftarrow \\tau \\theta^{Q} + (1-\\tau) \\theta^{Q^{'}}$$\n\n$$ \\theta^{\\mu^{'}} \\leftarrow \\tau \\theta^{\\mu} + (1-\\tau) \\theta^{\\mu^{'}}$$\n\n- DQN에서는 일정 주기마다 origin network의 weight를 target network로 직접 복사해서 사용합니다.\n- DDPG에서는 exponential moving average(지수이동평균) 식으로 대체합니다.\n- soft update가 DQN에서 사용했던 방식에 비해 어떤 장점이 있는지는 명확하게 설명되어있지 않지만 stochatic gradient descent와 같이 급격하게 학습이 진행되는 것을 막기 위해 사용하는 것 같습니다.\n\n<br>\n## 3.3 Batch Normalization\n\n<center> <img src=\"https://www.dropbox.com/s/1erxzrgk69x04j8/Screenshot%202018-06-23%2016.51.56.png?dl=1\" width=\"500px\"> </center>\n\n- 서로 scale이 다른 feature를 state로 사용할 때에 Neural Net이 일반화에서 어려움을 겪습니다.\n    - 이걸 해결하기 위해서는 원래 직접 스케일을 조정해주었습니다.\n\n- 하지만 각 layer의 Input을 Unit Gaussian이 되도록 강제하는 BatchNormalization을 사용하여 이 문제를 해결합니다.\n\n<br>\n## 3.4 Noise Process\n\nDDPG 에서는 Exploration을 위해서 output으로 나온 행동에 노이즈를 추가해줍니다.\n\nORNSTEIN UHLENBECK PROCESS(OU)\n\n$$dx_t = \\theta (\\mu - x_t) dt + \\sigma dW_t$$\n\n- OU Process는 평균으로 회귀하는 random process입니다.\n- $\\theta$는 얼마나 빨리 평균으로 회귀할 지를 나타내는 파라미터이며 $\\mu$는 평균을 의미합니다.\n- $\\sigma$는 process의 변동성을 의미하며 $W_t$는 Wiener process를 의미합니다.\n- 따라서 이전의 noise들과 temporally correlated입니다.\n- 위와 같은 temporally correlated noise process를 사용하는 이유는 physical control과 같은 관성이 있는 환경에서 학습 시킬 때 보다 효과적이기 때문입니다.\n\n<br>\n## 3.5 Diagram & Pseudocode \n\n- DDPG의 학습 과정을 간단히 도식화 해본 다이어그램입니다.\n\n<img src=\"https://www.dropbox.com/s/0ffb2c9irctjx2n/Screenshot%202018-06-23%2017.58.57.png?dl=1\"> \n\n- DDPG의 알고리즘 수도코드입니다. 모든 항목을 위에서 설명했으니 순서대로 보시면 이해에 도움이 될것입니다.\n\n<center> <img src=\"https://www.dropbox.com/s/fd0nj7goixfnd6z/Screenshot%202018-06-23%2018.02.13.png?dl=1\" width=\"700px\"> </center>\n\n<br><br>\n\n# 4. Results\n\n<br>\n## 4.1 Variants of DPG\n\n<center> <img src=\"https://www.dropbox.com/s/k8q6tih85lgow5l/Screenshot%202018-06-23%2018.08.05.png?dl=1\" width=\"800px\"> </center>\n\n- original DPG에 batchnorm만 추가(연한 회색), target network만 추가(진한 회색), 둘 다 추가(초록), pixel로만 학습(파랑). Target network가 성능을 가장 좌지우지합니다.\n\n<br>\n## 4.2 Q estimation of DDPG\n\n<center> <img src=\"https://www.dropbox.com/s/vvcfoni0lqrisst/Screenshot%202018-06-23%2018.10.23.png?dl=1\" width=\"650px\"> </center>\n\n- DQN은 Q value를 Over-estimate하는 경향이 있었지만, DDPG는 simple task에 대해서는 잘한다. 복잡한 문제에 대해서는 estimation을 잘 못했지만, 여전히 좋은 Policy를 찾았습니다.\n\n<br>\n## 4.3 Performance Comparison\n\n<center> <img src=\"https://www.dropbox.com/s/u8ibmz9q4kfxh6l/Screenshot%202018-06-23%2018.11.40.png?dl=1\" width=\"800px\"> </center>\n\n- Score는 naive policy를 0, ILQG (planning algorithm)의 mean score를 1점으로 놓았을 때의 점수 Torcs 환경에 대해서만 raw reward를 score로 사용합니다.\n\n<br><br>\n\n# 5. Implementation Details\n\n<br>\n## 5.1 Hyper parameters\n\n- Optimizer : Adam\n    - actor lr : 0.0001, critic lr : 0.001\n- Weight decay(L2) for critic(Q) : 0.001\n- Discount factor : $\\gamma = 0.99 $\n- Soft target updates : $\\tau = 0.001 $\n- Size of replay buffer : 1,000,000\n- Orstein Uhlenbeck Process : $\\theta = 0.15$, $\\sigma = 0.2$\n\n<br>\n## 5.2 Etc.\n\n- Final output layer of actor : tanh (행동의 최소 최대를 맞춰주기 위해서)\n- low-dimentional 문제에서 네트워크는 2개의 hidden layer (1st layer 400 units, 2nd layer 300 units)를 가집니다.\n- 이미지를 통해서 학습시킬 때 : 3 convolutional layers (no pooling) with 32 filters at each layer.\n- actor와 critic 각각의 final layer(weight, bias 모두)는 다음 범위의 uniform distribution에서 샘플링합니다. [ - 0.003, 0.003], [ - 0.0003 , 0.0003]. 이렇게 하는 이유는 가장 처음의 policy와 value의 output이 0에 가깝게 나오도록 하기 위합니다.\n\n<br><br>\n\n# 6.Conclusion\n\n- 이 연구는 최근 딥러닝의 발전과 강화학습을 엮은 것으로 Continuous action space를 가지는 문제를 robust하게 풀어냅니다.\n- non-linear function approximators을 쓰는 것은 수렴을 보장하지 않지만, 여러 환경에 대해서 특별한 조작 없이 안정적으로 수렴하는 것을 실험으로 보여냅니다.\n- Atari 도메인에서 DQN보다 상당히 적은 step만에 수렴하는 것을 실험을 통해서 알아냅니다.\n- model-free 알고리즘은 좋은 solution을 찾기 위해서는 많은 sample을 필요로 한다는 한계가 있지만 더 큰 시스템에서는 이러한 한계를 물리칠 정도로 중요한 역할을 하게 될 것이라고 합니다.\n\n<br><br>\n\n# 처음으로\n\n## [PG Travel Guide](https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/)\n\n<br>\n\n# 이전으로\n\n## [DPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/27/2_dpg/)\n\n<br>\n\n# 다음으로\n\n## [NPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/25/4_npg/)","slug":"3_ddpg","published":1,"updated":"2018-09-08T11:00:20.606Z","_id":"cjkj6tyra0000j715ow4qussb","comments":1,"layout":"post","photos":[],"link":"","content":"<center> <img src=\"https://www.dropbox.com/s/zv8rk0uf87ipiaj/Screenshot%202018-06-23%2010.01.48.png?dl=1\" width=\"600\"> </center>\n\n<p>논문 저자 : Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver &amp; Daan Wierstra<br>논문 링크 : <a href=\"https://arxiv.org/pdf/1509.02971.pdf\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/pdf/1509.02971.pdf</a><br>Proceeding : International Conference on Learning Representations (ICLR) 2016<br>정리 : 양혁렬, 이동민, 차금강</p>\n<hr>\n<h1 id=\"1-들어가며…\"><a href=\"#1-들어가며…\" class=\"headerlink\" title=\"1. 들어가며…\"></a>1. 들어가며…</h1><p><br></p>\n<h2 id=\"1-1-Success-amp-Limination-of-DQN\"><a href=\"#1-1-Success-amp-Limination-of-DQN\" class=\"headerlink\" title=\"1.1 Success &amp; Limination of DQN\"></a>1.1 Success &amp; Limination of DQN</h2><ul>\n<li>Success<ul>\n<li>sensor로부터 나오는 전처리를 거친 input 대신에 raw pixel input을 사용합니다. 이렇게 함으로써 High dimensional observation space 문제를 풀어냅니다.</li>\n</ul>\n</li>\n<li>Limitation<ul>\n<li>discrete &amp; low dimensional action space만 다룰 수 있습니다. Continuous action space를 다루기 위해서는 매 스텝 이를 위한 iterative optimization process를 거쳐야 합니다.</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"1-2-Problems-of-discritization\"><a href=\"#1-2-Problems-of-discritization\" class=\"headerlink\" title=\"1.2 Problems of discritization\"></a>1.2 Problems of discritization</h2><center> <img src=\"https://www.dropbox.com/s/nulhzxs8bak2fn6/Screenshot%202018-06-23%2012.22.20.png?dl=1\" width=\"450px\"> </center>\n\n<ul>\n<li>만약 7개의 관절을 가진 로봇 팔이 있다면, 가장 간단한 discretization은 각 관절을 다음과 같이 $a_{i}\\in \\{ -k, 0, k \\}$ 3개의 값을 가지도록 하는 것입니다.</li>\n<li>그렇다면 $3 \\times 3 \\times 3 \\times 3 \\times 3 \\times 3 \\times 3 = 3^{7} = 2187$ 가지의 dimension을 가진 action space가 만들어집니다. 이와같이 Discretization을 하면 action space가 exponential하게 늘어납니다.</li>\n<li>충분히 큰 action space 임에도 discretization으로 인한 정보의 손실이 있을 수 있고, 섬세한 Control을 할 수 없습니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"1-3-New-approach-for-continuous-control\"><a href=\"#1-3-New-approach-for-continuous-control\" class=\"headerlink\" title=\"1.3 New approach for continuous control\"></a>1.3 New approach for continuous control</h2><ul>\n<li>Model-free, Off-policy, Actor-critic algorithm을 제안합니다.</li>\n<li>Deep Deterministic Policy(이하 DPG)를 기반으로 합니다.</li>\n<li>Actor-Critic approach와 DQN의 성공적이었던 부분을 합칩니다.<ul>\n<li>Replay buffer : 샘플들 사이의 상관관계를 줄여줍니다.</li>\n<li>target Q Network : Update 동안 target을 안정적으로 만듭니다.</li>\n</ul>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"2-Background\"><a href=\"#2-Background\" class=\"headerlink\" title=\"2.Background\"></a>2.Background</h1><p><br></p>\n<h2 id=\"2-1-Notation\"><a href=\"#2-1-Notation\" class=\"headerlink\" title=\"2.1 Notation\"></a>2.1 Notation</h2><ul>\n<li>Observation : $x_{t}$</li>\n<li>Action : $a_t \\in {\\rm IR}^N $</li>\n<li>Reward : $r_t$</li>\n<li>Discount factor : $\\gamma$</li>\n<li>Environment : $E$</li>\n<li>Policy : $\\pi : S \\rightarrow P(A)  $</li>\n<li>Transition dynamics : $p(s_{t+1} \\vert s_t, a_t) $</li>\n<li>Reward function : $r(s_t, a_t)$</li>\n<li>Return : $ \\sum_{ i=t }^{ T } \\gamma^{(i-t)}r(s_i, a_i) $</li>\n<li>Discounted state visitation distribution for a policy : $\\rho^\\pi $</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-2-Bellman-Equation\"><a href=\"#2-2-Bellman-Equation\" class=\"headerlink\" title=\"2.2 Bellman Equation\"></a>2.2 Bellman Equation</h2><ul>\n<li>상태 $s_t$에서 행동 $a_t$를 취했을 때 Expected return은 다음과 같습니다.</li>\n</ul>\n<p>$$Q^{\\pi}(s_t, a_t)={\\rm E}_{r_{i \\geqq t},s_{i \\geqq t} \\backsim E, a_{i \\geqq t} \\backsim \\pi } [R_{t} \\vert s_t, a_t  ]$$</p>\n<ul>\n<li>벨만 방정식을 사용하여 위의 식을 변형합니다.</li>\n</ul>\n<p>$$ Q^{\\pi}(s_t, a_t)={\\rm E}_{r_{t},s_{t} \\backsim E } [r(s_t,a_t)+\\gamma {\\rm E}_{a_{t+1} \\backsim \\pi } [ Q^{\\pi}(s_{t+1}, a_{t+1}) ] ]$$</p>\n<ul>\n<li>Determinsitc policy를 가정합니다.</li>\n</ul>\n<p>$$Q^{\\mu}(s_t, a_t)={\\rm E}_{r_{t},s_{t} \\backsim E } [r(s_t,a_t)+\\gamma Q^{\\mu}(s_{t+1}, \\mu (s_{t+1})) ]$$</p>\n<ul>\n<li><p>위의 수식에 대한 추가설명</p>\n<ul>\n<li>두 번째 수식에서 위의 수식으로 내려오면서 policy가 determinstic하기 때문에 policy에 dependent한 Expectation이 빠진 것을 알 수 있습니다.</li>\n<li>Deterministic policy를 가정하기 전의 수식에서는 $a_{t+1}$을 골랐던 순간의 policy로 Q에 대한 Expection을 원래 구해야하기 때문에 off-policy가 아니지만, Determinsitic policy를 가정한다면 update 할 당시의 policy로 $a_{t+1}$를 구할 수 있기 때문에 off-policy가 됩니다.</li>\n</ul>\n</li>\n<li><p>Q learning<br>$$L(\\theta^{Q}) = {\\rm E}_{s_t \\backsim \\rho^\\beta , a_t \\backsim \\beta , r_t \\backsim E} [(Q(s_t, a_t \\vert \\theta^Q)-y_t)^2]$$ </p>\n</li>\n<li><p>위의 수식에 대한 추가설명</p>\n<ul>\n<li>$\\beta$는 behavior policy를 의미합니다.</li>\n<li>$ y_t = r(s_t, a_t) + \\gamma Q^{\\mu}(s_{t+1},\\mu(s_{t+1}))$  </li>\n<li>$\\mu(s) = argmax_{a}Q(s,a)$<ul>\n<li>Q learning은 위와 같이 $argmax$라는 deterministic policy를 사용하기 때문에 off policy로 사용할 수 있습니다. </li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-3-DPG\"><a href=\"#2-3-DPG\" class=\"headerlink\" title=\"2.3 DPG\"></a>2.3 DPG</h2><p>$$\\nabla_{\\theta^\\mu} J \\approx  {\\rm E}_{s_t \\backsim \\rho^\\beta} [ \\nabla_{\\theta^\\mu} Q(s, a \\vert \\theta ^ Q) \\vert_{s=s_t, a=\\mu(s_t)}] = {\\rm E}_{s_t \\backsim \\rho^\\beta} [ \\nabla_{a} Q(s, a \\vert \\theta ^ Q) \\vert_{s=s_t, a=\\mu(s_t)} \\nabla_{\\theta^\\mu} \\mu(s \\vert Q^{\\mu})\\vert_{s=s_t}]$$</p>\n<ul>\n<li>위의 수식은 피지여행 DPG 글 4-2.Q-learning을 이용한 off-policy actor-critic에서 이미 정리 한 바 있습니다. <a href=\"http://localhost:4000/2018/06/27/2_dpg/\" target=\"_blank\" rel=\"noopener\">DPG</a>를 참고해주세요.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"3-Algorithm\"><a href=\"#3-Algorithm\" class=\"headerlink\" title=\"3.Algorithm\"></a>3.Algorithm</h1><p>Continous control을 위한 새로운 알고리즘을 제안합니다. 제안하는 알고리즘의 특징은 다음과 같습니다.</p>\n<ul>\n<li>Replay buffer를 사용합니다.</li>\n<li>“soft” target update를 사용합니다.</li>\n<li>각 차원의 scale이 다른 low dimension vector로 부터 학습할 때 Batch Normalization을 사용합니다. </li>\n<li>탐험을 위해 action에 Noise를 추가합니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-1-Replay-buffer\"><a href=\"#3-1-Replay-buffer\" class=\"headerlink\" title=\"3.1 Replay buffer\"></a>3.1 Replay buffer</h2><center> <img src=\"https://www.dropbox.com/s/lc61b8nas1clqme/Screenshot%202018-06-23%2016.32.53.png?dl=1\" width=\"700px\"> </center>\n\n<ul>\n<li>큰 State space를 학습하고 일반화 하기위해서는 Neural Network와 같은 non-linear approximator가 필수적이지만 수렴한다는 보장이 없습니다.</li>\n<li>NFQCA에서는 수렴의 안정성을 위해서 batch learning을 도입합니다. 하지만 NFQCA에서는 업데이트시에 policy를 reset하지 않습니다.</li>\n<li>DDPG는 DQN에서 사용된 Replay buffer를 사용하여 online batch update를 가능하게 합니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-2-Soft-target-update\"><a href=\"#3-2-Soft-target-update\" class=\"headerlink\" title=\"3.2 Soft target update\"></a>3.2 Soft target update</h2><p>$$ \\theta^{Q^{‘}} \\leftarrow \\tau \\theta^{Q} + (1-\\tau) \\theta^{Q^{‘}}$$</p>\n<p>$$ \\theta^{\\mu^{‘}} \\leftarrow \\tau \\theta^{\\mu} + (1-\\tau) \\theta^{\\mu^{‘}}$$</p>\n<ul>\n<li>DQN에서는 일정 주기마다 origin network의 weight를 target network로 직접 복사해서 사용합니다.</li>\n<li>DDPG에서는 exponential moving average(지수이동평균) 식으로 대체합니다.</li>\n<li>soft update가 DQN에서 사용했던 방식에 비해 어떤 장점이 있는지는 명확하게 설명되어있지 않지만 stochatic gradient descent와 같이 급격하게 학습이 진행되는 것을 막기 위해 사용하는 것 같습니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-3-Batch-Normalization\"><a href=\"#3-3-Batch-Normalization\" class=\"headerlink\" title=\"3.3 Batch Normalization\"></a>3.3 Batch Normalization</h2><center> <img src=\"https://www.dropbox.com/s/1erxzrgk69x04j8/Screenshot%202018-06-23%2016.51.56.png?dl=1\" width=\"500px\"> </center>\n\n<ul>\n<li><p>서로 scale이 다른 feature를 state로 사용할 때에 Neural Net이 일반화에서 어려움을 겪습니다.</p>\n<ul>\n<li>이걸 해결하기 위해서는 원래 직접 스케일을 조정해주었습니다.</li>\n</ul>\n</li>\n<li><p>하지만 각 layer의 Input을 Unit Gaussian이 되도록 강제하는 BatchNormalization을 사용하여 이 문제를 해결합니다.</p>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-4-Noise-Process\"><a href=\"#3-4-Noise-Process\" class=\"headerlink\" title=\"3.4 Noise Process\"></a>3.4 Noise Process</h2><p>DDPG 에서는 Exploration을 위해서 output으로 나온 행동에 노이즈를 추가해줍니다.</p>\n<p>ORNSTEIN UHLENBECK PROCESS(OU)</p>\n<p>$$dx_t = \\theta (\\mu - x_t) dt + \\sigma dW_t$$</p>\n<ul>\n<li>OU Process는 평균으로 회귀하는 random process입니다.</li>\n<li>$\\theta$는 얼마나 빨리 평균으로 회귀할 지를 나타내는 파라미터이며 $\\mu$는 평균을 의미합니다.</li>\n<li>$\\sigma$는 process의 변동성을 의미하며 $W_t$는 Wiener process를 의미합니다.</li>\n<li>따라서 이전의 noise들과 temporally correlated입니다.</li>\n<li>위와 같은 temporally correlated noise process를 사용하는 이유는 physical control과 같은 관성이 있는 환경에서 학습 시킬 때 보다 효과적이기 때문입니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-5-Diagram-amp-Pseudocode\"><a href=\"#3-5-Diagram-amp-Pseudocode\" class=\"headerlink\" title=\"3.5 Diagram &amp; Pseudocode\"></a>3.5 Diagram &amp; Pseudocode</h2><ul>\n<li>DDPG의 학습 과정을 간단히 도식화 해본 다이어그램입니다.</li>\n</ul>\n<p><img src=\"https://www.dropbox.com/s/0ffb2c9irctjx2n/Screenshot%202018-06-23%2017.58.57.png?dl=1\"> </p>\n<ul>\n<li>DDPG의 알고리즘 수도코드입니다. 모든 항목을 위에서 설명했으니 순서대로 보시면 이해에 도움이 될것입니다.</li>\n</ul>\n<center> <img src=\"https://www.dropbox.com/s/fd0nj7goixfnd6z/Screenshot%202018-06-23%2018.02.13.png?dl=1\" width=\"700px\"> </center>\n\n<p><br><br></p>\n<h1 id=\"4-Results\"><a href=\"#4-Results\" class=\"headerlink\" title=\"4. Results\"></a>4. Results</h1><p><br></p>\n<h2 id=\"4-1-Variants-of-DPG\"><a href=\"#4-1-Variants-of-DPG\" class=\"headerlink\" title=\"4.1 Variants of DPG\"></a>4.1 Variants of DPG</h2><center> <img src=\"https://www.dropbox.com/s/k8q6tih85lgow5l/Screenshot%202018-06-23%2018.08.05.png?dl=1\" width=\"800px\"> </center>\n\n<ul>\n<li>original DPG에 batchnorm만 추가(연한 회색), target network만 추가(진한 회색), 둘 다 추가(초록), pixel로만 학습(파랑). Target network가 성능을 가장 좌지우지합니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"4-2-Q-estimation-of-DDPG\"><a href=\"#4-2-Q-estimation-of-DDPG\" class=\"headerlink\" title=\"4.2 Q estimation of DDPG\"></a>4.2 Q estimation of DDPG</h2><center> <img src=\"https://www.dropbox.com/s/vvcfoni0lqrisst/Screenshot%202018-06-23%2018.10.23.png?dl=1\" width=\"650px\"> </center>\n\n<ul>\n<li>DQN은 Q value를 Over-estimate하는 경향이 있었지만, DDPG는 simple task에 대해서는 잘한다. 복잡한 문제에 대해서는 estimation을 잘 못했지만, 여전히 좋은 Policy를 찾았습니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"4-3-Performance-Comparison\"><a href=\"#4-3-Performance-Comparison\" class=\"headerlink\" title=\"4.3 Performance Comparison\"></a>4.3 Performance Comparison</h2><center> <img src=\"https://www.dropbox.com/s/u8ibmz9q4kfxh6l/Screenshot%202018-06-23%2018.11.40.png?dl=1\" width=\"800px\"> </center>\n\n<ul>\n<li>Score는 naive policy를 0, ILQG (planning algorithm)의 mean score를 1점으로 놓았을 때의 점수 Torcs 환경에 대해서만 raw reward를 score로 사용합니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"5-Implementation-Details\"><a href=\"#5-Implementation-Details\" class=\"headerlink\" title=\"5. Implementation Details\"></a>5. Implementation Details</h1><p><br></p>\n<h2 id=\"5-1-Hyper-parameters\"><a href=\"#5-1-Hyper-parameters\" class=\"headerlink\" title=\"5.1 Hyper parameters\"></a>5.1 Hyper parameters</h2><ul>\n<li>Optimizer : Adam<ul>\n<li>actor lr : 0.0001, critic lr : 0.001</li>\n</ul>\n</li>\n<li>Weight decay(L2) for critic(Q) : 0.001</li>\n<li>Discount factor : $\\gamma = 0.99 $</li>\n<li>Soft target updates : $\\tau = 0.001 $</li>\n<li>Size of replay buffer : 1,000,000</li>\n<li>Orstein Uhlenbeck Process : $\\theta = 0.15$, $\\sigma = 0.2$</li>\n</ul>\n<p><br></p>\n<h2 id=\"5-2-Etc\"><a href=\"#5-2-Etc\" class=\"headerlink\" title=\"5.2 Etc.\"></a>5.2 Etc.</h2><ul>\n<li>Final output layer of actor : tanh (행동의 최소 최대를 맞춰주기 위해서)</li>\n<li>low-dimentional 문제에서 네트워크는 2개의 hidden layer (1st layer 400 units, 2nd layer 300 units)를 가집니다.</li>\n<li>이미지를 통해서 학습시킬 때 : 3 convolutional layers (no pooling) with 32 filters at each layer.</li>\n<li>actor와 critic 각각의 final layer(weight, bias 모두)는 다음 범위의 uniform distribution에서 샘플링합니다. [ - 0.003, 0.003], [ - 0.0003 , 0.0003]. 이렇게 하는 이유는 가장 처음의 policy와 value의 output이 0에 가깝게 나오도록 하기 위합니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"6-Conclusion\"><a href=\"#6-Conclusion\" class=\"headerlink\" title=\"6.Conclusion\"></a>6.Conclusion</h1><ul>\n<li>이 연구는 최근 딥러닝의 발전과 강화학습을 엮은 것으로 Continuous action space를 가지는 문제를 robust하게 풀어냅니다.</li>\n<li>non-linear function approximators을 쓰는 것은 수렴을 보장하지 않지만, 여러 환경에 대해서 특별한 조작 없이 안정적으로 수렴하는 것을 실험으로 보여냅니다.</li>\n<li>Atari 도메인에서 DQN보다 상당히 적은 step만에 수렴하는 것을 실험을 통해서 알아냅니다.</li>\n<li>model-free 알고리즘은 좋은 solution을 찾기 위해서는 많은 sample을 필요로 한다는 한계가 있지만 더 큰 시스템에서는 이러한 한계를 물리칠 정도로 중요한 역할을 하게 될 것이라고 합니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"처음으로\"><a href=\"#처음으로\" class=\"headerlink\" title=\"처음으로\"></a>처음으로</h1><h2 id=\"PG-Travel-Guide\"><a href=\"#PG-Travel-Guide\" class=\"headerlink\" title=\"PG Travel Guide\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/\">PG Travel Guide</a></h2><p><br></p>\n<h1 id=\"이전으로\"><a href=\"#이전으로\" class=\"headerlink\" title=\"이전으로\"></a>이전으로</h1><h2 id=\"DPG-여행하기\"><a href=\"#DPG-여행하기\" class=\"headerlink\" title=\"DPG 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/27/2_dpg/\">DPG 여행하기</a></h2><p><br></p>\n<h1 id=\"다음으로\"><a href=\"#다음으로\" class=\"headerlink\" title=\"다음으로\"></a>다음으로</h1><h2 id=\"NPG-여행하기\"><a href=\"#NPG-여행하기\" class=\"headerlink\" title=\"NPG 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/25/4_npg/\">NPG 여행하기</a></h2>","site":{"data":{}},"excerpt":"","more":"<center> <img src=\"https://www.dropbox.com/s/zv8rk0uf87ipiaj/Screenshot%202018-06-23%2010.01.48.png?dl=1\" width=\"600\"> </center>\n\n<p>논문 저자 : Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver &amp; Daan Wierstra<br>논문 링크 : <a href=\"https://arxiv.org/pdf/1509.02971.pdf\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/pdf/1509.02971.pdf</a><br>Proceeding : International Conference on Learning Representations (ICLR) 2016<br>정리 : 양혁렬, 이동민, 차금강</p>\n<hr>\n<h1 id=\"1-들어가며…\"><a href=\"#1-들어가며…\" class=\"headerlink\" title=\"1. 들어가며…\"></a>1. 들어가며…</h1><p><br></p>\n<h2 id=\"1-1-Success-amp-Limination-of-DQN\"><a href=\"#1-1-Success-amp-Limination-of-DQN\" class=\"headerlink\" title=\"1.1 Success &amp; Limination of DQN\"></a>1.1 Success &amp; Limination of DQN</h2><ul>\n<li>Success<ul>\n<li>sensor로부터 나오는 전처리를 거친 input 대신에 raw pixel input을 사용합니다. 이렇게 함으로써 High dimensional observation space 문제를 풀어냅니다.</li>\n</ul>\n</li>\n<li>Limitation<ul>\n<li>discrete &amp; low dimensional action space만 다룰 수 있습니다. Continuous action space를 다루기 위해서는 매 스텝 이를 위한 iterative optimization process를 거쳐야 합니다.</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"1-2-Problems-of-discritization\"><a href=\"#1-2-Problems-of-discritization\" class=\"headerlink\" title=\"1.2 Problems of discritization\"></a>1.2 Problems of discritization</h2><center> <img src=\"https://www.dropbox.com/s/nulhzxs8bak2fn6/Screenshot%202018-06-23%2012.22.20.png?dl=1\" width=\"450px\"> </center>\n\n<ul>\n<li>만약 7개의 관절을 가진 로봇 팔이 있다면, 가장 간단한 discretization은 각 관절을 다음과 같이 $a_{i}\\in \\{ -k, 0, k \\}$ 3개의 값을 가지도록 하는 것입니다.</li>\n<li>그렇다면 $3 \\times 3 \\times 3 \\times 3 \\times 3 \\times 3 \\times 3 = 3^{7} = 2187$ 가지의 dimension을 가진 action space가 만들어집니다. 이와같이 Discretization을 하면 action space가 exponential하게 늘어납니다.</li>\n<li>충분히 큰 action space 임에도 discretization으로 인한 정보의 손실이 있을 수 있고, 섬세한 Control을 할 수 없습니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"1-3-New-approach-for-continuous-control\"><a href=\"#1-3-New-approach-for-continuous-control\" class=\"headerlink\" title=\"1.3 New approach for continuous control\"></a>1.3 New approach for continuous control</h2><ul>\n<li>Model-free, Off-policy, Actor-critic algorithm을 제안합니다.</li>\n<li>Deep Deterministic Policy(이하 DPG)를 기반으로 합니다.</li>\n<li>Actor-Critic approach와 DQN의 성공적이었던 부분을 합칩니다.<ul>\n<li>Replay buffer : 샘플들 사이의 상관관계를 줄여줍니다.</li>\n<li>target Q Network : Update 동안 target을 안정적으로 만듭니다.</li>\n</ul>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"2-Background\"><a href=\"#2-Background\" class=\"headerlink\" title=\"2.Background\"></a>2.Background</h1><p><br></p>\n<h2 id=\"2-1-Notation\"><a href=\"#2-1-Notation\" class=\"headerlink\" title=\"2.1 Notation\"></a>2.1 Notation</h2><ul>\n<li>Observation : $x_{t}$</li>\n<li>Action : $a_t \\in {\\rm IR}^N $</li>\n<li>Reward : $r_t$</li>\n<li>Discount factor : $\\gamma$</li>\n<li>Environment : $E$</li>\n<li>Policy : $\\pi : S \\rightarrow P(A)  $</li>\n<li>Transition dynamics : $p(s_{t+1} \\vert s_t, a_t) $</li>\n<li>Reward function : $r(s_t, a_t)$</li>\n<li>Return : $ \\sum_{ i=t }^{ T } \\gamma^{(i-t)}r(s_i, a_i) $</li>\n<li>Discounted state visitation distribution for a policy : $\\rho^\\pi $</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-2-Bellman-Equation\"><a href=\"#2-2-Bellman-Equation\" class=\"headerlink\" title=\"2.2 Bellman Equation\"></a>2.2 Bellman Equation</h2><ul>\n<li>상태 $s_t$에서 행동 $a_t$를 취했을 때 Expected return은 다음과 같습니다.</li>\n</ul>\n<p>$$Q^{\\pi}(s_t, a_t)={\\rm E}_{r_{i \\geqq t},s_{i \\geqq t} \\backsim E, a_{i \\geqq t} \\backsim \\pi } [R_{t} \\vert s_t, a_t  ]$$</p>\n<ul>\n<li>벨만 방정식을 사용하여 위의 식을 변형합니다.</li>\n</ul>\n<p>$$ Q^{\\pi}(s_t, a_t)={\\rm E}_{r_{t},s_{t} \\backsim E } [r(s_t,a_t)+\\gamma {\\rm E}_{a_{t+1} \\backsim \\pi } [ Q^{\\pi}(s_{t+1}, a_{t+1}) ] ]$$</p>\n<ul>\n<li>Determinsitc policy를 가정합니다.</li>\n</ul>\n<p>$$Q^{\\mu}(s_t, a_t)={\\rm E}_{r_{t},s_{t} \\backsim E } [r(s_t,a_t)+\\gamma Q^{\\mu}(s_{t+1}, \\mu (s_{t+1})) ]$$</p>\n<ul>\n<li><p>위의 수식에 대한 추가설명</p>\n<ul>\n<li>두 번째 수식에서 위의 수식으로 내려오면서 policy가 determinstic하기 때문에 policy에 dependent한 Expectation이 빠진 것을 알 수 있습니다.</li>\n<li>Deterministic policy를 가정하기 전의 수식에서는 $a_{t+1}$을 골랐던 순간의 policy로 Q에 대한 Expection을 원래 구해야하기 때문에 off-policy가 아니지만, Determinsitic policy를 가정한다면 update 할 당시의 policy로 $a_{t+1}$를 구할 수 있기 때문에 off-policy가 됩니다.</li>\n</ul>\n</li>\n<li><p>Q learning<br>$$L(\\theta^{Q}) = {\\rm E}_{s_t \\backsim \\rho^\\beta , a_t \\backsim \\beta , r_t \\backsim E} [(Q(s_t, a_t \\vert \\theta^Q)-y_t)^2]$$ </p>\n</li>\n<li><p>위의 수식에 대한 추가설명</p>\n<ul>\n<li>$\\beta$는 behavior policy를 의미합니다.</li>\n<li>$ y_t = r(s_t, a_t) + \\gamma Q^{\\mu}(s_{t+1},\\mu(s_{t+1}))$  </li>\n<li>$\\mu(s) = argmax_{a}Q(s,a)$<ul>\n<li>Q learning은 위와 같이 $argmax$라는 deterministic policy를 사용하기 때문에 off policy로 사용할 수 있습니다. </li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-3-DPG\"><a href=\"#2-3-DPG\" class=\"headerlink\" title=\"2.3 DPG\"></a>2.3 DPG</h2><p>$$\\nabla_{\\theta^\\mu} J \\approx  {\\rm E}_{s_t \\backsim \\rho^\\beta} [ \\nabla_{\\theta^\\mu} Q(s, a \\vert \\theta ^ Q) \\vert_{s=s_t, a=\\mu(s_t)}] = {\\rm E}_{s_t \\backsim \\rho^\\beta} [ \\nabla_{a} Q(s, a \\vert \\theta ^ Q) \\vert_{s=s_t, a=\\mu(s_t)} \\nabla_{\\theta^\\mu} \\mu(s \\vert Q^{\\mu})\\vert_{s=s_t}]$$</p>\n<ul>\n<li>위의 수식은 피지여행 DPG 글 4-2.Q-learning을 이용한 off-policy actor-critic에서 이미 정리 한 바 있습니다. <a href=\"http://localhost:4000/2018/06/27/2_dpg/\" target=\"_blank\" rel=\"noopener\">DPG</a>를 참고해주세요.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"3-Algorithm\"><a href=\"#3-Algorithm\" class=\"headerlink\" title=\"3.Algorithm\"></a>3.Algorithm</h1><p>Continous control을 위한 새로운 알고리즘을 제안합니다. 제안하는 알고리즘의 특징은 다음과 같습니다.</p>\n<ul>\n<li>Replay buffer를 사용합니다.</li>\n<li>“soft” target update를 사용합니다.</li>\n<li>각 차원의 scale이 다른 low dimension vector로 부터 학습할 때 Batch Normalization을 사용합니다. </li>\n<li>탐험을 위해 action에 Noise를 추가합니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-1-Replay-buffer\"><a href=\"#3-1-Replay-buffer\" class=\"headerlink\" title=\"3.1 Replay buffer\"></a>3.1 Replay buffer</h2><center> <img src=\"https://www.dropbox.com/s/lc61b8nas1clqme/Screenshot%202018-06-23%2016.32.53.png?dl=1\" width=\"700px\"> </center>\n\n<ul>\n<li>큰 State space를 학습하고 일반화 하기위해서는 Neural Network와 같은 non-linear approximator가 필수적이지만 수렴한다는 보장이 없습니다.</li>\n<li>NFQCA에서는 수렴의 안정성을 위해서 batch learning을 도입합니다. 하지만 NFQCA에서는 업데이트시에 policy를 reset하지 않습니다.</li>\n<li>DDPG는 DQN에서 사용된 Replay buffer를 사용하여 online batch update를 가능하게 합니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-2-Soft-target-update\"><a href=\"#3-2-Soft-target-update\" class=\"headerlink\" title=\"3.2 Soft target update\"></a>3.2 Soft target update</h2><p>$$ \\theta^{Q^{‘}} \\leftarrow \\tau \\theta^{Q} + (1-\\tau) \\theta^{Q^{‘}}$$</p>\n<p>$$ \\theta^{\\mu^{‘}} \\leftarrow \\tau \\theta^{\\mu} + (1-\\tau) \\theta^{\\mu^{‘}}$$</p>\n<ul>\n<li>DQN에서는 일정 주기마다 origin network의 weight를 target network로 직접 복사해서 사용합니다.</li>\n<li>DDPG에서는 exponential moving average(지수이동평균) 식으로 대체합니다.</li>\n<li>soft update가 DQN에서 사용했던 방식에 비해 어떤 장점이 있는지는 명확하게 설명되어있지 않지만 stochatic gradient descent와 같이 급격하게 학습이 진행되는 것을 막기 위해 사용하는 것 같습니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-3-Batch-Normalization\"><a href=\"#3-3-Batch-Normalization\" class=\"headerlink\" title=\"3.3 Batch Normalization\"></a>3.3 Batch Normalization</h2><center> <img src=\"https://www.dropbox.com/s/1erxzrgk69x04j8/Screenshot%202018-06-23%2016.51.56.png?dl=1\" width=\"500px\"> </center>\n\n<ul>\n<li><p>서로 scale이 다른 feature를 state로 사용할 때에 Neural Net이 일반화에서 어려움을 겪습니다.</p>\n<ul>\n<li>이걸 해결하기 위해서는 원래 직접 스케일을 조정해주었습니다.</li>\n</ul>\n</li>\n<li><p>하지만 각 layer의 Input을 Unit Gaussian이 되도록 강제하는 BatchNormalization을 사용하여 이 문제를 해결합니다.</p>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-4-Noise-Process\"><a href=\"#3-4-Noise-Process\" class=\"headerlink\" title=\"3.4 Noise Process\"></a>3.4 Noise Process</h2><p>DDPG 에서는 Exploration을 위해서 output으로 나온 행동에 노이즈를 추가해줍니다.</p>\n<p>ORNSTEIN UHLENBECK PROCESS(OU)</p>\n<p>$$dx_t = \\theta (\\mu - x_t) dt + \\sigma dW_t$$</p>\n<ul>\n<li>OU Process는 평균으로 회귀하는 random process입니다.</li>\n<li>$\\theta$는 얼마나 빨리 평균으로 회귀할 지를 나타내는 파라미터이며 $\\mu$는 평균을 의미합니다.</li>\n<li>$\\sigma$는 process의 변동성을 의미하며 $W_t$는 Wiener process를 의미합니다.</li>\n<li>따라서 이전의 noise들과 temporally correlated입니다.</li>\n<li>위와 같은 temporally correlated noise process를 사용하는 이유는 physical control과 같은 관성이 있는 환경에서 학습 시킬 때 보다 효과적이기 때문입니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-5-Diagram-amp-Pseudocode\"><a href=\"#3-5-Diagram-amp-Pseudocode\" class=\"headerlink\" title=\"3.5 Diagram &amp; Pseudocode\"></a>3.5 Diagram &amp; Pseudocode</h2><ul>\n<li>DDPG의 학습 과정을 간단히 도식화 해본 다이어그램입니다.</li>\n</ul>\n<p><img src=\"https://www.dropbox.com/s/0ffb2c9irctjx2n/Screenshot%202018-06-23%2017.58.57.png?dl=1\"> </p>\n<ul>\n<li>DDPG의 알고리즘 수도코드입니다. 모든 항목을 위에서 설명했으니 순서대로 보시면 이해에 도움이 될것입니다.</li>\n</ul>\n<center> <img src=\"https://www.dropbox.com/s/fd0nj7goixfnd6z/Screenshot%202018-06-23%2018.02.13.png?dl=1\" width=\"700px\"> </center>\n\n<p><br><br></p>\n<h1 id=\"4-Results\"><a href=\"#4-Results\" class=\"headerlink\" title=\"4. Results\"></a>4. Results</h1><p><br></p>\n<h2 id=\"4-1-Variants-of-DPG\"><a href=\"#4-1-Variants-of-DPG\" class=\"headerlink\" title=\"4.1 Variants of DPG\"></a>4.1 Variants of DPG</h2><center> <img src=\"https://www.dropbox.com/s/k8q6tih85lgow5l/Screenshot%202018-06-23%2018.08.05.png?dl=1\" width=\"800px\"> </center>\n\n<ul>\n<li>original DPG에 batchnorm만 추가(연한 회색), target network만 추가(진한 회색), 둘 다 추가(초록), pixel로만 학습(파랑). Target network가 성능을 가장 좌지우지합니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"4-2-Q-estimation-of-DDPG\"><a href=\"#4-2-Q-estimation-of-DDPG\" class=\"headerlink\" title=\"4.2 Q estimation of DDPG\"></a>4.2 Q estimation of DDPG</h2><center> <img src=\"https://www.dropbox.com/s/vvcfoni0lqrisst/Screenshot%202018-06-23%2018.10.23.png?dl=1\" width=\"650px\"> </center>\n\n<ul>\n<li>DQN은 Q value를 Over-estimate하는 경향이 있었지만, DDPG는 simple task에 대해서는 잘한다. 복잡한 문제에 대해서는 estimation을 잘 못했지만, 여전히 좋은 Policy를 찾았습니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"4-3-Performance-Comparison\"><a href=\"#4-3-Performance-Comparison\" class=\"headerlink\" title=\"4.3 Performance Comparison\"></a>4.3 Performance Comparison</h2><center> <img src=\"https://www.dropbox.com/s/u8ibmz9q4kfxh6l/Screenshot%202018-06-23%2018.11.40.png?dl=1\" width=\"800px\"> </center>\n\n<ul>\n<li>Score는 naive policy를 0, ILQG (planning algorithm)의 mean score를 1점으로 놓았을 때의 점수 Torcs 환경에 대해서만 raw reward를 score로 사용합니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"5-Implementation-Details\"><a href=\"#5-Implementation-Details\" class=\"headerlink\" title=\"5. Implementation Details\"></a>5. Implementation Details</h1><p><br></p>\n<h2 id=\"5-1-Hyper-parameters\"><a href=\"#5-1-Hyper-parameters\" class=\"headerlink\" title=\"5.1 Hyper parameters\"></a>5.1 Hyper parameters</h2><ul>\n<li>Optimizer : Adam<ul>\n<li>actor lr : 0.0001, critic lr : 0.001</li>\n</ul>\n</li>\n<li>Weight decay(L2) for critic(Q) : 0.001</li>\n<li>Discount factor : $\\gamma = 0.99 $</li>\n<li>Soft target updates : $\\tau = 0.001 $</li>\n<li>Size of replay buffer : 1,000,000</li>\n<li>Orstein Uhlenbeck Process : $\\theta = 0.15$, $\\sigma = 0.2$</li>\n</ul>\n<p><br></p>\n<h2 id=\"5-2-Etc\"><a href=\"#5-2-Etc\" class=\"headerlink\" title=\"5.2 Etc.\"></a>5.2 Etc.</h2><ul>\n<li>Final output layer of actor : tanh (행동의 최소 최대를 맞춰주기 위해서)</li>\n<li>low-dimentional 문제에서 네트워크는 2개의 hidden layer (1st layer 400 units, 2nd layer 300 units)를 가집니다.</li>\n<li>이미지를 통해서 학습시킬 때 : 3 convolutional layers (no pooling) with 32 filters at each layer.</li>\n<li>actor와 critic 각각의 final layer(weight, bias 모두)는 다음 범위의 uniform distribution에서 샘플링합니다. [ - 0.003, 0.003], [ - 0.0003 , 0.0003]. 이렇게 하는 이유는 가장 처음의 policy와 value의 output이 0에 가깝게 나오도록 하기 위합니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"6-Conclusion\"><a href=\"#6-Conclusion\" class=\"headerlink\" title=\"6.Conclusion\"></a>6.Conclusion</h1><ul>\n<li>이 연구는 최근 딥러닝의 발전과 강화학습을 엮은 것으로 Continuous action space를 가지는 문제를 robust하게 풀어냅니다.</li>\n<li>non-linear function approximators을 쓰는 것은 수렴을 보장하지 않지만, 여러 환경에 대해서 특별한 조작 없이 안정적으로 수렴하는 것을 실험으로 보여냅니다.</li>\n<li>Atari 도메인에서 DQN보다 상당히 적은 step만에 수렴하는 것을 실험을 통해서 알아냅니다.</li>\n<li>model-free 알고리즘은 좋은 solution을 찾기 위해서는 많은 sample을 필요로 한다는 한계가 있지만 더 큰 시스템에서는 이러한 한계를 물리칠 정도로 중요한 역할을 하게 될 것이라고 합니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"처음으로\"><a href=\"#처음으로\" class=\"headerlink\" title=\"처음으로\"></a>처음으로</h1><h2 id=\"PG-Travel-Guide\"><a href=\"#PG-Travel-Guide\" class=\"headerlink\" title=\"PG Travel Guide\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/\">PG Travel Guide</a></h2><p><br></p>\n<h1 id=\"이전으로\"><a href=\"#이전으로\" class=\"headerlink\" title=\"이전으로\"></a>이전으로</h1><h2 id=\"DPG-여행하기\"><a href=\"#DPG-여행하기\" class=\"headerlink\" title=\"DPG 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/27/2_dpg/\">DPG 여행하기</a></h2><p><br></p>\n<h1 id=\"다음으로\"><a href=\"#다음으로\" class=\"headerlink\" title=\"다음으로\"></a>다음으로</h1><h2 id=\"NPG-여행하기\"><a href=\"#NPG-여행하기\" class=\"headerlink\" title=\"NPG 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/25/4_npg/\">NPG 여행하기</a></h2>"},{"title":"PG Travel Guide","date":"2018-06-28T16:11:26.000Z","author":"김동민, 이동민, 차금강","subtitle":"피지여행에 관한 개략적 기록","_content":"\n---\n\n# 0. Policy Gradient의 세계로\n\n반갑습니다! 저희는 PG여행을 위해 모인 PG탐험대입니다. 강화학습하면 보통 Q-learning을 떠올립니다. 그렇지만 오래전부터 Policy Gradient라는 Q-learning 못지 않은 훌륭한 방법론이 연구되어 왔고, 최근에는 강화학습의 최정점의 기술로 자리매김하고 있습니다. 강화학습의 아버지인 Sutton의 논문을 필두로 하여 기존의 DQN보다 뛰어난 성능을 내는 DPG와 DDPG, 그리고 현재 가장 주목받는 강화학습 연구자인 John Schulmann의 TRPO, GAE, PPO와 이를 이해하기 위해 필요한 Natural Policy Gardient까지 더불어 살펴보고자 합니다.\n\n<center> <img src=\"https://www.dropbox.com/s/tbcyhvilaqy4ra0/Policy%20Optimization%20in%20the%20RL%20Algorithm%20Landscape.png?dl=1\" width=\"800\"> </center>\n\n위의 그림은 강화학습 알고리즘 landscape에서 Policy Optimization의 관점을 중점적으로 하여 나타낸 그림입니다. 위의 그림에서 빨간색 작은 숫자로 나타낸 것이 저희 PG여행에서 다룰 논문들입니다. 순서는 다음과 같습니다.\n\n1. [Sutton_PG](http://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf)\n2. [DPG](http://proceedings.mlr.press/v32/silver14.pdf)\n3. [DDPG](https://arxiv.org/pdf/1509.02971.pdf)\n4. [NPG](https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf)\n5. [TRPO](https://arxiv.org/pdf/1502.05477.pdf)\n6. [GAE](https://arxiv.org/pdf/1506.02438.pdf)\n7. [PPO](https://arxiv.org/pdf/1707.06347.pdf)\n\n위와 같이 총 7가지 논문들을 리뷰하여 블로그로 정리하였습니다. 각 순서에 맞춰 보시는 것을 권장해드립니다.\n\n이 블로그에는 각각의 기술을 제안한 논문을 PG탐험대분들이 자세하게 리뷰한 포스트들이 있습니다. 우리나라에서 PG에 대해서 이렇게 자세하게 리뷰한 포스트들은 없었다고 감히 말씀드리고 싶습니다. 본 글에서는 이 포스트들을 읽기 전에 전체 내용을 개략적으로 소개하고 각각의 포스트들로 안내하고자 합니다. 자, 저희와 함께 PG여행을 즐겨보시겠습니까?\n\n<br><br>\n\n# 1. \\[Sutton PG\\] Policy gradient methods for reinforcement learning with function approximation\n\n[Sutton PG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/28/1_sutton-pg/)\n\npolicy gradient (PG)는 expected reward를 policy의 파라미터에 대한 함수로 모델링하고 이 reward를 최대화하는 policy를 gradient ascent 기법을 이용해서 찾는 기법입니다. 강화학습의 대표격이라고 할 수 있는 Q-learning이라는 훌륭한 방법론이 이미 존재하고 있었지만 Q값의 작은 변화에도 policy가 크게 변할 수도 있다는 단점이 있기 때문에 policy의 점진적인 변화를 통해 더 나은 policy를 찾아가는 PG기법이 개발되었습니다.\n\n이 PG기법은 먼저 개발되었던 [REINFORCE](https://link.springer.com/content/pdf/10.1007/BF00992696.pdf)라는 기법과 관련이 아주 많습니다. 서튼의 PG기법은 REINFORCE 기법을 [actor-critic algorithm](http://papers.nips.cc/paper/1786-actor-critic-algorithms.pdf)을 사용하여 개선시킨 것이라고 볼 수도 있습니다. 저희 PG여행 팀도 처음에는 REINFORCE를 출발지로 삼으려고 했었지만 예전 논문이다보니 논문의 가독성이 너무 떨어져서 강화학습의 아버지라고 할 수 있는 서튼의 논문을 출발지로 삼았습니다. 하지만 이 논문도 만만치 않았습니다. 이 논문을 읽으시려는 분들께 저희의 여행기가 도움이 될 것입니다. PG기법에 대해서 먼저 감을 잡고 시작하시려면 [Andre Karpathy의 PG에 대한 블로그](http://karpathy.github.io/2016/05/31/rl/)를 먼저 한 번 읽어보세요.  한글번역도 있습니다! 1) http://keunwoochoi.blogspot.com/2016/06/andrej-karpathy.html 2) https://tensorflow.blog/2016/07/13/reinforce-pong-at-gym/\n\n[Sutton PG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/28/1_sutton-pg/)\n\n<br><br>\n\n# 2. \\[DPG\\] Deterministic policy gradient algorithms\n\n[DPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/27/2_dpg/)\n\ndeterministic policy gradeint (DPG)는 어찌보면 상당히 도전적인 아이디어였던 것 같습니다. Sutton PG 논문에서는 DPG 스타일의 기법이 가진 단점에 대해서 언급하면서 stochastic policy gradient (SPG)를 써야 optimal을 찾을 수 있다고 기술하고 있었기 때문입니다.\n\n그런데 이 논문에서 높은 차원의 action space를 가지는 문제들에(예를 들면 문어발 제어) 대해서는 DPG가 상당히 좋은 성능을 내는 것을 저자들이 보였습니다. 그리고 DPG는 SPG와 대척점에 있는 기술이 아니고 SPG의 special case 중 하나임을 증명하면서 SPG를 가정하고 만들어진 기술들을 DPG에서도 그대로 이용할 수 있음을 보였습니다. David Silver의 [동영상 강의](http://techtalks.tv/talks/deterministic-policy-gradient-algorithms/61098/)를 한 번 보시길 추천드립니다. 짧은 강의지만 랩을 하듯이 쉴새없이 설명하는 Silver의 모습에서 천재성이 엿보이는 것을 확인하실 수 있습니다. \n\n[DPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/27/2_dpg/)\n\n<br><br>\n\n# 3. \\[DDPG\\] Continuous control with deep reinforcement learning\n\n[DDPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/26/3_ddpg/)\n\nDPG의 후속 연구로 DPG보다 더 큰 주목을 받은 논문입니다. 소위 말하는 deep deterministic policy gradient (DDPG)로 불리는 기술을 제안한 논문입니다. 이 논문의 저자 중 일부는 그 유명한 DQN 논문의 저자이기도 합니다. Q-learning과 deep neural network를 접목시켰던 DQN처럼 이 논문도 DPG와 deep neural network를 접목시킨 논문입니다.\n\n이 논문은 DQN으로는 좋은 성능을 내지 못했던 continuous action을 가지는 상황들에 대해서 상당히 훌륭한 결과를 보이면서 큰 주목을 받았습니다. 소위 말하는 deep reinforcement learning (DRL)에서 Q-learning 계열의 DQN, PG 계열의 DDPG로 양대산맥을 이루는 논문이라고 할 수 있습니다. 두 논문 모두 Deepmind에서 나왔다는 것은 Deepmind 기술력이 DRL 분야에서 최정점에 있음을 보여주는 상징이 아닌가 싶습니다. 논문 자체는 그리 어렵지 않습니다. 새로운 아이디어를 제시했다기보다는 딥러닝을 활용한 강화학습의 가능성을 보여주는 논문이라는 점에서 큰 의의를 가지는 것 같습니다. 여러분도 한번 코딩에 도전해보시는게 어떨까요?\n\n[DDPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/26/3_ddpg/)\n\n<br><br>\n\n# 4. \\[NPG\\] A natural policy gradient\n\n[NPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/25/4_npg/)\n[NPG Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/tnpg.py)\n\n이 논문은 뒤이어 나오는 TRPO를 더 잘 이해하기 위해서 보는 논문입니다. 이번 논문부터 내용이 상당히 어려워집니다. 다소 생소한 수학 개념들이 많이 나오기 때문입니다. 하지만 이 블로그를 보시면 많은 부분들이 채워질 것이라고 믿습니다.\n\n2002년 당시에도 많은 연구자들이 objective function의 gradient 값을 따라서 좋은 policy $\\pi$를 찾고자 하였습니다. 하지만 기존의 우리가 알던 gradient descent method는 steepest descent direction이 아닐 수 있기 때문에(쉽게 말해 가장 가파른 방향을 따라서 내려가야 하는데 그러지 못할 수도 있다는 것입니다.) 이 논문에서 steepest descent direction를 나타내는 natural gradient method를 policy gradient에 적용하여 좋은 policy $\\pi$를 찾습니다. gradient descent는 parameter를 한 번에 많이 update 할 수 없는 반면, natural gradient는 가장 좋은 action을 고르도록 학습이 됩니다.\n\n또한 natural gradient method는 Line search 기법과 함께 사용하면 더 Policy Iteration과 비슷해집니다. Greedy Policy Iteration에서와 비교하면 Performance Improvement가 보장됩니다. 하지만 Fisher Information Matrix(FIM)이 asymtotically Hessian으로 수렴하지 않습니다. 그렇기 때문에 Conjugate Gradient Method로 구하는 방법이 더 좋을수 있습니다.\n\n위의 요약한 문장들만 봤을 때는 생소한 용어들이 많이 나와서 무슨 말인지 감이 안잡히실 수 있습니다. 저희가 포스팅한 블로그 글에는 다음과 같은 추가적인 내용이 나옵니다. 반드시 알고 가야 TRPO를 이해하는 것은 아닙니다. 다만 NPG를 이해하면 할 수록 TRPO를 접하기가 더 쉬울 수 있습니다.\n\n- Euclidean space와 Riemannian space의 차이\n- Natural Gradient 증명\n- Fisher Information Matrix(FIM)\n- Line Search\n- FIM과 Hessian 방법의 차이\n- Conjugate Gradient Method\n\n아래의 NPG Code는 Hessian 방법이 아닌 Conjugate Gradient Method를 사용한 \"Truncated Natural Policy Gradient(TNPG)\"라고 하는 방법의 코드입니다.\n\n마지막으로 프로젝트 내에 있는 한 팀원의 문장을 인용하겠습니다. \"머리가 아프고 힘들수록 잘하고 있는겁니다.\" NPG 논문을 보시는 분들 화이팅입니다!\n\n[NPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/25/4_npg/)\n[NPG Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/tnpg.py)\n\n<br><br>\n\n# 5. \\[TRPO\\] Trust region policy optimization\n\n[TRPO 여행하기](https://reinforcement-learning-kr.github.io/2018/06/24/5_trpo/)\n[TRPO Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/trpo_gae.py)\n\nPG기법이 각광을 받게 된 계기는 아마도 TRPO 때문이 아닌가 싶습니다. DQN이라는 인공지능이 아타리게임을 사람보다 잘 플레이했던 영상을 보고 충격을 받았던 사람들이 많았던 것처럼 TRPO가 난제로 여겨졌던 서기, 걷기, 뛰기 같은 로봇에 적용할 수 있는 continuous action을 잘 수행하는 것을 보고 많은 사람들이 놀라워했습니다. (솔직히 처음 아타리게임을 봤을 때 만큼은 아니지만...) 이것들은 DQN으로는 달성할 수 없었던 난제였습니다. \n\n그렇지만 거인의 어깨 위에서 세상을 바라보았던 많은 과학자들이 그랬던 것처럼 Schulmann도 TRPO를 갑자기 생각낸 것은 아닙니다. policy를 업데이트할 때 기존의 policy와 너무 많이 달라지면 문제가 생깁니다. 일단, state distribution이 변합니다. state distribution은 간단히 얘기하면 각각의 state를 방문하는 빈도를 의미한다고 할 수 있습니다. policy가 많이 바뀌면 기존의 state distribution을 그대로 이용할 수 없고 이렇게 되면 policy gradient를 구하기 어려워집니다. \n\n그래서 착안한 것이 policy의 변화 정도를 제한하는 것입니다. 여기서 우리는 stochastic policy, 즉, 확률적 policy를 이용한다고 전제하고 있기 때문에, policy의 변화는 policy 확률 분포의 변화를 의미합니다. 현재 policy와 새로운 policy 사이의 확률 분포의 변화를 어떻게 측정할 수 있을까요? Schulmann은 [KL divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)라는 척도를 이용하였습니다. 이 변화량을 특정값 이하로 제한하고 이것을 만족하는 영역을 trust region이라고 불렀습니다. \n\n그렇지만 문제는 풀기에는 복잡했습니다. 이것을 풀기 위해서 여러 번의 근사화를 적용시켰습니다. objective function에는 linear approximation을 적용하고 trust region constraint에는 second-order approximation을 적용하면 문제가 좀 더 풀기 쉬운 형태로 바뀐다는 것을 알아냈습니다. 이것은 사실 natural gradient를 이용하는 것이었습니다. 이러한 이유로 TRPO를 이해하기 위해서 natural gradient도 살펴보았던 것입니다. \n\nSchulmann이 너무 똑똑해서 일까요? 훌륭한 아이디어로 policy gradient 기법의 르네상스를 열은 Schulmann의 역작인 TRPO 논문은 이해하기 쉽게 쓰여지지 않은 것 같습니다. (더 잘 쓸 수 있었잖아 Schulmann...) 저희의 포스트와 함께 보다 편하게 여행하시길 바랍니다. 이 [유투브 영상](https://youtu.be/CKaN5PgkSBc)도 무조건 보세요~\n\n[TRPO 여행하기](https://reinforcement-learning-kr.github.io/2018/06/24/5_trpo/)\n[TRPO Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/trpo_gae.py)\n\n<br><br>\n\n# 6. \\[GAE\\] High-Dimensional Continuous Control Using Generalized Advantage Estimation\n\n[GAE 여행하기](https://reinforcement-learning-kr.github.io/2018/06/23/6_gae/)\n\nTRPO가 나오고 난 뒤로도 복잡하고 어려운 control problem에서 Reinforcement Learning (RL)은 high sample complexity 때문에 제한이 되어왔습니다. 따라서 이 논문에서 그 제한을 풀고자 advantage function의 good estimate를 얻는 \"variance reduction\"에 대해 연구하였습니다.\n\n\"Generalized Advantage Estimator (GAE)\"라는 것을 제안했고, 이것은 bias-variance tradeoff를 조절하는 두 개의 parameter $\\gamma,\\lambda$를 가집니다.\n또한 어떻게 Trust Region Policy Optimization과 value function을 optimize하는 Trust Region Algorithm의 idea를 합치는지를 보였습니다.\n\n이렇게 함으로써 보다 더 복잡하고 어려운 control task들을 해결할 수 있었습니다.\n\nGAE의 실험적인 입증으로는 robotic locomotion을 simulation하는 domain입니다. 실험에서도 보여준 것처럼 [0.9, 0.99]의 범위에서 $\\lambda$의 적절한 중간의 값을 통해 best performance를 얻습니다. 좀 더 나아가 연구되어야할 점은 adaptive or automatic하도록 estimator parameter $\\gamma,\\lambda$를 조절하는 방법입니다.\n\n추가적으로 앞으로 연구되어야할 부분은 만약 value function estimation error와 policy gradient estimation error 사이의 관계를 알아낸다면, value function fitting에 더 잘 맞는 error metric(policy gradient estimation의 정확성과 더 잘 맞는 value function)을 사용할 수 있을 것입니다. 여기서 policy와 value function의 파라미터를 공유하는 모델을 만드는 것은 아주 흥미롭고 이점이 많습니다. 하지만 수렴을 보장하도록 적절한 numerical optimization을 제시해야 할 것입니다.\n\n[GAE 여행하기](https://reinforcement-learning-kr.github.io/2018/06/23/6_gae/)\n\n<br><br>\n\n# 7. \\[PPO\\] Proximal policy optimization algorithms\n\n[PPO 여행하기](https://reinforcement-learning-kr.github.io/2018/06/22/7_ppo/)\n[PPO Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/ppo_gae.py)\n\n이 논문에서는 Reinforcement Learning에서 Policy Gradient Method의 새로운 방법인 PPO를 제안합니다. 이 방법은 agent가 환경과의 상호작용을 통해 data를 sampling하는 것과 stochastic gradient ascent를 사용하여 \"surrogate\" objective function을 optimizing하는 것을 번갈아가면서 하는 방법입니다. data sample마다 one gradient update를 수행하는 기존의 방법과는 달리, minibatch update의 multiple epochs를 가능하게 하는 새로운 objective function을 말합니다.\n\n또한 PPO는 TRPO의 연장선상에 있는 알고리즘이라고 할 수 있습니다. TRPO에서는 문제를 단순하게 만들기 위해서 최적화 문제를 여러 번 변형시켰지만, PPO는 단순하게 clip이라는 개념을 사용합니다. TRPO에서 이용했던 surrogate objective function을 reward가 특정값 이상이거나 이하가 될 때 더 이상 변화시키지 않는 것입니다.\n\n이 알고리즘의 장점으로는\n\n- TRPO의 장점만을 가집니다. 다시 말해 알고리즘으로 학습하기에 훨씬 더 간단하고, 더 일반적이고, 더 좋은 sample complexity (empirically)를 가집니다.\n- 또한 다른 online policy gradient method들을 능가했고, 전반적으로 sample complexity(특히 computational complexity), simplicity, wall-time가 좋다고 합니다.\n\n[PPO 여행하기](https://reinforcement-learning-kr.github.io/2018/06/22/7_ppo/)\n[PPO Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/ppo_gae.py)\n","source":"_posts/0_pg-travel-guide.md","raw":"---\ntitle: PG Travel Guide\ndate: 2018-06-29 01:11:26\ntags: [\"프로젝트\", \"피지여행\"]\ncategories: 프로젝트\nauthor: 김동민, 이동민, 차금강\nsubtitle: 피지여행에 관한 개략적 기록\n---\n\n---\n\n# 0. Policy Gradient의 세계로\n\n반갑습니다! 저희는 PG여행을 위해 모인 PG탐험대입니다. 강화학습하면 보통 Q-learning을 떠올립니다. 그렇지만 오래전부터 Policy Gradient라는 Q-learning 못지 않은 훌륭한 방법론이 연구되어 왔고, 최근에는 강화학습의 최정점의 기술로 자리매김하고 있습니다. 강화학습의 아버지인 Sutton의 논문을 필두로 하여 기존의 DQN보다 뛰어난 성능을 내는 DPG와 DDPG, 그리고 현재 가장 주목받는 강화학습 연구자인 John Schulmann의 TRPO, GAE, PPO와 이를 이해하기 위해 필요한 Natural Policy Gardient까지 더불어 살펴보고자 합니다.\n\n<center> <img src=\"https://www.dropbox.com/s/tbcyhvilaqy4ra0/Policy%20Optimization%20in%20the%20RL%20Algorithm%20Landscape.png?dl=1\" width=\"800\"> </center>\n\n위의 그림은 강화학습 알고리즘 landscape에서 Policy Optimization의 관점을 중점적으로 하여 나타낸 그림입니다. 위의 그림에서 빨간색 작은 숫자로 나타낸 것이 저희 PG여행에서 다룰 논문들입니다. 순서는 다음과 같습니다.\n\n1. [Sutton_PG](http://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf)\n2. [DPG](http://proceedings.mlr.press/v32/silver14.pdf)\n3. [DDPG](https://arxiv.org/pdf/1509.02971.pdf)\n4. [NPG](https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf)\n5. [TRPO](https://arxiv.org/pdf/1502.05477.pdf)\n6. [GAE](https://arxiv.org/pdf/1506.02438.pdf)\n7. [PPO](https://arxiv.org/pdf/1707.06347.pdf)\n\n위와 같이 총 7가지 논문들을 리뷰하여 블로그로 정리하였습니다. 각 순서에 맞춰 보시는 것을 권장해드립니다.\n\n이 블로그에는 각각의 기술을 제안한 논문을 PG탐험대분들이 자세하게 리뷰한 포스트들이 있습니다. 우리나라에서 PG에 대해서 이렇게 자세하게 리뷰한 포스트들은 없었다고 감히 말씀드리고 싶습니다. 본 글에서는 이 포스트들을 읽기 전에 전체 내용을 개략적으로 소개하고 각각의 포스트들로 안내하고자 합니다. 자, 저희와 함께 PG여행을 즐겨보시겠습니까?\n\n<br><br>\n\n# 1. \\[Sutton PG\\] Policy gradient methods for reinforcement learning with function approximation\n\n[Sutton PG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/28/1_sutton-pg/)\n\npolicy gradient (PG)는 expected reward를 policy의 파라미터에 대한 함수로 모델링하고 이 reward를 최대화하는 policy를 gradient ascent 기법을 이용해서 찾는 기법입니다. 강화학습의 대표격이라고 할 수 있는 Q-learning이라는 훌륭한 방법론이 이미 존재하고 있었지만 Q값의 작은 변화에도 policy가 크게 변할 수도 있다는 단점이 있기 때문에 policy의 점진적인 변화를 통해 더 나은 policy를 찾아가는 PG기법이 개발되었습니다.\n\n이 PG기법은 먼저 개발되었던 [REINFORCE](https://link.springer.com/content/pdf/10.1007/BF00992696.pdf)라는 기법과 관련이 아주 많습니다. 서튼의 PG기법은 REINFORCE 기법을 [actor-critic algorithm](http://papers.nips.cc/paper/1786-actor-critic-algorithms.pdf)을 사용하여 개선시킨 것이라고 볼 수도 있습니다. 저희 PG여행 팀도 처음에는 REINFORCE를 출발지로 삼으려고 했었지만 예전 논문이다보니 논문의 가독성이 너무 떨어져서 강화학습의 아버지라고 할 수 있는 서튼의 논문을 출발지로 삼았습니다. 하지만 이 논문도 만만치 않았습니다. 이 논문을 읽으시려는 분들께 저희의 여행기가 도움이 될 것입니다. PG기법에 대해서 먼저 감을 잡고 시작하시려면 [Andre Karpathy의 PG에 대한 블로그](http://karpathy.github.io/2016/05/31/rl/)를 먼저 한 번 읽어보세요.  한글번역도 있습니다! 1) http://keunwoochoi.blogspot.com/2016/06/andrej-karpathy.html 2) https://tensorflow.blog/2016/07/13/reinforce-pong-at-gym/\n\n[Sutton PG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/28/1_sutton-pg/)\n\n<br><br>\n\n# 2. \\[DPG\\] Deterministic policy gradient algorithms\n\n[DPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/27/2_dpg/)\n\ndeterministic policy gradeint (DPG)는 어찌보면 상당히 도전적인 아이디어였던 것 같습니다. Sutton PG 논문에서는 DPG 스타일의 기법이 가진 단점에 대해서 언급하면서 stochastic policy gradient (SPG)를 써야 optimal을 찾을 수 있다고 기술하고 있었기 때문입니다.\n\n그런데 이 논문에서 높은 차원의 action space를 가지는 문제들에(예를 들면 문어발 제어) 대해서는 DPG가 상당히 좋은 성능을 내는 것을 저자들이 보였습니다. 그리고 DPG는 SPG와 대척점에 있는 기술이 아니고 SPG의 special case 중 하나임을 증명하면서 SPG를 가정하고 만들어진 기술들을 DPG에서도 그대로 이용할 수 있음을 보였습니다. David Silver의 [동영상 강의](http://techtalks.tv/talks/deterministic-policy-gradient-algorithms/61098/)를 한 번 보시길 추천드립니다. 짧은 강의지만 랩을 하듯이 쉴새없이 설명하는 Silver의 모습에서 천재성이 엿보이는 것을 확인하실 수 있습니다. \n\n[DPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/27/2_dpg/)\n\n<br><br>\n\n# 3. \\[DDPG\\] Continuous control with deep reinforcement learning\n\n[DDPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/26/3_ddpg/)\n\nDPG의 후속 연구로 DPG보다 더 큰 주목을 받은 논문입니다. 소위 말하는 deep deterministic policy gradient (DDPG)로 불리는 기술을 제안한 논문입니다. 이 논문의 저자 중 일부는 그 유명한 DQN 논문의 저자이기도 합니다. Q-learning과 deep neural network를 접목시켰던 DQN처럼 이 논문도 DPG와 deep neural network를 접목시킨 논문입니다.\n\n이 논문은 DQN으로는 좋은 성능을 내지 못했던 continuous action을 가지는 상황들에 대해서 상당히 훌륭한 결과를 보이면서 큰 주목을 받았습니다. 소위 말하는 deep reinforcement learning (DRL)에서 Q-learning 계열의 DQN, PG 계열의 DDPG로 양대산맥을 이루는 논문이라고 할 수 있습니다. 두 논문 모두 Deepmind에서 나왔다는 것은 Deepmind 기술력이 DRL 분야에서 최정점에 있음을 보여주는 상징이 아닌가 싶습니다. 논문 자체는 그리 어렵지 않습니다. 새로운 아이디어를 제시했다기보다는 딥러닝을 활용한 강화학습의 가능성을 보여주는 논문이라는 점에서 큰 의의를 가지는 것 같습니다. 여러분도 한번 코딩에 도전해보시는게 어떨까요?\n\n[DDPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/26/3_ddpg/)\n\n<br><br>\n\n# 4. \\[NPG\\] A natural policy gradient\n\n[NPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/25/4_npg/)\n[NPG Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/tnpg.py)\n\n이 논문은 뒤이어 나오는 TRPO를 더 잘 이해하기 위해서 보는 논문입니다. 이번 논문부터 내용이 상당히 어려워집니다. 다소 생소한 수학 개념들이 많이 나오기 때문입니다. 하지만 이 블로그를 보시면 많은 부분들이 채워질 것이라고 믿습니다.\n\n2002년 당시에도 많은 연구자들이 objective function의 gradient 값을 따라서 좋은 policy $\\pi$를 찾고자 하였습니다. 하지만 기존의 우리가 알던 gradient descent method는 steepest descent direction이 아닐 수 있기 때문에(쉽게 말해 가장 가파른 방향을 따라서 내려가야 하는데 그러지 못할 수도 있다는 것입니다.) 이 논문에서 steepest descent direction를 나타내는 natural gradient method를 policy gradient에 적용하여 좋은 policy $\\pi$를 찾습니다. gradient descent는 parameter를 한 번에 많이 update 할 수 없는 반면, natural gradient는 가장 좋은 action을 고르도록 학습이 됩니다.\n\n또한 natural gradient method는 Line search 기법과 함께 사용하면 더 Policy Iteration과 비슷해집니다. Greedy Policy Iteration에서와 비교하면 Performance Improvement가 보장됩니다. 하지만 Fisher Information Matrix(FIM)이 asymtotically Hessian으로 수렴하지 않습니다. 그렇기 때문에 Conjugate Gradient Method로 구하는 방법이 더 좋을수 있습니다.\n\n위의 요약한 문장들만 봤을 때는 생소한 용어들이 많이 나와서 무슨 말인지 감이 안잡히실 수 있습니다. 저희가 포스팅한 블로그 글에는 다음과 같은 추가적인 내용이 나옵니다. 반드시 알고 가야 TRPO를 이해하는 것은 아닙니다. 다만 NPG를 이해하면 할 수록 TRPO를 접하기가 더 쉬울 수 있습니다.\n\n- Euclidean space와 Riemannian space의 차이\n- Natural Gradient 증명\n- Fisher Information Matrix(FIM)\n- Line Search\n- FIM과 Hessian 방법의 차이\n- Conjugate Gradient Method\n\n아래의 NPG Code는 Hessian 방법이 아닌 Conjugate Gradient Method를 사용한 \"Truncated Natural Policy Gradient(TNPG)\"라고 하는 방법의 코드입니다.\n\n마지막으로 프로젝트 내에 있는 한 팀원의 문장을 인용하겠습니다. \"머리가 아프고 힘들수록 잘하고 있는겁니다.\" NPG 논문을 보시는 분들 화이팅입니다!\n\n[NPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/25/4_npg/)\n[NPG Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/tnpg.py)\n\n<br><br>\n\n# 5. \\[TRPO\\] Trust region policy optimization\n\n[TRPO 여행하기](https://reinforcement-learning-kr.github.io/2018/06/24/5_trpo/)\n[TRPO Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/trpo_gae.py)\n\nPG기법이 각광을 받게 된 계기는 아마도 TRPO 때문이 아닌가 싶습니다. DQN이라는 인공지능이 아타리게임을 사람보다 잘 플레이했던 영상을 보고 충격을 받았던 사람들이 많았던 것처럼 TRPO가 난제로 여겨졌던 서기, 걷기, 뛰기 같은 로봇에 적용할 수 있는 continuous action을 잘 수행하는 것을 보고 많은 사람들이 놀라워했습니다. (솔직히 처음 아타리게임을 봤을 때 만큼은 아니지만...) 이것들은 DQN으로는 달성할 수 없었던 난제였습니다. \n\n그렇지만 거인의 어깨 위에서 세상을 바라보았던 많은 과학자들이 그랬던 것처럼 Schulmann도 TRPO를 갑자기 생각낸 것은 아닙니다. policy를 업데이트할 때 기존의 policy와 너무 많이 달라지면 문제가 생깁니다. 일단, state distribution이 변합니다. state distribution은 간단히 얘기하면 각각의 state를 방문하는 빈도를 의미한다고 할 수 있습니다. policy가 많이 바뀌면 기존의 state distribution을 그대로 이용할 수 없고 이렇게 되면 policy gradient를 구하기 어려워집니다. \n\n그래서 착안한 것이 policy의 변화 정도를 제한하는 것입니다. 여기서 우리는 stochastic policy, 즉, 확률적 policy를 이용한다고 전제하고 있기 때문에, policy의 변화는 policy 확률 분포의 변화를 의미합니다. 현재 policy와 새로운 policy 사이의 확률 분포의 변화를 어떻게 측정할 수 있을까요? Schulmann은 [KL divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)라는 척도를 이용하였습니다. 이 변화량을 특정값 이하로 제한하고 이것을 만족하는 영역을 trust region이라고 불렀습니다. \n\n그렇지만 문제는 풀기에는 복잡했습니다. 이것을 풀기 위해서 여러 번의 근사화를 적용시켰습니다. objective function에는 linear approximation을 적용하고 trust region constraint에는 second-order approximation을 적용하면 문제가 좀 더 풀기 쉬운 형태로 바뀐다는 것을 알아냈습니다. 이것은 사실 natural gradient를 이용하는 것이었습니다. 이러한 이유로 TRPO를 이해하기 위해서 natural gradient도 살펴보았던 것입니다. \n\nSchulmann이 너무 똑똑해서 일까요? 훌륭한 아이디어로 policy gradient 기법의 르네상스를 열은 Schulmann의 역작인 TRPO 논문은 이해하기 쉽게 쓰여지지 않은 것 같습니다. (더 잘 쓸 수 있었잖아 Schulmann...) 저희의 포스트와 함께 보다 편하게 여행하시길 바랍니다. 이 [유투브 영상](https://youtu.be/CKaN5PgkSBc)도 무조건 보세요~\n\n[TRPO 여행하기](https://reinforcement-learning-kr.github.io/2018/06/24/5_trpo/)\n[TRPO Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/trpo_gae.py)\n\n<br><br>\n\n# 6. \\[GAE\\] High-Dimensional Continuous Control Using Generalized Advantage Estimation\n\n[GAE 여행하기](https://reinforcement-learning-kr.github.io/2018/06/23/6_gae/)\n\nTRPO가 나오고 난 뒤로도 복잡하고 어려운 control problem에서 Reinforcement Learning (RL)은 high sample complexity 때문에 제한이 되어왔습니다. 따라서 이 논문에서 그 제한을 풀고자 advantage function의 good estimate를 얻는 \"variance reduction\"에 대해 연구하였습니다.\n\n\"Generalized Advantage Estimator (GAE)\"라는 것을 제안했고, 이것은 bias-variance tradeoff를 조절하는 두 개의 parameter $\\gamma,\\lambda$를 가집니다.\n또한 어떻게 Trust Region Policy Optimization과 value function을 optimize하는 Trust Region Algorithm의 idea를 합치는지를 보였습니다.\n\n이렇게 함으로써 보다 더 복잡하고 어려운 control task들을 해결할 수 있었습니다.\n\nGAE의 실험적인 입증으로는 robotic locomotion을 simulation하는 domain입니다. 실험에서도 보여준 것처럼 [0.9, 0.99]의 범위에서 $\\lambda$의 적절한 중간의 값을 통해 best performance를 얻습니다. 좀 더 나아가 연구되어야할 점은 adaptive or automatic하도록 estimator parameter $\\gamma,\\lambda$를 조절하는 방법입니다.\n\n추가적으로 앞으로 연구되어야할 부분은 만약 value function estimation error와 policy gradient estimation error 사이의 관계를 알아낸다면, value function fitting에 더 잘 맞는 error metric(policy gradient estimation의 정확성과 더 잘 맞는 value function)을 사용할 수 있을 것입니다. 여기서 policy와 value function의 파라미터를 공유하는 모델을 만드는 것은 아주 흥미롭고 이점이 많습니다. 하지만 수렴을 보장하도록 적절한 numerical optimization을 제시해야 할 것입니다.\n\n[GAE 여행하기](https://reinforcement-learning-kr.github.io/2018/06/23/6_gae/)\n\n<br><br>\n\n# 7. \\[PPO\\] Proximal policy optimization algorithms\n\n[PPO 여행하기](https://reinforcement-learning-kr.github.io/2018/06/22/7_ppo/)\n[PPO Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/ppo_gae.py)\n\n이 논문에서는 Reinforcement Learning에서 Policy Gradient Method의 새로운 방법인 PPO를 제안합니다. 이 방법은 agent가 환경과의 상호작용을 통해 data를 sampling하는 것과 stochastic gradient ascent를 사용하여 \"surrogate\" objective function을 optimizing하는 것을 번갈아가면서 하는 방법입니다. data sample마다 one gradient update를 수행하는 기존의 방법과는 달리, minibatch update의 multiple epochs를 가능하게 하는 새로운 objective function을 말합니다.\n\n또한 PPO는 TRPO의 연장선상에 있는 알고리즘이라고 할 수 있습니다. TRPO에서는 문제를 단순하게 만들기 위해서 최적화 문제를 여러 번 변형시켰지만, PPO는 단순하게 clip이라는 개념을 사용합니다. TRPO에서 이용했던 surrogate objective function을 reward가 특정값 이상이거나 이하가 될 때 더 이상 변화시키지 않는 것입니다.\n\n이 알고리즘의 장점으로는\n\n- TRPO의 장점만을 가집니다. 다시 말해 알고리즘으로 학습하기에 훨씬 더 간단하고, 더 일반적이고, 더 좋은 sample complexity (empirically)를 가집니다.\n- 또한 다른 online policy gradient method들을 능가했고, 전반적으로 sample complexity(특히 computational complexity), simplicity, wall-time가 좋다고 합니다.\n\n[PPO 여행하기](https://reinforcement-learning-kr.github.io/2018/06/22/7_ppo/)\n[PPO Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/ppo_gae.py)\n","slug":"0_pg-travel-guide","published":1,"updated":"2018-09-08T11:00:20.604Z","_id":"cjkj6tyrj0004j7150bs8twv4","comments":1,"layout":"post","photos":[],"link":"","content":"<hr>\n<h1 id=\"0-Policy-Gradient의-세계로\"><a href=\"#0-Policy-Gradient의-세계로\" class=\"headerlink\" title=\"0. Policy Gradient의 세계로\"></a>0. Policy Gradient의 세계로</h1><p>반갑습니다! 저희는 PG여행을 위해 모인 PG탐험대입니다. 강화학습하면 보통 Q-learning을 떠올립니다. 그렇지만 오래전부터 Policy Gradient라는 Q-learning 못지 않은 훌륭한 방법론이 연구되어 왔고, 최근에는 강화학습의 최정점의 기술로 자리매김하고 있습니다. 강화학습의 아버지인 Sutton의 논문을 필두로 하여 기존의 DQN보다 뛰어난 성능을 내는 DPG와 DDPG, 그리고 현재 가장 주목받는 강화학습 연구자인 John Schulmann의 TRPO, GAE, PPO와 이를 이해하기 위해 필요한 Natural Policy Gardient까지 더불어 살펴보고자 합니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/tbcyhvilaqy4ra0/Policy%20Optimization%20in%20the%20RL%20Algorithm%20Landscape.png?dl=1\" width=\"800\"> </center>\n\n<p>위의 그림은 강화학습 알고리즘 landscape에서 Policy Optimization의 관점을 중점적으로 하여 나타낸 그림입니다. 위의 그림에서 빨간색 작은 숫자로 나타낸 것이 저희 PG여행에서 다룰 논문들입니다. 순서는 다음과 같습니다.</p>\n<ol>\n<li><a href=\"http://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf\" target=\"_blank\" rel=\"noopener\">Sutton_PG</a></li>\n<li><a href=\"http://proceedings.mlr.press/v32/silver14.pdf\" target=\"_blank\" rel=\"noopener\">DPG</a></li>\n<li><a href=\"https://arxiv.org/pdf/1509.02971.pdf\" target=\"_blank\" rel=\"noopener\">DDPG</a></li>\n<li><a href=\"https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf\" target=\"_blank\" rel=\"noopener\">NPG</a></li>\n<li><a href=\"https://arxiv.org/pdf/1502.05477.pdf\" target=\"_blank\" rel=\"noopener\">TRPO</a></li>\n<li><a href=\"https://arxiv.org/pdf/1506.02438.pdf\" target=\"_blank\" rel=\"noopener\">GAE</a></li>\n<li><a href=\"https://arxiv.org/pdf/1707.06347.pdf\" target=\"_blank\" rel=\"noopener\">PPO</a></li>\n</ol>\n<p>위와 같이 총 7가지 논문들을 리뷰하여 블로그로 정리하였습니다. 각 순서에 맞춰 보시는 것을 권장해드립니다.</p>\n<p>이 블로그에는 각각의 기술을 제안한 논문을 PG탐험대분들이 자세하게 리뷰한 포스트들이 있습니다. 우리나라에서 PG에 대해서 이렇게 자세하게 리뷰한 포스트들은 없었다고 감히 말씀드리고 싶습니다. 본 글에서는 이 포스트들을 읽기 전에 전체 내용을 개략적으로 소개하고 각각의 포스트들로 안내하고자 합니다. 자, 저희와 함께 PG여행을 즐겨보시겠습니까?</p>\n<p><br><br></p>\n<h1 id=\"1-Sutton-PG-Policy-gradient-methods-for-reinforcement-learning-with-function-approximation\"><a href=\"#1-Sutton-PG-Policy-gradient-methods-for-reinforcement-learning-with-function-approximation\" class=\"headerlink\" title=\"1. [Sutton PG] Policy gradient methods for reinforcement learning with function approximation\"></a>1. [Sutton PG] Policy gradient methods for reinforcement learning with function approximation</h1><p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/28/1_sutton-pg/\">Sutton PG 여행하기</a></p>\n<p>policy gradient (PG)는 expected reward를 policy의 파라미터에 대한 함수로 모델링하고 이 reward를 최대화하는 policy를 gradient ascent 기법을 이용해서 찾는 기법입니다. 강화학습의 대표격이라고 할 수 있는 Q-learning이라는 훌륭한 방법론이 이미 존재하고 있었지만 Q값의 작은 변화에도 policy가 크게 변할 수도 있다는 단점이 있기 때문에 policy의 점진적인 변화를 통해 더 나은 policy를 찾아가는 PG기법이 개발되었습니다.</p>\n<p>이 PG기법은 먼저 개발되었던 <a href=\"https://link.springer.com/content/pdf/10.1007/BF00992696.pdf\" target=\"_blank\" rel=\"noopener\">REINFORCE</a>라는 기법과 관련이 아주 많습니다. 서튼의 PG기법은 REINFORCE 기법을 <a href=\"http://papers.nips.cc/paper/1786-actor-critic-algorithms.pdf\" target=\"_blank\" rel=\"noopener\">actor-critic algorithm</a>을 사용하여 개선시킨 것이라고 볼 수도 있습니다. 저희 PG여행 팀도 처음에는 REINFORCE를 출발지로 삼으려고 했었지만 예전 논문이다보니 논문의 가독성이 너무 떨어져서 강화학습의 아버지라고 할 수 있는 서튼의 논문을 출발지로 삼았습니다. 하지만 이 논문도 만만치 않았습니다. 이 논문을 읽으시려는 분들께 저희의 여행기가 도움이 될 것입니다. PG기법에 대해서 먼저 감을 잡고 시작하시려면 <a href=\"http://karpathy.github.io/2016/05/31/rl/\" target=\"_blank\" rel=\"noopener\">Andre Karpathy의 PG에 대한 블로그</a>를 먼저 한 번 읽어보세요.  한글번역도 있습니다! 1) <a href=\"http://keunwoochoi.blogspot.com/2016/06/andrej-karpathy.html\" target=\"_blank\" rel=\"noopener\">http://keunwoochoi.blogspot.com/2016/06/andrej-karpathy.html</a> 2) <a href=\"https://tensorflow.blog/2016/07/13/reinforce-pong-at-gym/\" target=\"_blank\" rel=\"noopener\">https://tensorflow.blog/2016/07/13/reinforce-pong-at-gym/</a></p>\n<p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/28/1_sutton-pg/\">Sutton PG 여행하기</a></p>\n<p><br><br></p>\n<h1 id=\"2-DPG-Deterministic-policy-gradient-algorithms\"><a href=\"#2-DPG-Deterministic-policy-gradient-algorithms\" class=\"headerlink\" title=\"2. [DPG] Deterministic policy gradient algorithms\"></a>2. [DPG] Deterministic policy gradient algorithms</h1><p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/27/2_dpg/\">DPG 여행하기</a></p>\n<p>deterministic policy gradeint (DPG)는 어찌보면 상당히 도전적인 아이디어였던 것 같습니다. Sutton PG 논문에서는 DPG 스타일의 기법이 가진 단점에 대해서 언급하면서 stochastic policy gradient (SPG)를 써야 optimal을 찾을 수 있다고 기술하고 있었기 때문입니다.</p>\n<p>그런데 이 논문에서 높은 차원의 action space를 가지는 문제들에(예를 들면 문어발 제어) 대해서는 DPG가 상당히 좋은 성능을 내는 것을 저자들이 보였습니다. 그리고 DPG는 SPG와 대척점에 있는 기술이 아니고 SPG의 special case 중 하나임을 증명하면서 SPG를 가정하고 만들어진 기술들을 DPG에서도 그대로 이용할 수 있음을 보였습니다. David Silver의 <a href=\"http://techtalks.tv/talks/deterministic-policy-gradient-algorithms/61098/\" target=\"_blank\" rel=\"noopener\">동영상 강의</a>를 한 번 보시길 추천드립니다. 짧은 강의지만 랩을 하듯이 쉴새없이 설명하는 Silver의 모습에서 천재성이 엿보이는 것을 확인하실 수 있습니다. </p>\n<p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/27/2_dpg/\">DPG 여행하기</a></p>\n<p><br><br></p>\n<h1 id=\"3-DDPG-Continuous-control-with-deep-reinforcement-learning\"><a href=\"#3-DDPG-Continuous-control-with-deep-reinforcement-learning\" class=\"headerlink\" title=\"3. [DDPG] Continuous control with deep reinforcement learning\"></a>3. [DDPG] Continuous control with deep reinforcement learning</h1><p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/26/3_ddpg/\">DDPG 여행하기</a></p>\n<p>DPG의 후속 연구로 DPG보다 더 큰 주목을 받은 논문입니다. 소위 말하는 deep deterministic policy gradient (DDPG)로 불리는 기술을 제안한 논문입니다. 이 논문의 저자 중 일부는 그 유명한 DQN 논문의 저자이기도 합니다. Q-learning과 deep neural network를 접목시켰던 DQN처럼 이 논문도 DPG와 deep neural network를 접목시킨 논문입니다.</p>\n<p>이 논문은 DQN으로는 좋은 성능을 내지 못했던 continuous action을 가지는 상황들에 대해서 상당히 훌륭한 결과를 보이면서 큰 주목을 받았습니다. 소위 말하는 deep reinforcement learning (DRL)에서 Q-learning 계열의 DQN, PG 계열의 DDPG로 양대산맥을 이루는 논문이라고 할 수 있습니다. 두 논문 모두 Deepmind에서 나왔다는 것은 Deepmind 기술력이 DRL 분야에서 최정점에 있음을 보여주는 상징이 아닌가 싶습니다. 논문 자체는 그리 어렵지 않습니다. 새로운 아이디어를 제시했다기보다는 딥러닝을 활용한 강화학습의 가능성을 보여주는 논문이라는 점에서 큰 의의를 가지는 것 같습니다. 여러분도 한번 코딩에 도전해보시는게 어떨까요?</p>\n<p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/26/3_ddpg/\">DDPG 여행하기</a></p>\n<p><br><br></p>\n<h1 id=\"4-NPG-A-natural-policy-gradient\"><a href=\"#4-NPG-A-natural-policy-gradient\" class=\"headerlink\" title=\"4. [NPG] A natural policy gradient\"></a>4. [NPG] A natural policy gradient</h1><p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/25/4_npg/\">NPG 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/tnpg.py\" target=\"_blank\" rel=\"noopener\">NPG Code</a></p>\n<p>이 논문은 뒤이어 나오는 TRPO를 더 잘 이해하기 위해서 보는 논문입니다. 이번 논문부터 내용이 상당히 어려워집니다. 다소 생소한 수학 개념들이 많이 나오기 때문입니다. 하지만 이 블로그를 보시면 많은 부분들이 채워질 것이라고 믿습니다.</p>\n<p>2002년 당시에도 많은 연구자들이 objective function의 gradient 값을 따라서 좋은 policy $\\pi$를 찾고자 하였습니다. 하지만 기존의 우리가 알던 gradient descent method는 steepest descent direction이 아닐 수 있기 때문에(쉽게 말해 가장 가파른 방향을 따라서 내려가야 하는데 그러지 못할 수도 있다는 것입니다.) 이 논문에서 steepest descent direction를 나타내는 natural gradient method를 policy gradient에 적용하여 좋은 policy $\\pi$를 찾습니다. gradient descent는 parameter를 한 번에 많이 update 할 수 없는 반면, natural gradient는 가장 좋은 action을 고르도록 학습이 됩니다.</p>\n<p>또한 natural gradient method는 Line search 기법과 함께 사용하면 더 Policy Iteration과 비슷해집니다. Greedy Policy Iteration에서와 비교하면 Performance Improvement가 보장됩니다. 하지만 Fisher Information Matrix(FIM)이 asymtotically Hessian으로 수렴하지 않습니다. 그렇기 때문에 Conjugate Gradient Method로 구하는 방법이 더 좋을수 있습니다.</p>\n<p>위의 요약한 문장들만 봤을 때는 생소한 용어들이 많이 나와서 무슨 말인지 감이 안잡히실 수 있습니다. 저희가 포스팅한 블로그 글에는 다음과 같은 추가적인 내용이 나옵니다. 반드시 알고 가야 TRPO를 이해하는 것은 아닙니다. 다만 NPG를 이해하면 할 수록 TRPO를 접하기가 더 쉬울 수 있습니다.</p>\n<ul>\n<li>Euclidean space와 Riemannian space의 차이</li>\n<li>Natural Gradient 증명</li>\n<li>Fisher Information Matrix(FIM)</li>\n<li>Line Search</li>\n<li>FIM과 Hessian 방법의 차이</li>\n<li>Conjugate Gradient Method</li>\n</ul>\n<p>아래의 NPG Code는 Hessian 방법이 아닌 Conjugate Gradient Method를 사용한 “Truncated Natural Policy Gradient(TNPG)”라고 하는 방법의 코드입니다.</p>\n<p>마지막으로 프로젝트 내에 있는 한 팀원의 문장을 인용하겠습니다. “머리가 아프고 힘들수록 잘하고 있는겁니다.” NPG 논문을 보시는 분들 화이팅입니다!</p>\n<p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/25/4_npg/\">NPG 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/tnpg.py\" target=\"_blank\" rel=\"noopener\">NPG Code</a></p>\n<p><br><br></p>\n<h1 id=\"5-TRPO-Trust-region-policy-optimization\"><a href=\"#5-TRPO-Trust-region-policy-optimization\" class=\"headerlink\" title=\"5. [TRPO] Trust region policy optimization\"></a>5. [TRPO] Trust region policy optimization</h1><p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/24/5_trpo/\">TRPO 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/trpo_gae.py\" target=\"_blank\" rel=\"noopener\">TRPO Code</a></p>\n<p>PG기법이 각광을 받게 된 계기는 아마도 TRPO 때문이 아닌가 싶습니다. DQN이라는 인공지능이 아타리게임을 사람보다 잘 플레이했던 영상을 보고 충격을 받았던 사람들이 많았던 것처럼 TRPO가 난제로 여겨졌던 서기, 걷기, 뛰기 같은 로봇에 적용할 수 있는 continuous action을 잘 수행하는 것을 보고 많은 사람들이 놀라워했습니다. (솔직히 처음 아타리게임을 봤을 때 만큼은 아니지만…) 이것들은 DQN으로는 달성할 수 없었던 난제였습니다. </p>\n<p>그렇지만 거인의 어깨 위에서 세상을 바라보았던 많은 과학자들이 그랬던 것처럼 Schulmann도 TRPO를 갑자기 생각낸 것은 아닙니다. policy를 업데이트할 때 기존의 policy와 너무 많이 달라지면 문제가 생깁니다. 일단, state distribution이 변합니다. state distribution은 간단히 얘기하면 각각의 state를 방문하는 빈도를 의미한다고 할 수 있습니다. policy가 많이 바뀌면 기존의 state distribution을 그대로 이용할 수 없고 이렇게 되면 policy gradient를 구하기 어려워집니다. </p>\n<p>그래서 착안한 것이 policy의 변화 정도를 제한하는 것입니다. 여기서 우리는 stochastic policy, 즉, 확률적 policy를 이용한다고 전제하고 있기 때문에, policy의 변화는 policy 확률 분포의 변화를 의미합니다. 현재 policy와 새로운 policy 사이의 확률 분포의 변화를 어떻게 측정할 수 있을까요? Schulmann은 <a href=\"https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\" target=\"_blank\" rel=\"noopener\">KL divergence</a>라는 척도를 이용하였습니다. 이 변화량을 특정값 이하로 제한하고 이것을 만족하는 영역을 trust region이라고 불렀습니다. </p>\n<p>그렇지만 문제는 풀기에는 복잡했습니다. 이것을 풀기 위해서 여러 번의 근사화를 적용시켰습니다. objective function에는 linear approximation을 적용하고 trust region constraint에는 second-order approximation을 적용하면 문제가 좀 더 풀기 쉬운 형태로 바뀐다는 것을 알아냈습니다. 이것은 사실 natural gradient를 이용하는 것이었습니다. 이러한 이유로 TRPO를 이해하기 위해서 natural gradient도 살펴보았던 것입니다. </p>\n<p>Schulmann이 너무 똑똑해서 일까요? 훌륭한 아이디어로 policy gradient 기법의 르네상스를 열은 Schulmann의 역작인 TRPO 논문은 이해하기 쉽게 쓰여지지 않은 것 같습니다. (더 잘 쓸 수 있었잖아 Schulmann…) 저희의 포스트와 함께 보다 편하게 여행하시길 바랍니다. 이 <a href=\"https://youtu.be/CKaN5PgkSBc\" target=\"_blank\" rel=\"noopener\">유투브 영상</a>도 무조건 보세요~</p>\n<p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/24/5_trpo/\">TRPO 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/trpo_gae.py\" target=\"_blank\" rel=\"noopener\">TRPO Code</a></p>\n<p><br><br></p>\n<h1 id=\"6-GAE-High-Dimensional-Continuous-Control-Using-Generalized-Advantage-Estimation\"><a href=\"#6-GAE-High-Dimensional-Continuous-Control-Using-Generalized-Advantage-Estimation\" class=\"headerlink\" title=\"6. [GAE] High-Dimensional Continuous Control Using Generalized Advantage Estimation\"></a>6. [GAE] High-Dimensional Continuous Control Using Generalized Advantage Estimation</h1><p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/23/6_gae/\">GAE 여행하기</a></p>\n<p>TRPO가 나오고 난 뒤로도 복잡하고 어려운 control problem에서 Reinforcement Learning (RL)은 high sample complexity 때문에 제한이 되어왔습니다. 따라서 이 논문에서 그 제한을 풀고자 advantage function의 good estimate를 얻는 “variance reduction”에 대해 연구하였습니다.</p>\n<p>“Generalized Advantage Estimator (GAE)”라는 것을 제안했고, 이것은 bias-variance tradeoff를 조절하는 두 개의 parameter $\\gamma,\\lambda$를 가집니다.<br>또한 어떻게 Trust Region Policy Optimization과 value function을 optimize하는 Trust Region Algorithm의 idea를 합치는지를 보였습니다.</p>\n<p>이렇게 함으로써 보다 더 복잡하고 어려운 control task들을 해결할 수 있었습니다.</p>\n<p>GAE의 실험적인 입증으로는 robotic locomotion을 simulation하는 domain입니다. 실험에서도 보여준 것처럼 [0.9, 0.99]의 범위에서 $\\lambda$의 적절한 중간의 값을 통해 best performance를 얻습니다. 좀 더 나아가 연구되어야할 점은 adaptive or automatic하도록 estimator parameter $\\gamma,\\lambda$를 조절하는 방법입니다.</p>\n<p>추가적으로 앞으로 연구되어야할 부분은 만약 value function estimation error와 policy gradient estimation error 사이의 관계를 알아낸다면, value function fitting에 더 잘 맞는 error metric(policy gradient estimation의 정확성과 더 잘 맞는 value function)을 사용할 수 있을 것입니다. 여기서 policy와 value function의 파라미터를 공유하는 모델을 만드는 것은 아주 흥미롭고 이점이 많습니다. 하지만 수렴을 보장하도록 적절한 numerical optimization을 제시해야 할 것입니다.</p>\n<p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/23/6_gae/\">GAE 여행하기</a></p>\n<p><br><br></p>\n<h1 id=\"7-PPO-Proximal-policy-optimization-algorithms\"><a href=\"#7-PPO-Proximal-policy-optimization-algorithms\" class=\"headerlink\" title=\"7. [PPO] Proximal policy optimization algorithms\"></a>7. [PPO] Proximal policy optimization algorithms</h1><p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/22/7_ppo/\">PPO 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/ppo_gae.py\" target=\"_blank\" rel=\"noopener\">PPO Code</a></p>\n<p>이 논문에서는 Reinforcement Learning에서 Policy Gradient Method의 새로운 방법인 PPO를 제안합니다. 이 방법은 agent가 환경과의 상호작용을 통해 data를 sampling하는 것과 stochastic gradient ascent를 사용하여 “surrogate” objective function을 optimizing하는 것을 번갈아가면서 하는 방법입니다. data sample마다 one gradient update를 수행하는 기존의 방법과는 달리, minibatch update의 multiple epochs를 가능하게 하는 새로운 objective function을 말합니다.</p>\n<p>또한 PPO는 TRPO의 연장선상에 있는 알고리즘이라고 할 수 있습니다. TRPO에서는 문제를 단순하게 만들기 위해서 최적화 문제를 여러 번 변형시켰지만, PPO는 단순하게 clip이라는 개념을 사용합니다. TRPO에서 이용했던 surrogate objective function을 reward가 특정값 이상이거나 이하가 될 때 더 이상 변화시키지 않는 것입니다.</p>\n<p>이 알고리즘의 장점으로는</p>\n<ul>\n<li>TRPO의 장점만을 가집니다. 다시 말해 알고리즘으로 학습하기에 훨씬 더 간단하고, 더 일반적이고, 더 좋은 sample complexity (empirically)를 가집니다.</li>\n<li>또한 다른 online policy gradient method들을 능가했고, 전반적으로 sample complexity(특히 computational complexity), simplicity, wall-time가 좋다고 합니다.</li>\n</ul>\n<p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/22/7_ppo/\">PPO 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/ppo_gae.py\" target=\"_blank\" rel=\"noopener\">PPO Code</a></p>\n","site":{"data":{}},"excerpt":"","more":"<hr>\n<h1 id=\"0-Policy-Gradient의-세계로\"><a href=\"#0-Policy-Gradient의-세계로\" class=\"headerlink\" title=\"0. Policy Gradient의 세계로\"></a>0. Policy Gradient의 세계로</h1><p>반갑습니다! 저희는 PG여행을 위해 모인 PG탐험대입니다. 강화학습하면 보통 Q-learning을 떠올립니다. 그렇지만 오래전부터 Policy Gradient라는 Q-learning 못지 않은 훌륭한 방법론이 연구되어 왔고, 최근에는 강화학습의 최정점의 기술로 자리매김하고 있습니다. 강화학습의 아버지인 Sutton의 논문을 필두로 하여 기존의 DQN보다 뛰어난 성능을 내는 DPG와 DDPG, 그리고 현재 가장 주목받는 강화학습 연구자인 John Schulmann의 TRPO, GAE, PPO와 이를 이해하기 위해 필요한 Natural Policy Gardient까지 더불어 살펴보고자 합니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/tbcyhvilaqy4ra0/Policy%20Optimization%20in%20the%20RL%20Algorithm%20Landscape.png?dl=1\" width=\"800\"> </center>\n\n<p>위의 그림은 강화학습 알고리즘 landscape에서 Policy Optimization의 관점을 중점적으로 하여 나타낸 그림입니다. 위의 그림에서 빨간색 작은 숫자로 나타낸 것이 저희 PG여행에서 다룰 논문들입니다. 순서는 다음과 같습니다.</p>\n<ol>\n<li><a href=\"http://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf\" target=\"_blank\" rel=\"noopener\">Sutton_PG</a></li>\n<li><a href=\"http://proceedings.mlr.press/v32/silver14.pdf\" target=\"_blank\" rel=\"noopener\">DPG</a></li>\n<li><a href=\"https://arxiv.org/pdf/1509.02971.pdf\" target=\"_blank\" rel=\"noopener\">DDPG</a></li>\n<li><a href=\"https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf\" target=\"_blank\" rel=\"noopener\">NPG</a></li>\n<li><a href=\"https://arxiv.org/pdf/1502.05477.pdf\" target=\"_blank\" rel=\"noopener\">TRPO</a></li>\n<li><a href=\"https://arxiv.org/pdf/1506.02438.pdf\" target=\"_blank\" rel=\"noopener\">GAE</a></li>\n<li><a href=\"https://arxiv.org/pdf/1707.06347.pdf\" target=\"_blank\" rel=\"noopener\">PPO</a></li>\n</ol>\n<p>위와 같이 총 7가지 논문들을 리뷰하여 블로그로 정리하였습니다. 각 순서에 맞춰 보시는 것을 권장해드립니다.</p>\n<p>이 블로그에는 각각의 기술을 제안한 논문을 PG탐험대분들이 자세하게 리뷰한 포스트들이 있습니다. 우리나라에서 PG에 대해서 이렇게 자세하게 리뷰한 포스트들은 없었다고 감히 말씀드리고 싶습니다. 본 글에서는 이 포스트들을 읽기 전에 전체 내용을 개략적으로 소개하고 각각의 포스트들로 안내하고자 합니다. 자, 저희와 함께 PG여행을 즐겨보시겠습니까?</p>\n<p><br><br></p>\n<h1 id=\"1-Sutton-PG-Policy-gradient-methods-for-reinforcement-learning-with-function-approximation\"><a href=\"#1-Sutton-PG-Policy-gradient-methods-for-reinforcement-learning-with-function-approximation\" class=\"headerlink\" title=\"1. [Sutton PG] Policy gradient methods for reinforcement learning with function approximation\"></a>1. [Sutton PG] Policy gradient methods for reinforcement learning with function approximation</h1><p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/28/1_sutton-pg/\">Sutton PG 여행하기</a></p>\n<p>policy gradient (PG)는 expected reward를 policy의 파라미터에 대한 함수로 모델링하고 이 reward를 최대화하는 policy를 gradient ascent 기법을 이용해서 찾는 기법입니다. 강화학습의 대표격이라고 할 수 있는 Q-learning이라는 훌륭한 방법론이 이미 존재하고 있었지만 Q값의 작은 변화에도 policy가 크게 변할 수도 있다는 단점이 있기 때문에 policy의 점진적인 변화를 통해 더 나은 policy를 찾아가는 PG기법이 개발되었습니다.</p>\n<p>이 PG기법은 먼저 개발되었던 <a href=\"https://link.springer.com/content/pdf/10.1007/BF00992696.pdf\" target=\"_blank\" rel=\"noopener\">REINFORCE</a>라는 기법과 관련이 아주 많습니다. 서튼의 PG기법은 REINFORCE 기법을 <a href=\"http://papers.nips.cc/paper/1786-actor-critic-algorithms.pdf\" target=\"_blank\" rel=\"noopener\">actor-critic algorithm</a>을 사용하여 개선시킨 것이라고 볼 수도 있습니다. 저희 PG여행 팀도 처음에는 REINFORCE를 출발지로 삼으려고 했었지만 예전 논문이다보니 논문의 가독성이 너무 떨어져서 강화학습의 아버지라고 할 수 있는 서튼의 논문을 출발지로 삼았습니다. 하지만 이 논문도 만만치 않았습니다. 이 논문을 읽으시려는 분들께 저희의 여행기가 도움이 될 것입니다. PG기법에 대해서 먼저 감을 잡고 시작하시려면 <a href=\"http://karpathy.github.io/2016/05/31/rl/\" target=\"_blank\" rel=\"noopener\">Andre Karpathy의 PG에 대한 블로그</a>를 먼저 한 번 읽어보세요.  한글번역도 있습니다! 1) <a href=\"http://keunwoochoi.blogspot.com/2016/06/andrej-karpathy.html\" target=\"_blank\" rel=\"noopener\">http://keunwoochoi.blogspot.com/2016/06/andrej-karpathy.html</a> 2) <a href=\"https://tensorflow.blog/2016/07/13/reinforce-pong-at-gym/\" target=\"_blank\" rel=\"noopener\">https://tensorflow.blog/2016/07/13/reinforce-pong-at-gym/</a></p>\n<p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/28/1_sutton-pg/\">Sutton PG 여행하기</a></p>\n<p><br><br></p>\n<h1 id=\"2-DPG-Deterministic-policy-gradient-algorithms\"><a href=\"#2-DPG-Deterministic-policy-gradient-algorithms\" class=\"headerlink\" title=\"2. [DPG] Deterministic policy gradient algorithms\"></a>2. [DPG] Deterministic policy gradient algorithms</h1><p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/27/2_dpg/\">DPG 여행하기</a></p>\n<p>deterministic policy gradeint (DPG)는 어찌보면 상당히 도전적인 아이디어였던 것 같습니다. Sutton PG 논문에서는 DPG 스타일의 기법이 가진 단점에 대해서 언급하면서 stochastic policy gradient (SPG)를 써야 optimal을 찾을 수 있다고 기술하고 있었기 때문입니다.</p>\n<p>그런데 이 논문에서 높은 차원의 action space를 가지는 문제들에(예를 들면 문어발 제어) 대해서는 DPG가 상당히 좋은 성능을 내는 것을 저자들이 보였습니다. 그리고 DPG는 SPG와 대척점에 있는 기술이 아니고 SPG의 special case 중 하나임을 증명하면서 SPG를 가정하고 만들어진 기술들을 DPG에서도 그대로 이용할 수 있음을 보였습니다. David Silver의 <a href=\"http://techtalks.tv/talks/deterministic-policy-gradient-algorithms/61098/\" target=\"_blank\" rel=\"noopener\">동영상 강의</a>를 한 번 보시길 추천드립니다. 짧은 강의지만 랩을 하듯이 쉴새없이 설명하는 Silver의 모습에서 천재성이 엿보이는 것을 확인하실 수 있습니다. </p>\n<p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/27/2_dpg/\">DPG 여행하기</a></p>\n<p><br><br></p>\n<h1 id=\"3-DDPG-Continuous-control-with-deep-reinforcement-learning\"><a href=\"#3-DDPG-Continuous-control-with-deep-reinforcement-learning\" class=\"headerlink\" title=\"3. [DDPG] Continuous control with deep reinforcement learning\"></a>3. [DDPG] Continuous control with deep reinforcement learning</h1><p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/26/3_ddpg/\">DDPG 여행하기</a></p>\n<p>DPG의 후속 연구로 DPG보다 더 큰 주목을 받은 논문입니다. 소위 말하는 deep deterministic policy gradient (DDPG)로 불리는 기술을 제안한 논문입니다. 이 논문의 저자 중 일부는 그 유명한 DQN 논문의 저자이기도 합니다. Q-learning과 deep neural network를 접목시켰던 DQN처럼 이 논문도 DPG와 deep neural network를 접목시킨 논문입니다.</p>\n<p>이 논문은 DQN으로는 좋은 성능을 내지 못했던 continuous action을 가지는 상황들에 대해서 상당히 훌륭한 결과를 보이면서 큰 주목을 받았습니다. 소위 말하는 deep reinforcement learning (DRL)에서 Q-learning 계열의 DQN, PG 계열의 DDPG로 양대산맥을 이루는 논문이라고 할 수 있습니다. 두 논문 모두 Deepmind에서 나왔다는 것은 Deepmind 기술력이 DRL 분야에서 최정점에 있음을 보여주는 상징이 아닌가 싶습니다. 논문 자체는 그리 어렵지 않습니다. 새로운 아이디어를 제시했다기보다는 딥러닝을 활용한 강화학습의 가능성을 보여주는 논문이라는 점에서 큰 의의를 가지는 것 같습니다. 여러분도 한번 코딩에 도전해보시는게 어떨까요?</p>\n<p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/26/3_ddpg/\">DDPG 여행하기</a></p>\n<p><br><br></p>\n<h1 id=\"4-NPG-A-natural-policy-gradient\"><a href=\"#4-NPG-A-natural-policy-gradient\" class=\"headerlink\" title=\"4. [NPG] A natural policy gradient\"></a>4. [NPG] A natural policy gradient</h1><p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/25/4_npg/\">NPG 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/tnpg.py\" target=\"_blank\" rel=\"noopener\">NPG Code</a></p>\n<p>이 논문은 뒤이어 나오는 TRPO를 더 잘 이해하기 위해서 보는 논문입니다. 이번 논문부터 내용이 상당히 어려워집니다. 다소 생소한 수학 개념들이 많이 나오기 때문입니다. 하지만 이 블로그를 보시면 많은 부분들이 채워질 것이라고 믿습니다.</p>\n<p>2002년 당시에도 많은 연구자들이 objective function의 gradient 값을 따라서 좋은 policy $\\pi$를 찾고자 하였습니다. 하지만 기존의 우리가 알던 gradient descent method는 steepest descent direction이 아닐 수 있기 때문에(쉽게 말해 가장 가파른 방향을 따라서 내려가야 하는데 그러지 못할 수도 있다는 것입니다.) 이 논문에서 steepest descent direction를 나타내는 natural gradient method를 policy gradient에 적용하여 좋은 policy $\\pi$를 찾습니다. gradient descent는 parameter를 한 번에 많이 update 할 수 없는 반면, natural gradient는 가장 좋은 action을 고르도록 학습이 됩니다.</p>\n<p>또한 natural gradient method는 Line search 기법과 함께 사용하면 더 Policy Iteration과 비슷해집니다. Greedy Policy Iteration에서와 비교하면 Performance Improvement가 보장됩니다. 하지만 Fisher Information Matrix(FIM)이 asymtotically Hessian으로 수렴하지 않습니다. 그렇기 때문에 Conjugate Gradient Method로 구하는 방법이 더 좋을수 있습니다.</p>\n<p>위의 요약한 문장들만 봤을 때는 생소한 용어들이 많이 나와서 무슨 말인지 감이 안잡히실 수 있습니다. 저희가 포스팅한 블로그 글에는 다음과 같은 추가적인 내용이 나옵니다. 반드시 알고 가야 TRPO를 이해하는 것은 아닙니다. 다만 NPG를 이해하면 할 수록 TRPO를 접하기가 더 쉬울 수 있습니다.</p>\n<ul>\n<li>Euclidean space와 Riemannian space의 차이</li>\n<li>Natural Gradient 증명</li>\n<li>Fisher Information Matrix(FIM)</li>\n<li>Line Search</li>\n<li>FIM과 Hessian 방법의 차이</li>\n<li>Conjugate Gradient Method</li>\n</ul>\n<p>아래의 NPG Code는 Hessian 방법이 아닌 Conjugate Gradient Method를 사용한 “Truncated Natural Policy Gradient(TNPG)”라고 하는 방법의 코드입니다.</p>\n<p>마지막으로 프로젝트 내에 있는 한 팀원의 문장을 인용하겠습니다. “머리가 아프고 힘들수록 잘하고 있는겁니다.” NPG 논문을 보시는 분들 화이팅입니다!</p>\n<p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/25/4_npg/\">NPG 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/tnpg.py\" target=\"_blank\" rel=\"noopener\">NPG Code</a></p>\n<p><br><br></p>\n<h1 id=\"5-TRPO-Trust-region-policy-optimization\"><a href=\"#5-TRPO-Trust-region-policy-optimization\" class=\"headerlink\" title=\"5. [TRPO] Trust region policy optimization\"></a>5. [TRPO] Trust region policy optimization</h1><p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/24/5_trpo/\">TRPO 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/trpo_gae.py\" target=\"_blank\" rel=\"noopener\">TRPO Code</a></p>\n<p>PG기법이 각광을 받게 된 계기는 아마도 TRPO 때문이 아닌가 싶습니다. DQN이라는 인공지능이 아타리게임을 사람보다 잘 플레이했던 영상을 보고 충격을 받았던 사람들이 많았던 것처럼 TRPO가 난제로 여겨졌던 서기, 걷기, 뛰기 같은 로봇에 적용할 수 있는 continuous action을 잘 수행하는 것을 보고 많은 사람들이 놀라워했습니다. (솔직히 처음 아타리게임을 봤을 때 만큼은 아니지만…) 이것들은 DQN으로는 달성할 수 없었던 난제였습니다. </p>\n<p>그렇지만 거인의 어깨 위에서 세상을 바라보았던 많은 과학자들이 그랬던 것처럼 Schulmann도 TRPO를 갑자기 생각낸 것은 아닙니다. policy를 업데이트할 때 기존의 policy와 너무 많이 달라지면 문제가 생깁니다. 일단, state distribution이 변합니다. state distribution은 간단히 얘기하면 각각의 state를 방문하는 빈도를 의미한다고 할 수 있습니다. policy가 많이 바뀌면 기존의 state distribution을 그대로 이용할 수 없고 이렇게 되면 policy gradient를 구하기 어려워집니다. </p>\n<p>그래서 착안한 것이 policy의 변화 정도를 제한하는 것입니다. 여기서 우리는 stochastic policy, 즉, 확률적 policy를 이용한다고 전제하고 있기 때문에, policy의 변화는 policy 확률 분포의 변화를 의미합니다. 현재 policy와 새로운 policy 사이의 확률 분포의 변화를 어떻게 측정할 수 있을까요? Schulmann은 <a href=\"https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\" target=\"_blank\" rel=\"noopener\">KL divergence</a>라는 척도를 이용하였습니다. 이 변화량을 특정값 이하로 제한하고 이것을 만족하는 영역을 trust region이라고 불렀습니다. </p>\n<p>그렇지만 문제는 풀기에는 복잡했습니다. 이것을 풀기 위해서 여러 번의 근사화를 적용시켰습니다. objective function에는 linear approximation을 적용하고 trust region constraint에는 second-order approximation을 적용하면 문제가 좀 더 풀기 쉬운 형태로 바뀐다는 것을 알아냈습니다. 이것은 사실 natural gradient를 이용하는 것이었습니다. 이러한 이유로 TRPO를 이해하기 위해서 natural gradient도 살펴보았던 것입니다. </p>\n<p>Schulmann이 너무 똑똑해서 일까요? 훌륭한 아이디어로 policy gradient 기법의 르네상스를 열은 Schulmann의 역작인 TRPO 논문은 이해하기 쉽게 쓰여지지 않은 것 같습니다. (더 잘 쓸 수 있었잖아 Schulmann…) 저희의 포스트와 함께 보다 편하게 여행하시길 바랍니다. 이 <a href=\"https://youtu.be/CKaN5PgkSBc\" target=\"_blank\" rel=\"noopener\">유투브 영상</a>도 무조건 보세요~</p>\n<p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/24/5_trpo/\">TRPO 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/trpo_gae.py\" target=\"_blank\" rel=\"noopener\">TRPO Code</a></p>\n<p><br><br></p>\n<h1 id=\"6-GAE-High-Dimensional-Continuous-Control-Using-Generalized-Advantage-Estimation\"><a href=\"#6-GAE-High-Dimensional-Continuous-Control-Using-Generalized-Advantage-Estimation\" class=\"headerlink\" title=\"6. [GAE] High-Dimensional Continuous Control Using Generalized Advantage Estimation\"></a>6. [GAE] High-Dimensional Continuous Control Using Generalized Advantage Estimation</h1><p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/23/6_gae/\">GAE 여행하기</a></p>\n<p>TRPO가 나오고 난 뒤로도 복잡하고 어려운 control problem에서 Reinforcement Learning (RL)은 high sample complexity 때문에 제한이 되어왔습니다. 따라서 이 논문에서 그 제한을 풀고자 advantage function의 good estimate를 얻는 “variance reduction”에 대해 연구하였습니다.</p>\n<p>“Generalized Advantage Estimator (GAE)”라는 것을 제안했고, 이것은 bias-variance tradeoff를 조절하는 두 개의 parameter $\\gamma,\\lambda$를 가집니다.<br>또한 어떻게 Trust Region Policy Optimization과 value function을 optimize하는 Trust Region Algorithm의 idea를 합치는지를 보였습니다.</p>\n<p>이렇게 함으로써 보다 더 복잡하고 어려운 control task들을 해결할 수 있었습니다.</p>\n<p>GAE의 실험적인 입증으로는 robotic locomotion을 simulation하는 domain입니다. 실험에서도 보여준 것처럼 [0.9, 0.99]의 범위에서 $\\lambda$의 적절한 중간의 값을 통해 best performance를 얻습니다. 좀 더 나아가 연구되어야할 점은 adaptive or automatic하도록 estimator parameter $\\gamma,\\lambda$를 조절하는 방법입니다.</p>\n<p>추가적으로 앞으로 연구되어야할 부분은 만약 value function estimation error와 policy gradient estimation error 사이의 관계를 알아낸다면, value function fitting에 더 잘 맞는 error metric(policy gradient estimation의 정확성과 더 잘 맞는 value function)을 사용할 수 있을 것입니다. 여기서 policy와 value function의 파라미터를 공유하는 모델을 만드는 것은 아주 흥미롭고 이점이 많습니다. 하지만 수렴을 보장하도록 적절한 numerical optimization을 제시해야 할 것입니다.</p>\n<p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/23/6_gae/\">GAE 여행하기</a></p>\n<p><br><br></p>\n<h1 id=\"7-PPO-Proximal-policy-optimization-algorithms\"><a href=\"#7-PPO-Proximal-policy-optimization-algorithms\" class=\"headerlink\" title=\"7. [PPO] Proximal policy optimization algorithms\"></a>7. [PPO] Proximal policy optimization algorithms</h1><p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/22/7_ppo/\">PPO 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/ppo_gae.py\" target=\"_blank\" rel=\"noopener\">PPO Code</a></p>\n<p>이 논문에서는 Reinforcement Learning에서 Policy Gradient Method의 새로운 방법인 PPO를 제안합니다. 이 방법은 agent가 환경과의 상호작용을 통해 data를 sampling하는 것과 stochastic gradient ascent를 사용하여 “surrogate” objective function을 optimizing하는 것을 번갈아가면서 하는 방법입니다. data sample마다 one gradient update를 수행하는 기존의 방법과는 달리, minibatch update의 multiple epochs를 가능하게 하는 새로운 objective function을 말합니다.</p>\n<p>또한 PPO는 TRPO의 연장선상에 있는 알고리즘이라고 할 수 있습니다. TRPO에서는 문제를 단순하게 만들기 위해서 최적화 문제를 여러 번 변형시켰지만, PPO는 단순하게 clip이라는 개념을 사용합니다. TRPO에서 이용했던 surrogate objective function을 reward가 특정값 이상이거나 이하가 될 때 더 이상 변화시키지 않는 것입니다.</p>\n<p>이 알고리즘의 장점으로는</p>\n<ul>\n<li>TRPO의 장점만을 가집니다. 다시 말해 알고리즘으로 학습하기에 훨씬 더 간단하고, 더 일반적이고, 더 좋은 sample complexity (empirically)를 가집니다.</li>\n<li>또한 다른 online policy gradient method들을 능가했고, 전반적으로 sample complexity(특히 computational complexity), simplicity, wall-time가 좋다고 합니다.</li>\n</ul>\n<p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/22/7_ppo/\">PPO 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/ppo_gae.py\" target=\"_blank\" rel=\"noopener\">PPO Code</a></p>\n"},{"title":"Natural Policy Gradient","date":"2018-06-25T02:36:45.000Z","author":"김동민, 이동민, 이웅원, 차금강","subtitle":"피지여행 4번째 논문","_content":"\n<center> <img src=\"https://www.dropbox.com/s/yd0x14ljrhpnj1b/Screen%20Shot%202018-07-18%20at%201.08.05%20AM.png?dl=1\" width=\"600\"> </center>\n\n논문 저자 : Sham Kakade\n논문 링크 : https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf\nProceeding : Advances in Neural Information Processing Systems (NIPS) 2002\n정리 : 김동민, 이동민, 이웅원, 차금강\n\n---\n\n# 1. 들어가며...\n\n이 논문이 발표된 2002년 당시에도 많은 연구자들이 objective function의 gradient 값을 따라서 좋은 policy $\\pi$를 찾고자 하였습니다. 하지만 기존의 우리가 알던 gradient descent method는 steepest descent direction이 아닐 수 있기 때문에(쉽게 말해 가장 가파른 방향을 따라서 내려가야 하는데 그러지 못할 수도 있다는 것입니다.) 이 논문에서 steepest descent direction를 나타내는 natural gradient method를 policy gradient에 적용하여 좋은 policy $\\pi$를 찾습니다. \n\n<br>\n## 1.1 NPG 흐름 잡기\n\n### 1.1.1 매니폴드(manifold)\n\n<center> <img src=\"https://www.dropbox.com/s/cstjgemqpby4ysr/Screen%20Shot%202018-08-12%20at%208.28.29%20PM.png?dl=1\" width=\"300\"> </center>\n\n매니폴드는 간단하게 말해 위의 그림에서의 점들을 아우르는 subspace입니다. 그래서 NPG 공부하기 전에 매니폴드를 왜 배울까라는 의문이 들으실텐데 그 이유는 위에서도 언급했듯이 natural gradient method는 어떠한 파라미터 공간에서의 steepest descent direction을 강조하기 때문입니다. 여기서 파라미터 공간이 바로 리만 매니폴드입니다. 리만 매니폴드는 매니폴드가 각지지 않고 미분가능하게 부드럽게 곡률을 가진 면이라고 생각하면 편합니다.\n\nNeural Network(NN)을 사용할 경우 NN의 parameter space가 우리가 보통 생각하는 직선으로 쭉쭉 뻗어있는 유클리디안 공간(Euclidean space)가 아닙니다. 좀 더 일반적으로는 구의 표면과 같이 휘어져있는 공간 즉, 리만 공간(Riemannian space)로 표현할 수 있습니다. 다음 그림들을 보겠습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/jnlm6he18ar64yc/Screen%20Shot%202018-08-12%20at%208.14.13%20PM.png?dl=1\" width=\"400\"> </center>\n\n<center> <img src=\"https://www.dropbox.com/s/tufjqmaeaz29kaz/Screen%20Shot%202018-08-12%20at%208.15.53%20PM.png?dl=1\" width=\"400\"> </center>\n\n아래의 그림처럼 어떠한 확률 분포가 있다고 해봅시다.\n\n<center> <img src=\"https://www.dropbox.com/s/mpoop19eyu1vp0a/Screen%20Shot%202018-08-13%20at%2012.52.09%20PM.png?dl=1\" width=\"600\"> </center>\n\n어떠한 공간의 확률 분포에 있는 한 점은 다른 공간의 확률 분포에서의 한 점이 될 것입니다. 이렇게 위의 그림들과 같이 유클리디안 공간의 확률 분포에서의 한 점이 리만 공간의 확률 분포의 한 점이 되는 것이고, 곡률의 일차 근사가 유클리디안 공간에서는 일차 근사가 아닌 이차 근사이고, 리만 공간에서는 휘어진 공간이기 때문에 곡률을 일차 근사라고 보는 것입니다. 추가적으로 일차 근사와 이차 근사의 차이는 [다크 프로그래머님의 블로그](http://darkpgmr.tistory.com/149)를 참고해주시기 바랍니다. (꼭 보세요!) \n\n일차 근사와 이차 근사의 차이점과 각각의 장단점, 그리고 추가적으로 Line Search까지 알고 난 후에 이 논문을 보시는 것을 권장해드립니다.\n\n### 1.1.2 Natural Gradient + Policy Gradient\n\n먼저 아래의 그림들을 보여드리겠습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/65hra43zadsubff/Screen%20Shot%202018-08-13%20at%2012.51.45%20PM.png?dl=1\" width=\"500\"> <img src=\"https://www.dropbox.com/s/8g332zceeordqwv/Screen%20Shot%202018-08-13%20at%2012.51.51%20PM.png?dl=1\" width=\"400\"> </center>\n\n위에서도 언급했듯이 natural gradient가 steepest direction이 된다는 연구가 이뤄지고 있었습니다. 강화학습의 policy gradient은 objective function의 gradient를 따라 policy를 업데이트를 합니다. 이 때 policy는 parameterization되는데 이 경우에도 gradient 대신에 natural gradient가 좋다는 것을 실험해보는 논문이 지금 다루고 있는 논문입니다.\n\ngradient가 non-covariant(1차 근사)해서 생기는 문제는 간단히 말하자면 다음과 같습니다. policy가 parameterized된 상황에서는 같은 policy라도 다른 parameter를 가질 수 있습니다. 이 때, steepest direction은 두 경우에 같은 방향을 가리켜야 하는데 non-covariant한 경우 그렇지 못합니다. 이러한 부분이 바로 결국 느린 학습으로 연결이 되는 것입니다.\n\n논문에서 2차미분 방법론들과 짧게 비교합니다. 하지만 개인적인 의견으로 2차미분을 이용한 다른 방법들과의 비교가 생각보다 없는 점이 부족해 보였습니다.(Hessian을 이용한다거나 conjugate gradient method를 이용한다거나). 실험을 통해 Fisher Information Matrix(FIM)가 hessian에 수렴안하는 거라던지 Hessian 방법론이 local maxima 부근에서 상당히 느리다던지의 결과를 보여줬었으면 좋았을 것 같습니다.\n\n또한 natural gradient의 단점으로 natural gradient 만으로 업데이트하면 policy의 improvement 보장이 안될 수 있습니다. policy의 improvement를 보장하기 위해 line search도 써야하는데 line search를 어떻게 쓰는지에 대한 자세한 언급이 없습니다. 다시 말해 자세한 algorithm 설명이 없다는 것입니다.\n\nnatural policy gradient 논문은 natural gradient + policy gradient를 처음 적용했다는데 의의가 있습니다. 하지만 이 논문이 문제 삼은 gradient는 non-covariant하다라는 문제를 natural gradient를 통해 해결하지는 못했습니다(Experiment를 통해 covariant gradient가 되지 못했다는 것이 보입니다). NPG의 뒤를 잇는 논문이 “covariant policy search”와 “natural actor-critic”에서 covariant(2차 근사)하지 못하다는 것을 해결하기 위해 Fisher Information Matrix를 sample 하나 하나에 대해서 구하는 것이 아니라 trajectory 전체에 대해서 구합니다.\n\n또한 논문은 pg의 두 가지 세팅 중에 average-reward setting(infinite horizon)에서만 NPG를 다룹니다. “covariant policy search” 논문에서는 average-reward setting과 start-state setting 모두에 대해서 npg를 적용합니다.\n\nnatural gradient + policy gradient를 처음 제시했다는 것은 좋지만 npg 학습의 과정을 자세하게 설명하지 않았고 다른 2차 미분 방법들과 비교를 많이 하지 않은 점이 아쉬운 논문이었습니다.\n\n<br><br>\n\n# 2. Introduction\n\n소개는 앞에서 다 했기 때문에 간략하게 다시 한 번 정리하겠습니다. direct policy gradient method는 future reward의 gradient를 따라 policy를 update합니다. 하지만 gradient descent는 non-covariant입니다. 따라서 이 논문에서는 covarient gradient를 제시합니다. 바로 \"Natural Gradient\" 입니다. \n\n또한 natural gradient와 policy iteration의 연관성을 설명합니다. natural policy gradient is moving toward choosing a greedy optimal action (이런 연결점은 아마도 step-size를 덜 신경쓰고 싶어서 그런게 아닌가 싶습니다)\n\n논문의 Introduction 부분에 다음 멘트가 있습니다. 이 글만 봐서는 이해가 안갔는데 Mackay 논문에 좀 더 자세히 나와있었습니다.\n<center> <img src=\"https://www.dropbox.com/s/41xhhr7lgfk24a1/Screenshot%202018-06-10%2011.45.18.png?dl=1\"> </center>\n\n[Mackay](http://www.inference.org.uk/mackay/ica.pdf)논문에서는 다음과 같이 언급하고 있습니다. Back-propagation을 사용할 경우에 learning rate를 dimension에 1/n로 사용하면 수렴한다는 것이 증명됐습니다. 하지만 너무 느리다고 합니다.\n<center> <img src=\"https://www.dropbox.com/s/us9ezc7vxgrkez6/Screenshot%202018-06-10%2011.47.21.png?dl=1\"> </center>\n\n<br><br>\n\n# 3. A Natural Gradient\n\n<br>\n## 3.1 Notation\n\n이 논문에서 제시하는 Notation은 다음과 같습니다.\n\n- MDP : tuple $(S, s_0, A, R, P)$\n- $S$ : a finite set of states\n- $s_0$ : a start state\n- $A$ : a finite set of actions\n- $R$ : reward function $R: S \\times A -> [0, R_{max}]$\n- $\\pi(a;s, \\theta)$ : stochastic policy parameterized by $\\theta$\n- 모든 정책 $\\pi$는 ergodic : stationary distribution $\\rho^{\\pi}$이 잘 정의되어 있다고 봅니다.\n- 이 논문에서는 sutton의 pg 논문의 두 세팅(start-state formulation, average-reward formulation) 중에 두 번째인 average-reward formulation을 가정합니다.\n- performance or average reward : $\\eta(\\pi)=\\sum_{s,a}\\rho^{\\pi}(s)\\pi(a;s)R(s,a)$\n- state-action value : $Q^{\\pi}(s,a)=E_{\\pi}[\\sum_{t=0}^{\\infty}R(s_t, a_t)-\\eta(\\pi)\\vert s_0=s, a_0=a]$ ([Sutton PG](https://reinforcement-learning-kr.github.io/2018/06/28/sutton-pg/) 를 참고해주시기 바랍니다.)\n- 정책이 $\\theta$로 parameterize되어있으므로 performance는 $\\eta(\\pi_{\\theta})$인데 $\\eta(\\theta)$로 씁니다.\n\n<br>\n## 3.2 Natural Gradient\n\nSutton PG 논문의 policy gradient theorem에 따라 exact gradient of the average reward는 다음과 같습니다. 다음 수식이 어떻게 유도되었는지, 어떤 의미인지 모른다면 [Sutton PG](https://reinforcement-learning-kr.github.io/2018/06/28/sutton-pg/)을 통해 제대로 이해하는 것이 좋습니다. Euclidean space에서는 objective function의 일반적인 policy gradient는 Sutton PG에서 논의한 바와 같이 다음과 같이 구할 수 있습니다.\n\n$$\\nabla\\eta(\\pi_\\theta) = \\Sigma_{s,a}\\rho^\\pi(s)\\nabla\\pi(a;s,\\theta)Q^\\pi(s,a)$$\n\n여기서 steepest descent direction of $\\eta(\\theta)$는 $\\eta(\\theta + d\\theta)$를 최소화하는 $d\\theta$로 정의됩니다. 이 때, $\\vert d\\theta \\vert^2$가 일정 크기 이하인 것으로 제약조건을 주는데(held to small constant) Euclidian space에서는 $\\eta(\\theta)$가 steepest direction이지만 Riemannian space에서는 natural gradient가 steepest direction입니다.\n\n추가적으로 결국 우리가 원하는 건 ‘policy 최적화를 좀 더 스마트하게 해 보자!’입니다. Policy 최적화를 잘한다는 것은 Policy를 어느 방향으로 얼만큼 업데이트 하느냐에 따라 달라집니다.\n\n### 3.2.1 Natural gradient 증명\n\nRiemannian space에서 거리는 다음과 같이 정의됩니다. 여기서 $G(\\theta)$는 특정한 양수로 이루어진 matrix입니다. 자세한 내용은 [양의 정부호 행렬(positive definite matrix)이란?](http://bskyvision.com/205)을 참고해주시기 바랍니다.\n\n$$\\vert d\\theta \\vert^2=\\sum_{ij}(\\theta)d\\theta_id\\theta_i=d\\theta^TG(\\theta)d\\theta$$\n\n이 수식은 [Natural gradient works in efficiently in learning](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.452.7280&rep=rep1&type=pdf) 논문에서 증명되어 있습니다. 다음은 natural gradient 증명입니다.\n\nsteepest direction을 구할 때 $\\theta$의 크기를 제약조건으로 줍니다. 제약조건은 다음과 같습니다.\n\n$$\\vert d\\theta \\vert^2 = \\epsilon^2$$\n\n그리고 steepest vector인 $d\\theta$는 다음과 같이 정의할 수 있습니다.\n\n$$d\\theta = \\epsilon a$$\n\n$$\\vert a \\vert^2=a^TG(\\theta)a = 1$$\n\n이 때, $a$가 steepest direction unit vector이 되려면 다음 수식을 최소로 만들어야 합니다.\n\n$$\\eta(\\theta + d\\theta) = \\eta(\\theta) + \\epsilon\\nabla\\eta(\\theta)^Ta$$\n\n위 수식이 제약조건 아래 최소가 되는 $a$를 구하기 위해 Lagrangian method를 사용합니다. Lagrangian method를 모른다면 [위키피디아](https://en.wikipedia.org/wiki/Lagrange_multiplier)를 참고하는 것을 추천합니다. 위 수식이 최소라는 것은 $\\nabla\\eta(\\theta)^Ta$가 최소라는 것입니다.\n\n$$\\frac{\\partial}{\\partial a_i}(\\nabla\\eta(\\theta)^Ta - \\lambda a^TG(\\theta)a)=0$$\n\n따라서 $(\\nabla\\eta(\\theta)^Ta - \\lambda a^TG(\\theta)a)=0$는 상수입니다. 상수를 미분하면 0이므로 이 식을 $a$로 미분합니다. 그러면 다음과 같이 steepest direction을 구한 것입니다.\n\n$$\\nabla\\eta(\\theta) = 2 \\lambda G(\\theta)a$$\n\n$$a=\\frac{1}{2\\lambda}G^{-1}\\nabla\\eta(\\theta)$$\n\n이 때, 다음 식을 natural gradient라고 정의합니다.\n\n$$\\bar{\\nabla}\\eta(\\theta) = G^{-1}\\nabla\\eta(\\theta)$$\n\nnatural gradient를 이용한 업데이트는 다음과 같습니다.\n\n$$\\theta_{t+1}=\\theta_t - \\alpha_tG^{-1}\\nabla\\eta(\\theta)$$\n\n여기까지는 natural gradient의 증명이었습니다. 이 natural gradient를 policy gradient에 적용한 것이 natural policy gradient입니다. natural policy gradient는 다음과 같이 정의됩니다.\n\n$$\\bar{\\nabla}\\eta(\\theta) = F^{-1}\\nabla\\eta(\\theta)$$\n\n여기서 $G$대신 $F$를 사용했는데 $F$는 Fisher information matix입니다. 수식은 다음과 같습니다.\n\n$$F(\\theta) = E_{\\rho^\\pi(s)}[F_s(\\theta)]$$\n\n$$F_s(\\theta)=E_{\\pi(a;s,\\theta)}[\\frac{\\partial log \\pi(a;s, \\theta)}{\\partial \\theta_i}\\frac{\\partial log \\pi(a;s, \\theta)}{\\partial\\theta_j}]$$\n\n### 3.2.2 Fisher Information Matrix에 정의된 Metric\n\n추가적으로 Fisher Information Matrix(FIM)에 대해서 설명하겠습니다. 위의 문제를 우리가 생각하기 쉬운 Neural Network(NN)으로 구성하고 해결할 수 있다고 해봅시다. NN은 여러가지 parameter set들로 구성될 수 있습니다. 게다가 다른 parameter set을 가지지만 같은 policy를 가질 수도 있습니다. 이 경우 steepest direction은 같은 policy이기 때문에 같은 방향을 가리키고 있어야 하는데 non-covariant한 경우 그렇지 못합니다. 떄문에 학습이 느려지고, 이러한 문제를 해결하기 위해 단순히 Positive-Definite Matrix $G(\\theta)$를 사용하지 않고 FIM $F_s(\\theta)$를 사용합니다. 어떠한 확률 변수 $X$가 임의의 매개변수 $\\theta$에 의해 정의되는 분포를 따른다고 하면 $X=x$일때 FIM은 다음과 같이 정의됩니다.\n\n$$F_x(\\theta)=E\\left[\\left(\\dfrac{\\partial}{\\partial\\theta}\\log\\Pr(x|\\theta)\\right)^2\\right]$$\n\n강화학습 관점에서 생각해보면 정보 $x$는 에피소드에 의해 관측된 상태값 $s$이며 매개변수 $\\theta$에 의해 선택될 수 있는 행동에 대한 분포가 나오게 됩니다. 이에 의해 위의 FIM는 다음과 같이 표현할 수 있습니다.\n\n$$F_s(\\theta) \\equiv E_{\\pi(a;s,\\theta)}\\left[\\left(\\dfrac{\\partial}{\\partial\\theta}\\log\\pi(a;s,\\theta)\\right)^2\\right] =E_{\\pi(a;s,\\theta)}\\left[\\dfrac{\\partial \\log\\pi(a;s,\\theta)}{\\partial \\theta_i}\\dfrac{\\partial \\log\\pi(a;s,\\theta)}{\\partial\\theta_j}\\right]$$\n\n그리고 위의 식들을 이용하여 objective function을 정리하면 아래의 식과 같이 표현됩니다.\n\n$$F(\\theta) = E_{\\rho^{\\pi}(s)}[F_s(\\theta)]$$\n\n또한 이 Fisher Information Matrix에 정의된 이 metric은 다음과 같은 성질을 가지고 있습니다.\n\n1. 업데이트되는 파라미터에 의해 구성되는 매니폴드에 기반한 metric입니다.\n2. 확률분포($\\pi(a;s,\\theta)$)를 구성하는 파라미터($\\theta$)의 변화에 독립적입니다.\n3. 마지막으로 positive-definite한 값을 가집니다. 이렇기 때문에 steepest gradient에서 objective function의 방향을 알기 위해 사용한 방법과 같은 방법으로 natural gradient direction을 다음과 같이 구할 수 있는 것입니다.\n\n$$\\bar{\\nabla}\\eta(\\theta) \\equiv F(\\theta)^{-1}\\nabla\\eta(\\theta)$$\n\n<br><br>\n\n# 4. The Natural Gradient and Policy Iteration\n\n4장에서는 Natural gradient를 통한 policy iteration을 수행하여 실제로 정책의 향상이 있는지를 증명합니다. 여기서 $Q^\\pi(s,a)$는 compatible function approximator $f^\\pi(s,a;w)$로 근사됩니다. ([Sutton PG](https://reinforcement-learning-kr.github.io/2018/06/28/sutton-pg/)를 참고해주시기 바랍니다.)\n\n<br>\n## 4.1 Theorem 1: Compatible Function Approximation\n\napproximate하는 함수 $f^{\\pi}(s,a;w)$는 다음과 같습니다.(compatible value function)\n\n$$f^{\\pi}(s,a;w)=w^T\\psi^{\\pi}(s,a)$$\n\n$$\\psi^{\\pi}(s,a) = \\nabla log\\pi(a;s,\\theta)$$\n\n여기서 $[\\nabla \\log\\pi(a;s,\\theta)]_i=\\partial \\log\\pi(a;s,\\theta)/\\partial\\theta_i$입니다. $w$는 원래 approximate하는 함수 $Q$와 $f$의 차이를 줄이도록 학습합니다(mean square error). 수렴한 local minima의 $w$를 $\\bar{w}$라고 가정 하겠습니다. 에러는 다음과 같은 수식으로 나타낼 수 있습니다.\n\n$$\\epsilon(w,\\pi)\\equiv\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)(f^{\\pi}(s,a;w)-Q^{\\pi}(s,a))^2$$\n\n그 때, $\\bar{\\omega} = \\bar{\\nabla}\\eta(\\theta)$이면 function approximator의 gradient 방향과 정책의 gradient 방향이 같다는 것을 의미합니다.\n\n아래의 내용은 위의 Theorem에 대한 증명입니다.\n\n위의 수식이 local minima이면 미분값이 0이 됩니다. $w$에 대해서 미분하면 다음과 같습니다.\n\n$$\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)(\\psi^{\\pi}(s,a)^T\\bar{w}-Q^{\\pi}(s,a))=0$$\n\n$$\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)\\psi^{\\pi}(s,a)^T\\bar{w}=\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)Q^{\\pi}(s,a)$$\n\n이 때, 위 식의 우변은 $\\psi$의 정의에 의해 policy gradient가 됩니다. 또한 왼쪽 항에서는 Fisher information matrix가 나옵니다.\n\n$$F(\\theta)=\\sum_{s,a}\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)\\psi^{\\pi}(s,a)=E_{\\rho^\\pi(s)}[F_s(\\theta)]$$\n\n따라서 다음과 같이 쓸 수 있습니다.\n\n$$F(\\theta)\\bar{w}=\\nabla\\eta(\\theta)$$\n\n$$\\bar{w}=F(\\theta)^{-1}\\nabla\\eta(\\theta)$$\n\n위의 수식은 natural gradient식과 동일합니다. 위의 수식은 policy가 update 될 때, value function approximator의 parameter 방향으로 이동한다는 것을 의미합니다. function approximation이 정확하다면 그 parameter의 natural policy gradient와 inner product가 커야합니다.\n\n<br>\n## 4.2 Theorem 2: Greedy Policy Improvement\n\n이번 장에서는 natrual gradient는 다른 policy iteration 방법처럼 단순히 더 좋은 행동을 고르도록 학습하는 것이 아니라 가장 좋은(greedy) 행동을 고르도록 학습한다는 것을 증명하는 부분입니다. 이것을 일반적인 형태의 policy에 대해서 증명하기 전에 exponential 형태의 policy에 대해서 증명하는 것이 Theorem 2입니다. 특수한 정책을 가지는 상황안에서 학습속도($\\alpha$)를 무한대로 가져감으로서 어떤 action을 선택하는지를 알아봅니다.\n\npolicy를 다음과 같이 정의합니다.\n\n$$\\pi(a;s,\\theta) \\propto \\exp(\\theta^T\\phi_{sa})$$\n\n여기서 $\\bar{\\nabla}\\eta(\\theta)$가 0이 아니고 $\\bar{w}$는 approximation error를 최소화된 $w$라고 가정합니다. 이 상태에서 natural gradient update를 생각해봅시다. 그리고 policy gradient는 gradient ascent임을 기억합시다.\n\n$$\\theta_{t+1}=\\theta_t + \\alpha_t\\bar{\\nabla}\\eta(\\theta)$$\n\n이 때 $\\alpha$가 learning rate로 parameter를 얼마나 업데이트하는지를 결정합니다. 이 값을 무한대로 늘렸을 때 policy가 어떻게 업데이트되는지 생각해봅시다.\n\n$$\\pi_{\\infty}(a;s)=lim_{\\alpha\\rightarrow\\infty}\\pi(a;s,\\theta+\\alpha\\bar{\\nabla}\\eta(\\theta))-(1)$$\n\n이 때,\n\n$$\\pi_{\\infty}=0 \\, \\, \\, if \\, and \\, only \\, if(필요충분조건) \\, \\, \\, a \\notin argmax_{a'}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa'}$$\n\n이라고 말할 수 있습니다.\n\n아래의 내용은 위의 Theorem에 대한 증명입니다.\n\n먼저 function approximator는 앞서 다뤘듯이 아래와 같습니다.\n\n$$f^{\\pi}(s,a;w)=w^T\\psi^{\\pi}(s,a)$$\n\n여기서 function approximator는 Theorem 1에 의해 아래와 같이 쓸 수 있습니다.\n\n$$f^{\\pi}(s,a;w)=\\bar{\\nabla}\\eta(\\theta)^T\\psi^{\\pi}(s,a)$$\n\n$\\theta$의 정의에 의해 $\\psi$는 다음과 같습니다.\n\n$$\\psi^{\\pi}(s,a)=\\phi_{sa}-E_{\\pi(a';s,\\theta)}[\\phi_{sa'}]$$\n\n따라서 function approximator는 다음과 같이 다시 쓸 수 있습니다.\n\n$$f^{\\pi}(s,a;w)=\\bar{\\nabla}\\eta(\\theta)^T(\\phi_{sa}-E_{\\pi(a';s,\\theta)}[\\phi_{sa'}])$$\n\ngreedy policy improvement가 Q function 값 중 가장 큰 값을 가지는 action을 선택하듯이 여기서도 function approximator의 값이 가장 큰 action을 선택하는 상황을 가정해봅시다. 이 때 function approximator의 argmax는 다음과 같이 쓸 수 있습니다.\n\n$$argmax_{a'}f^{\\pi}(s,a)=argmax_{a'}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa'}$$\n\n(1) 식을 다시 살펴봅시다. 그러면 policy의 정의에 따라 다음과 같이 쓸 수 있습니다.\n\n$$\\pi(a;s,\\theta + \\alpha\\bar{\\nabla}\\eta(\\theta)) \\propto exp(\\theta^T\\phi_{sa} + \\alpha\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa})$$\n\n$\\bar{\\nabla}\\eta(\\theta) \\neq 0$이고 $\\alpha\\rightarrow\\infty$이면 exp안의 항 중에서 뒤의 항이 dominate하게 됩니다. 여러 행동 중에 $\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa}$가 가장 큰 행동이 있다면 이 행동의 policy probability가 1이 되고 나머지는 0이 됩니다. 따라서 다음이 성립합니다.\n\n$$\\pi_{\\infty}=0 \\, \\, \\, if \\, and \\, only \\, if \\, \\, \\, a \\notin argmax_{a'}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa'}$$\n\n이 결과로부터 natural policy gradient는 단지 더 좋은 action이 아니라 best action을 고르도록 학습이 됩니다. 반면에 non-covariant gradient(1차미분)에서는 그저 더 좋은 action을 고르도록 학습이 됩니다. 이 natural policy gradient에 대한 결과는 infinite learning rate 세팅에서만 성립합니다. 좀 더 일반적인 경우에 대해서 살펴봅시다.\n\n<br>\n## 4.3 Theorem 3: General Parameterized Policy\n\nTheorem 2에서와는 달리 일반적인 policy를 가정해봅시다(general parameterized policy). Theorem 3는 이 상황에서 natural gradient를 통한 업데이트가 best action를 고르는 방향으로 학습이 된다는 것을 보여줍니다.\n\nnatural gradien에 따른 policy parameter의 업데이트는 다음과 같습니다. $\\bar{w}$는 approximation error를 minimize하는 $w$입니다.\n\n$$\\delta\\theta = \\theta' - \\theta = \\alpha\\bar{\\nabla}\\eta(\\theta)=\\alpha\\bar{w}$$\n\npolicy에 대해서 1차근사를 하면 다음과 같습니다.\n\n$$\\pi(a;s,\\theta')=\\pi(a;s,\\theta)+\\frac{\\partial\\pi(a;s,\\theta)^T}{\\partial\\theta}\\delta\\theta + O(\\delta\\theta^2)$$\n\n$$=\\pi(a;s,\\theta)(1+\\psi(s,a)^T\\delta\\theta) + O(\\delta\\theta^2)$$\n\n$$=\\pi(a;s,\\theta)(1+\\alpha\\psi(s,a)^T\\bar{w}) + O(\\delta\\theta^2)$$\n\n$$=\\pi(a;s,\\theta)(1+\\alpha f^{\\pi}(s,a;\\bar{w}) + O(\\delta\\theta^2)$$\n\npolicy 자체가 function approximator의 크기대로 업데이트가 되므로 local하게 best action의 probability는 커지고 다른 probability의 크기는 작아질 것입니다. 하지만 만약 greedy improvement가 된다하더라도 그게 performance의 improvement를 보장하는 것은 아닙니다. 하지만 line search와 함께 사용할 경우 improvement를 보장할 수 있습니다. 왜 그런지는 처음에 말씀드린 블로그 [다크 프로그래머님의 블로그](http://darkpgmr.tistory.com/149)를 참고해주시기 바랍니다.\n\n<br><br>\n\n# 5. Metrics and Curvatures\n\n$$\\vert d\\theta \\vert^2=\\sum_{ij}(\\theta)d\\theta_id\\theta_i=d\\theta^TG(\\theta)d\\theta$$\n\n이 파트에서는 FIM과 다른 metric 사이의 관계를 다룹니다.\n\n위의 식에 해당하는 G는 Fisher Information Matrix만 사용할 수 있는 것이 아닙니다. Positive-Definite Matrix인 FIM이외의 다른 Matrix도 사용할 수 있습니다. 또한 다양한 파라미터 추정에서 FIM은 Hessian Matrix에 수렴하지 않을 수 있다고 합니다. 이 말은 2nd order(2차 근사) 수렴이 보장되지 않는다는 말입니다.\n\n논문에 있는 내용들을 조금 더 추가적으로 보면 아래와 같이 나옵니다.\n\nIn the different setting of parameter estimation, the Fisher information converges to the ```Hessian```, so it is [asymptotically efficient](https://en.wikipedia.org/wiki/Efficiency_(statistics)) 이지만,\n\n이 논문의 경우, 아마리 논문의 'blind separation case'와 유사한데 이 때는 꼭 asymtotically efficient하지 않다고 말합니다. 이 말은 즉 2nd order 수렴이 보장되지 않는다는 것이다.\n\n[Mackay](http://www.inference.org.uk/mackay/ica.pdf) 논문에서는 Hessian에서 data independant한 term을 metric으로 가져오는 방법을 제안했습니다. 그래서 performance를 2번 미분해보면 아래의 수식과 같습니다. 하지만 다음 식에서는 모든 항이 data dependent합니다(Q가 있으니까). 첫 번째 항이 그나마 FIM과의 관련성이 있을 수 있지만 Q 값이 curvature에 weight를 주는 방식 때문에 다르다고 할 수 있습니다.\n\n$$\\nabla^2\\eta(\\theta)=\\sum_{sa}\\rho^{\\pi}(s)(\\nabla^2\\pi(a;s)Q^{\\pi}(s,a)+\\nabla\\pi(a;s)\\nabla Q^{\\pi}(s,a)^T+\\nabla Q^{\\pi}(s,a)\\nabla\\pi(a;s)^T)$$\n\nhessian은 보통 positive definite가 아닐수도 있습니다. 따라서 local maxima가 될 때까지 Hessian이 사용하기 별로 안좋습니다. 그리고 local maxima에서는 Hessian보다는 Conjugate methods가 더 효율적이라고 합니다.\n\n사실 이 파트에서는 무엇을 말하고 있는지 알기가 어렵습니다. FIM과 Hessian이 관련이 있다는 것을 알겠는데 asymtotically efficient와 같은 내용을 모르므로 내용의 이해가 어려웠습니다.\n\nMackay 논문에서 해당 부분은 다음과 같습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/x4n6z6pdyi7xtb9/Screenshot%202018-06-10%2012.04.13.png?dl=1\"> </center>\n\n<br>\n## 5.1 Fisher Information Matrix(FIM) vs. Hessian\n\nFIM과 Hessian에 대해서 추가적인 설명을 하고자 합니다.\n\n- FIM\n\n일단 정의되려면 space 자체가 stochastic한 성질을 가지고 있어야 합니다. 모든 parameter를 표현하는 함수가 deterministic한 함수가 아니라 확률로 나타낸 분포입니다. (probability distribution)\n\n결국은 FIM에서도 hessian처럼 비슷한 과정을 취하고 싶은 것입니다. 따라서 확률 변수이기 때문에 어떠한 특정 sample을 취하면 그게 항상 다른값이 됩니다. 그래서 FIM에다가 expectation을 취한 것입니다. expectation을 취함으로써 hessian같은 성질을 가집니다. 왜냐하면 expectation을 취함으로써 constant한 값이 되기 때문입니다.\n\n- Hessian\n\n그냥 deterministic한 함수에서 정의됩니다. 그냥 함수를 두 번 미분 한 것이라고 보면 됩니다.\n\n<br>\n## 5.2 Conjugate Gradient Method\n\n추가적으로 Conjugate Gradient Method(CGM)에 대해서 다루고자 합니다.\n\n- 참고자료\n    - https://www.quora.com/What-is-an-intuitive-explanation-of-what-the-conjugate-gradient-method-is\n    - https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf\n    - https://en.wikipedia.org/wiki/Conjugate_gradient_method\n\n### $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$의 해 구하기\n\n$\\mathbf{A}\\mathbf{x} = \\mathbf{b}$의 해를 구하는 문제를 생각해봅시다. $\\mathbf{A}$의 역행렬을 구해서 양변에 곱해주면 $\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}$가 되어 쉽게 해를 구할 수 있습니다. 하지만 $\\mathbf{A}^{-1}$은 계산이 많이 필요 자원소모가 큰 연산입니다. 역행렬을 취하지 않고 해를 구할 수 있는 방법이 있을까요?\n\n위의 방정식에서 $\\mathbf{b}$를 이항시키면 $\\mathbf{A}\\mathbf{x} - \\mathbf{b} = 0$이 됩니다. 최적화문제는 많은 경우 1차 미분이 0이 되는 지점이 해일 확률이 높습니다. $\\mathbf{A}\\mathbf{x} - \\mathbf{b} = 0$을 1차 미분으로 가지는 함수는 무엇일까요?\n\n$$f( \\mathbf{x} ) = \\frac{1}{2}\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x} - \\mathbf{x}^\\mathrm{T}\\mathbf{b}$$\n\n위의 함수는 $\\mathbf{A}\\mathbf{x} - \\mathbf{b}$를 1차 미분값으로 가집니다. 이 때 $\\mathbf{A}$는 symmetric positive definite해야 합니다.\n\n- symmetric: $\\mathbf{A}=\\mathbf{A}^\\mathrm{T}$\n- positive definite: $\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x}>0,\\quad\\forall\\mathbf{x}$ or all eigenvalues of $\\mathbf{A}$ are positive\n\nsymmetric positive definite한 성질은 $\\mathbf{x}$를 strict convex function이 되게 만들어서 unique solution이 존재하게 합니다. $\\mathbf{A}$는 $\\mathbf{x}$의 Hessian이기도 합니다.\n\n자, 우리는 위의 방정식의 해 $\\mathbf{x}^*$를 iterative한 방식으로 찾고자 합니다. $\\mathbf{x}_0$을 초기값이라고 합시다.\n\n우리는 <img src=\"https://www.dropbox.com/s/g91yqajf72jzjoc/Screen%20Shot%202018-08-14%20at%208.43.28%20AM.png?dl=1\" width=\"80\">가 되는 <img src=\"https://www.dropbox.com/s/aj3j5fjtiyewlo1/Screen%20Shot%202018-08-14%20at%208.43.37%20AM.png?dl=1\" width=\"30\">를 찾고 싶은데 이를 위한 현재의 추정값은 $\\mathbf{A} \\mathbf{x}_0$입니다. 최적의 값을 찾았을 때와 비교하면 현재의 오차는 $\\mathbf{b} - \\mathbf{A}\\mathbf{x}_0 = \\mathbf{r}_0$입니다. 이것을 residual이라고 합니다. 어떻게 효율적으로 값을 변화시켜서 오차를 0으로 만들 수 있을까요? 일단 매 iteration마다 residual이 작아지는 방향으로 나아가야겠죠?\n\n가장 널리 알려진 방법은 gradient descent 방향입니다. gradient가 증가하면 반대로 가고 gradient가 감소하면 그 방향으로 가는 것입니다. 그리고 어떤 방향으로든 gradient가 0이면 그 지점에 멈추는 것이죠. 이 방법은 수렴은 하지만 zigzag하게 움직여서 느립니다. 이것보다 더 좋은 방법이 없을까요?\n\n### Conjugate Gradient Method\n\n#### Gram-Schmidt orthgonalization\n\n다음과 같은 vector들의 집합 <img src=\"https://www.dropbox.com/s/csgdsfofe19q5r6/Screen%20Shot%202018-08-14%20at%208.56.04%20AM.png?dl=1\" width=\"135\">이 있다고 해봅시다. 이 vector들이 서로 linearly independent하다면 이 vector들은 $\\mathbb{R}^n$ 공간 상의 basis입니다. 이 vector들을 이용해서 orthogonal한 vector들을 만들어보겠습니다. 새로운 vector들을 <img src=\"https://www.dropbox.com/s/7xg7pciar1ndu44/Screen%20Shot%202018-08-14%20at%208.58.25%20AM.png?dl=1\" width=\"130\">라고 한다면 아래와 같은 과정을 통해 만들 수 있습니다. 이 방법을 Gram-Schmidt orthogonalization이라고 합니다.\n\n- $\\mathbf{d}_1=\\mathbf{v}_1$\n- $\\mathbf{d}_2=\\mathbf{v}_2 - \\left\\langle\\mathbf{v}_2,\\frac{\\mathbf{d}_1}{\\parallel\\mathbf{d}_1\\parallel}\\right\\rangle\\frac{\\mathbf{d}_1}{\\parallel\\mathbf{d}_1\\parallel}$\n...\n- <img src=\"https://www.dropbox.com/s/arzc5cez8az0j7h/Screen%20Shot%202018-08-14%20at%209.00.04%20AM.png?dl=1\" width=\"290\">\n\n간단히 설명을 하면, 첫 vector는 basis와 같은 방향으로 출발합니다. 그 다음 vector는 그 다음 basis를 이전 vector로 projection한 다음 해당 basis로부터 빼줍니다. 그 다음 vector를 만들 때는 이전에 만든 모든 vector들에 대해서 projection을 취한 다음 모두 더한 것을 해당 basis에서 빼줍니다. \n\n#### $A$-conjugate\n\n이 방법을 이용하여 matrix $A$에 관하여 orthogonal한 새로운 vector들의 집합을 만들어보겠습니다. 일반적인 inner product와 약간 다른 다음과 같은 inner product를 정의할 수 있습니다.\n\n$$<\\mathbf{x},\\mathbf{y}>_\\mathbf{A} = \\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{y}$$\n\n이 때 $<\\mathbf{x},\\mathbf{y}>_\\mathbf{A} = 0$이면 $\\mathbf{x}와 \\mathbf{y}$는 $\\mathbf{A}$-orthogonal 또는 $\\mathbf{A}$-conjugate하다라고 합니다. vector norm $\\parallel\\cdot\\parallel$도 일반적인 vector norm이 아닌 A-norm을 다음과 같이 정의합니다.\n\n$$\\parallel\\mathbf{x}\\parallel_\\mathbf{A}^2=\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x}$$\n\nGram-Schmidt orthogonalization을 또 이용해볼까요? 아래와 같습니다.\n\n* $\\mathbf{d}_1=\\mathbf{v}_1$\n* <img src=\"https://www.dropbox.com/s/i8wnvoccefgg7t8/Screen%20Shot%202018-08-14%20at%209.08.42%20AM.png?dl=1\" width=\"400\">\n...\n* <img src=\"https://www.dropbox.com/s/045x2v5sx7s0wot/Screen%20Shot%202018-08-14%20at%209.09.03%20AM.png?dl=1\" width=\"500\">\n\n그런데 이 Gram-Schmidt 방법은 단점이 하나 있습니다. 새로운 vector를 만들어내기 위해서 이전에 만든 vector들을 모두 가지고 있어야 하고 projection도 다시 계산해야 한다는 점입니다. 그런데 마법같은 이유($\\approx$ 여기서 설명하지 않는 이유)로 인해서 오직 마지막으로 만들어낸 vector만 가지고 있어도 기존의 모든 vector들이 성질을 표현해낼 수 있습니다. 그러면 우리가 만들어낸 vector는 다음과 같이 간단해집니다.\n\n<center> <img src=\"https://www.dropbox.com/s/wlyu7ax6qt0aw9w/Screen%20Shot%202018-08-14%20at%209.09.43%20AM.png?dl=1\" width=\"550\"> </center>\n\n이 방향이 우리가 업데이트시키고 싶은 방향입니다. 즉 우리는 다음 수식처럼 움직이고자 합니다.\n\n<center> <img src=\"https://www.dropbox.com/s/053o4ocvxsgxfl9/Screen%20Shot%202018-08-14%20at%209.09.56%20AM.png?dl=1\" width=\"180\"> </center>\n\n$d_k$는 방향이고 $\\alpha_{k}$는 step size입니다. 이러한 방법을 일반적으로 line search method라고 부릅니다. 방향은 위에서 구한 conjugate 방향을 이용합니다. 그래서 이 line search를 conjugate gradient method라고 부릅니다. 이 방법의 한가지 중요한 장점은 업데이트가 무조건 n번만 일어난다는 것입니다! n-dimensional space에서는 basis가 n개 밖에 없거든요. 그렇다면 얼마만큼 많이 이 방향으로 움직여야 할까요? 더 이상 $f(\\mathbf{x})$가 감소하지 않을 때까지 움직이는게 좋지 않을까요? 이것은 다시 말하면 gradient가 0이 될 때까지 움직인다는 뜻입니다. 위에서 설명한 residual과도 관계있을 것 같지 않나요?\n \n$\\nabla f(\\mathbf{x})=\\mathbf{A}\\mathbf{x} - \\mathbf{b}$에 $x_k + \\alpha_{k} d_k$를 대입해 봅시다. 아래와 같이 step size를 구할 수 있습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/xikpfgcix64ngmp/Screen%20Shot%202018-08-14%20at%209.13.20%20AM.png?dl=1\" width=\"300\"> </center>\n\n네, 이제 우리는 방향과 step size를 모두 알았습니다. 이제 update만 하면 해를 찾을 수 있겠군요!\n\n<center> <img src=\"https://www.dropbox.com/s/avjw4kbzd4ormv5/conjugate_gradient_wikipedia.png?dl=1\" width=\"200\"> </center>\n\n위키피디아에서 가져온 위의 그림을 봅시다. 그림의 녹색선이 gradient descent이고 빨간선이 conjugate gradient입니다. conjugate gradient는 gradient descent보다 빨리 수렴합니다. 녹색선은 항상 이전 이동 방향과 직각으로 이동합니다. 빨간선은 보다 빠르게 더 낮은 값이 있는 곳으로 이동합니다.\n\n위에서 설명을 생략했지만 왜 conjugate gradient는 빠르게 수렴(n-step)하며 Gram-Schmidt orthogonalization을 할 때도 메모리가 적게 필요할까요? 수학적으로 엄밀하게 설명하기 보다는 개념적으로 설명을 하겠습니다. 그 비밀은 바로 $\\mathbf{A}$-orthogonal 또는 $\\mathbf{A}$-conjugate vector들을 이용한데 있습니다. 이 vector들은 basis라고 했습니다. 즉, 처음에 우리가 구하고자 했던 해 $\\mathbf{x}^*$를 이 vector들을 이용해서 표현할 수 있습니다. 또한 iterative하게 찾아가기 위한 초기 vector $\\mathbf{x}_0$도 이 vector들로 표현할 수 있습니다. 그렇다면 이 둘 간의 오차도 이 vector들도 표현할 수 있게 됩니다. 다음 수식처럼 말입니다.\n\n$$\\mathbf{x}^\\* - \\mathbf{x}_0 = \\epsilon_1\\mathbf{d}_1 + \\epsilon_2\\mathbf{d}_2 + \\cdots + \\epsilon_n\\mathbf{d}_n$$\n\n우리의 목표는 이 오차를 줄이는 것입니다. 각각의 basis마다 오차가 있으며 이들은 서로 independent합니다. 즉, 특정 iteration에서 특정 basis에 대한 오차를 0으로 만들면 다음 번 iteration에서는 이 오차는 영향을 받지 않습니다! 이러한 이유로 n번의 iteration만으로 해를 찾을 수 있고 다른 vector들을 저장할 필요도 없는 것입니다. 선형대수의 아름다움이 느껴지지 않으시나요? 오래되었지만 참 잘 디자인된 기법이라는 생각이 듭니다.\n\n<br><br>\n\n# 6. Experiment\n\n이 논문에서는 natural gradient를 simple MDP와 tetris MDP에 대해서 실험을 진행했습니다. FIM은 다음과 같은 식으로 업데이트합니다.\n\n$$f\\leftarrow f+\\nabla \\log \\pi(a_t; s_t, \\theta)\\nabla \\log \\pi(a_t; s_t, \\theta)^T$$\n\n$T$ length trajectory에 대해서 $f/T$를 이용해 $F$의 기대값($E$)를 구합니다.\n\n<br>\n## 6.1 LQR(Linear Quadratic Regulator)\n\nAgent를 실험할 환경은 다음과 같은 dynamics를 가지고 있습니다.\n\n$x(t+1) = 0.7x(t)+u(t)+\\epsilon(t)$\n\n$u(t)$는 control 신호로서 에이전트의 행동입니다. $\\epsilon$은 noise distribution으로 환경에 가해지는 노이즈입니다. 에이전트의 목표는 적절한 $u(t)$를 통해 $x(t)$를 0으로 유지하는 것입니다. $x(t)$를 0으로 유지하기 위해서 필요한 소모값(cost)는 $x(t)^2$로 정의하며 cost를 최소화하도록 학습합니다. 이 논문에서는 실험할 때 복잡성을 더 해주기 위해 noise distribution인 $\\epsilon$을 dynamics에 추가하였습니다.\n\n이 실험에서 policy는 다음과 같이 설정하였습니다. 파라미터가 2개 밖에 없는 간단한 policy입니다.\n\n$\\pi(u;x,\\theta)\\propto \\exp(\\theta_1s_1x^2+\\theta_2s_2x)$\n\n이 policy를 간단히 그래프로 그려보면 다음과 같습니다. 가로축은 $x$, 세로축은 cost입니다. $\\theta_1$과 $\\theta_2$를 (0.5, 0.5), (1, 0), (0, 1)로 설정하고 $s_1$, $s_2$는, 1로 두었습니다. $x$는 -1~1사이의 범위로 그렸습니다. \n\n<center> <img src='https://www.dropbox.com/s/v69qyrwn7zurk8c/Screenshot%202018-06-08%2014.57.07.png?dl=1' width='500px'> </center>\n\n아래의 그림은 LQR을 학습한 그래프입니다. cost가 $x^2$이기 때문에 cost가 0으로 갈수록 agent는 0에서 안정적으로 머무른다고 볼 수 있습니다. 6개의 선 중에서 오른쪽 세 개가 일반적인 gradient 방법을 사용해서 학습한 결과입니다. 그리고 왼쪽의 세 개의 선이 natural policy gradient를 통해 학습한 곡선입니다. 일반 gradient 방법보다 natural gradient가 훨씬 빠르게 학습합니다(time 축이 log scale인 것을 감안하면 차이가 많이 납니다.). \n\n하지만 문제가 있습니다. NPG를 학습한 세 개의 곡선은 $\\theta$를 rescale 한 것입니다. $\\theta$앞에 곱해지는 숫자에 따라 학습의 과정이 다릅니다. 이 것은 coordinate에 따라 steepest gradient가 다르게 측정된다는 것입니다. 즉, covariant gradient가 아니라는 뜻입니다. 이 논문에서는 natural gradient를 통해 gradient가 covariant하도록 만들고 싶었는데 실패한 것입니다.\n\n<center> <img src=\"https://www.dropbox.com/s/fhn8cgje0rdws0i/Screenshot%202018-06-08%2023.13.37.png?dl=1\" width=\"300px\"> </center>\n\nnatural gradient가 covariant하지 않은 이유는 Fisher Information Matrix가 예상했던 바와는 달리 invariant metric이 아니기 때문입니다. 또한 FIM이 invariant metric이 아닌 이유는 FIM을 계산할 때 $\\rho_s$가 곱해지기 때문입니다. 하지만 여전히 의의가 있는 것은 기존 gradient 방법들보다 훨씬 빠르게 학습한다는 것입니다!\n\n<br>\n## 6.2 Simple 2-state MDP\n\n이제 다른 예제에서 NPG를 테스트합니다. 2개의 state만 가지는 MDP를 고려해봅시다. [그림출처](http://repository.cmu.edu/cgi/viewcontent.cgi?article=1080&context=robotics). 그림으로보면 다음과 같습니다. $x=0$ 상태와 $x=1$ 상태 두 개가 존재합니다. 에이전트는 각 상태에서 다시 자신의 상태로 되돌아오는 행동을 하거나 다른 상태로 가는 행동을 할 수 있습니다. 상태 $x=0$에서 다시 자기 자신으로 돌아오면 1의 보상을 받고 상태 $x=1$에서 자기 자신으로 돌아오면 2의 보상을 받습니다. 결국 optimal policy는 상태 $x=1$에서 계속 자기 자신으로 돌아오는 행동을 취하는 것입니다. \n\n<img src=\"https://www.dropbox.com/s/g1x9yknzsrip59i/Screenshot%202018-06-08%2023.06.50.png?dl=1\">\n\n문제를 좀 어렵게 만들기 위해 state distribution을 다음과 같이 설정합니다. 즉, 대부분의 경우에 상태 x=0에서 에이전트가 시작하는 것입니다.\n\n$$\\rho(x=0)=0.8,  \\rho(x=1)=0.2$$\n\n일반적인 gradient 방법을 사용하면 다음과 같은 policy gradient 식에 따라서 업데이트를 하게 됩니다. 이 때, $\\rho(s)$가 gradient에 곱해지므로 상대적으로 상태 0에서의 gradient 값이 커지게 됩니다. 따라서 에이전트는 상태 0에서의 gradient(상태 0에서 스스로에게 돌아오는 행동을 취하도록 정책을 업데이트하는 gradient)를 따라 parameterized policy를 update합니다. 따라서 아래 그림의 첫번째 그림에서처럼 reward가 1에서 오랜 시간동안 머무릅니다. 즉, 에이전트가 상태 0에서 self-loop를 계속 돌고 있다는 것입니다.\n\n$$\\nabla\\eta(\\theta)=\\sum_{s,a}\\rho^{\\pi}(s)\\nabla\\pi(a;s,\\theta)Q^{\\pi}(s,a)$$\n\n<center><img src=\"https://www.dropbox.com/s/xtb77mfazbppnss/Screenshot%202018-06-08%2023.14.24.png?dl=1\" width=\"300px\"></center>\n\n하지만 NPG를 사용할 경우에는 훨씬 빠르게 average reward가 2에 도달합니다. gradient 방법이 $1.7\\times 10^7$정도의 시간만에 2에 도달한 반면 NPG는 2의 시간만에 도달합니다. 또한 $\\rho(x=0)$가 $10^{-5}$이하로 떨어지지 않습니다.\n\n한 가지 그래프를 더 살펴봅시다. 다음 그래프는 parameter $\\theta$가 업데이트 되는 과정을 보여줍니다. 이 그래프에서는 parameter가 2개 있습니다. 일반적인 gradient가 아래 그래프에서 실선에 해당합니다. 이 실선의 그래프는 보면 처음부터 중반까지 $\\theta_i$만 거의 업데이트하는 것을 볼 수 있습니다. 그에 비해 NPG는 두 개의 parameter를 균등하게 업데이트하는 것을 볼 수 있습니다. \n\n<center><img src=\"https://www.dropbox.com/s/g7pazozw2k6rd7x/Screenshot%202018-06-08%2023.23.25.png?dl=1\" width=\"300px\"></center>\n\npolicy가 $\\pi(a;s,\\theta)\\propto \\exp(\\theta_{sa})$일 때, 다음과 같이 $F_{-1}$이 gradient 앞에 weight로 곱해지는데 이게 $\\rho$와는 달리 각 parameter에 대해 균등합니다. 따라서 위 그래프에서와 같이 각 parameter는 비슷한 비율로 업데이트가 되는 것입니다.\n\n$$\\bar{\\nabla}\\eta(\\theta) = F^{-1}\\nabla\\eta(\\theta)$$\n\n<br>\n## 6.3 Tetris\n\nNPG를 테스트할 tetris 예제는 Neuro Dynamic Programming 책에 소개되어 있습니다. 다음 그림은 tetris 예제를 보여줍니다. 보통 그림에서와 같이 state의 feature를 정해줍니다. [그림 출처](http://slideplayer.com/slide/5215520/)\n\n<img src=\"https://www.dropbox.com/s/y1halso9yermy8s/Screenshot%202018-06-08%2023.44.34.png?dl=1\">\n\n이 예제에서도 exponential family로 policy를 표현합니다. $\\pi(a;s,\\theta) \\propto \\exp(\\theta^T\\phi_{sa})$ 로 표현합니다.\n\ntetris는 linear function approximator와 greedy policy iteration을 사용할 경우 performance가 갑자기 떨어지는 현상이 있습니다. 밑의 그림에서 A의 spike가 있는 그래프가 이 경우입니다. 그 밑에 낮게 누워있는 그래프는 일반적인 policy gradient 방법입니다. 하지만 Natural policy gradient를 사용할 경우 B 그림에서 오른쪽 그래프와 같이 성능개선이 뚜렷합니다. Policy Iteration 처럼 성능이 뚝 떨어지지 않고 안정적으로 유지합니다. 또한 그림 C에서 보는 것처럼 오른쪽 그래프인 일반적인 gradient 방법보다 훨씬 빠르게 학습하는 것을 볼 수 있습니다.\n\n<img src=\"https://www.dropbox.com/s/pr6s2qrqaic0wyj/Screenshot%202018-06-08%2023.40.16.png?dl=1\">\n\n<br><br>\n\n# 7. Discussion\n\nNatural Gradient Method는  Policy Iteration에서와 같이 Greedy Action을 선택하도록 학습합니다. Line search 기법과 함께 사용하면 더 Policy Iteration과 비슷해집니다. Greedy Policy Iteration에서와 비교하면 Performance Improvement가 보장됩니다. 하지만 FIM이 asymtotically Hessian으로 수렴하지 않습니다. 그렇기 때문에 Conjugate Gradient Method로 구하는 방법이 더 좋을수 있습니다.\n\n살펴봤듯이 본 논문의 NPG는 완벽하지 않습니다. 위의 몇가지 문제점을 극복하기 위한 추가적인 연구가 필요하다고 할 수 있습니다.\n\n<br><br>\n\n# 처음으로\n\n## [PG Travel Guide](https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/)\n\n<br>\n\n# 이전으로\n\n## [DDPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/26/3_ddpg/)\n\n<br>\n\n# 다음으로\n\n## [NPG Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/tnpg.py)\n\n## [TRPO 여행하기](https://reinforcement-learning-kr.github.io/2018/06/24/5_trpo/)","source":"_posts/4_npg.md","raw":"---\ntitle: Natural Policy Gradient\ndate: 2018-06-25 11:36:45\ntags: [\"프로젝트\", \"피지여행\"]\ncategories: 프로젝트\nauthor: 김동민, 이동민, 이웅원, 차금강\nsubtitle: 피지여행 4번째 논문\n---\n\n<center> <img src=\"https://www.dropbox.com/s/yd0x14ljrhpnj1b/Screen%20Shot%202018-07-18%20at%201.08.05%20AM.png?dl=1\" width=\"600\"> </center>\n\n논문 저자 : Sham Kakade\n논문 링크 : https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf\nProceeding : Advances in Neural Information Processing Systems (NIPS) 2002\n정리 : 김동민, 이동민, 이웅원, 차금강\n\n---\n\n# 1. 들어가며...\n\n이 논문이 발표된 2002년 당시에도 많은 연구자들이 objective function의 gradient 값을 따라서 좋은 policy $\\pi$를 찾고자 하였습니다. 하지만 기존의 우리가 알던 gradient descent method는 steepest descent direction이 아닐 수 있기 때문에(쉽게 말해 가장 가파른 방향을 따라서 내려가야 하는데 그러지 못할 수도 있다는 것입니다.) 이 논문에서 steepest descent direction를 나타내는 natural gradient method를 policy gradient에 적용하여 좋은 policy $\\pi$를 찾습니다. \n\n<br>\n## 1.1 NPG 흐름 잡기\n\n### 1.1.1 매니폴드(manifold)\n\n<center> <img src=\"https://www.dropbox.com/s/cstjgemqpby4ysr/Screen%20Shot%202018-08-12%20at%208.28.29%20PM.png?dl=1\" width=\"300\"> </center>\n\n매니폴드는 간단하게 말해 위의 그림에서의 점들을 아우르는 subspace입니다. 그래서 NPG 공부하기 전에 매니폴드를 왜 배울까라는 의문이 들으실텐데 그 이유는 위에서도 언급했듯이 natural gradient method는 어떠한 파라미터 공간에서의 steepest descent direction을 강조하기 때문입니다. 여기서 파라미터 공간이 바로 리만 매니폴드입니다. 리만 매니폴드는 매니폴드가 각지지 않고 미분가능하게 부드럽게 곡률을 가진 면이라고 생각하면 편합니다.\n\nNeural Network(NN)을 사용할 경우 NN의 parameter space가 우리가 보통 생각하는 직선으로 쭉쭉 뻗어있는 유클리디안 공간(Euclidean space)가 아닙니다. 좀 더 일반적으로는 구의 표면과 같이 휘어져있는 공간 즉, 리만 공간(Riemannian space)로 표현할 수 있습니다. 다음 그림들을 보겠습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/jnlm6he18ar64yc/Screen%20Shot%202018-08-12%20at%208.14.13%20PM.png?dl=1\" width=\"400\"> </center>\n\n<center> <img src=\"https://www.dropbox.com/s/tufjqmaeaz29kaz/Screen%20Shot%202018-08-12%20at%208.15.53%20PM.png?dl=1\" width=\"400\"> </center>\n\n아래의 그림처럼 어떠한 확률 분포가 있다고 해봅시다.\n\n<center> <img src=\"https://www.dropbox.com/s/mpoop19eyu1vp0a/Screen%20Shot%202018-08-13%20at%2012.52.09%20PM.png?dl=1\" width=\"600\"> </center>\n\n어떠한 공간의 확률 분포에 있는 한 점은 다른 공간의 확률 분포에서의 한 점이 될 것입니다. 이렇게 위의 그림들과 같이 유클리디안 공간의 확률 분포에서의 한 점이 리만 공간의 확률 분포의 한 점이 되는 것이고, 곡률의 일차 근사가 유클리디안 공간에서는 일차 근사가 아닌 이차 근사이고, 리만 공간에서는 휘어진 공간이기 때문에 곡률을 일차 근사라고 보는 것입니다. 추가적으로 일차 근사와 이차 근사의 차이는 [다크 프로그래머님의 블로그](http://darkpgmr.tistory.com/149)를 참고해주시기 바랍니다. (꼭 보세요!) \n\n일차 근사와 이차 근사의 차이점과 각각의 장단점, 그리고 추가적으로 Line Search까지 알고 난 후에 이 논문을 보시는 것을 권장해드립니다.\n\n### 1.1.2 Natural Gradient + Policy Gradient\n\n먼저 아래의 그림들을 보여드리겠습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/65hra43zadsubff/Screen%20Shot%202018-08-13%20at%2012.51.45%20PM.png?dl=1\" width=\"500\"> <img src=\"https://www.dropbox.com/s/8g332zceeordqwv/Screen%20Shot%202018-08-13%20at%2012.51.51%20PM.png?dl=1\" width=\"400\"> </center>\n\n위에서도 언급했듯이 natural gradient가 steepest direction이 된다는 연구가 이뤄지고 있었습니다. 강화학습의 policy gradient은 objective function의 gradient를 따라 policy를 업데이트를 합니다. 이 때 policy는 parameterization되는데 이 경우에도 gradient 대신에 natural gradient가 좋다는 것을 실험해보는 논문이 지금 다루고 있는 논문입니다.\n\ngradient가 non-covariant(1차 근사)해서 생기는 문제는 간단히 말하자면 다음과 같습니다. policy가 parameterized된 상황에서는 같은 policy라도 다른 parameter를 가질 수 있습니다. 이 때, steepest direction은 두 경우에 같은 방향을 가리켜야 하는데 non-covariant한 경우 그렇지 못합니다. 이러한 부분이 바로 결국 느린 학습으로 연결이 되는 것입니다.\n\n논문에서 2차미분 방법론들과 짧게 비교합니다. 하지만 개인적인 의견으로 2차미분을 이용한 다른 방법들과의 비교가 생각보다 없는 점이 부족해 보였습니다.(Hessian을 이용한다거나 conjugate gradient method를 이용한다거나). 실험을 통해 Fisher Information Matrix(FIM)가 hessian에 수렴안하는 거라던지 Hessian 방법론이 local maxima 부근에서 상당히 느리다던지의 결과를 보여줬었으면 좋았을 것 같습니다.\n\n또한 natural gradient의 단점으로 natural gradient 만으로 업데이트하면 policy의 improvement 보장이 안될 수 있습니다. policy의 improvement를 보장하기 위해 line search도 써야하는데 line search를 어떻게 쓰는지에 대한 자세한 언급이 없습니다. 다시 말해 자세한 algorithm 설명이 없다는 것입니다.\n\nnatural policy gradient 논문은 natural gradient + policy gradient를 처음 적용했다는데 의의가 있습니다. 하지만 이 논문이 문제 삼은 gradient는 non-covariant하다라는 문제를 natural gradient를 통해 해결하지는 못했습니다(Experiment를 통해 covariant gradient가 되지 못했다는 것이 보입니다). NPG의 뒤를 잇는 논문이 “covariant policy search”와 “natural actor-critic”에서 covariant(2차 근사)하지 못하다는 것을 해결하기 위해 Fisher Information Matrix를 sample 하나 하나에 대해서 구하는 것이 아니라 trajectory 전체에 대해서 구합니다.\n\n또한 논문은 pg의 두 가지 세팅 중에 average-reward setting(infinite horizon)에서만 NPG를 다룹니다. “covariant policy search” 논문에서는 average-reward setting과 start-state setting 모두에 대해서 npg를 적용합니다.\n\nnatural gradient + policy gradient를 처음 제시했다는 것은 좋지만 npg 학습의 과정을 자세하게 설명하지 않았고 다른 2차 미분 방법들과 비교를 많이 하지 않은 점이 아쉬운 논문이었습니다.\n\n<br><br>\n\n# 2. Introduction\n\n소개는 앞에서 다 했기 때문에 간략하게 다시 한 번 정리하겠습니다. direct policy gradient method는 future reward의 gradient를 따라 policy를 update합니다. 하지만 gradient descent는 non-covariant입니다. 따라서 이 논문에서는 covarient gradient를 제시합니다. 바로 \"Natural Gradient\" 입니다. \n\n또한 natural gradient와 policy iteration의 연관성을 설명합니다. natural policy gradient is moving toward choosing a greedy optimal action (이런 연결점은 아마도 step-size를 덜 신경쓰고 싶어서 그런게 아닌가 싶습니다)\n\n논문의 Introduction 부분에 다음 멘트가 있습니다. 이 글만 봐서는 이해가 안갔는데 Mackay 논문에 좀 더 자세히 나와있었습니다.\n<center> <img src=\"https://www.dropbox.com/s/41xhhr7lgfk24a1/Screenshot%202018-06-10%2011.45.18.png?dl=1\"> </center>\n\n[Mackay](http://www.inference.org.uk/mackay/ica.pdf)논문에서는 다음과 같이 언급하고 있습니다. Back-propagation을 사용할 경우에 learning rate를 dimension에 1/n로 사용하면 수렴한다는 것이 증명됐습니다. 하지만 너무 느리다고 합니다.\n<center> <img src=\"https://www.dropbox.com/s/us9ezc7vxgrkez6/Screenshot%202018-06-10%2011.47.21.png?dl=1\"> </center>\n\n<br><br>\n\n# 3. A Natural Gradient\n\n<br>\n## 3.1 Notation\n\n이 논문에서 제시하는 Notation은 다음과 같습니다.\n\n- MDP : tuple $(S, s_0, A, R, P)$\n- $S$ : a finite set of states\n- $s_0$ : a start state\n- $A$ : a finite set of actions\n- $R$ : reward function $R: S \\times A -> [0, R_{max}]$\n- $\\pi(a;s, \\theta)$ : stochastic policy parameterized by $\\theta$\n- 모든 정책 $\\pi$는 ergodic : stationary distribution $\\rho^{\\pi}$이 잘 정의되어 있다고 봅니다.\n- 이 논문에서는 sutton의 pg 논문의 두 세팅(start-state formulation, average-reward formulation) 중에 두 번째인 average-reward formulation을 가정합니다.\n- performance or average reward : $\\eta(\\pi)=\\sum_{s,a}\\rho^{\\pi}(s)\\pi(a;s)R(s,a)$\n- state-action value : $Q^{\\pi}(s,a)=E_{\\pi}[\\sum_{t=0}^{\\infty}R(s_t, a_t)-\\eta(\\pi)\\vert s_0=s, a_0=a]$ ([Sutton PG](https://reinforcement-learning-kr.github.io/2018/06/28/sutton-pg/) 를 참고해주시기 바랍니다.)\n- 정책이 $\\theta$로 parameterize되어있으므로 performance는 $\\eta(\\pi_{\\theta})$인데 $\\eta(\\theta)$로 씁니다.\n\n<br>\n## 3.2 Natural Gradient\n\nSutton PG 논문의 policy gradient theorem에 따라 exact gradient of the average reward는 다음과 같습니다. 다음 수식이 어떻게 유도되었는지, 어떤 의미인지 모른다면 [Sutton PG](https://reinforcement-learning-kr.github.io/2018/06/28/sutton-pg/)을 통해 제대로 이해하는 것이 좋습니다. Euclidean space에서는 objective function의 일반적인 policy gradient는 Sutton PG에서 논의한 바와 같이 다음과 같이 구할 수 있습니다.\n\n$$\\nabla\\eta(\\pi_\\theta) = \\Sigma_{s,a}\\rho^\\pi(s)\\nabla\\pi(a;s,\\theta)Q^\\pi(s,a)$$\n\n여기서 steepest descent direction of $\\eta(\\theta)$는 $\\eta(\\theta + d\\theta)$를 최소화하는 $d\\theta$로 정의됩니다. 이 때, $\\vert d\\theta \\vert^2$가 일정 크기 이하인 것으로 제약조건을 주는데(held to small constant) Euclidian space에서는 $\\eta(\\theta)$가 steepest direction이지만 Riemannian space에서는 natural gradient가 steepest direction입니다.\n\n추가적으로 결국 우리가 원하는 건 ‘policy 최적화를 좀 더 스마트하게 해 보자!’입니다. Policy 최적화를 잘한다는 것은 Policy를 어느 방향으로 얼만큼 업데이트 하느냐에 따라 달라집니다.\n\n### 3.2.1 Natural gradient 증명\n\nRiemannian space에서 거리는 다음과 같이 정의됩니다. 여기서 $G(\\theta)$는 특정한 양수로 이루어진 matrix입니다. 자세한 내용은 [양의 정부호 행렬(positive definite matrix)이란?](http://bskyvision.com/205)을 참고해주시기 바랍니다.\n\n$$\\vert d\\theta \\vert^2=\\sum_{ij}(\\theta)d\\theta_id\\theta_i=d\\theta^TG(\\theta)d\\theta$$\n\n이 수식은 [Natural gradient works in efficiently in learning](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.452.7280&rep=rep1&type=pdf) 논문에서 증명되어 있습니다. 다음은 natural gradient 증명입니다.\n\nsteepest direction을 구할 때 $\\theta$의 크기를 제약조건으로 줍니다. 제약조건은 다음과 같습니다.\n\n$$\\vert d\\theta \\vert^2 = \\epsilon^2$$\n\n그리고 steepest vector인 $d\\theta$는 다음과 같이 정의할 수 있습니다.\n\n$$d\\theta = \\epsilon a$$\n\n$$\\vert a \\vert^2=a^TG(\\theta)a = 1$$\n\n이 때, $a$가 steepest direction unit vector이 되려면 다음 수식을 최소로 만들어야 합니다.\n\n$$\\eta(\\theta + d\\theta) = \\eta(\\theta) + \\epsilon\\nabla\\eta(\\theta)^Ta$$\n\n위 수식이 제약조건 아래 최소가 되는 $a$를 구하기 위해 Lagrangian method를 사용합니다. Lagrangian method를 모른다면 [위키피디아](https://en.wikipedia.org/wiki/Lagrange_multiplier)를 참고하는 것을 추천합니다. 위 수식이 최소라는 것은 $\\nabla\\eta(\\theta)^Ta$가 최소라는 것입니다.\n\n$$\\frac{\\partial}{\\partial a_i}(\\nabla\\eta(\\theta)^Ta - \\lambda a^TG(\\theta)a)=0$$\n\n따라서 $(\\nabla\\eta(\\theta)^Ta - \\lambda a^TG(\\theta)a)=0$는 상수입니다. 상수를 미분하면 0이므로 이 식을 $a$로 미분합니다. 그러면 다음과 같이 steepest direction을 구한 것입니다.\n\n$$\\nabla\\eta(\\theta) = 2 \\lambda G(\\theta)a$$\n\n$$a=\\frac{1}{2\\lambda}G^{-1}\\nabla\\eta(\\theta)$$\n\n이 때, 다음 식을 natural gradient라고 정의합니다.\n\n$$\\bar{\\nabla}\\eta(\\theta) = G^{-1}\\nabla\\eta(\\theta)$$\n\nnatural gradient를 이용한 업데이트는 다음과 같습니다.\n\n$$\\theta_{t+1}=\\theta_t - \\alpha_tG^{-1}\\nabla\\eta(\\theta)$$\n\n여기까지는 natural gradient의 증명이었습니다. 이 natural gradient를 policy gradient에 적용한 것이 natural policy gradient입니다. natural policy gradient는 다음과 같이 정의됩니다.\n\n$$\\bar{\\nabla}\\eta(\\theta) = F^{-1}\\nabla\\eta(\\theta)$$\n\n여기서 $G$대신 $F$를 사용했는데 $F$는 Fisher information matix입니다. 수식은 다음과 같습니다.\n\n$$F(\\theta) = E_{\\rho^\\pi(s)}[F_s(\\theta)]$$\n\n$$F_s(\\theta)=E_{\\pi(a;s,\\theta)}[\\frac{\\partial log \\pi(a;s, \\theta)}{\\partial \\theta_i}\\frac{\\partial log \\pi(a;s, \\theta)}{\\partial\\theta_j}]$$\n\n### 3.2.2 Fisher Information Matrix에 정의된 Metric\n\n추가적으로 Fisher Information Matrix(FIM)에 대해서 설명하겠습니다. 위의 문제를 우리가 생각하기 쉬운 Neural Network(NN)으로 구성하고 해결할 수 있다고 해봅시다. NN은 여러가지 parameter set들로 구성될 수 있습니다. 게다가 다른 parameter set을 가지지만 같은 policy를 가질 수도 있습니다. 이 경우 steepest direction은 같은 policy이기 때문에 같은 방향을 가리키고 있어야 하는데 non-covariant한 경우 그렇지 못합니다. 떄문에 학습이 느려지고, 이러한 문제를 해결하기 위해 단순히 Positive-Definite Matrix $G(\\theta)$를 사용하지 않고 FIM $F_s(\\theta)$를 사용합니다. 어떠한 확률 변수 $X$가 임의의 매개변수 $\\theta$에 의해 정의되는 분포를 따른다고 하면 $X=x$일때 FIM은 다음과 같이 정의됩니다.\n\n$$F_x(\\theta)=E\\left[\\left(\\dfrac{\\partial}{\\partial\\theta}\\log\\Pr(x|\\theta)\\right)^2\\right]$$\n\n강화학습 관점에서 생각해보면 정보 $x$는 에피소드에 의해 관측된 상태값 $s$이며 매개변수 $\\theta$에 의해 선택될 수 있는 행동에 대한 분포가 나오게 됩니다. 이에 의해 위의 FIM는 다음과 같이 표현할 수 있습니다.\n\n$$F_s(\\theta) \\equiv E_{\\pi(a;s,\\theta)}\\left[\\left(\\dfrac{\\partial}{\\partial\\theta}\\log\\pi(a;s,\\theta)\\right)^2\\right] =E_{\\pi(a;s,\\theta)}\\left[\\dfrac{\\partial \\log\\pi(a;s,\\theta)}{\\partial \\theta_i}\\dfrac{\\partial \\log\\pi(a;s,\\theta)}{\\partial\\theta_j}\\right]$$\n\n그리고 위의 식들을 이용하여 objective function을 정리하면 아래의 식과 같이 표현됩니다.\n\n$$F(\\theta) = E_{\\rho^{\\pi}(s)}[F_s(\\theta)]$$\n\n또한 이 Fisher Information Matrix에 정의된 이 metric은 다음과 같은 성질을 가지고 있습니다.\n\n1. 업데이트되는 파라미터에 의해 구성되는 매니폴드에 기반한 metric입니다.\n2. 확률분포($\\pi(a;s,\\theta)$)를 구성하는 파라미터($\\theta$)의 변화에 독립적입니다.\n3. 마지막으로 positive-definite한 값을 가집니다. 이렇기 때문에 steepest gradient에서 objective function의 방향을 알기 위해 사용한 방법과 같은 방법으로 natural gradient direction을 다음과 같이 구할 수 있는 것입니다.\n\n$$\\bar{\\nabla}\\eta(\\theta) \\equiv F(\\theta)^{-1}\\nabla\\eta(\\theta)$$\n\n<br><br>\n\n# 4. The Natural Gradient and Policy Iteration\n\n4장에서는 Natural gradient를 통한 policy iteration을 수행하여 실제로 정책의 향상이 있는지를 증명합니다. 여기서 $Q^\\pi(s,a)$는 compatible function approximator $f^\\pi(s,a;w)$로 근사됩니다. ([Sutton PG](https://reinforcement-learning-kr.github.io/2018/06/28/sutton-pg/)를 참고해주시기 바랍니다.)\n\n<br>\n## 4.1 Theorem 1: Compatible Function Approximation\n\napproximate하는 함수 $f^{\\pi}(s,a;w)$는 다음과 같습니다.(compatible value function)\n\n$$f^{\\pi}(s,a;w)=w^T\\psi^{\\pi}(s,a)$$\n\n$$\\psi^{\\pi}(s,a) = \\nabla log\\pi(a;s,\\theta)$$\n\n여기서 $[\\nabla \\log\\pi(a;s,\\theta)]_i=\\partial \\log\\pi(a;s,\\theta)/\\partial\\theta_i$입니다. $w$는 원래 approximate하는 함수 $Q$와 $f$의 차이를 줄이도록 학습합니다(mean square error). 수렴한 local minima의 $w$를 $\\bar{w}$라고 가정 하겠습니다. 에러는 다음과 같은 수식으로 나타낼 수 있습니다.\n\n$$\\epsilon(w,\\pi)\\equiv\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)(f^{\\pi}(s,a;w)-Q^{\\pi}(s,a))^2$$\n\n그 때, $\\bar{\\omega} = \\bar{\\nabla}\\eta(\\theta)$이면 function approximator의 gradient 방향과 정책의 gradient 방향이 같다는 것을 의미합니다.\n\n아래의 내용은 위의 Theorem에 대한 증명입니다.\n\n위의 수식이 local minima이면 미분값이 0이 됩니다. $w$에 대해서 미분하면 다음과 같습니다.\n\n$$\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)(\\psi^{\\pi}(s,a)^T\\bar{w}-Q^{\\pi}(s,a))=0$$\n\n$$\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)\\psi^{\\pi}(s,a)^T\\bar{w}=\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)Q^{\\pi}(s,a)$$\n\n이 때, 위 식의 우변은 $\\psi$의 정의에 의해 policy gradient가 됩니다. 또한 왼쪽 항에서는 Fisher information matrix가 나옵니다.\n\n$$F(\\theta)=\\sum_{s,a}\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)\\psi^{\\pi}(s,a)=E_{\\rho^\\pi(s)}[F_s(\\theta)]$$\n\n따라서 다음과 같이 쓸 수 있습니다.\n\n$$F(\\theta)\\bar{w}=\\nabla\\eta(\\theta)$$\n\n$$\\bar{w}=F(\\theta)^{-1}\\nabla\\eta(\\theta)$$\n\n위의 수식은 natural gradient식과 동일합니다. 위의 수식은 policy가 update 될 때, value function approximator의 parameter 방향으로 이동한다는 것을 의미합니다. function approximation이 정확하다면 그 parameter의 natural policy gradient와 inner product가 커야합니다.\n\n<br>\n## 4.2 Theorem 2: Greedy Policy Improvement\n\n이번 장에서는 natrual gradient는 다른 policy iteration 방법처럼 단순히 더 좋은 행동을 고르도록 학습하는 것이 아니라 가장 좋은(greedy) 행동을 고르도록 학습한다는 것을 증명하는 부분입니다. 이것을 일반적인 형태의 policy에 대해서 증명하기 전에 exponential 형태의 policy에 대해서 증명하는 것이 Theorem 2입니다. 특수한 정책을 가지는 상황안에서 학습속도($\\alpha$)를 무한대로 가져감으로서 어떤 action을 선택하는지를 알아봅니다.\n\npolicy를 다음과 같이 정의합니다.\n\n$$\\pi(a;s,\\theta) \\propto \\exp(\\theta^T\\phi_{sa})$$\n\n여기서 $\\bar{\\nabla}\\eta(\\theta)$가 0이 아니고 $\\bar{w}$는 approximation error를 최소화된 $w$라고 가정합니다. 이 상태에서 natural gradient update를 생각해봅시다. 그리고 policy gradient는 gradient ascent임을 기억합시다.\n\n$$\\theta_{t+1}=\\theta_t + \\alpha_t\\bar{\\nabla}\\eta(\\theta)$$\n\n이 때 $\\alpha$가 learning rate로 parameter를 얼마나 업데이트하는지를 결정합니다. 이 값을 무한대로 늘렸을 때 policy가 어떻게 업데이트되는지 생각해봅시다.\n\n$$\\pi_{\\infty}(a;s)=lim_{\\alpha\\rightarrow\\infty}\\pi(a;s,\\theta+\\alpha\\bar{\\nabla}\\eta(\\theta))-(1)$$\n\n이 때,\n\n$$\\pi_{\\infty}=0 \\, \\, \\, if \\, and \\, only \\, if(필요충분조건) \\, \\, \\, a \\notin argmax_{a'}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa'}$$\n\n이라고 말할 수 있습니다.\n\n아래의 내용은 위의 Theorem에 대한 증명입니다.\n\n먼저 function approximator는 앞서 다뤘듯이 아래와 같습니다.\n\n$$f^{\\pi}(s,a;w)=w^T\\psi^{\\pi}(s,a)$$\n\n여기서 function approximator는 Theorem 1에 의해 아래와 같이 쓸 수 있습니다.\n\n$$f^{\\pi}(s,a;w)=\\bar{\\nabla}\\eta(\\theta)^T\\psi^{\\pi}(s,a)$$\n\n$\\theta$의 정의에 의해 $\\psi$는 다음과 같습니다.\n\n$$\\psi^{\\pi}(s,a)=\\phi_{sa}-E_{\\pi(a';s,\\theta)}[\\phi_{sa'}]$$\n\n따라서 function approximator는 다음과 같이 다시 쓸 수 있습니다.\n\n$$f^{\\pi}(s,a;w)=\\bar{\\nabla}\\eta(\\theta)^T(\\phi_{sa}-E_{\\pi(a';s,\\theta)}[\\phi_{sa'}])$$\n\ngreedy policy improvement가 Q function 값 중 가장 큰 값을 가지는 action을 선택하듯이 여기서도 function approximator의 값이 가장 큰 action을 선택하는 상황을 가정해봅시다. 이 때 function approximator의 argmax는 다음과 같이 쓸 수 있습니다.\n\n$$argmax_{a'}f^{\\pi}(s,a)=argmax_{a'}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa'}$$\n\n(1) 식을 다시 살펴봅시다. 그러면 policy의 정의에 따라 다음과 같이 쓸 수 있습니다.\n\n$$\\pi(a;s,\\theta + \\alpha\\bar{\\nabla}\\eta(\\theta)) \\propto exp(\\theta^T\\phi_{sa} + \\alpha\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa})$$\n\n$\\bar{\\nabla}\\eta(\\theta) \\neq 0$이고 $\\alpha\\rightarrow\\infty$이면 exp안의 항 중에서 뒤의 항이 dominate하게 됩니다. 여러 행동 중에 $\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa}$가 가장 큰 행동이 있다면 이 행동의 policy probability가 1이 되고 나머지는 0이 됩니다. 따라서 다음이 성립합니다.\n\n$$\\pi_{\\infty}=0 \\, \\, \\, if \\, and \\, only \\, if \\, \\, \\, a \\notin argmax_{a'}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa'}$$\n\n이 결과로부터 natural policy gradient는 단지 더 좋은 action이 아니라 best action을 고르도록 학습이 됩니다. 반면에 non-covariant gradient(1차미분)에서는 그저 더 좋은 action을 고르도록 학습이 됩니다. 이 natural policy gradient에 대한 결과는 infinite learning rate 세팅에서만 성립합니다. 좀 더 일반적인 경우에 대해서 살펴봅시다.\n\n<br>\n## 4.3 Theorem 3: General Parameterized Policy\n\nTheorem 2에서와는 달리 일반적인 policy를 가정해봅시다(general parameterized policy). Theorem 3는 이 상황에서 natural gradient를 통한 업데이트가 best action를 고르는 방향으로 학습이 된다는 것을 보여줍니다.\n\nnatural gradien에 따른 policy parameter의 업데이트는 다음과 같습니다. $\\bar{w}$는 approximation error를 minimize하는 $w$입니다.\n\n$$\\delta\\theta = \\theta' - \\theta = \\alpha\\bar{\\nabla}\\eta(\\theta)=\\alpha\\bar{w}$$\n\npolicy에 대해서 1차근사를 하면 다음과 같습니다.\n\n$$\\pi(a;s,\\theta')=\\pi(a;s,\\theta)+\\frac{\\partial\\pi(a;s,\\theta)^T}{\\partial\\theta}\\delta\\theta + O(\\delta\\theta^2)$$\n\n$$=\\pi(a;s,\\theta)(1+\\psi(s,a)^T\\delta\\theta) + O(\\delta\\theta^2)$$\n\n$$=\\pi(a;s,\\theta)(1+\\alpha\\psi(s,a)^T\\bar{w}) + O(\\delta\\theta^2)$$\n\n$$=\\pi(a;s,\\theta)(1+\\alpha f^{\\pi}(s,a;\\bar{w}) + O(\\delta\\theta^2)$$\n\npolicy 자체가 function approximator의 크기대로 업데이트가 되므로 local하게 best action의 probability는 커지고 다른 probability의 크기는 작아질 것입니다. 하지만 만약 greedy improvement가 된다하더라도 그게 performance의 improvement를 보장하는 것은 아닙니다. 하지만 line search와 함께 사용할 경우 improvement를 보장할 수 있습니다. 왜 그런지는 처음에 말씀드린 블로그 [다크 프로그래머님의 블로그](http://darkpgmr.tistory.com/149)를 참고해주시기 바랍니다.\n\n<br><br>\n\n# 5. Metrics and Curvatures\n\n$$\\vert d\\theta \\vert^2=\\sum_{ij}(\\theta)d\\theta_id\\theta_i=d\\theta^TG(\\theta)d\\theta$$\n\n이 파트에서는 FIM과 다른 metric 사이의 관계를 다룹니다.\n\n위의 식에 해당하는 G는 Fisher Information Matrix만 사용할 수 있는 것이 아닙니다. Positive-Definite Matrix인 FIM이외의 다른 Matrix도 사용할 수 있습니다. 또한 다양한 파라미터 추정에서 FIM은 Hessian Matrix에 수렴하지 않을 수 있다고 합니다. 이 말은 2nd order(2차 근사) 수렴이 보장되지 않는다는 말입니다.\n\n논문에 있는 내용들을 조금 더 추가적으로 보면 아래와 같이 나옵니다.\n\nIn the different setting of parameter estimation, the Fisher information converges to the ```Hessian```, so it is [asymptotically efficient](https://en.wikipedia.org/wiki/Efficiency_(statistics)) 이지만,\n\n이 논문의 경우, 아마리 논문의 'blind separation case'와 유사한데 이 때는 꼭 asymtotically efficient하지 않다고 말합니다. 이 말은 즉 2nd order 수렴이 보장되지 않는다는 것이다.\n\n[Mackay](http://www.inference.org.uk/mackay/ica.pdf) 논문에서는 Hessian에서 data independant한 term을 metric으로 가져오는 방법을 제안했습니다. 그래서 performance를 2번 미분해보면 아래의 수식과 같습니다. 하지만 다음 식에서는 모든 항이 data dependent합니다(Q가 있으니까). 첫 번째 항이 그나마 FIM과의 관련성이 있을 수 있지만 Q 값이 curvature에 weight를 주는 방식 때문에 다르다고 할 수 있습니다.\n\n$$\\nabla^2\\eta(\\theta)=\\sum_{sa}\\rho^{\\pi}(s)(\\nabla^2\\pi(a;s)Q^{\\pi}(s,a)+\\nabla\\pi(a;s)\\nabla Q^{\\pi}(s,a)^T+\\nabla Q^{\\pi}(s,a)\\nabla\\pi(a;s)^T)$$\n\nhessian은 보통 positive definite가 아닐수도 있습니다. 따라서 local maxima가 될 때까지 Hessian이 사용하기 별로 안좋습니다. 그리고 local maxima에서는 Hessian보다는 Conjugate methods가 더 효율적이라고 합니다.\n\n사실 이 파트에서는 무엇을 말하고 있는지 알기가 어렵습니다. FIM과 Hessian이 관련이 있다는 것을 알겠는데 asymtotically efficient와 같은 내용을 모르므로 내용의 이해가 어려웠습니다.\n\nMackay 논문에서 해당 부분은 다음과 같습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/x4n6z6pdyi7xtb9/Screenshot%202018-06-10%2012.04.13.png?dl=1\"> </center>\n\n<br>\n## 5.1 Fisher Information Matrix(FIM) vs. Hessian\n\nFIM과 Hessian에 대해서 추가적인 설명을 하고자 합니다.\n\n- FIM\n\n일단 정의되려면 space 자체가 stochastic한 성질을 가지고 있어야 합니다. 모든 parameter를 표현하는 함수가 deterministic한 함수가 아니라 확률로 나타낸 분포입니다. (probability distribution)\n\n결국은 FIM에서도 hessian처럼 비슷한 과정을 취하고 싶은 것입니다. 따라서 확률 변수이기 때문에 어떠한 특정 sample을 취하면 그게 항상 다른값이 됩니다. 그래서 FIM에다가 expectation을 취한 것입니다. expectation을 취함으로써 hessian같은 성질을 가집니다. 왜냐하면 expectation을 취함으로써 constant한 값이 되기 때문입니다.\n\n- Hessian\n\n그냥 deterministic한 함수에서 정의됩니다. 그냥 함수를 두 번 미분 한 것이라고 보면 됩니다.\n\n<br>\n## 5.2 Conjugate Gradient Method\n\n추가적으로 Conjugate Gradient Method(CGM)에 대해서 다루고자 합니다.\n\n- 참고자료\n    - https://www.quora.com/What-is-an-intuitive-explanation-of-what-the-conjugate-gradient-method-is\n    - https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf\n    - https://en.wikipedia.org/wiki/Conjugate_gradient_method\n\n### $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$의 해 구하기\n\n$\\mathbf{A}\\mathbf{x} = \\mathbf{b}$의 해를 구하는 문제를 생각해봅시다. $\\mathbf{A}$의 역행렬을 구해서 양변에 곱해주면 $\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}$가 되어 쉽게 해를 구할 수 있습니다. 하지만 $\\mathbf{A}^{-1}$은 계산이 많이 필요 자원소모가 큰 연산입니다. 역행렬을 취하지 않고 해를 구할 수 있는 방법이 있을까요?\n\n위의 방정식에서 $\\mathbf{b}$를 이항시키면 $\\mathbf{A}\\mathbf{x} - \\mathbf{b} = 0$이 됩니다. 최적화문제는 많은 경우 1차 미분이 0이 되는 지점이 해일 확률이 높습니다. $\\mathbf{A}\\mathbf{x} - \\mathbf{b} = 0$을 1차 미분으로 가지는 함수는 무엇일까요?\n\n$$f( \\mathbf{x} ) = \\frac{1}{2}\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x} - \\mathbf{x}^\\mathrm{T}\\mathbf{b}$$\n\n위의 함수는 $\\mathbf{A}\\mathbf{x} - \\mathbf{b}$를 1차 미분값으로 가집니다. 이 때 $\\mathbf{A}$는 symmetric positive definite해야 합니다.\n\n- symmetric: $\\mathbf{A}=\\mathbf{A}^\\mathrm{T}$\n- positive definite: $\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x}>0,\\quad\\forall\\mathbf{x}$ or all eigenvalues of $\\mathbf{A}$ are positive\n\nsymmetric positive definite한 성질은 $\\mathbf{x}$를 strict convex function이 되게 만들어서 unique solution이 존재하게 합니다. $\\mathbf{A}$는 $\\mathbf{x}$의 Hessian이기도 합니다.\n\n자, 우리는 위의 방정식의 해 $\\mathbf{x}^*$를 iterative한 방식으로 찾고자 합니다. $\\mathbf{x}_0$을 초기값이라고 합시다.\n\n우리는 <img src=\"https://www.dropbox.com/s/g91yqajf72jzjoc/Screen%20Shot%202018-08-14%20at%208.43.28%20AM.png?dl=1\" width=\"80\">가 되는 <img src=\"https://www.dropbox.com/s/aj3j5fjtiyewlo1/Screen%20Shot%202018-08-14%20at%208.43.37%20AM.png?dl=1\" width=\"30\">를 찾고 싶은데 이를 위한 현재의 추정값은 $\\mathbf{A} \\mathbf{x}_0$입니다. 최적의 값을 찾았을 때와 비교하면 현재의 오차는 $\\mathbf{b} - \\mathbf{A}\\mathbf{x}_0 = \\mathbf{r}_0$입니다. 이것을 residual이라고 합니다. 어떻게 효율적으로 값을 변화시켜서 오차를 0으로 만들 수 있을까요? 일단 매 iteration마다 residual이 작아지는 방향으로 나아가야겠죠?\n\n가장 널리 알려진 방법은 gradient descent 방향입니다. gradient가 증가하면 반대로 가고 gradient가 감소하면 그 방향으로 가는 것입니다. 그리고 어떤 방향으로든 gradient가 0이면 그 지점에 멈추는 것이죠. 이 방법은 수렴은 하지만 zigzag하게 움직여서 느립니다. 이것보다 더 좋은 방법이 없을까요?\n\n### Conjugate Gradient Method\n\n#### Gram-Schmidt orthgonalization\n\n다음과 같은 vector들의 집합 <img src=\"https://www.dropbox.com/s/csgdsfofe19q5r6/Screen%20Shot%202018-08-14%20at%208.56.04%20AM.png?dl=1\" width=\"135\">이 있다고 해봅시다. 이 vector들이 서로 linearly independent하다면 이 vector들은 $\\mathbb{R}^n$ 공간 상의 basis입니다. 이 vector들을 이용해서 orthogonal한 vector들을 만들어보겠습니다. 새로운 vector들을 <img src=\"https://www.dropbox.com/s/7xg7pciar1ndu44/Screen%20Shot%202018-08-14%20at%208.58.25%20AM.png?dl=1\" width=\"130\">라고 한다면 아래와 같은 과정을 통해 만들 수 있습니다. 이 방법을 Gram-Schmidt orthogonalization이라고 합니다.\n\n- $\\mathbf{d}_1=\\mathbf{v}_1$\n- $\\mathbf{d}_2=\\mathbf{v}_2 - \\left\\langle\\mathbf{v}_2,\\frac{\\mathbf{d}_1}{\\parallel\\mathbf{d}_1\\parallel}\\right\\rangle\\frac{\\mathbf{d}_1}{\\parallel\\mathbf{d}_1\\parallel}$\n...\n- <img src=\"https://www.dropbox.com/s/arzc5cez8az0j7h/Screen%20Shot%202018-08-14%20at%209.00.04%20AM.png?dl=1\" width=\"290\">\n\n간단히 설명을 하면, 첫 vector는 basis와 같은 방향으로 출발합니다. 그 다음 vector는 그 다음 basis를 이전 vector로 projection한 다음 해당 basis로부터 빼줍니다. 그 다음 vector를 만들 때는 이전에 만든 모든 vector들에 대해서 projection을 취한 다음 모두 더한 것을 해당 basis에서 빼줍니다. \n\n#### $A$-conjugate\n\n이 방법을 이용하여 matrix $A$에 관하여 orthogonal한 새로운 vector들의 집합을 만들어보겠습니다. 일반적인 inner product와 약간 다른 다음과 같은 inner product를 정의할 수 있습니다.\n\n$$<\\mathbf{x},\\mathbf{y}>_\\mathbf{A} = \\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{y}$$\n\n이 때 $<\\mathbf{x},\\mathbf{y}>_\\mathbf{A} = 0$이면 $\\mathbf{x}와 \\mathbf{y}$는 $\\mathbf{A}$-orthogonal 또는 $\\mathbf{A}$-conjugate하다라고 합니다. vector norm $\\parallel\\cdot\\parallel$도 일반적인 vector norm이 아닌 A-norm을 다음과 같이 정의합니다.\n\n$$\\parallel\\mathbf{x}\\parallel_\\mathbf{A}^2=\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x}$$\n\nGram-Schmidt orthogonalization을 또 이용해볼까요? 아래와 같습니다.\n\n* $\\mathbf{d}_1=\\mathbf{v}_1$\n* <img src=\"https://www.dropbox.com/s/i8wnvoccefgg7t8/Screen%20Shot%202018-08-14%20at%209.08.42%20AM.png?dl=1\" width=\"400\">\n...\n* <img src=\"https://www.dropbox.com/s/045x2v5sx7s0wot/Screen%20Shot%202018-08-14%20at%209.09.03%20AM.png?dl=1\" width=\"500\">\n\n그런데 이 Gram-Schmidt 방법은 단점이 하나 있습니다. 새로운 vector를 만들어내기 위해서 이전에 만든 vector들을 모두 가지고 있어야 하고 projection도 다시 계산해야 한다는 점입니다. 그런데 마법같은 이유($\\approx$ 여기서 설명하지 않는 이유)로 인해서 오직 마지막으로 만들어낸 vector만 가지고 있어도 기존의 모든 vector들이 성질을 표현해낼 수 있습니다. 그러면 우리가 만들어낸 vector는 다음과 같이 간단해집니다.\n\n<center> <img src=\"https://www.dropbox.com/s/wlyu7ax6qt0aw9w/Screen%20Shot%202018-08-14%20at%209.09.43%20AM.png?dl=1\" width=\"550\"> </center>\n\n이 방향이 우리가 업데이트시키고 싶은 방향입니다. 즉 우리는 다음 수식처럼 움직이고자 합니다.\n\n<center> <img src=\"https://www.dropbox.com/s/053o4ocvxsgxfl9/Screen%20Shot%202018-08-14%20at%209.09.56%20AM.png?dl=1\" width=\"180\"> </center>\n\n$d_k$는 방향이고 $\\alpha_{k}$는 step size입니다. 이러한 방법을 일반적으로 line search method라고 부릅니다. 방향은 위에서 구한 conjugate 방향을 이용합니다. 그래서 이 line search를 conjugate gradient method라고 부릅니다. 이 방법의 한가지 중요한 장점은 업데이트가 무조건 n번만 일어난다는 것입니다! n-dimensional space에서는 basis가 n개 밖에 없거든요. 그렇다면 얼마만큼 많이 이 방향으로 움직여야 할까요? 더 이상 $f(\\mathbf{x})$가 감소하지 않을 때까지 움직이는게 좋지 않을까요? 이것은 다시 말하면 gradient가 0이 될 때까지 움직인다는 뜻입니다. 위에서 설명한 residual과도 관계있을 것 같지 않나요?\n \n$\\nabla f(\\mathbf{x})=\\mathbf{A}\\mathbf{x} - \\mathbf{b}$에 $x_k + \\alpha_{k} d_k$를 대입해 봅시다. 아래와 같이 step size를 구할 수 있습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/xikpfgcix64ngmp/Screen%20Shot%202018-08-14%20at%209.13.20%20AM.png?dl=1\" width=\"300\"> </center>\n\n네, 이제 우리는 방향과 step size를 모두 알았습니다. 이제 update만 하면 해를 찾을 수 있겠군요!\n\n<center> <img src=\"https://www.dropbox.com/s/avjw4kbzd4ormv5/conjugate_gradient_wikipedia.png?dl=1\" width=\"200\"> </center>\n\n위키피디아에서 가져온 위의 그림을 봅시다. 그림의 녹색선이 gradient descent이고 빨간선이 conjugate gradient입니다. conjugate gradient는 gradient descent보다 빨리 수렴합니다. 녹색선은 항상 이전 이동 방향과 직각으로 이동합니다. 빨간선은 보다 빠르게 더 낮은 값이 있는 곳으로 이동합니다.\n\n위에서 설명을 생략했지만 왜 conjugate gradient는 빠르게 수렴(n-step)하며 Gram-Schmidt orthogonalization을 할 때도 메모리가 적게 필요할까요? 수학적으로 엄밀하게 설명하기 보다는 개념적으로 설명을 하겠습니다. 그 비밀은 바로 $\\mathbf{A}$-orthogonal 또는 $\\mathbf{A}$-conjugate vector들을 이용한데 있습니다. 이 vector들은 basis라고 했습니다. 즉, 처음에 우리가 구하고자 했던 해 $\\mathbf{x}^*$를 이 vector들을 이용해서 표현할 수 있습니다. 또한 iterative하게 찾아가기 위한 초기 vector $\\mathbf{x}_0$도 이 vector들로 표현할 수 있습니다. 그렇다면 이 둘 간의 오차도 이 vector들도 표현할 수 있게 됩니다. 다음 수식처럼 말입니다.\n\n$$\\mathbf{x}^\\* - \\mathbf{x}_0 = \\epsilon_1\\mathbf{d}_1 + \\epsilon_2\\mathbf{d}_2 + \\cdots + \\epsilon_n\\mathbf{d}_n$$\n\n우리의 목표는 이 오차를 줄이는 것입니다. 각각의 basis마다 오차가 있으며 이들은 서로 independent합니다. 즉, 특정 iteration에서 특정 basis에 대한 오차를 0으로 만들면 다음 번 iteration에서는 이 오차는 영향을 받지 않습니다! 이러한 이유로 n번의 iteration만으로 해를 찾을 수 있고 다른 vector들을 저장할 필요도 없는 것입니다. 선형대수의 아름다움이 느껴지지 않으시나요? 오래되었지만 참 잘 디자인된 기법이라는 생각이 듭니다.\n\n<br><br>\n\n# 6. Experiment\n\n이 논문에서는 natural gradient를 simple MDP와 tetris MDP에 대해서 실험을 진행했습니다. FIM은 다음과 같은 식으로 업데이트합니다.\n\n$$f\\leftarrow f+\\nabla \\log \\pi(a_t; s_t, \\theta)\\nabla \\log \\pi(a_t; s_t, \\theta)^T$$\n\n$T$ length trajectory에 대해서 $f/T$를 이용해 $F$의 기대값($E$)를 구합니다.\n\n<br>\n## 6.1 LQR(Linear Quadratic Regulator)\n\nAgent를 실험할 환경은 다음과 같은 dynamics를 가지고 있습니다.\n\n$x(t+1) = 0.7x(t)+u(t)+\\epsilon(t)$\n\n$u(t)$는 control 신호로서 에이전트의 행동입니다. $\\epsilon$은 noise distribution으로 환경에 가해지는 노이즈입니다. 에이전트의 목표는 적절한 $u(t)$를 통해 $x(t)$를 0으로 유지하는 것입니다. $x(t)$를 0으로 유지하기 위해서 필요한 소모값(cost)는 $x(t)^2$로 정의하며 cost를 최소화하도록 학습합니다. 이 논문에서는 실험할 때 복잡성을 더 해주기 위해 noise distribution인 $\\epsilon$을 dynamics에 추가하였습니다.\n\n이 실험에서 policy는 다음과 같이 설정하였습니다. 파라미터가 2개 밖에 없는 간단한 policy입니다.\n\n$\\pi(u;x,\\theta)\\propto \\exp(\\theta_1s_1x^2+\\theta_2s_2x)$\n\n이 policy를 간단히 그래프로 그려보면 다음과 같습니다. 가로축은 $x$, 세로축은 cost입니다. $\\theta_1$과 $\\theta_2$를 (0.5, 0.5), (1, 0), (0, 1)로 설정하고 $s_1$, $s_2$는, 1로 두었습니다. $x$는 -1~1사이의 범위로 그렸습니다. \n\n<center> <img src='https://www.dropbox.com/s/v69qyrwn7zurk8c/Screenshot%202018-06-08%2014.57.07.png?dl=1' width='500px'> </center>\n\n아래의 그림은 LQR을 학습한 그래프입니다. cost가 $x^2$이기 때문에 cost가 0으로 갈수록 agent는 0에서 안정적으로 머무른다고 볼 수 있습니다. 6개의 선 중에서 오른쪽 세 개가 일반적인 gradient 방법을 사용해서 학습한 결과입니다. 그리고 왼쪽의 세 개의 선이 natural policy gradient를 통해 학습한 곡선입니다. 일반 gradient 방법보다 natural gradient가 훨씬 빠르게 학습합니다(time 축이 log scale인 것을 감안하면 차이가 많이 납니다.). \n\n하지만 문제가 있습니다. NPG를 학습한 세 개의 곡선은 $\\theta$를 rescale 한 것입니다. $\\theta$앞에 곱해지는 숫자에 따라 학습의 과정이 다릅니다. 이 것은 coordinate에 따라 steepest gradient가 다르게 측정된다는 것입니다. 즉, covariant gradient가 아니라는 뜻입니다. 이 논문에서는 natural gradient를 통해 gradient가 covariant하도록 만들고 싶었는데 실패한 것입니다.\n\n<center> <img src=\"https://www.dropbox.com/s/fhn8cgje0rdws0i/Screenshot%202018-06-08%2023.13.37.png?dl=1\" width=\"300px\"> </center>\n\nnatural gradient가 covariant하지 않은 이유는 Fisher Information Matrix가 예상했던 바와는 달리 invariant metric이 아니기 때문입니다. 또한 FIM이 invariant metric이 아닌 이유는 FIM을 계산할 때 $\\rho_s$가 곱해지기 때문입니다. 하지만 여전히 의의가 있는 것은 기존 gradient 방법들보다 훨씬 빠르게 학습한다는 것입니다!\n\n<br>\n## 6.2 Simple 2-state MDP\n\n이제 다른 예제에서 NPG를 테스트합니다. 2개의 state만 가지는 MDP를 고려해봅시다. [그림출처](http://repository.cmu.edu/cgi/viewcontent.cgi?article=1080&context=robotics). 그림으로보면 다음과 같습니다. $x=0$ 상태와 $x=1$ 상태 두 개가 존재합니다. 에이전트는 각 상태에서 다시 자신의 상태로 되돌아오는 행동을 하거나 다른 상태로 가는 행동을 할 수 있습니다. 상태 $x=0$에서 다시 자기 자신으로 돌아오면 1의 보상을 받고 상태 $x=1$에서 자기 자신으로 돌아오면 2의 보상을 받습니다. 결국 optimal policy는 상태 $x=1$에서 계속 자기 자신으로 돌아오는 행동을 취하는 것입니다. \n\n<img src=\"https://www.dropbox.com/s/g1x9yknzsrip59i/Screenshot%202018-06-08%2023.06.50.png?dl=1\">\n\n문제를 좀 어렵게 만들기 위해 state distribution을 다음과 같이 설정합니다. 즉, 대부분의 경우에 상태 x=0에서 에이전트가 시작하는 것입니다.\n\n$$\\rho(x=0)=0.8,  \\rho(x=1)=0.2$$\n\n일반적인 gradient 방법을 사용하면 다음과 같은 policy gradient 식에 따라서 업데이트를 하게 됩니다. 이 때, $\\rho(s)$가 gradient에 곱해지므로 상대적으로 상태 0에서의 gradient 값이 커지게 됩니다. 따라서 에이전트는 상태 0에서의 gradient(상태 0에서 스스로에게 돌아오는 행동을 취하도록 정책을 업데이트하는 gradient)를 따라 parameterized policy를 update합니다. 따라서 아래 그림의 첫번째 그림에서처럼 reward가 1에서 오랜 시간동안 머무릅니다. 즉, 에이전트가 상태 0에서 self-loop를 계속 돌고 있다는 것입니다.\n\n$$\\nabla\\eta(\\theta)=\\sum_{s,a}\\rho^{\\pi}(s)\\nabla\\pi(a;s,\\theta)Q^{\\pi}(s,a)$$\n\n<center><img src=\"https://www.dropbox.com/s/xtb77mfazbppnss/Screenshot%202018-06-08%2023.14.24.png?dl=1\" width=\"300px\"></center>\n\n하지만 NPG를 사용할 경우에는 훨씬 빠르게 average reward가 2에 도달합니다. gradient 방법이 $1.7\\times 10^7$정도의 시간만에 2에 도달한 반면 NPG는 2의 시간만에 도달합니다. 또한 $\\rho(x=0)$가 $10^{-5}$이하로 떨어지지 않습니다.\n\n한 가지 그래프를 더 살펴봅시다. 다음 그래프는 parameter $\\theta$가 업데이트 되는 과정을 보여줍니다. 이 그래프에서는 parameter가 2개 있습니다. 일반적인 gradient가 아래 그래프에서 실선에 해당합니다. 이 실선의 그래프는 보면 처음부터 중반까지 $\\theta_i$만 거의 업데이트하는 것을 볼 수 있습니다. 그에 비해 NPG는 두 개의 parameter를 균등하게 업데이트하는 것을 볼 수 있습니다. \n\n<center><img src=\"https://www.dropbox.com/s/g7pazozw2k6rd7x/Screenshot%202018-06-08%2023.23.25.png?dl=1\" width=\"300px\"></center>\n\npolicy가 $\\pi(a;s,\\theta)\\propto \\exp(\\theta_{sa})$일 때, 다음과 같이 $F_{-1}$이 gradient 앞에 weight로 곱해지는데 이게 $\\rho$와는 달리 각 parameter에 대해 균등합니다. 따라서 위 그래프에서와 같이 각 parameter는 비슷한 비율로 업데이트가 되는 것입니다.\n\n$$\\bar{\\nabla}\\eta(\\theta) = F^{-1}\\nabla\\eta(\\theta)$$\n\n<br>\n## 6.3 Tetris\n\nNPG를 테스트할 tetris 예제는 Neuro Dynamic Programming 책에 소개되어 있습니다. 다음 그림은 tetris 예제를 보여줍니다. 보통 그림에서와 같이 state의 feature를 정해줍니다. [그림 출처](http://slideplayer.com/slide/5215520/)\n\n<img src=\"https://www.dropbox.com/s/y1halso9yermy8s/Screenshot%202018-06-08%2023.44.34.png?dl=1\">\n\n이 예제에서도 exponential family로 policy를 표현합니다. $\\pi(a;s,\\theta) \\propto \\exp(\\theta^T\\phi_{sa})$ 로 표현합니다.\n\ntetris는 linear function approximator와 greedy policy iteration을 사용할 경우 performance가 갑자기 떨어지는 현상이 있습니다. 밑의 그림에서 A의 spike가 있는 그래프가 이 경우입니다. 그 밑에 낮게 누워있는 그래프는 일반적인 policy gradient 방법입니다. 하지만 Natural policy gradient를 사용할 경우 B 그림에서 오른쪽 그래프와 같이 성능개선이 뚜렷합니다. Policy Iteration 처럼 성능이 뚝 떨어지지 않고 안정적으로 유지합니다. 또한 그림 C에서 보는 것처럼 오른쪽 그래프인 일반적인 gradient 방법보다 훨씬 빠르게 학습하는 것을 볼 수 있습니다.\n\n<img src=\"https://www.dropbox.com/s/pr6s2qrqaic0wyj/Screenshot%202018-06-08%2023.40.16.png?dl=1\">\n\n<br><br>\n\n# 7. Discussion\n\nNatural Gradient Method는  Policy Iteration에서와 같이 Greedy Action을 선택하도록 학습합니다. Line search 기법과 함께 사용하면 더 Policy Iteration과 비슷해집니다. Greedy Policy Iteration에서와 비교하면 Performance Improvement가 보장됩니다. 하지만 FIM이 asymtotically Hessian으로 수렴하지 않습니다. 그렇기 때문에 Conjugate Gradient Method로 구하는 방법이 더 좋을수 있습니다.\n\n살펴봤듯이 본 논문의 NPG는 완벽하지 않습니다. 위의 몇가지 문제점을 극복하기 위한 추가적인 연구가 필요하다고 할 수 있습니다.\n\n<br><br>\n\n# 처음으로\n\n## [PG Travel Guide](https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/)\n\n<br>\n\n# 이전으로\n\n## [DDPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/26/3_ddpg/)\n\n<br>\n\n# 다음으로\n\n## [NPG Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/tnpg.py)\n\n## [TRPO 여행하기](https://reinforcement-learning-kr.github.io/2018/06/24/5_trpo/)","slug":"4_npg","published":1,"updated":"2018-09-08T11:00:20.606Z","_id":"cjkj6tyrk0005j715nj1djy8z","comments":1,"layout":"post","photos":[],"link":"","content":"<center> <img src=\"https://www.dropbox.com/s/yd0x14ljrhpnj1b/Screen%20Shot%202018-07-18%20at%201.08.05%20AM.png?dl=1\" width=\"600\"> </center>\n\n<p>논문 저자 : Sham Kakade<br>논문 링크 : <a href=\"https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf\" target=\"_blank\" rel=\"noopener\">https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf</a><br>Proceeding : Advances in Neural Information Processing Systems (NIPS) 2002<br>정리 : 김동민, 이동민, 이웅원, 차금강</p>\n<hr>\n<h1 id=\"1-들어가며…\"><a href=\"#1-들어가며…\" class=\"headerlink\" title=\"1. 들어가며…\"></a>1. 들어가며…</h1><p>이 논문이 발표된 2002년 당시에도 많은 연구자들이 objective function의 gradient 값을 따라서 좋은 policy $\\pi$를 찾고자 하였습니다. 하지만 기존의 우리가 알던 gradient descent method는 steepest descent direction이 아닐 수 있기 때문에(쉽게 말해 가장 가파른 방향을 따라서 내려가야 하는데 그러지 못할 수도 있다는 것입니다.) 이 논문에서 steepest descent direction를 나타내는 natural gradient method를 policy gradient에 적용하여 좋은 policy $\\pi$를 찾습니다. </p>\n<p><br></p>\n<h2 id=\"1-1-NPG-흐름-잡기\"><a href=\"#1-1-NPG-흐름-잡기\" class=\"headerlink\" title=\"1.1 NPG 흐름 잡기\"></a>1.1 NPG 흐름 잡기</h2><h3 id=\"1-1-1-매니폴드-manifold\"><a href=\"#1-1-1-매니폴드-manifold\" class=\"headerlink\" title=\"1.1.1 매니폴드(manifold)\"></a>1.1.1 매니폴드(manifold)</h3><center> <img src=\"https://www.dropbox.com/s/cstjgemqpby4ysr/Screen%20Shot%202018-08-12%20at%208.28.29%20PM.png?dl=1\" width=\"300\"> </center>\n\n<p>매니폴드는 간단하게 말해 위의 그림에서의 점들을 아우르는 subspace입니다. 그래서 NPG 공부하기 전에 매니폴드를 왜 배울까라는 의문이 들으실텐데 그 이유는 위에서도 언급했듯이 natural gradient method는 어떠한 파라미터 공간에서의 steepest descent direction을 강조하기 때문입니다. 여기서 파라미터 공간이 바로 리만 매니폴드입니다. 리만 매니폴드는 매니폴드가 각지지 않고 미분가능하게 부드럽게 곡률을 가진 면이라고 생각하면 편합니다.</p>\n<p>Neural Network(NN)을 사용할 경우 NN의 parameter space가 우리가 보통 생각하는 직선으로 쭉쭉 뻗어있는 유클리디안 공간(Euclidean space)가 아닙니다. 좀 더 일반적으로는 구의 표면과 같이 휘어져있는 공간 즉, 리만 공간(Riemannian space)로 표현할 수 있습니다. 다음 그림들을 보겠습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/jnlm6he18ar64yc/Screen%20Shot%202018-08-12%20at%208.14.13%20PM.png?dl=1\" width=\"400\"> </center>\n\n<center> <img src=\"https://www.dropbox.com/s/tufjqmaeaz29kaz/Screen%20Shot%202018-08-12%20at%208.15.53%20PM.png?dl=1\" width=\"400\"> </center>\n\n<p>아래의 그림처럼 어떠한 확률 분포가 있다고 해봅시다.</p>\n<center> <img src=\"https://www.dropbox.com/s/mpoop19eyu1vp0a/Screen%20Shot%202018-08-13%20at%2012.52.09%20PM.png?dl=1\" width=\"600\"> </center>\n\n<p>어떠한 공간의 확률 분포에 있는 한 점은 다른 공간의 확률 분포에서의 한 점이 될 것입니다. 이렇게 위의 그림들과 같이 유클리디안 공간의 확률 분포에서의 한 점이 리만 공간의 확률 분포의 한 점이 되는 것이고, 곡률의 일차 근사가 유클리디안 공간에서는 일차 근사가 아닌 이차 근사이고, 리만 공간에서는 휘어진 공간이기 때문에 곡률을 일차 근사라고 보는 것입니다. 추가적으로 일차 근사와 이차 근사의 차이는 <a href=\"http://darkpgmr.tistory.com/149\" target=\"_blank\" rel=\"noopener\">다크 프로그래머님의 블로그</a>를 참고해주시기 바랍니다. (꼭 보세요!) </p>\n<p>일차 근사와 이차 근사의 차이점과 각각의 장단점, 그리고 추가적으로 Line Search까지 알고 난 후에 이 논문을 보시는 것을 권장해드립니다.</p>\n<h3 id=\"1-1-2-Natural-Gradient-Policy-Gradient\"><a href=\"#1-1-2-Natural-Gradient-Policy-Gradient\" class=\"headerlink\" title=\"1.1.2 Natural Gradient + Policy Gradient\"></a>1.1.2 Natural Gradient + Policy Gradient</h3><p>먼저 아래의 그림들을 보여드리겠습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/65hra43zadsubff/Screen%20Shot%202018-08-13%20at%2012.51.45%20PM.png?dl=1\" width=\"500\"> <img src=\"https://www.dropbox.com/s/8g332zceeordqwv/Screen%20Shot%202018-08-13%20at%2012.51.51%20PM.png?dl=1\" width=\"400\"> </center>\n\n<p>위에서도 언급했듯이 natural gradient가 steepest direction이 된다는 연구가 이뤄지고 있었습니다. 강화학습의 policy gradient은 objective function의 gradient를 따라 policy를 업데이트를 합니다. 이 때 policy는 parameterization되는데 이 경우에도 gradient 대신에 natural gradient가 좋다는 것을 실험해보는 논문이 지금 다루고 있는 논문입니다.</p>\n<p>gradient가 non-covariant(1차 근사)해서 생기는 문제는 간단히 말하자면 다음과 같습니다. policy가 parameterized된 상황에서는 같은 policy라도 다른 parameter를 가질 수 있습니다. 이 때, steepest direction은 두 경우에 같은 방향을 가리켜야 하는데 non-covariant한 경우 그렇지 못합니다. 이러한 부분이 바로 결국 느린 학습으로 연결이 되는 것입니다.</p>\n<p>논문에서 2차미분 방법론들과 짧게 비교합니다. 하지만 개인적인 의견으로 2차미분을 이용한 다른 방법들과의 비교가 생각보다 없는 점이 부족해 보였습니다.(Hessian을 이용한다거나 conjugate gradient method를 이용한다거나). 실험을 통해 Fisher Information Matrix(FIM)가 hessian에 수렴안하는 거라던지 Hessian 방법론이 local maxima 부근에서 상당히 느리다던지의 결과를 보여줬었으면 좋았을 것 같습니다.</p>\n<p>또한 natural gradient의 단점으로 natural gradient 만으로 업데이트하면 policy의 improvement 보장이 안될 수 있습니다. policy의 improvement를 보장하기 위해 line search도 써야하는데 line search를 어떻게 쓰는지에 대한 자세한 언급이 없습니다. 다시 말해 자세한 algorithm 설명이 없다는 것입니다.</p>\n<p>natural policy gradient 논문은 natural gradient + policy gradient를 처음 적용했다는데 의의가 있습니다. 하지만 이 논문이 문제 삼은 gradient는 non-covariant하다라는 문제를 natural gradient를 통해 해결하지는 못했습니다(Experiment를 통해 covariant gradient가 되지 못했다는 것이 보입니다). NPG의 뒤를 잇는 논문이 “covariant policy search”와 “natural actor-critic”에서 covariant(2차 근사)하지 못하다는 것을 해결하기 위해 Fisher Information Matrix를 sample 하나 하나에 대해서 구하는 것이 아니라 trajectory 전체에 대해서 구합니다.</p>\n<p>또한 논문은 pg의 두 가지 세팅 중에 average-reward setting(infinite horizon)에서만 NPG를 다룹니다. “covariant policy search” 논문에서는 average-reward setting과 start-state setting 모두에 대해서 npg를 적용합니다.</p>\n<p>natural gradient + policy gradient를 처음 제시했다는 것은 좋지만 npg 학습의 과정을 자세하게 설명하지 않았고 다른 2차 미분 방법들과 비교를 많이 하지 않은 점이 아쉬운 논문이었습니다.</p>\n<p><br><br></p>\n<h1 id=\"2-Introduction\"><a href=\"#2-Introduction\" class=\"headerlink\" title=\"2. Introduction\"></a>2. Introduction</h1><p>소개는 앞에서 다 했기 때문에 간략하게 다시 한 번 정리하겠습니다. direct policy gradient method는 future reward의 gradient를 따라 policy를 update합니다. 하지만 gradient descent는 non-covariant입니다. 따라서 이 논문에서는 covarient gradient를 제시합니다. 바로 “Natural Gradient” 입니다. </p>\n<p>또한 natural gradient와 policy iteration의 연관성을 설명합니다. natural policy gradient is moving toward choosing a greedy optimal action (이런 연결점은 아마도 step-size를 덜 신경쓰고 싶어서 그런게 아닌가 싶습니다)</p>\n<p>논문의 Introduction 부분에 다음 멘트가 있습니다. 이 글만 봐서는 이해가 안갔는데 Mackay 논문에 좀 더 자세히 나와있었습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/41xhhr7lgfk24a1/Screenshot%202018-06-10%2011.45.18.png?dl=1\"> </center>\n\n<p><a href=\"http://www.inference.org.uk/mackay/ica.pdf\" target=\"_blank\" rel=\"noopener\">Mackay</a>논문에서는 다음과 같이 언급하고 있습니다. Back-propagation을 사용할 경우에 learning rate를 dimension에 1/n로 사용하면 수렴한다는 것이 증명됐습니다. 하지만 너무 느리다고 합니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/us9ezc7vxgrkez6/Screenshot%202018-06-10%2011.47.21.png?dl=1\"> </center>\n\n<p><br><br></p>\n<h1 id=\"3-A-Natural-Gradient\"><a href=\"#3-A-Natural-Gradient\" class=\"headerlink\" title=\"3. A Natural Gradient\"></a>3. A Natural Gradient</h1><p><br></p>\n<h2 id=\"3-1-Notation\"><a href=\"#3-1-Notation\" class=\"headerlink\" title=\"3.1 Notation\"></a>3.1 Notation</h2><p>이 논문에서 제시하는 Notation은 다음과 같습니다.</p>\n<ul>\n<li>MDP : tuple $(S, s_0, A, R, P)$</li>\n<li>$S$ : a finite set of states</li>\n<li>$s_0$ : a start state</li>\n<li>$A$ : a finite set of actions</li>\n<li>$R$ : reward function $R: S \\times A -&gt; [0, R_{max}]$</li>\n<li>$\\pi(a;s, \\theta)$ : stochastic policy parameterized by $\\theta$</li>\n<li>모든 정책 $\\pi$는 ergodic : stationary distribution $\\rho^{\\pi}$이 잘 정의되어 있다고 봅니다.</li>\n<li>이 논문에서는 sutton의 pg 논문의 두 세팅(start-state formulation, average-reward formulation) 중에 두 번째인 average-reward formulation을 가정합니다.</li>\n<li>performance or average reward : $\\eta(\\pi)=\\sum_{s,a}\\rho^{\\pi}(s)\\pi(a;s)R(s,a)$</li>\n<li>state-action value : $Q^{\\pi}(s,a)=E_{\\pi}[\\sum_{t=0}^{\\infty}R(s_t, a_t)-\\eta(\\pi)\\vert s_0=s, a_0=a]$ (<a href=\"https://reinforcement-learning-kr.github.io/2018/06/28/sutton-pg/\">Sutton PG</a> 를 참고해주시기 바랍니다.)</li>\n<li>정책이 $\\theta$로 parameterize되어있으므로 performance는 $\\eta(\\pi_{\\theta})$인데 $\\eta(\\theta)$로 씁니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-2-Natural-Gradient\"><a href=\"#3-2-Natural-Gradient\" class=\"headerlink\" title=\"3.2 Natural Gradient\"></a>3.2 Natural Gradient</h2><p>Sutton PG 논문의 policy gradient theorem에 따라 exact gradient of the average reward는 다음과 같습니다. 다음 수식이 어떻게 유도되었는지, 어떤 의미인지 모른다면 <a href=\"https://reinforcement-learning-kr.github.io/2018/06/28/sutton-pg/\">Sutton PG</a>을 통해 제대로 이해하는 것이 좋습니다. Euclidean space에서는 objective function의 일반적인 policy gradient는 Sutton PG에서 논의한 바와 같이 다음과 같이 구할 수 있습니다.</p>\n<p>$$\\nabla\\eta(\\pi_\\theta) = \\Sigma_{s,a}\\rho^\\pi(s)\\nabla\\pi(a;s,\\theta)Q^\\pi(s,a)$$</p>\n<p>여기서 steepest descent direction of $\\eta(\\theta)$는 $\\eta(\\theta + d\\theta)$를 최소화하는 $d\\theta$로 정의됩니다. 이 때, $\\vert d\\theta \\vert^2$가 일정 크기 이하인 것으로 제약조건을 주는데(held to small constant) Euclidian space에서는 $\\eta(\\theta)$가 steepest direction이지만 Riemannian space에서는 natural gradient가 steepest direction입니다.</p>\n<p>추가적으로 결국 우리가 원하는 건 ‘policy 최적화를 좀 더 스마트하게 해 보자!’입니다. Policy 최적화를 잘한다는 것은 Policy를 어느 방향으로 얼만큼 업데이트 하느냐에 따라 달라집니다.</p>\n<h3 id=\"3-2-1-Natural-gradient-증명\"><a href=\"#3-2-1-Natural-gradient-증명\" class=\"headerlink\" title=\"3.2.1 Natural gradient 증명\"></a>3.2.1 Natural gradient 증명</h3><p>Riemannian space에서 거리는 다음과 같이 정의됩니다. 여기서 $G(\\theta)$는 특정한 양수로 이루어진 matrix입니다. 자세한 내용은 <a href=\"http://bskyvision.com/205\" target=\"_blank\" rel=\"noopener\">양의 정부호 행렬(positive definite matrix)이란?</a>을 참고해주시기 바랍니다.</p>\n<p>$$\\vert d\\theta \\vert^2=\\sum_{ij}(\\theta)d\\theta_id\\theta_i=d\\theta^TG(\\theta)d\\theta$$</p>\n<p>이 수식은 <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.452.7280&amp;rep=rep1&amp;type=pdf\" target=\"_blank\" rel=\"noopener\">Natural gradient works in efficiently in learning</a> 논문에서 증명되어 있습니다. 다음은 natural gradient 증명입니다.</p>\n<p>steepest direction을 구할 때 $\\theta$의 크기를 제약조건으로 줍니다. 제약조건은 다음과 같습니다.</p>\n<p>$$\\vert d\\theta \\vert^2 = \\epsilon^2$$</p>\n<p>그리고 steepest vector인 $d\\theta$는 다음과 같이 정의할 수 있습니다.</p>\n<p>$$d\\theta = \\epsilon a$$</p>\n<p>$$\\vert a \\vert^2=a^TG(\\theta)a = 1$$</p>\n<p>이 때, $a$가 steepest direction unit vector이 되려면 다음 수식을 최소로 만들어야 합니다.</p>\n<p>$$\\eta(\\theta + d\\theta) = \\eta(\\theta) + \\epsilon\\nabla\\eta(\\theta)^Ta$$</p>\n<p>위 수식이 제약조건 아래 최소가 되는 $a$를 구하기 위해 Lagrangian method를 사용합니다. Lagrangian method를 모른다면 <a href=\"https://en.wikipedia.org/wiki/Lagrange_multiplier\" target=\"_blank\" rel=\"noopener\">위키피디아</a>를 참고하는 것을 추천합니다. 위 수식이 최소라는 것은 $\\nabla\\eta(\\theta)^Ta$가 최소라는 것입니다.</p>\n<p>$$\\frac{\\partial}{\\partial a_i}(\\nabla\\eta(\\theta)^Ta - \\lambda a^TG(\\theta)a)=0$$</p>\n<p>따라서 $(\\nabla\\eta(\\theta)^Ta - \\lambda a^TG(\\theta)a)=0$는 상수입니다. 상수를 미분하면 0이므로 이 식을 $a$로 미분합니다. 그러면 다음과 같이 steepest direction을 구한 것입니다.</p>\n<p>$$\\nabla\\eta(\\theta) = 2 \\lambda G(\\theta)a$$</p>\n<p>$$a=\\frac{1}{2\\lambda}G^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>이 때, 다음 식을 natural gradient라고 정의합니다.</p>\n<p>$$\\bar{\\nabla}\\eta(\\theta) = G^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>natural gradient를 이용한 업데이트는 다음과 같습니다.</p>\n<p>$$\\theta_{t+1}=\\theta_t - \\alpha_tG^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>여기까지는 natural gradient의 증명이었습니다. 이 natural gradient를 policy gradient에 적용한 것이 natural policy gradient입니다. natural policy gradient는 다음과 같이 정의됩니다.</p>\n<p>$$\\bar{\\nabla}\\eta(\\theta) = F^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>여기서 $G$대신 $F$를 사용했는데 $F$는 Fisher information matix입니다. 수식은 다음과 같습니다.</p>\n<p>$$F(\\theta) = E_{\\rho^\\pi(s)}[F_s(\\theta)]$$</p>\n<p>$$F_s(\\theta)=E_{\\pi(a;s,\\theta)}[\\frac{\\partial log \\pi(a;s, \\theta)}{\\partial \\theta_i}\\frac{\\partial log \\pi(a;s, \\theta)}{\\partial\\theta_j}]$$</p>\n<h3 id=\"3-2-2-Fisher-Information-Matrix에-정의된-Metric\"><a href=\"#3-2-2-Fisher-Information-Matrix에-정의된-Metric\" class=\"headerlink\" title=\"3.2.2 Fisher Information Matrix에 정의된 Metric\"></a>3.2.2 Fisher Information Matrix에 정의된 Metric</h3><p>추가적으로 Fisher Information Matrix(FIM)에 대해서 설명하겠습니다. 위의 문제를 우리가 생각하기 쉬운 Neural Network(NN)으로 구성하고 해결할 수 있다고 해봅시다. NN은 여러가지 parameter set들로 구성될 수 있습니다. 게다가 다른 parameter set을 가지지만 같은 policy를 가질 수도 있습니다. 이 경우 steepest direction은 같은 policy이기 때문에 같은 방향을 가리키고 있어야 하는데 non-covariant한 경우 그렇지 못합니다. 떄문에 학습이 느려지고, 이러한 문제를 해결하기 위해 단순히 Positive-Definite Matrix $G(\\theta)$를 사용하지 않고 FIM $F_s(\\theta)$를 사용합니다. 어떠한 확률 변수 $X$가 임의의 매개변수 $\\theta$에 의해 정의되는 분포를 따른다고 하면 $X=x$일때 FIM은 다음과 같이 정의됩니다.</p>\n<p>$$F_x(\\theta)=E\\left[\\left(\\dfrac{\\partial}{\\partial\\theta}\\log\\Pr(x|\\theta)\\right)^2\\right]$$</p>\n<p>강화학습 관점에서 생각해보면 정보 $x$는 에피소드에 의해 관측된 상태값 $s$이며 매개변수 $\\theta$에 의해 선택될 수 있는 행동에 대한 분포가 나오게 됩니다. 이에 의해 위의 FIM는 다음과 같이 표현할 수 있습니다.</p>\n<p>$$F_s(\\theta) \\equiv E_{\\pi(a;s,\\theta)}\\left[\\left(\\dfrac{\\partial}{\\partial\\theta}\\log\\pi(a;s,\\theta)\\right)^2\\right] =E_{\\pi(a;s,\\theta)}\\left[\\dfrac{\\partial \\log\\pi(a;s,\\theta)}{\\partial \\theta_i}\\dfrac{\\partial \\log\\pi(a;s,\\theta)}{\\partial\\theta_j}\\right]$$</p>\n<p>그리고 위의 식들을 이용하여 objective function을 정리하면 아래의 식과 같이 표현됩니다.</p>\n<p>$$F(\\theta) = E_{\\rho^{\\pi}(s)}[F_s(\\theta)]$$</p>\n<p>또한 이 Fisher Information Matrix에 정의된 이 metric은 다음과 같은 성질을 가지고 있습니다.</p>\n<ol>\n<li>업데이트되는 파라미터에 의해 구성되는 매니폴드에 기반한 metric입니다.</li>\n<li>확률분포($\\pi(a;s,\\theta)$)를 구성하는 파라미터($\\theta$)의 변화에 독립적입니다.</li>\n<li>마지막으로 positive-definite한 값을 가집니다. 이렇기 때문에 steepest gradient에서 objective function의 방향을 알기 위해 사용한 방법과 같은 방법으로 natural gradient direction을 다음과 같이 구할 수 있는 것입니다.</li>\n</ol>\n<p>$$\\bar{\\nabla}\\eta(\\theta) \\equiv F(\\theta)^{-1}\\nabla\\eta(\\theta)$$</p>\n<p><br><br></p>\n<h1 id=\"4-The-Natural-Gradient-and-Policy-Iteration\"><a href=\"#4-The-Natural-Gradient-and-Policy-Iteration\" class=\"headerlink\" title=\"4. The Natural Gradient and Policy Iteration\"></a>4. The Natural Gradient and Policy Iteration</h1><p>4장에서는 Natural gradient를 통한 policy iteration을 수행하여 실제로 정책의 향상이 있는지를 증명합니다. 여기서 $Q^\\pi(s,a)$는 compatible function approximator $f^\\pi(s,a;w)$로 근사됩니다. (<a href=\"https://reinforcement-learning-kr.github.io/2018/06/28/sutton-pg/\">Sutton PG</a>를 참고해주시기 바랍니다.)</p>\n<p><br></p>\n<h2 id=\"4-1-Theorem-1-Compatible-Function-Approximation\"><a href=\"#4-1-Theorem-1-Compatible-Function-Approximation\" class=\"headerlink\" title=\"4.1 Theorem 1: Compatible Function Approximation\"></a>4.1 Theorem 1: Compatible Function Approximation</h2><p>approximate하는 함수 $f^{\\pi}(s,a;w)$는 다음과 같습니다.(compatible value function)</p>\n<p>$$f^{\\pi}(s,a;w)=w^T\\psi^{\\pi}(s,a)$$</p>\n<p>$$\\psi^{\\pi}(s,a) = \\nabla log\\pi(a;s,\\theta)$$</p>\n<p>여기서 $[\\nabla \\log\\pi(a;s,\\theta)]_i=\\partial \\log\\pi(a;s,\\theta)/\\partial\\theta_i$입니다. $w$는 원래 approximate하는 함수 $Q$와 $f$의 차이를 줄이도록 학습합니다(mean square error). 수렴한 local minima의 $w$를 $\\bar{w}$라고 가정 하겠습니다. 에러는 다음과 같은 수식으로 나타낼 수 있습니다.</p>\n<p>$$\\epsilon(w,\\pi)\\equiv\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)(f^{\\pi}(s,a;w)-Q^{\\pi}(s,a))^2$$</p>\n<p>그 때, $\\bar{\\omega} = \\bar{\\nabla}\\eta(\\theta)$이면 function approximator의 gradient 방향과 정책의 gradient 방향이 같다는 것을 의미합니다.</p>\n<p>아래의 내용은 위의 Theorem에 대한 증명입니다.</p>\n<p>위의 수식이 local minima이면 미분값이 0이 됩니다. $w$에 대해서 미분하면 다음과 같습니다.</p>\n<p>$$\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)(\\psi^{\\pi}(s,a)^T\\bar{w}-Q^{\\pi}(s,a))=0$$</p>\n<p>$$\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)\\psi^{\\pi}(s,a)^T\\bar{w}=\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)Q^{\\pi}(s,a)$$</p>\n<p>이 때, 위 식의 우변은 $\\psi$의 정의에 의해 policy gradient가 됩니다. 또한 왼쪽 항에서는 Fisher information matrix가 나옵니다.</p>\n<p>$$F(\\theta)=\\sum_{s,a}\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)\\psi^{\\pi}(s,a)=E_{\\rho^\\pi(s)}[F_s(\\theta)]$$</p>\n<p>따라서 다음과 같이 쓸 수 있습니다.</p>\n<p>$$F(\\theta)\\bar{w}=\\nabla\\eta(\\theta)$$</p>\n<p>$$\\bar{w}=F(\\theta)^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>위의 수식은 natural gradient식과 동일합니다. 위의 수식은 policy가 update 될 때, value function approximator의 parameter 방향으로 이동한다는 것을 의미합니다. function approximation이 정확하다면 그 parameter의 natural policy gradient와 inner product가 커야합니다.</p>\n<p><br></p>\n<h2 id=\"4-2-Theorem-2-Greedy-Policy-Improvement\"><a href=\"#4-2-Theorem-2-Greedy-Policy-Improvement\" class=\"headerlink\" title=\"4.2 Theorem 2: Greedy Policy Improvement\"></a>4.2 Theorem 2: Greedy Policy Improvement</h2><p>이번 장에서는 natrual gradient는 다른 policy iteration 방법처럼 단순히 더 좋은 행동을 고르도록 학습하는 것이 아니라 가장 좋은(greedy) 행동을 고르도록 학습한다는 것을 증명하는 부분입니다. 이것을 일반적인 형태의 policy에 대해서 증명하기 전에 exponential 형태의 policy에 대해서 증명하는 것이 Theorem 2입니다. 특수한 정책을 가지는 상황안에서 학습속도($\\alpha$)를 무한대로 가져감으로서 어떤 action을 선택하는지를 알아봅니다.</p>\n<p>policy를 다음과 같이 정의합니다.</p>\n<p>$$\\pi(a;s,\\theta) \\propto \\exp(\\theta^T\\phi_{sa})$$</p>\n<p>여기서 $\\bar{\\nabla}\\eta(\\theta)$가 0이 아니고 $\\bar{w}$는 approximation error를 최소화된 $w$라고 가정합니다. 이 상태에서 natural gradient update를 생각해봅시다. 그리고 policy gradient는 gradient ascent임을 기억합시다.</p>\n<p>$$\\theta_{t+1}=\\theta_t + \\alpha_t\\bar{\\nabla}\\eta(\\theta)$$</p>\n<p>이 때 $\\alpha$가 learning rate로 parameter를 얼마나 업데이트하는지를 결정합니다. 이 값을 무한대로 늘렸을 때 policy가 어떻게 업데이트되는지 생각해봅시다.</p>\n<p>$$\\pi_{\\infty}(a;s)=lim_{\\alpha\\rightarrow\\infty}\\pi(a;s,\\theta+\\alpha\\bar{\\nabla}\\eta(\\theta))-(1)$$</p>\n<p>이 때,</p>\n<p>$$\\pi_{\\infty}=0 \\, \\, \\, if \\, and \\, only \\, if(필요충분조건) \\, \\, \\, a \\notin argmax_{a’}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa’}$$</p>\n<p>이라고 말할 수 있습니다.</p>\n<p>아래의 내용은 위의 Theorem에 대한 증명입니다.</p>\n<p>먼저 function approximator는 앞서 다뤘듯이 아래와 같습니다.</p>\n<p>$$f^{\\pi}(s,a;w)=w^T\\psi^{\\pi}(s,a)$$</p>\n<p>여기서 function approximator는 Theorem 1에 의해 아래와 같이 쓸 수 있습니다.</p>\n<p>$$f^{\\pi}(s,a;w)=\\bar{\\nabla}\\eta(\\theta)^T\\psi^{\\pi}(s,a)$$</p>\n<p>$\\theta$의 정의에 의해 $\\psi$는 다음과 같습니다.</p>\n<p>$$\\psi^{\\pi}(s,a)=\\phi_{sa}-E_{\\pi(a’;s,\\theta)}[\\phi_{sa’}]$$</p>\n<p>따라서 function approximator는 다음과 같이 다시 쓸 수 있습니다.</p>\n<p>$$f^{\\pi}(s,a;w)=\\bar{\\nabla}\\eta(\\theta)^T(\\phi_{sa}-E_{\\pi(a’;s,\\theta)}[\\phi_{sa’}])$$</p>\n<p>greedy policy improvement가 Q function 값 중 가장 큰 값을 가지는 action을 선택하듯이 여기서도 function approximator의 값이 가장 큰 action을 선택하는 상황을 가정해봅시다. 이 때 function approximator의 argmax는 다음과 같이 쓸 수 있습니다.</p>\n<p>$$argmax_{a’}f^{\\pi}(s,a)=argmax_{a’}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa’}$$</p>\n<p>(1) 식을 다시 살펴봅시다. 그러면 policy의 정의에 따라 다음과 같이 쓸 수 있습니다.</p>\n<p>$$\\pi(a;s,\\theta + \\alpha\\bar{\\nabla}\\eta(\\theta)) \\propto exp(\\theta^T\\phi_{sa} + \\alpha\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa})$$</p>\n<p>$\\bar{\\nabla}\\eta(\\theta) \\neq 0$이고 $\\alpha\\rightarrow\\infty$이면 exp안의 항 중에서 뒤의 항이 dominate하게 됩니다. 여러 행동 중에 $\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa}$가 가장 큰 행동이 있다면 이 행동의 policy probability가 1이 되고 나머지는 0이 됩니다. 따라서 다음이 성립합니다.</p>\n<p>$$\\pi_{\\infty}=0 \\, \\, \\, if \\, and \\, only \\, if \\, \\, \\, a \\notin argmax_{a’}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa’}$$</p>\n<p>이 결과로부터 natural policy gradient는 단지 더 좋은 action이 아니라 best action을 고르도록 학습이 됩니다. 반면에 non-covariant gradient(1차미분)에서는 그저 더 좋은 action을 고르도록 학습이 됩니다. 이 natural policy gradient에 대한 결과는 infinite learning rate 세팅에서만 성립합니다. 좀 더 일반적인 경우에 대해서 살펴봅시다.</p>\n<p><br></p>\n<h2 id=\"4-3-Theorem-3-General-Parameterized-Policy\"><a href=\"#4-3-Theorem-3-General-Parameterized-Policy\" class=\"headerlink\" title=\"4.3 Theorem 3: General Parameterized Policy\"></a>4.3 Theorem 3: General Parameterized Policy</h2><p>Theorem 2에서와는 달리 일반적인 policy를 가정해봅시다(general parameterized policy). Theorem 3는 이 상황에서 natural gradient를 통한 업데이트가 best action를 고르는 방향으로 학습이 된다는 것을 보여줍니다.</p>\n<p>natural gradien에 따른 policy parameter의 업데이트는 다음과 같습니다. $\\bar{w}$는 approximation error를 minimize하는 $w$입니다.</p>\n<p>$$\\delta\\theta = \\theta’ - \\theta = \\alpha\\bar{\\nabla}\\eta(\\theta)=\\alpha\\bar{w}$$</p>\n<p>policy에 대해서 1차근사를 하면 다음과 같습니다.</p>\n<p>$$\\pi(a;s,\\theta’)=\\pi(a;s,\\theta)+\\frac{\\partial\\pi(a;s,\\theta)^T}{\\partial\\theta}\\delta\\theta + O(\\delta\\theta^2)$$</p>\n<p>$$=\\pi(a;s,\\theta)(1+\\psi(s,a)^T\\delta\\theta) + O(\\delta\\theta^2)$$</p>\n<p>$$=\\pi(a;s,\\theta)(1+\\alpha\\psi(s,a)^T\\bar{w}) + O(\\delta\\theta^2)$$</p>\n<p>$$=\\pi(a;s,\\theta)(1+\\alpha f^{\\pi}(s,a;\\bar{w}) + O(\\delta\\theta^2)$$</p>\n<p>policy 자체가 function approximator의 크기대로 업데이트가 되므로 local하게 best action의 probability는 커지고 다른 probability의 크기는 작아질 것입니다. 하지만 만약 greedy improvement가 된다하더라도 그게 performance의 improvement를 보장하는 것은 아닙니다. 하지만 line search와 함께 사용할 경우 improvement를 보장할 수 있습니다. 왜 그런지는 처음에 말씀드린 블로그 <a href=\"http://darkpgmr.tistory.com/149\" target=\"_blank\" rel=\"noopener\">다크 프로그래머님의 블로그</a>를 참고해주시기 바랍니다.</p>\n<p><br><br></p>\n<h1 id=\"5-Metrics-and-Curvatures\"><a href=\"#5-Metrics-and-Curvatures\" class=\"headerlink\" title=\"5. Metrics and Curvatures\"></a>5. Metrics and Curvatures</h1><p>$$\\vert d\\theta \\vert^2=\\sum_{ij}(\\theta)d\\theta_id\\theta_i=d\\theta^TG(\\theta)d\\theta$$</p>\n<p>이 파트에서는 FIM과 다른 metric 사이의 관계를 다룹니다.</p>\n<p>위의 식에 해당하는 G는 Fisher Information Matrix만 사용할 수 있는 것이 아닙니다. Positive-Definite Matrix인 FIM이외의 다른 Matrix도 사용할 수 있습니다. 또한 다양한 파라미터 추정에서 FIM은 Hessian Matrix에 수렴하지 않을 수 있다고 합니다. 이 말은 2nd order(2차 근사) 수렴이 보장되지 않는다는 말입니다.</p>\n<p>논문에 있는 내용들을 조금 더 추가적으로 보면 아래와 같이 나옵니다.</p>\n<p>In the different setting of parameter estimation, the Fisher information converges to the <code>Hessian</code>, so it is <a href=\"https://en.wikipedia.org/wiki/Efficiency_(statistics\" target=\"_blank\" rel=\"noopener\">asymptotically efficient</a>) 이지만,</p>\n<p>이 논문의 경우, 아마리 논문의 ‘blind separation case’와 유사한데 이 때는 꼭 asymtotically efficient하지 않다고 말합니다. 이 말은 즉 2nd order 수렴이 보장되지 않는다는 것이다.</p>\n<p><a href=\"http://www.inference.org.uk/mackay/ica.pdf\" target=\"_blank\" rel=\"noopener\">Mackay</a> 논문에서는 Hessian에서 data independant한 term을 metric으로 가져오는 방법을 제안했습니다. 그래서 performance를 2번 미분해보면 아래의 수식과 같습니다. 하지만 다음 식에서는 모든 항이 data dependent합니다(Q가 있으니까). 첫 번째 항이 그나마 FIM과의 관련성이 있을 수 있지만 Q 값이 curvature에 weight를 주는 방식 때문에 다르다고 할 수 있습니다.</p>\n<p>$$\\nabla^2\\eta(\\theta)=\\sum_{sa}\\rho^{\\pi}(s)(\\nabla^2\\pi(a;s)Q^{\\pi}(s,a)+\\nabla\\pi(a;s)\\nabla Q^{\\pi}(s,a)^T+\\nabla Q^{\\pi}(s,a)\\nabla\\pi(a;s)^T)$$</p>\n<p>hessian은 보통 positive definite가 아닐수도 있습니다. 따라서 local maxima가 될 때까지 Hessian이 사용하기 별로 안좋습니다. 그리고 local maxima에서는 Hessian보다는 Conjugate methods가 더 효율적이라고 합니다.</p>\n<p>사실 이 파트에서는 무엇을 말하고 있는지 알기가 어렵습니다. FIM과 Hessian이 관련이 있다는 것을 알겠는데 asymtotically efficient와 같은 내용을 모르므로 내용의 이해가 어려웠습니다.</p>\n<p>Mackay 논문에서 해당 부분은 다음과 같습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/x4n6z6pdyi7xtb9/Screenshot%202018-06-10%2012.04.13.png?dl=1\"> </center>\n\n<p><br></p>\n<h2 id=\"5-1-Fisher-Information-Matrix-FIM-vs-Hessian\"><a href=\"#5-1-Fisher-Information-Matrix-FIM-vs-Hessian\" class=\"headerlink\" title=\"5.1 Fisher Information Matrix(FIM) vs. Hessian\"></a>5.1 Fisher Information Matrix(FIM) vs. Hessian</h2><p>FIM과 Hessian에 대해서 추가적인 설명을 하고자 합니다.</p>\n<ul>\n<li>FIM</li>\n</ul>\n<p>일단 정의되려면 space 자체가 stochastic한 성질을 가지고 있어야 합니다. 모든 parameter를 표현하는 함수가 deterministic한 함수가 아니라 확률로 나타낸 분포입니다. (probability distribution)</p>\n<p>결국은 FIM에서도 hessian처럼 비슷한 과정을 취하고 싶은 것입니다. 따라서 확률 변수이기 때문에 어떠한 특정 sample을 취하면 그게 항상 다른값이 됩니다. 그래서 FIM에다가 expectation을 취한 것입니다. expectation을 취함으로써 hessian같은 성질을 가집니다. 왜냐하면 expectation을 취함으로써 constant한 값이 되기 때문입니다.</p>\n<ul>\n<li>Hessian</li>\n</ul>\n<p>그냥 deterministic한 함수에서 정의됩니다. 그냥 함수를 두 번 미분 한 것이라고 보면 됩니다.</p>\n<p><br></p>\n<h2 id=\"5-2-Conjugate-Gradient-Method\"><a href=\"#5-2-Conjugate-Gradient-Method\" class=\"headerlink\" title=\"5.2 Conjugate Gradient Method\"></a>5.2 Conjugate Gradient Method</h2><p>추가적으로 Conjugate Gradient Method(CGM)에 대해서 다루고자 합니다.</p>\n<ul>\n<li>참고자료<ul>\n<li><a href=\"https://www.quora.com/What-is-an-intuitive-explanation-of-what-the-conjugate-gradient-method-is\" target=\"_blank\" rel=\"noopener\">https://www.quora.com/What-is-an-intuitive-explanation-of-what-the-conjugate-gradient-method-is</a></li>\n<li><a href=\"https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf\" target=\"_blank\" rel=\"noopener\">https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Conjugate_gradient_method\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Conjugate_gradient_method</a></li>\n</ul>\n</li>\n</ul>\n<h3 id=\"mathbf-A-mathbf-x-mathbf-b-의-해-구하기\"><a href=\"#mathbf-A-mathbf-x-mathbf-b-의-해-구하기\" class=\"headerlink\" title=\"$\\mathbf{A}\\mathbf{x} = \\mathbf{b}$의 해 구하기\"></a>$\\mathbf{A}\\mathbf{x} = \\mathbf{b}$의 해 구하기</h3><p>$\\mathbf{A}\\mathbf{x} = \\mathbf{b}$의 해를 구하는 문제를 생각해봅시다. $\\mathbf{A}$의 역행렬을 구해서 양변에 곱해주면 $\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}$가 되어 쉽게 해를 구할 수 있습니다. 하지만 $\\mathbf{A}^{-1}$은 계산이 많이 필요 자원소모가 큰 연산입니다. 역행렬을 취하지 않고 해를 구할 수 있는 방법이 있을까요?</p>\n<p>위의 방정식에서 $\\mathbf{b}$를 이항시키면 $\\mathbf{A}\\mathbf{x} - \\mathbf{b} = 0$이 됩니다. 최적화문제는 많은 경우 1차 미분이 0이 되는 지점이 해일 확률이 높습니다. $\\mathbf{A}\\mathbf{x} - \\mathbf{b} = 0$을 1차 미분으로 가지는 함수는 무엇일까요?</p>\n<p>$$f( \\mathbf{x} ) = \\frac{1}{2}\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x} - \\mathbf{x}^\\mathrm{T}\\mathbf{b}$$</p>\n<p>위의 함수는 $\\mathbf{A}\\mathbf{x} - \\mathbf{b}$를 1차 미분값으로 가집니다. 이 때 $\\mathbf{A}$는 symmetric positive definite해야 합니다.</p>\n<ul>\n<li>symmetric: $\\mathbf{A}=\\mathbf{A}^\\mathrm{T}$</li>\n<li>positive definite: $\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x}&gt;0,\\quad\\forall\\mathbf{x}$ or all eigenvalues of $\\mathbf{A}$ are positive</li>\n</ul>\n<p>symmetric positive definite한 성질은 $\\mathbf{x}$를 strict convex function이 되게 만들어서 unique solution이 존재하게 합니다. $\\mathbf{A}$는 $\\mathbf{x}$의 Hessian이기도 합니다.</p>\n<p>자, 우리는 위의 방정식의 해 $\\mathbf{x}^*$를 iterative한 방식으로 찾고자 합니다. $\\mathbf{x}_0$을 초기값이라고 합시다.</p>\n<p>우리는 <img src=\"https://www.dropbox.com/s/g91yqajf72jzjoc/Screen%20Shot%202018-08-14%20at%208.43.28%20AM.png?dl=1\" width=\"80\">가 되는 <img src=\"https://www.dropbox.com/s/aj3j5fjtiyewlo1/Screen%20Shot%202018-08-14%20at%208.43.37%20AM.png?dl=1\" width=\"30\">를 찾고 싶은데 이를 위한 현재의 추정값은 $\\mathbf{A} \\mathbf{x}_0$입니다. 최적의 값을 찾았을 때와 비교하면 현재의 오차는 $\\mathbf{b} - \\mathbf{A}\\mathbf{x}_0 = \\mathbf{r}_0$입니다. 이것을 residual이라고 합니다. 어떻게 효율적으로 값을 변화시켜서 오차를 0으로 만들 수 있을까요? 일단 매 iteration마다 residual이 작아지는 방향으로 나아가야겠죠?</p>\n<p>가장 널리 알려진 방법은 gradient descent 방향입니다. gradient가 증가하면 반대로 가고 gradient가 감소하면 그 방향으로 가는 것입니다. 그리고 어떤 방향으로든 gradient가 0이면 그 지점에 멈추는 것이죠. 이 방법은 수렴은 하지만 zigzag하게 움직여서 느립니다. 이것보다 더 좋은 방법이 없을까요?</p>\n<h3 id=\"Conjugate-Gradient-Method\"><a href=\"#Conjugate-Gradient-Method\" class=\"headerlink\" title=\"Conjugate Gradient Method\"></a>Conjugate Gradient Method</h3><h4 id=\"Gram-Schmidt-orthgonalization\"><a href=\"#Gram-Schmidt-orthgonalization\" class=\"headerlink\" title=\"Gram-Schmidt orthgonalization\"></a>Gram-Schmidt orthgonalization</h4><p>다음과 같은 vector들의 집합 <img src=\"https://www.dropbox.com/s/csgdsfofe19q5r6/Screen%20Shot%202018-08-14%20at%208.56.04%20AM.png?dl=1\" width=\"135\">이 있다고 해봅시다. 이 vector들이 서로 linearly independent하다면 이 vector들은 $\\mathbb{R}^n$ 공간 상의 basis입니다. 이 vector들을 이용해서 orthogonal한 vector들을 만들어보겠습니다. 새로운 vector들을 <img src=\"https://www.dropbox.com/s/7xg7pciar1ndu44/Screen%20Shot%202018-08-14%20at%208.58.25%20AM.png?dl=1\" width=\"130\">라고 한다면 아래와 같은 과정을 통해 만들 수 있습니다. 이 방법을 Gram-Schmidt orthogonalization이라고 합니다.</p>\n<ul>\n<li>$\\mathbf{d}_1=\\mathbf{v}_1$</li>\n<li>$\\mathbf{d}_2=\\mathbf{v}_2 - \\left\\langle\\mathbf{v}_2,\\frac{\\mathbf{d}_1}{\\parallel\\mathbf{d}_1\\parallel}\\right\\rangle\\frac{\\mathbf{d}_1}{\\parallel\\mathbf{d}_1\\parallel}$<br>…</li>\n<li><img src=\"https://www.dropbox.com/s/arzc5cez8az0j7h/Screen%20Shot%202018-08-14%20at%209.00.04%20AM.png?dl=1\" width=\"290\"></li>\n</ul>\n<p>간단히 설명을 하면, 첫 vector는 basis와 같은 방향으로 출발합니다. 그 다음 vector는 그 다음 basis를 이전 vector로 projection한 다음 해당 basis로부터 빼줍니다. 그 다음 vector를 만들 때는 이전에 만든 모든 vector들에 대해서 projection을 취한 다음 모두 더한 것을 해당 basis에서 빼줍니다. </p>\n<h4 id=\"A-conjugate\"><a href=\"#A-conjugate\" class=\"headerlink\" title=\"$A$-conjugate\"></a>$A$-conjugate</h4><p>이 방법을 이용하여 matrix $A$에 관하여 orthogonal한 새로운 vector들의 집합을 만들어보겠습니다. 일반적인 inner product와 약간 다른 다음과 같은 inner product를 정의할 수 있습니다.</p>\n<p>$$&lt;\\mathbf{x},\\mathbf{y}&gt;_\\mathbf{A} = \\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{y}$$</p>\n<p>이 때 $&lt;\\mathbf{x},\\mathbf{y}&gt;_\\mathbf{A} = 0$이면 $\\mathbf{x}와 \\mathbf{y}$는 $\\mathbf{A}$-orthogonal 또는 $\\mathbf{A}$-conjugate하다라고 합니다. vector norm $\\parallel\\cdot\\parallel$도 일반적인 vector norm이 아닌 A-norm을 다음과 같이 정의합니다.</p>\n<p>$$\\parallel\\mathbf{x}\\parallel_\\mathbf{A}^2=\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x}$$</p>\n<p>Gram-Schmidt orthogonalization을 또 이용해볼까요? 아래와 같습니다.</p>\n<ul>\n<li>$\\mathbf{d}_1=\\mathbf{v}_1$</li>\n<li><img src=\"https://www.dropbox.com/s/i8wnvoccefgg7t8/Screen%20Shot%202018-08-14%20at%209.08.42%20AM.png?dl=1\" width=\"400\"><br>…</li>\n<li><img src=\"https://www.dropbox.com/s/045x2v5sx7s0wot/Screen%20Shot%202018-08-14%20at%209.09.03%20AM.png?dl=1\" width=\"500\"></li>\n</ul>\n<p>그런데 이 Gram-Schmidt 방법은 단점이 하나 있습니다. 새로운 vector를 만들어내기 위해서 이전에 만든 vector들을 모두 가지고 있어야 하고 projection도 다시 계산해야 한다는 점입니다. 그런데 마법같은 이유($\\approx$ 여기서 설명하지 않는 이유)로 인해서 오직 마지막으로 만들어낸 vector만 가지고 있어도 기존의 모든 vector들이 성질을 표현해낼 수 있습니다. 그러면 우리가 만들어낸 vector는 다음과 같이 간단해집니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/wlyu7ax6qt0aw9w/Screen%20Shot%202018-08-14%20at%209.09.43%20AM.png?dl=1\" width=\"550\"> </center>\n\n<p>이 방향이 우리가 업데이트시키고 싶은 방향입니다. 즉 우리는 다음 수식처럼 움직이고자 합니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/053o4ocvxsgxfl9/Screen%20Shot%202018-08-14%20at%209.09.56%20AM.png?dl=1\" width=\"180\"> </center>\n\n<p>$d_k$는 방향이고 $\\alpha_{k}$는 step size입니다. 이러한 방법을 일반적으로 line search method라고 부릅니다. 방향은 위에서 구한 conjugate 방향을 이용합니다. 그래서 이 line search를 conjugate gradient method라고 부릅니다. 이 방법의 한가지 중요한 장점은 업데이트가 무조건 n번만 일어난다는 것입니다! n-dimensional space에서는 basis가 n개 밖에 없거든요. 그렇다면 얼마만큼 많이 이 방향으로 움직여야 할까요? 더 이상 $f(\\mathbf{x})$가 감소하지 않을 때까지 움직이는게 좋지 않을까요? 이것은 다시 말하면 gradient가 0이 될 때까지 움직인다는 뜻입니다. 위에서 설명한 residual과도 관계있을 것 같지 않나요?</p>\n<p>$\\nabla f(\\mathbf{x})=\\mathbf{A}\\mathbf{x} - \\mathbf{b}$에 $x_k + \\alpha_{k} d_k$를 대입해 봅시다. 아래와 같이 step size를 구할 수 있습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/xikpfgcix64ngmp/Screen%20Shot%202018-08-14%20at%209.13.20%20AM.png?dl=1\" width=\"300\"> </center>\n\n<p>네, 이제 우리는 방향과 step size를 모두 알았습니다. 이제 update만 하면 해를 찾을 수 있겠군요!</p>\n<center> <img src=\"https://www.dropbox.com/s/avjw4kbzd4ormv5/conjugate_gradient_wikipedia.png?dl=1\" width=\"200\"> </center>\n\n<p>위키피디아에서 가져온 위의 그림을 봅시다. 그림의 녹색선이 gradient descent이고 빨간선이 conjugate gradient입니다. conjugate gradient는 gradient descent보다 빨리 수렴합니다. 녹색선은 항상 이전 이동 방향과 직각으로 이동합니다. 빨간선은 보다 빠르게 더 낮은 값이 있는 곳으로 이동합니다.</p>\n<p>위에서 설명을 생략했지만 왜 conjugate gradient는 빠르게 수렴(n-step)하며 Gram-Schmidt orthogonalization을 할 때도 메모리가 적게 필요할까요? 수학적으로 엄밀하게 설명하기 보다는 개념적으로 설명을 하겠습니다. 그 비밀은 바로 $\\mathbf{A}$-orthogonal 또는 $\\mathbf{A}$-conjugate vector들을 이용한데 있습니다. 이 vector들은 basis라고 했습니다. 즉, 처음에 우리가 구하고자 했던 해 $\\mathbf{x}^*$를 이 vector들을 이용해서 표현할 수 있습니다. 또한 iterative하게 찾아가기 위한 초기 vector $\\mathbf{x}_0$도 이 vector들로 표현할 수 있습니다. 그렇다면 이 둘 간의 오차도 이 vector들도 표현할 수 있게 됩니다. 다음 수식처럼 말입니다.</p>\n<p>$$\\mathbf{x}^* - \\mathbf{x}_0 = \\epsilon_1\\mathbf{d}_1 + \\epsilon_2\\mathbf{d}_2 + \\cdots + \\epsilon_n\\mathbf{d}_n$$</p>\n<p>우리의 목표는 이 오차를 줄이는 것입니다. 각각의 basis마다 오차가 있으며 이들은 서로 independent합니다. 즉, 특정 iteration에서 특정 basis에 대한 오차를 0으로 만들면 다음 번 iteration에서는 이 오차는 영향을 받지 않습니다! 이러한 이유로 n번의 iteration만으로 해를 찾을 수 있고 다른 vector들을 저장할 필요도 없는 것입니다. 선형대수의 아름다움이 느껴지지 않으시나요? 오래되었지만 참 잘 디자인된 기법이라는 생각이 듭니다.</p>\n<p><br><br></p>\n<h1 id=\"6-Experiment\"><a href=\"#6-Experiment\" class=\"headerlink\" title=\"6. Experiment\"></a>6. Experiment</h1><p>이 논문에서는 natural gradient를 simple MDP와 tetris MDP에 대해서 실험을 진행했습니다. FIM은 다음과 같은 식으로 업데이트합니다.</p>\n<p>$$f\\leftarrow f+\\nabla \\log \\pi(a_t; s_t, \\theta)\\nabla \\log \\pi(a_t; s_t, \\theta)^T$$</p>\n<p>$T$ length trajectory에 대해서 $f/T$를 이용해 $F$의 기대값($E$)를 구합니다.</p>\n<p><br></p>\n<h2 id=\"6-1-LQR-Linear-Quadratic-Regulator\"><a href=\"#6-1-LQR-Linear-Quadratic-Regulator\" class=\"headerlink\" title=\"6.1 LQR(Linear Quadratic Regulator)\"></a>6.1 LQR(Linear Quadratic Regulator)</h2><p>Agent를 실험할 환경은 다음과 같은 dynamics를 가지고 있습니다.</p>\n<p>$x(t+1) = 0.7x(t)+u(t)+\\epsilon(t)$</p>\n<p>$u(t)$는 control 신호로서 에이전트의 행동입니다. $\\epsilon$은 noise distribution으로 환경에 가해지는 노이즈입니다. 에이전트의 목표는 적절한 $u(t)$를 통해 $x(t)$를 0으로 유지하는 것입니다. $x(t)$를 0으로 유지하기 위해서 필요한 소모값(cost)는 $x(t)^2$로 정의하며 cost를 최소화하도록 학습합니다. 이 논문에서는 실험할 때 복잡성을 더 해주기 위해 noise distribution인 $\\epsilon$을 dynamics에 추가하였습니다.</p>\n<p>이 실험에서 policy는 다음과 같이 설정하였습니다. 파라미터가 2개 밖에 없는 간단한 policy입니다.</p>\n<p>$\\pi(u;x,\\theta)\\propto \\exp(\\theta_1s_1x^2+\\theta_2s_2x)$</p>\n<p>이 policy를 간단히 그래프로 그려보면 다음과 같습니다. 가로축은 $x$, 세로축은 cost입니다. $\\theta_1$과 $\\theta_2$를 (0.5, 0.5), (1, 0), (0, 1)로 설정하고 $s_1$, $s_2$는, 1로 두었습니다. $x$는 -1~1사이의 범위로 그렸습니다. </p>\n<center> <img src=\"https://www.dropbox.com/s/v69qyrwn7zurk8c/Screenshot%202018-06-08%2014.57.07.png?dl=1\" width=\"500px\"> </center>\n\n<p>아래의 그림은 LQR을 학습한 그래프입니다. cost가 $x^2$이기 때문에 cost가 0으로 갈수록 agent는 0에서 안정적으로 머무른다고 볼 수 있습니다. 6개의 선 중에서 오른쪽 세 개가 일반적인 gradient 방법을 사용해서 학습한 결과입니다. 그리고 왼쪽의 세 개의 선이 natural policy gradient를 통해 학습한 곡선입니다. 일반 gradient 방법보다 natural gradient가 훨씬 빠르게 학습합니다(time 축이 log scale인 것을 감안하면 차이가 많이 납니다.). </p>\n<p>하지만 문제가 있습니다. NPG를 학습한 세 개의 곡선은 $\\theta$를 rescale 한 것입니다. $\\theta$앞에 곱해지는 숫자에 따라 학습의 과정이 다릅니다. 이 것은 coordinate에 따라 steepest gradient가 다르게 측정된다는 것입니다. 즉, covariant gradient가 아니라는 뜻입니다. 이 논문에서는 natural gradient를 통해 gradient가 covariant하도록 만들고 싶었는데 실패한 것입니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/fhn8cgje0rdws0i/Screenshot%202018-06-08%2023.13.37.png?dl=1\" width=\"300px\"> </center>\n\n<p>natural gradient가 covariant하지 않은 이유는 Fisher Information Matrix가 예상했던 바와는 달리 invariant metric이 아니기 때문입니다. 또한 FIM이 invariant metric이 아닌 이유는 FIM을 계산할 때 $\\rho_s$가 곱해지기 때문입니다. 하지만 여전히 의의가 있는 것은 기존 gradient 방법들보다 훨씬 빠르게 학습한다는 것입니다!</p>\n<p><br></p>\n<h2 id=\"6-2-Simple-2-state-MDP\"><a href=\"#6-2-Simple-2-state-MDP\" class=\"headerlink\" title=\"6.2 Simple 2-state MDP\"></a>6.2 Simple 2-state MDP</h2><p>이제 다른 예제에서 NPG를 테스트합니다. 2개의 state만 가지는 MDP를 고려해봅시다. <a href=\"http://repository.cmu.edu/cgi/viewcontent.cgi?article=1080&amp;context=robotics\" target=\"_blank\" rel=\"noopener\">그림출처</a>. 그림으로보면 다음과 같습니다. $x=0$ 상태와 $x=1$ 상태 두 개가 존재합니다. 에이전트는 각 상태에서 다시 자신의 상태로 되돌아오는 행동을 하거나 다른 상태로 가는 행동을 할 수 있습니다. 상태 $x=0$에서 다시 자기 자신으로 돌아오면 1의 보상을 받고 상태 $x=1$에서 자기 자신으로 돌아오면 2의 보상을 받습니다. 결국 optimal policy는 상태 $x=1$에서 계속 자기 자신으로 돌아오는 행동을 취하는 것입니다. </p>\n<p><img src=\"https://www.dropbox.com/s/g1x9yknzsrip59i/Screenshot%202018-06-08%2023.06.50.png?dl=1\"></p>\n<p>문제를 좀 어렵게 만들기 위해 state distribution을 다음과 같이 설정합니다. 즉, 대부분의 경우에 상태 x=0에서 에이전트가 시작하는 것입니다.</p>\n<p>$$\\rho(x=0)=0.8,  \\rho(x=1)=0.2$$</p>\n<p>일반적인 gradient 방법을 사용하면 다음과 같은 policy gradient 식에 따라서 업데이트를 하게 됩니다. 이 때, $\\rho(s)$가 gradient에 곱해지므로 상대적으로 상태 0에서의 gradient 값이 커지게 됩니다. 따라서 에이전트는 상태 0에서의 gradient(상태 0에서 스스로에게 돌아오는 행동을 취하도록 정책을 업데이트하는 gradient)를 따라 parameterized policy를 update합니다. 따라서 아래 그림의 첫번째 그림에서처럼 reward가 1에서 오랜 시간동안 머무릅니다. 즉, 에이전트가 상태 0에서 self-loop를 계속 돌고 있다는 것입니다.</p>\n<p>$$\\nabla\\eta(\\theta)=\\sum_{s,a}\\rho^{\\pi}(s)\\nabla\\pi(a;s,\\theta)Q^{\\pi}(s,a)$$</p>\n<center><img src=\"https://www.dropbox.com/s/xtb77mfazbppnss/Screenshot%202018-06-08%2023.14.24.png?dl=1\" width=\"300px\"></center>\n\n<p>하지만 NPG를 사용할 경우에는 훨씬 빠르게 average reward가 2에 도달합니다. gradient 방법이 $1.7\\times 10^7$정도의 시간만에 2에 도달한 반면 NPG는 2의 시간만에 도달합니다. 또한 $\\rho(x=0)$가 $10^{-5}$이하로 떨어지지 않습니다.</p>\n<p>한 가지 그래프를 더 살펴봅시다. 다음 그래프는 parameter $\\theta$가 업데이트 되는 과정을 보여줍니다. 이 그래프에서는 parameter가 2개 있습니다. 일반적인 gradient가 아래 그래프에서 실선에 해당합니다. 이 실선의 그래프는 보면 처음부터 중반까지 $\\theta_i$만 거의 업데이트하는 것을 볼 수 있습니다. 그에 비해 NPG는 두 개의 parameter를 균등하게 업데이트하는 것을 볼 수 있습니다. </p>\n<center><img src=\"https://www.dropbox.com/s/g7pazozw2k6rd7x/Screenshot%202018-06-08%2023.23.25.png?dl=1\" width=\"300px\"></center>\n\n<p>policy가 $\\pi(a;s,\\theta)\\propto \\exp(\\theta_{sa})$일 때, 다음과 같이 $F_{-1}$이 gradient 앞에 weight로 곱해지는데 이게 $\\rho$와는 달리 각 parameter에 대해 균등합니다. 따라서 위 그래프에서와 같이 각 parameter는 비슷한 비율로 업데이트가 되는 것입니다.</p>\n<p>$$\\bar{\\nabla}\\eta(\\theta) = F^{-1}\\nabla\\eta(\\theta)$$</p>\n<p><br></p>\n<h2 id=\"6-3-Tetris\"><a href=\"#6-3-Tetris\" class=\"headerlink\" title=\"6.3 Tetris\"></a>6.3 Tetris</h2><p>NPG를 테스트할 tetris 예제는 Neuro Dynamic Programming 책에 소개되어 있습니다. 다음 그림은 tetris 예제를 보여줍니다. 보통 그림에서와 같이 state의 feature를 정해줍니다. <a href=\"http://slideplayer.com/slide/5215520/\" target=\"_blank\" rel=\"noopener\">그림 출처</a></p>\n<p><img src=\"https://www.dropbox.com/s/y1halso9yermy8s/Screenshot%202018-06-08%2023.44.34.png?dl=1\"></p>\n<p>이 예제에서도 exponential family로 policy를 표현합니다. $\\pi(a;s,\\theta) \\propto \\exp(\\theta^T\\phi_{sa})$ 로 표현합니다.</p>\n<p>tetris는 linear function approximator와 greedy policy iteration을 사용할 경우 performance가 갑자기 떨어지는 현상이 있습니다. 밑의 그림에서 A의 spike가 있는 그래프가 이 경우입니다. 그 밑에 낮게 누워있는 그래프는 일반적인 policy gradient 방법입니다. 하지만 Natural policy gradient를 사용할 경우 B 그림에서 오른쪽 그래프와 같이 성능개선이 뚜렷합니다. Policy Iteration 처럼 성능이 뚝 떨어지지 않고 안정적으로 유지합니다. 또한 그림 C에서 보는 것처럼 오른쪽 그래프인 일반적인 gradient 방법보다 훨씬 빠르게 학습하는 것을 볼 수 있습니다.</p>\n<p><img src=\"https://www.dropbox.com/s/pr6s2qrqaic0wyj/Screenshot%202018-06-08%2023.40.16.png?dl=1\"></p>\n<p><br><br></p>\n<h1 id=\"7-Discussion\"><a href=\"#7-Discussion\" class=\"headerlink\" title=\"7. Discussion\"></a>7. Discussion</h1><p>Natural Gradient Method는  Policy Iteration에서와 같이 Greedy Action을 선택하도록 학습합니다. Line search 기법과 함께 사용하면 더 Policy Iteration과 비슷해집니다. Greedy Policy Iteration에서와 비교하면 Performance Improvement가 보장됩니다. 하지만 FIM이 asymtotically Hessian으로 수렴하지 않습니다. 그렇기 때문에 Conjugate Gradient Method로 구하는 방법이 더 좋을수 있습니다.</p>\n<p>살펴봤듯이 본 논문의 NPG는 완벽하지 않습니다. 위의 몇가지 문제점을 극복하기 위한 추가적인 연구가 필요하다고 할 수 있습니다.</p>\n<p><br><br></p>\n<h1 id=\"처음으로\"><a href=\"#처음으로\" class=\"headerlink\" title=\"처음으로\"></a>처음으로</h1><h2 id=\"PG-Travel-Guide\"><a href=\"#PG-Travel-Guide\" class=\"headerlink\" title=\"PG Travel Guide\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/\">PG Travel Guide</a></h2><p><br></p>\n<h1 id=\"이전으로\"><a href=\"#이전으로\" class=\"headerlink\" title=\"이전으로\"></a>이전으로</h1><h2 id=\"DDPG-여행하기\"><a href=\"#DDPG-여행하기\" class=\"headerlink\" title=\"DDPG 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/26/3_ddpg/\">DDPG 여행하기</a></h2><p><br></p>\n<h1 id=\"다음으로\"><a href=\"#다음으로\" class=\"headerlink\" title=\"다음으로\"></a>다음으로</h1><h2 id=\"NPG-Code\"><a href=\"#NPG-Code\" class=\"headerlink\" title=\"NPG Code\"></a><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/tnpg.py\" target=\"_blank\" rel=\"noopener\">NPG Code</a></h2><h2 id=\"TRPO-여행하기\"><a href=\"#TRPO-여행하기\" class=\"headerlink\" title=\"TRPO 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/24/5_trpo/\">TRPO 여행하기</a></h2>","site":{"data":{}},"excerpt":"","more":"<center> <img src=\"https://www.dropbox.com/s/yd0x14ljrhpnj1b/Screen%20Shot%202018-07-18%20at%201.08.05%20AM.png?dl=1\" width=\"600\"> </center>\n\n<p>논문 저자 : Sham Kakade<br>논문 링크 : <a href=\"https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf\" target=\"_blank\" rel=\"noopener\">https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf</a><br>Proceeding : Advances in Neural Information Processing Systems (NIPS) 2002<br>정리 : 김동민, 이동민, 이웅원, 차금강</p>\n<hr>\n<h1 id=\"1-들어가며…\"><a href=\"#1-들어가며…\" class=\"headerlink\" title=\"1. 들어가며…\"></a>1. 들어가며…</h1><p>이 논문이 발표된 2002년 당시에도 많은 연구자들이 objective function의 gradient 값을 따라서 좋은 policy $\\pi$를 찾고자 하였습니다. 하지만 기존의 우리가 알던 gradient descent method는 steepest descent direction이 아닐 수 있기 때문에(쉽게 말해 가장 가파른 방향을 따라서 내려가야 하는데 그러지 못할 수도 있다는 것입니다.) 이 논문에서 steepest descent direction를 나타내는 natural gradient method를 policy gradient에 적용하여 좋은 policy $\\pi$를 찾습니다. </p>\n<p><br></p>\n<h2 id=\"1-1-NPG-흐름-잡기\"><a href=\"#1-1-NPG-흐름-잡기\" class=\"headerlink\" title=\"1.1 NPG 흐름 잡기\"></a>1.1 NPG 흐름 잡기</h2><h3 id=\"1-1-1-매니폴드-manifold\"><a href=\"#1-1-1-매니폴드-manifold\" class=\"headerlink\" title=\"1.1.1 매니폴드(manifold)\"></a>1.1.1 매니폴드(manifold)</h3><center> <img src=\"https://www.dropbox.com/s/cstjgemqpby4ysr/Screen%20Shot%202018-08-12%20at%208.28.29%20PM.png?dl=1\" width=\"300\"> </center>\n\n<p>매니폴드는 간단하게 말해 위의 그림에서의 점들을 아우르는 subspace입니다. 그래서 NPG 공부하기 전에 매니폴드를 왜 배울까라는 의문이 들으실텐데 그 이유는 위에서도 언급했듯이 natural gradient method는 어떠한 파라미터 공간에서의 steepest descent direction을 강조하기 때문입니다. 여기서 파라미터 공간이 바로 리만 매니폴드입니다. 리만 매니폴드는 매니폴드가 각지지 않고 미분가능하게 부드럽게 곡률을 가진 면이라고 생각하면 편합니다.</p>\n<p>Neural Network(NN)을 사용할 경우 NN의 parameter space가 우리가 보통 생각하는 직선으로 쭉쭉 뻗어있는 유클리디안 공간(Euclidean space)가 아닙니다. 좀 더 일반적으로는 구의 표면과 같이 휘어져있는 공간 즉, 리만 공간(Riemannian space)로 표현할 수 있습니다. 다음 그림들을 보겠습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/jnlm6he18ar64yc/Screen%20Shot%202018-08-12%20at%208.14.13%20PM.png?dl=1\" width=\"400\"> </center>\n\n<center> <img src=\"https://www.dropbox.com/s/tufjqmaeaz29kaz/Screen%20Shot%202018-08-12%20at%208.15.53%20PM.png?dl=1\" width=\"400\"> </center>\n\n<p>아래의 그림처럼 어떠한 확률 분포가 있다고 해봅시다.</p>\n<center> <img src=\"https://www.dropbox.com/s/mpoop19eyu1vp0a/Screen%20Shot%202018-08-13%20at%2012.52.09%20PM.png?dl=1\" width=\"600\"> </center>\n\n<p>어떠한 공간의 확률 분포에 있는 한 점은 다른 공간의 확률 분포에서의 한 점이 될 것입니다. 이렇게 위의 그림들과 같이 유클리디안 공간의 확률 분포에서의 한 점이 리만 공간의 확률 분포의 한 점이 되는 것이고, 곡률의 일차 근사가 유클리디안 공간에서는 일차 근사가 아닌 이차 근사이고, 리만 공간에서는 휘어진 공간이기 때문에 곡률을 일차 근사라고 보는 것입니다. 추가적으로 일차 근사와 이차 근사의 차이는 <a href=\"http://darkpgmr.tistory.com/149\" target=\"_blank\" rel=\"noopener\">다크 프로그래머님의 블로그</a>를 참고해주시기 바랍니다. (꼭 보세요!) </p>\n<p>일차 근사와 이차 근사의 차이점과 각각의 장단점, 그리고 추가적으로 Line Search까지 알고 난 후에 이 논문을 보시는 것을 권장해드립니다.</p>\n<h3 id=\"1-1-2-Natural-Gradient-Policy-Gradient\"><a href=\"#1-1-2-Natural-Gradient-Policy-Gradient\" class=\"headerlink\" title=\"1.1.2 Natural Gradient + Policy Gradient\"></a>1.1.2 Natural Gradient + Policy Gradient</h3><p>먼저 아래의 그림들을 보여드리겠습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/65hra43zadsubff/Screen%20Shot%202018-08-13%20at%2012.51.45%20PM.png?dl=1\" width=\"500\"> <img src=\"https://www.dropbox.com/s/8g332zceeordqwv/Screen%20Shot%202018-08-13%20at%2012.51.51%20PM.png?dl=1\" width=\"400\"> </center>\n\n<p>위에서도 언급했듯이 natural gradient가 steepest direction이 된다는 연구가 이뤄지고 있었습니다. 강화학습의 policy gradient은 objective function의 gradient를 따라 policy를 업데이트를 합니다. 이 때 policy는 parameterization되는데 이 경우에도 gradient 대신에 natural gradient가 좋다는 것을 실험해보는 논문이 지금 다루고 있는 논문입니다.</p>\n<p>gradient가 non-covariant(1차 근사)해서 생기는 문제는 간단히 말하자면 다음과 같습니다. policy가 parameterized된 상황에서는 같은 policy라도 다른 parameter를 가질 수 있습니다. 이 때, steepest direction은 두 경우에 같은 방향을 가리켜야 하는데 non-covariant한 경우 그렇지 못합니다. 이러한 부분이 바로 결국 느린 학습으로 연결이 되는 것입니다.</p>\n<p>논문에서 2차미분 방법론들과 짧게 비교합니다. 하지만 개인적인 의견으로 2차미분을 이용한 다른 방법들과의 비교가 생각보다 없는 점이 부족해 보였습니다.(Hessian을 이용한다거나 conjugate gradient method를 이용한다거나). 실험을 통해 Fisher Information Matrix(FIM)가 hessian에 수렴안하는 거라던지 Hessian 방법론이 local maxima 부근에서 상당히 느리다던지의 결과를 보여줬었으면 좋았을 것 같습니다.</p>\n<p>또한 natural gradient의 단점으로 natural gradient 만으로 업데이트하면 policy의 improvement 보장이 안될 수 있습니다. policy의 improvement를 보장하기 위해 line search도 써야하는데 line search를 어떻게 쓰는지에 대한 자세한 언급이 없습니다. 다시 말해 자세한 algorithm 설명이 없다는 것입니다.</p>\n<p>natural policy gradient 논문은 natural gradient + policy gradient를 처음 적용했다는데 의의가 있습니다. 하지만 이 논문이 문제 삼은 gradient는 non-covariant하다라는 문제를 natural gradient를 통해 해결하지는 못했습니다(Experiment를 통해 covariant gradient가 되지 못했다는 것이 보입니다). NPG의 뒤를 잇는 논문이 “covariant policy search”와 “natural actor-critic”에서 covariant(2차 근사)하지 못하다는 것을 해결하기 위해 Fisher Information Matrix를 sample 하나 하나에 대해서 구하는 것이 아니라 trajectory 전체에 대해서 구합니다.</p>\n<p>또한 논문은 pg의 두 가지 세팅 중에 average-reward setting(infinite horizon)에서만 NPG를 다룹니다. “covariant policy search” 논문에서는 average-reward setting과 start-state setting 모두에 대해서 npg를 적용합니다.</p>\n<p>natural gradient + policy gradient를 처음 제시했다는 것은 좋지만 npg 학습의 과정을 자세하게 설명하지 않았고 다른 2차 미분 방법들과 비교를 많이 하지 않은 점이 아쉬운 논문이었습니다.</p>\n<p><br><br></p>\n<h1 id=\"2-Introduction\"><a href=\"#2-Introduction\" class=\"headerlink\" title=\"2. Introduction\"></a>2. Introduction</h1><p>소개는 앞에서 다 했기 때문에 간략하게 다시 한 번 정리하겠습니다. direct policy gradient method는 future reward의 gradient를 따라 policy를 update합니다. 하지만 gradient descent는 non-covariant입니다. 따라서 이 논문에서는 covarient gradient를 제시합니다. 바로 “Natural Gradient” 입니다. </p>\n<p>또한 natural gradient와 policy iteration의 연관성을 설명합니다. natural policy gradient is moving toward choosing a greedy optimal action (이런 연결점은 아마도 step-size를 덜 신경쓰고 싶어서 그런게 아닌가 싶습니다)</p>\n<p>논문의 Introduction 부분에 다음 멘트가 있습니다. 이 글만 봐서는 이해가 안갔는데 Mackay 논문에 좀 더 자세히 나와있었습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/41xhhr7lgfk24a1/Screenshot%202018-06-10%2011.45.18.png?dl=1\"> </center>\n\n<p><a href=\"http://www.inference.org.uk/mackay/ica.pdf\" target=\"_blank\" rel=\"noopener\">Mackay</a>논문에서는 다음과 같이 언급하고 있습니다. Back-propagation을 사용할 경우에 learning rate를 dimension에 1/n로 사용하면 수렴한다는 것이 증명됐습니다. 하지만 너무 느리다고 합니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/us9ezc7vxgrkez6/Screenshot%202018-06-10%2011.47.21.png?dl=1\"> </center>\n\n<p><br><br></p>\n<h1 id=\"3-A-Natural-Gradient\"><a href=\"#3-A-Natural-Gradient\" class=\"headerlink\" title=\"3. A Natural Gradient\"></a>3. A Natural Gradient</h1><p><br></p>\n<h2 id=\"3-1-Notation\"><a href=\"#3-1-Notation\" class=\"headerlink\" title=\"3.1 Notation\"></a>3.1 Notation</h2><p>이 논문에서 제시하는 Notation은 다음과 같습니다.</p>\n<ul>\n<li>MDP : tuple $(S, s_0, A, R, P)$</li>\n<li>$S$ : a finite set of states</li>\n<li>$s_0$ : a start state</li>\n<li>$A$ : a finite set of actions</li>\n<li>$R$ : reward function $R: S \\times A -&gt; [0, R_{max}]$</li>\n<li>$\\pi(a;s, \\theta)$ : stochastic policy parameterized by $\\theta$</li>\n<li>모든 정책 $\\pi$는 ergodic : stationary distribution $\\rho^{\\pi}$이 잘 정의되어 있다고 봅니다.</li>\n<li>이 논문에서는 sutton의 pg 논문의 두 세팅(start-state formulation, average-reward formulation) 중에 두 번째인 average-reward formulation을 가정합니다.</li>\n<li>performance or average reward : $\\eta(\\pi)=\\sum_{s,a}\\rho^{\\pi}(s)\\pi(a;s)R(s,a)$</li>\n<li>state-action value : $Q^{\\pi}(s,a)=E_{\\pi}[\\sum_{t=0}^{\\infty}R(s_t, a_t)-\\eta(\\pi)\\vert s_0=s, a_0=a]$ (<a href=\"https://reinforcement-learning-kr.github.io/2018/06/28/sutton-pg/\">Sutton PG</a> 를 참고해주시기 바랍니다.)</li>\n<li>정책이 $\\theta$로 parameterize되어있으므로 performance는 $\\eta(\\pi_{\\theta})$인데 $\\eta(\\theta)$로 씁니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-2-Natural-Gradient\"><a href=\"#3-2-Natural-Gradient\" class=\"headerlink\" title=\"3.2 Natural Gradient\"></a>3.2 Natural Gradient</h2><p>Sutton PG 논문의 policy gradient theorem에 따라 exact gradient of the average reward는 다음과 같습니다. 다음 수식이 어떻게 유도되었는지, 어떤 의미인지 모른다면 <a href=\"https://reinforcement-learning-kr.github.io/2018/06/28/sutton-pg/\">Sutton PG</a>을 통해 제대로 이해하는 것이 좋습니다. Euclidean space에서는 objective function의 일반적인 policy gradient는 Sutton PG에서 논의한 바와 같이 다음과 같이 구할 수 있습니다.</p>\n<p>$$\\nabla\\eta(\\pi_\\theta) = \\Sigma_{s,a}\\rho^\\pi(s)\\nabla\\pi(a;s,\\theta)Q^\\pi(s,a)$$</p>\n<p>여기서 steepest descent direction of $\\eta(\\theta)$는 $\\eta(\\theta + d\\theta)$를 최소화하는 $d\\theta$로 정의됩니다. 이 때, $\\vert d\\theta \\vert^2$가 일정 크기 이하인 것으로 제약조건을 주는데(held to small constant) Euclidian space에서는 $\\eta(\\theta)$가 steepest direction이지만 Riemannian space에서는 natural gradient가 steepest direction입니다.</p>\n<p>추가적으로 결국 우리가 원하는 건 ‘policy 최적화를 좀 더 스마트하게 해 보자!’입니다. Policy 최적화를 잘한다는 것은 Policy를 어느 방향으로 얼만큼 업데이트 하느냐에 따라 달라집니다.</p>\n<h3 id=\"3-2-1-Natural-gradient-증명\"><a href=\"#3-2-1-Natural-gradient-증명\" class=\"headerlink\" title=\"3.2.1 Natural gradient 증명\"></a>3.2.1 Natural gradient 증명</h3><p>Riemannian space에서 거리는 다음과 같이 정의됩니다. 여기서 $G(\\theta)$는 특정한 양수로 이루어진 matrix입니다. 자세한 내용은 <a href=\"http://bskyvision.com/205\" target=\"_blank\" rel=\"noopener\">양의 정부호 행렬(positive definite matrix)이란?</a>을 참고해주시기 바랍니다.</p>\n<p>$$\\vert d\\theta \\vert^2=\\sum_{ij}(\\theta)d\\theta_id\\theta_i=d\\theta^TG(\\theta)d\\theta$$</p>\n<p>이 수식은 <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.452.7280&amp;rep=rep1&amp;type=pdf\" target=\"_blank\" rel=\"noopener\">Natural gradient works in efficiently in learning</a> 논문에서 증명되어 있습니다. 다음은 natural gradient 증명입니다.</p>\n<p>steepest direction을 구할 때 $\\theta$의 크기를 제약조건으로 줍니다. 제약조건은 다음과 같습니다.</p>\n<p>$$\\vert d\\theta \\vert^2 = \\epsilon^2$$</p>\n<p>그리고 steepest vector인 $d\\theta$는 다음과 같이 정의할 수 있습니다.</p>\n<p>$$d\\theta = \\epsilon a$$</p>\n<p>$$\\vert a \\vert^2=a^TG(\\theta)a = 1$$</p>\n<p>이 때, $a$가 steepest direction unit vector이 되려면 다음 수식을 최소로 만들어야 합니다.</p>\n<p>$$\\eta(\\theta + d\\theta) = \\eta(\\theta) + \\epsilon\\nabla\\eta(\\theta)^Ta$$</p>\n<p>위 수식이 제약조건 아래 최소가 되는 $a$를 구하기 위해 Lagrangian method를 사용합니다. Lagrangian method를 모른다면 <a href=\"https://en.wikipedia.org/wiki/Lagrange_multiplier\" target=\"_blank\" rel=\"noopener\">위키피디아</a>를 참고하는 것을 추천합니다. 위 수식이 최소라는 것은 $\\nabla\\eta(\\theta)^Ta$가 최소라는 것입니다.</p>\n<p>$$\\frac{\\partial}{\\partial a_i}(\\nabla\\eta(\\theta)^Ta - \\lambda a^TG(\\theta)a)=0$$</p>\n<p>따라서 $(\\nabla\\eta(\\theta)^Ta - \\lambda a^TG(\\theta)a)=0$는 상수입니다. 상수를 미분하면 0이므로 이 식을 $a$로 미분합니다. 그러면 다음과 같이 steepest direction을 구한 것입니다.</p>\n<p>$$\\nabla\\eta(\\theta) = 2 \\lambda G(\\theta)a$$</p>\n<p>$$a=\\frac{1}{2\\lambda}G^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>이 때, 다음 식을 natural gradient라고 정의합니다.</p>\n<p>$$\\bar{\\nabla}\\eta(\\theta) = G^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>natural gradient를 이용한 업데이트는 다음과 같습니다.</p>\n<p>$$\\theta_{t+1}=\\theta_t - \\alpha_tG^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>여기까지는 natural gradient의 증명이었습니다. 이 natural gradient를 policy gradient에 적용한 것이 natural policy gradient입니다. natural policy gradient는 다음과 같이 정의됩니다.</p>\n<p>$$\\bar{\\nabla}\\eta(\\theta) = F^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>여기서 $G$대신 $F$를 사용했는데 $F$는 Fisher information matix입니다. 수식은 다음과 같습니다.</p>\n<p>$$F(\\theta) = E_{\\rho^\\pi(s)}[F_s(\\theta)]$$</p>\n<p>$$F_s(\\theta)=E_{\\pi(a;s,\\theta)}[\\frac{\\partial log \\pi(a;s, \\theta)}{\\partial \\theta_i}\\frac{\\partial log \\pi(a;s, \\theta)}{\\partial\\theta_j}]$$</p>\n<h3 id=\"3-2-2-Fisher-Information-Matrix에-정의된-Metric\"><a href=\"#3-2-2-Fisher-Information-Matrix에-정의된-Metric\" class=\"headerlink\" title=\"3.2.2 Fisher Information Matrix에 정의된 Metric\"></a>3.2.2 Fisher Information Matrix에 정의된 Metric</h3><p>추가적으로 Fisher Information Matrix(FIM)에 대해서 설명하겠습니다. 위의 문제를 우리가 생각하기 쉬운 Neural Network(NN)으로 구성하고 해결할 수 있다고 해봅시다. NN은 여러가지 parameter set들로 구성될 수 있습니다. 게다가 다른 parameter set을 가지지만 같은 policy를 가질 수도 있습니다. 이 경우 steepest direction은 같은 policy이기 때문에 같은 방향을 가리키고 있어야 하는데 non-covariant한 경우 그렇지 못합니다. 떄문에 학습이 느려지고, 이러한 문제를 해결하기 위해 단순히 Positive-Definite Matrix $G(\\theta)$를 사용하지 않고 FIM $F_s(\\theta)$를 사용합니다. 어떠한 확률 변수 $X$가 임의의 매개변수 $\\theta$에 의해 정의되는 분포를 따른다고 하면 $X=x$일때 FIM은 다음과 같이 정의됩니다.</p>\n<p>$$F_x(\\theta)=E\\left[\\left(\\dfrac{\\partial}{\\partial\\theta}\\log\\Pr(x|\\theta)\\right)^2\\right]$$</p>\n<p>강화학습 관점에서 생각해보면 정보 $x$는 에피소드에 의해 관측된 상태값 $s$이며 매개변수 $\\theta$에 의해 선택될 수 있는 행동에 대한 분포가 나오게 됩니다. 이에 의해 위의 FIM는 다음과 같이 표현할 수 있습니다.</p>\n<p>$$F_s(\\theta) \\equiv E_{\\pi(a;s,\\theta)}\\left[\\left(\\dfrac{\\partial}{\\partial\\theta}\\log\\pi(a;s,\\theta)\\right)^2\\right] =E_{\\pi(a;s,\\theta)}\\left[\\dfrac{\\partial \\log\\pi(a;s,\\theta)}{\\partial \\theta_i}\\dfrac{\\partial \\log\\pi(a;s,\\theta)}{\\partial\\theta_j}\\right]$$</p>\n<p>그리고 위의 식들을 이용하여 objective function을 정리하면 아래의 식과 같이 표현됩니다.</p>\n<p>$$F(\\theta) = E_{\\rho^{\\pi}(s)}[F_s(\\theta)]$$</p>\n<p>또한 이 Fisher Information Matrix에 정의된 이 metric은 다음과 같은 성질을 가지고 있습니다.</p>\n<ol>\n<li>업데이트되는 파라미터에 의해 구성되는 매니폴드에 기반한 metric입니다.</li>\n<li>확률분포($\\pi(a;s,\\theta)$)를 구성하는 파라미터($\\theta$)의 변화에 독립적입니다.</li>\n<li>마지막으로 positive-definite한 값을 가집니다. 이렇기 때문에 steepest gradient에서 objective function의 방향을 알기 위해 사용한 방법과 같은 방법으로 natural gradient direction을 다음과 같이 구할 수 있는 것입니다.</li>\n</ol>\n<p>$$\\bar{\\nabla}\\eta(\\theta) \\equiv F(\\theta)^{-1}\\nabla\\eta(\\theta)$$</p>\n<p><br><br></p>\n<h1 id=\"4-The-Natural-Gradient-and-Policy-Iteration\"><a href=\"#4-The-Natural-Gradient-and-Policy-Iteration\" class=\"headerlink\" title=\"4. The Natural Gradient and Policy Iteration\"></a>4. The Natural Gradient and Policy Iteration</h1><p>4장에서는 Natural gradient를 통한 policy iteration을 수행하여 실제로 정책의 향상이 있는지를 증명합니다. 여기서 $Q^\\pi(s,a)$는 compatible function approximator $f^\\pi(s,a;w)$로 근사됩니다. (<a href=\"https://reinforcement-learning-kr.github.io/2018/06/28/sutton-pg/\">Sutton PG</a>를 참고해주시기 바랍니다.)</p>\n<p><br></p>\n<h2 id=\"4-1-Theorem-1-Compatible-Function-Approximation\"><a href=\"#4-1-Theorem-1-Compatible-Function-Approximation\" class=\"headerlink\" title=\"4.1 Theorem 1: Compatible Function Approximation\"></a>4.1 Theorem 1: Compatible Function Approximation</h2><p>approximate하는 함수 $f^{\\pi}(s,a;w)$는 다음과 같습니다.(compatible value function)</p>\n<p>$$f^{\\pi}(s,a;w)=w^T\\psi^{\\pi}(s,a)$$</p>\n<p>$$\\psi^{\\pi}(s,a) = \\nabla log\\pi(a;s,\\theta)$$</p>\n<p>여기서 $[\\nabla \\log\\pi(a;s,\\theta)]_i=\\partial \\log\\pi(a;s,\\theta)/\\partial\\theta_i$입니다. $w$는 원래 approximate하는 함수 $Q$와 $f$의 차이를 줄이도록 학습합니다(mean square error). 수렴한 local minima의 $w$를 $\\bar{w}$라고 가정 하겠습니다. 에러는 다음과 같은 수식으로 나타낼 수 있습니다.</p>\n<p>$$\\epsilon(w,\\pi)\\equiv\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)(f^{\\pi}(s,a;w)-Q^{\\pi}(s,a))^2$$</p>\n<p>그 때, $\\bar{\\omega} = \\bar{\\nabla}\\eta(\\theta)$이면 function approximator의 gradient 방향과 정책의 gradient 방향이 같다는 것을 의미합니다.</p>\n<p>아래의 내용은 위의 Theorem에 대한 증명입니다.</p>\n<p>위의 수식이 local minima이면 미분값이 0이 됩니다. $w$에 대해서 미분하면 다음과 같습니다.</p>\n<p>$$\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)(\\psi^{\\pi}(s,a)^T\\bar{w}-Q^{\\pi}(s,a))=0$$</p>\n<p>$$\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)\\psi^{\\pi}(s,a)^T\\bar{w}=\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)Q^{\\pi}(s,a)$$</p>\n<p>이 때, 위 식의 우변은 $\\psi$의 정의에 의해 policy gradient가 됩니다. 또한 왼쪽 항에서는 Fisher information matrix가 나옵니다.</p>\n<p>$$F(\\theta)=\\sum_{s,a}\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)\\psi^{\\pi}(s,a)=E_{\\rho^\\pi(s)}[F_s(\\theta)]$$</p>\n<p>따라서 다음과 같이 쓸 수 있습니다.</p>\n<p>$$F(\\theta)\\bar{w}=\\nabla\\eta(\\theta)$$</p>\n<p>$$\\bar{w}=F(\\theta)^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>위의 수식은 natural gradient식과 동일합니다. 위의 수식은 policy가 update 될 때, value function approximator의 parameter 방향으로 이동한다는 것을 의미합니다. function approximation이 정확하다면 그 parameter의 natural policy gradient와 inner product가 커야합니다.</p>\n<p><br></p>\n<h2 id=\"4-2-Theorem-2-Greedy-Policy-Improvement\"><a href=\"#4-2-Theorem-2-Greedy-Policy-Improvement\" class=\"headerlink\" title=\"4.2 Theorem 2: Greedy Policy Improvement\"></a>4.2 Theorem 2: Greedy Policy Improvement</h2><p>이번 장에서는 natrual gradient는 다른 policy iteration 방법처럼 단순히 더 좋은 행동을 고르도록 학습하는 것이 아니라 가장 좋은(greedy) 행동을 고르도록 학습한다는 것을 증명하는 부분입니다. 이것을 일반적인 형태의 policy에 대해서 증명하기 전에 exponential 형태의 policy에 대해서 증명하는 것이 Theorem 2입니다. 특수한 정책을 가지는 상황안에서 학습속도($\\alpha$)를 무한대로 가져감으로서 어떤 action을 선택하는지를 알아봅니다.</p>\n<p>policy를 다음과 같이 정의합니다.</p>\n<p>$$\\pi(a;s,\\theta) \\propto \\exp(\\theta^T\\phi_{sa})$$</p>\n<p>여기서 $\\bar{\\nabla}\\eta(\\theta)$가 0이 아니고 $\\bar{w}$는 approximation error를 최소화된 $w$라고 가정합니다. 이 상태에서 natural gradient update를 생각해봅시다. 그리고 policy gradient는 gradient ascent임을 기억합시다.</p>\n<p>$$\\theta_{t+1}=\\theta_t + \\alpha_t\\bar{\\nabla}\\eta(\\theta)$$</p>\n<p>이 때 $\\alpha$가 learning rate로 parameter를 얼마나 업데이트하는지를 결정합니다. 이 값을 무한대로 늘렸을 때 policy가 어떻게 업데이트되는지 생각해봅시다.</p>\n<p>$$\\pi_{\\infty}(a;s)=lim_{\\alpha\\rightarrow\\infty}\\pi(a;s,\\theta+\\alpha\\bar{\\nabla}\\eta(\\theta))-(1)$$</p>\n<p>이 때,</p>\n<p>$$\\pi_{\\infty}=0 \\, \\, \\, if \\, and \\, only \\, if(필요충분조건) \\, \\, \\, a \\notin argmax_{a’}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa’}$$</p>\n<p>이라고 말할 수 있습니다.</p>\n<p>아래의 내용은 위의 Theorem에 대한 증명입니다.</p>\n<p>먼저 function approximator는 앞서 다뤘듯이 아래와 같습니다.</p>\n<p>$$f^{\\pi}(s,a;w)=w^T\\psi^{\\pi}(s,a)$$</p>\n<p>여기서 function approximator는 Theorem 1에 의해 아래와 같이 쓸 수 있습니다.</p>\n<p>$$f^{\\pi}(s,a;w)=\\bar{\\nabla}\\eta(\\theta)^T\\psi^{\\pi}(s,a)$$</p>\n<p>$\\theta$의 정의에 의해 $\\psi$는 다음과 같습니다.</p>\n<p>$$\\psi^{\\pi}(s,a)=\\phi_{sa}-E_{\\pi(a’;s,\\theta)}[\\phi_{sa’}]$$</p>\n<p>따라서 function approximator는 다음과 같이 다시 쓸 수 있습니다.</p>\n<p>$$f^{\\pi}(s,a;w)=\\bar{\\nabla}\\eta(\\theta)^T(\\phi_{sa}-E_{\\pi(a’;s,\\theta)}[\\phi_{sa’}])$$</p>\n<p>greedy policy improvement가 Q function 값 중 가장 큰 값을 가지는 action을 선택하듯이 여기서도 function approximator의 값이 가장 큰 action을 선택하는 상황을 가정해봅시다. 이 때 function approximator의 argmax는 다음과 같이 쓸 수 있습니다.</p>\n<p>$$argmax_{a’}f^{\\pi}(s,a)=argmax_{a’}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa’}$$</p>\n<p>(1) 식을 다시 살펴봅시다. 그러면 policy의 정의에 따라 다음과 같이 쓸 수 있습니다.</p>\n<p>$$\\pi(a;s,\\theta + \\alpha\\bar{\\nabla}\\eta(\\theta)) \\propto exp(\\theta^T\\phi_{sa} + \\alpha\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa})$$</p>\n<p>$\\bar{\\nabla}\\eta(\\theta) \\neq 0$이고 $\\alpha\\rightarrow\\infty$이면 exp안의 항 중에서 뒤의 항이 dominate하게 됩니다. 여러 행동 중에 $\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa}$가 가장 큰 행동이 있다면 이 행동의 policy probability가 1이 되고 나머지는 0이 됩니다. 따라서 다음이 성립합니다.</p>\n<p>$$\\pi_{\\infty}=0 \\, \\, \\, if \\, and \\, only \\, if \\, \\, \\, a \\notin argmax_{a’}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa’}$$</p>\n<p>이 결과로부터 natural policy gradient는 단지 더 좋은 action이 아니라 best action을 고르도록 학습이 됩니다. 반면에 non-covariant gradient(1차미분)에서는 그저 더 좋은 action을 고르도록 학습이 됩니다. 이 natural policy gradient에 대한 결과는 infinite learning rate 세팅에서만 성립합니다. 좀 더 일반적인 경우에 대해서 살펴봅시다.</p>\n<p><br></p>\n<h2 id=\"4-3-Theorem-3-General-Parameterized-Policy\"><a href=\"#4-3-Theorem-3-General-Parameterized-Policy\" class=\"headerlink\" title=\"4.3 Theorem 3: General Parameterized Policy\"></a>4.3 Theorem 3: General Parameterized Policy</h2><p>Theorem 2에서와는 달리 일반적인 policy를 가정해봅시다(general parameterized policy). Theorem 3는 이 상황에서 natural gradient를 통한 업데이트가 best action를 고르는 방향으로 학습이 된다는 것을 보여줍니다.</p>\n<p>natural gradien에 따른 policy parameter의 업데이트는 다음과 같습니다. $\\bar{w}$는 approximation error를 minimize하는 $w$입니다.</p>\n<p>$$\\delta\\theta = \\theta’ - \\theta = \\alpha\\bar{\\nabla}\\eta(\\theta)=\\alpha\\bar{w}$$</p>\n<p>policy에 대해서 1차근사를 하면 다음과 같습니다.</p>\n<p>$$\\pi(a;s,\\theta’)=\\pi(a;s,\\theta)+\\frac{\\partial\\pi(a;s,\\theta)^T}{\\partial\\theta}\\delta\\theta + O(\\delta\\theta^2)$$</p>\n<p>$$=\\pi(a;s,\\theta)(1+\\psi(s,a)^T\\delta\\theta) + O(\\delta\\theta^2)$$</p>\n<p>$$=\\pi(a;s,\\theta)(1+\\alpha\\psi(s,a)^T\\bar{w}) + O(\\delta\\theta^2)$$</p>\n<p>$$=\\pi(a;s,\\theta)(1+\\alpha f^{\\pi}(s,a;\\bar{w}) + O(\\delta\\theta^2)$$</p>\n<p>policy 자체가 function approximator의 크기대로 업데이트가 되므로 local하게 best action의 probability는 커지고 다른 probability의 크기는 작아질 것입니다. 하지만 만약 greedy improvement가 된다하더라도 그게 performance의 improvement를 보장하는 것은 아닙니다. 하지만 line search와 함께 사용할 경우 improvement를 보장할 수 있습니다. 왜 그런지는 처음에 말씀드린 블로그 <a href=\"http://darkpgmr.tistory.com/149\" target=\"_blank\" rel=\"noopener\">다크 프로그래머님의 블로그</a>를 참고해주시기 바랍니다.</p>\n<p><br><br></p>\n<h1 id=\"5-Metrics-and-Curvatures\"><a href=\"#5-Metrics-and-Curvatures\" class=\"headerlink\" title=\"5. Metrics and Curvatures\"></a>5. Metrics and Curvatures</h1><p>$$\\vert d\\theta \\vert^2=\\sum_{ij}(\\theta)d\\theta_id\\theta_i=d\\theta^TG(\\theta)d\\theta$$</p>\n<p>이 파트에서는 FIM과 다른 metric 사이의 관계를 다룹니다.</p>\n<p>위의 식에 해당하는 G는 Fisher Information Matrix만 사용할 수 있는 것이 아닙니다. Positive-Definite Matrix인 FIM이외의 다른 Matrix도 사용할 수 있습니다. 또한 다양한 파라미터 추정에서 FIM은 Hessian Matrix에 수렴하지 않을 수 있다고 합니다. 이 말은 2nd order(2차 근사) 수렴이 보장되지 않는다는 말입니다.</p>\n<p>논문에 있는 내용들을 조금 더 추가적으로 보면 아래와 같이 나옵니다.</p>\n<p>In the different setting of parameter estimation, the Fisher information converges to the <code>Hessian</code>, so it is <a href=\"https://en.wikipedia.org/wiki/Efficiency_(statistics\" target=\"_blank\" rel=\"noopener\">asymptotically efficient</a>) 이지만,</p>\n<p>이 논문의 경우, 아마리 논문의 ‘blind separation case’와 유사한데 이 때는 꼭 asymtotically efficient하지 않다고 말합니다. 이 말은 즉 2nd order 수렴이 보장되지 않는다는 것이다.</p>\n<p><a href=\"http://www.inference.org.uk/mackay/ica.pdf\" target=\"_blank\" rel=\"noopener\">Mackay</a> 논문에서는 Hessian에서 data independant한 term을 metric으로 가져오는 방법을 제안했습니다. 그래서 performance를 2번 미분해보면 아래의 수식과 같습니다. 하지만 다음 식에서는 모든 항이 data dependent합니다(Q가 있으니까). 첫 번째 항이 그나마 FIM과의 관련성이 있을 수 있지만 Q 값이 curvature에 weight를 주는 방식 때문에 다르다고 할 수 있습니다.</p>\n<p>$$\\nabla^2\\eta(\\theta)=\\sum_{sa}\\rho^{\\pi}(s)(\\nabla^2\\pi(a;s)Q^{\\pi}(s,a)+\\nabla\\pi(a;s)\\nabla Q^{\\pi}(s,a)^T+\\nabla Q^{\\pi}(s,a)\\nabla\\pi(a;s)^T)$$</p>\n<p>hessian은 보통 positive definite가 아닐수도 있습니다. 따라서 local maxima가 될 때까지 Hessian이 사용하기 별로 안좋습니다. 그리고 local maxima에서는 Hessian보다는 Conjugate methods가 더 효율적이라고 합니다.</p>\n<p>사실 이 파트에서는 무엇을 말하고 있는지 알기가 어렵습니다. FIM과 Hessian이 관련이 있다는 것을 알겠는데 asymtotically efficient와 같은 내용을 모르므로 내용의 이해가 어려웠습니다.</p>\n<p>Mackay 논문에서 해당 부분은 다음과 같습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/x4n6z6pdyi7xtb9/Screenshot%202018-06-10%2012.04.13.png?dl=1\"> </center>\n\n<p><br></p>\n<h2 id=\"5-1-Fisher-Information-Matrix-FIM-vs-Hessian\"><a href=\"#5-1-Fisher-Information-Matrix-FIM-vs-Hessian\" class=\"headerlink\" title=\"5.1 Fisher Information Matrix(FIM) vs. Hessian\"></a>5.1 Fisher Information Matrix(FIM) vs. Hessian</h2><p>FIM과 Hessian에 대해서 추가적인 설명을 하고자 합니다.</p>\n<ul>\n<li>FIM</li>\n</ul>\n<p>일단 정의되려면 space 자체가 stochastic한 성질을 가지고 있어야 합니다. 모든 parameter를 표현하는 함수가 deterministic한 함수가 아니라 확률로 나타낸 분포입니다. (probability distribution)</p>\n<p>결국은 FIM에서도 hessian처럼 비슷한 과정을 취하고 싶은 것입니다. 따라서 확률 변수이기 때문에 어떠한 특정 sample을 취하면 그게 항상 다른값이 됩니다. 그래서 FIM에다가 expectation을 취한 것입니다. expectation을 취함으로써 hessian같은 성질을 가집니다. 왜냐하면 expectation을 취함으로써 constant한 값이 되기 때문입니다.</p>\n<ul>\n<li>Hessian</li>\n</ul>\n<p>그냥 deterministic한 함수에서 정의됩니다. 그냥 함수를 두 번 미분 한 것이라고 보면 됩니다.</p>\n<p><br></p>\n<h2 id=\"5-2-Conjugate-Gradient-Method\"><a href=\"#5-2-Conjugate-Gradient-Method\" class=\"headerlink\" title=\"5.2 Conjugate Gradient Method\"></a>5.2 Conjugate Gradient Method</h2><p>추가적으로 Conjugate Gradient Method(CGM)에 대해서 다루고자 합니다.</p>\n<ul>\n<li>참고자료<ul>\n<li><a href=\"https://www.quora.com/What-is-an-intuitive-explanation-of-what-the-conjugate-gradient-method-is\" target=\"_blank\" rel=\"noopener\">https://www.quora.com/What-is-an-intuitive-explanation-of-what-the-conjugate-gradient-method-is</a></li>\n<li><a href=\"https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf\" target=\"_blank\" rel=\"noopener\">https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Conjugate_gradient_method\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Conjugate_gradient_method</a></li>\n</ul>\n</li>\n</ul>\n<h3 id=\"mathbf-A-mathbf-x-mathbf-b-의-해-구하기\"><a href=\"#mathbf-A-mathbf-x-mathbf-b-의-해-구하기\" class=\"headerlink\" title=\"$\\mathbf{A}\\mathbf{x} = \\mathbf{b}$의 해 구하기\"></a>$\\mathbf{A}\\mathbf{x} = \\mathbf{b}$의 해 구하기</h3><p>$\\mathbf{A}\\mathbf{x} = \\mathbf{b}$의 해를 구하는 문제를 생각해봅시다. $\\mathbf{A}$의 역행렬을 구해서 양변에 곱해주면 $\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}$가 되어 쉽게 해를 구할 수 있습니다. 하지만 $\\mathbf{A}^{-1}$은 계산이 많이 필요 자원소모가 큰 연산입니다. 역행렬을 취하지 않고 해를 구할 수 있는 방법이 있을까요?</p>\n<p>위의 방정식에서 $\\mathbf{b}$를 이항시키면 $\\mathbf{A}\\mathbf{x} - \\mathbf{b} = 0$이 됩니다. 최적화문제는 많은 경우 1차 미분이 0이 되는 지점이 해일 확률이 높습니다. $\\mathbf{A}\\mathbf{x} - \\mathbf{b} = 0$을 1차 미분으로 가지는 함수는 무엇일까요?</p>\n<p>$$f( \\mathbf{x} ) = \\frac{1}{2}\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x} - \\mathbf{x}^\\mathrm{T}\\mathbf{b}$$</p>\n<p>위의 함수는 $\\mathbf{A}\\mathbf{x} - \\mathbf{b}$를 1차 미분값으로 가집니다. 이 때 $\\mathbf{A}$는 symmetric positive definite해야 합니다.</p>\n<ul>\n<li>symmetric: $\\mathbf{A}=\\mathbf{A}^\\mathrm{T}$</li>\n<li>positive definite: $\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x}&gt;0,\\quad\\forall\\mathbf{x}$ or all eigenvalues of $\\mathbf{A}$ are positive</li>\n</ul>\n<p>symmetric positive definite한 성질은 $\\mathbf{x}$를 strict convex function이 되게 만들어서 unique solution이 존재하게 합니다. $\\mathbf{A}$는 $\\mathbf{x}$의 Hessian이기도 합니다.</p>\n<p>자, 우리는 위의 방정식의 해 $\\mathbf{x}^*$를 iterative한 방식으로 찾고자 합니다. $\\mathbf{x}_0$을 초기값이라고 합시다.</p>\n<p>우리는 <img src=\"https://www.dropbox.com/s/g91yqajf72jzjoc/Screen%20Shot%202018-08-14%20at%208.43.28%20AM.png?dl=1\" width=\"80\">가 되는 <img src=\"https://www.dropbox.com/s/aj3j5fjtiyewlo1/Screen%20Shot%202018-08-14%20at%208.43.37%20AM.png?dl=1\" width=\"30\">를 찾고 싶은데 이를 위한 현재의 추정값은 $\\mathbf{A} \\mathbf{x}_0$입니다. 최적의 값을 찾았을 때와 비교하면 현재의 오차는 $\\mathbf{b} - \\mathbf{A}\\mathbf{x}_0 = \\mathbf{r}_0$입니다. 이것을 residual이라고 합니다. 어떻게 효율적으로 값을 변화시켜서 오차를 0으로 만들 수 있을까요? 일단 매 iteration마다 residual이 작아지는 방향으로 나아가야겠죠?</p>\n<p>가장 널리 알려진 방법은 gradient descent 방향입니다. gradient가 증가하면 반대로 가고 gradient가 감소하면 그 방향으로 가는 것입니다. 그리고 어떤 방향으로든 gradient가 0이면 그 지점에 멈추는 것이죠. 이 방법은 수렴은 하지만 zigzag하게 움직여서 느립니다. 이것보다 더 좋은 방법이 없을까요?</p>\n<h3 id=\"Conjugate-Gradient-Method\"><a href=\"#Conjugate-Gradient-Method\" class=\"headerlink\" title=\"Conjugate Gradient Method\"></a>Conjugate Gradient Method</h3><h4 id=\"Gram-Schmidt-orthgonalization\"><a href=\"#Gram-Schmidt-orthgonalization\" class=\"headerlink\" title=\"Gram-Schmidt orthgonalization\"></a>Gram-Schmidt orthgonalization</h4><p>다음과 같은 vector들의 집합 <img src=\"https://www.dropbox.com/s/csgdsfofe19q5r6/Screen%20Shot%202018-08-14%20at%208.56.04%20AM.png?dl=1\" width=\"135\">이 있다고 해봅시다. 이 vector들이 서로 linearly independent하다면 이 vector들은 $\\mathbb{R}^n$ 공간 상의 basis입니다. 이 vector들을 이용해서 orthogonal한 vector들을 만들어보겠습니다. 새로운 vector들을 <img src=\"https://www.dropbox.com/s/7xg7pciar1ndu44/Screen%20Shot%202018-08-14%20at%208.58.25%20AM.png?dl=1\" width=\"130\">라고 한다면 아래와 같은 과정을 통해 만들 수 있습니다. 이 방법을 Gram-Schmidt orthogonalization이라고 합니다.</p>\n<ul>\n<li>$\\mathbf{d}_1=\\mathbf{v}_1$</li>\n<li>$\\mathbf{d}_2=\\mathbf{v}_2 - \\left\\langle\\mathbf{v}_2,\\frac{\\mathbf{d}_1}{\\parallel\\mathbf{d}_1\\parallel}\\right\\rangle\\frac{\\mathbf{d}_1}{\\parallel\\mathbf{d}_1\\parallel}$<br>…</li>\n<li><img src=\"https://www.dropbox.com/s/arzc5cez8az0j7h/Screen%20Shot%202018-08-14%20at%209.00.04%20AM.png?dl=1\" width=\"290\"></li>\n</ul>\n<p>간단히 설명을 하면, 첫 vector는 basis와 같은 방향으로 출발합니다. 그 다음 vector는 그 다음 basis를 이전 vector로 projection한 다음 해당 basis로부터 빼줍니다. 그 다음 vector를 만들 때는 이전에 만든 모든 vector들에 대해서 projection을 취한 다음 모두 더한 것을 해당 basis에서 빼줍니다. </p>\n<h4 id=\"A-conjugate\"><a href=\"#A-conjugate\" class=\"headerlink\" title=\"$A$-conjugate\"></a>$A$-conjugate</h4><p>이 방법을 이용하여 matrix $A$에 관하여 orthogonal한 새로운 vector들의 집합을 만들어보겠습니다. 일반적인 inner product와 약간 다른 다음과 같은 inner product를 정의할 수 있습니다.</p>\n<p>$$&lt;\\mathbf{x},\\mathbf{y}&gt;_\\mathbf{A} = \\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{y}$$</p>\n<p>이 때 $&lt;\\mathbf{x},\\mathbf{y}&gt;_\\mathbf{A} = 0$이면 $\\mathbf{x}와 \\mathbf{y}$는 $\\mathbf{A}$-orthogonal 또는 $\\mathbf{A}$-conjugate하다라고 합니다. vector norm $\\parallel\\cdot\\parallel$도 일반적인 vector norm이 아닌 A-norm을 다음과 같이 정의합니다.</p>\n<p>$$\\parallel\\mathbf{x}\\parallel_\\mathbf{A}^2=\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x}$$</p>\n<p>Gram-Schmidt orthogonalization을 또 이용해볼까요? 아래와 같습니다.</p>\n<ul>\n<li>$\\mathbf{d}_1=\\mathbf{v}_1$</li>\n<li><img src=\"https://www.dropbox.com/s/i8wnvoccefgg7t8/Screen%20Shot%202018-08-14%20at%209.08.42%20AM.png?dl=1\" width=\"400\"><br>…</li>\n<li><img src=\"https://www.dropbox.com/s/045x2v5sx7s0wot/Screen%20Shot%202018-08-14%20at%209.09.03%20AM.png?dl=1\" width=\"500\"></li>\n</ul>\n<p>그런데 이 Gram-Schmidt 방법은 단점이 하나 있습니다. 새로운 vector를 만들어내기 위해서 이전에 만든 vector들을 모두 가지고 있어야 하고 projection도 다시 계산해야 한다는 점입니다. 그런데 마법같은 이유($\\approx$ 여기서 설명하지 않는 이유)로 인해서 오직 마지막으로 만들어낸 vector만 가지고 있어도 기존의 모든 vector들이 성질을 표현해낼 수 있습니다. 그러면 우리가 만들어낸 vector는 다음과 같이 간단해집니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/wlyu7ax6qt0aw9w/Screen%20Shot%202018-08-14%20at%209.09.43%20AM.png?dl=1\" width=\"550\"> </center>\n\n<p>이 방향이 우리가 업데이트시키고 싶은 방향입니다. 즉 우리는 다음 수식처럼 움직이고자 합니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/053o4ocvxsgxfl9/Screen%20Shot%202018-08-14%20at%209.09.56%20AM.png?dl=1\" width=\"180\"> </center>\n\n<p>$d_k$는 방향이고 $\\alpha_{k}$는 step size입니다. 이러한 방법을 일반적으로 line search method라고 부릅니다. 방향은 위에서 구한 conjugate 방향을 이용합니다. 그래서 이 line search를 conjugate gradient method라고 부릅니다. 이 방법의 한가지 중요한 장점은 업데이트가 무조건 n번만 일어난다는 것입니다! n-dimensional space에서는 basis가 n개 밖에 없거든요. 그렇다면 얼마만큼 많이 이 방향으로 움직여야 할까요? 더 이상 $f(\\mathbf{x})$가 감소하지 않을 때까지 움직이는게 좋지 않을까요? 이것은 다시 말하면 gradient가 0이 될 때까지 움직인다는 뜻입니다. 위에서 설명한 residual과도 관계있을 것 같지 않나요?</p>\n<p>$\\nabla f(\\mathbf{x})=\\mathbf{A}\\mathbf{x} - \\mathbf{b}$에 $x_k + \\alpha_{k} d_k$를 대입해 봅시다. 아래와 같이 step size를 구할 수 있습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/xikpfgcix64ngmp/Screen%20Shot%202018-08-14%20at%209.13.20%20AM.png?dl=1\" width=\"300\"> </center>\n\n<p>네, 이제 우리는 방향과 step size를 모두 알았습니다. 이제 update만 하면 해를 찾을 수 있겠군요!</p>\n<center> <img src=\"https://www.dropbox.com/s/avjw4kbzd4ormv5/conjugate_gradient_wikipedia.png?dl=1\" width=\"200\"> </center>\n\n<p>위키피디아에서 가져온 위의 그림을 봅시다. 그림의 녹색선이 gradient descent이고 빨간선이 conjugate gradient입니다. conjugate gradient는 gradient descent보다 빨리 수렴합니다. 녹색선은 항상 이전 이동 방향과 직각으로 이동합니다. 빨간선은 보다 빠르게 더 낮은 값이 있는 곳으로 이동합니다.</p>\n<p>위에서 설명을 생략했지만 왜 conjugate gradient는 빠르게 수렴(n-step)하며 Gram-Schmidt orthogonalization을 할 때도 메모리가 적게 필요할까요? 수학적으로 엄밀하게 설명하기 보다는 개념적으로 설명을 하겠습니다. 그 비밀은 바로 $\\mathbf{A}$-orthogonal 또는 $\\mathbf{A}$-conjugate vector들을 이용한데 있습니다. 이 vector들은 basis라고 했습니다. 즉, 처음에 우리가 구하고자 했던 해 $\\mathbf{x}^*$를 이 vector들을 이용해서 표현할 수 있습니다. 또한 iterative하게 찾아가기 위한 초기 vector $\\mathbf{x}_0$도 이 vector들로 표현할 수 있습니다. 그렇다면 이 둘 간의 오차도 이 vector들도 표현할 수 있게 됩니다. 다음 수식처럼 말입니다.</p>\n<p>$$\\mathbf{x}^* - \\mathbf{x}_0 = \\epsilon_1\\mathbf{d}_1 + \\epsilon_2\\mathbf{d}_2 + \\cdots + \\epsilon_n\\mathbf{d}_n$$</p>\n<p>우리의 목표는 이 오차를 줄이는 것입니다. 각각의 basis마다 오차가 있으며 이들은 서로 independent합니다. 즉, 특정 iteration에서 특정 basis에 대한 오차를 0으로 만들면 다음 번 iteration에서는 이 오차는 영향을 받지 않습니다! 이러한 이유로 n번의 iteration만으로 해를 찾을 수 있고 다른 vector들을 저장할 필요도 없는 것입니다. 선형대수의 아름다움이 느껴지지 않으시나요? 오래되었지만 참 잘 디자인된 기법이라는 생각이 듭니다.</p>\n<p><br><br></p>\n<h1 id=\"6-Experiment\"><a href=\"#6-Experiment\" class=\"headerlink\" title=\"6. Experiment\"></a>6. Experiment</h1><p>이 논문에서는 natural gradient를 simple MDP와 tetris MDP에 대해서 실험을 진행했습니다. FIM은 다음과 같은 식으로 업데이트합니다.</p>\n<p>$$f\\leftarrow f+\\nabla \\log \\pi(a_t; s_t, \\theta)\\nabla \\log \\pi(a_t; s_t, \\theta)^T$$</p>\n<p>$T$ length trajectory에 대해서 $f/T$를 이용해 $F$의 기대값($E$)를 구합니다.</p>\n<p><br></p>\n<h2 id=\"6-1-LQR-Linear-Quadratic-Regulator\"><a href=\"#6-1-LQR-Linear-Quadratic-Regulator\" class=\"headerlink\" title=\"6.1 LQR(Linear Quadratic Regulator)\"></a>6.1 LQR(Linear Quadratic Regulator)</h2><p>Agent를 실험할 환경은 다음과 같은 dynamics를 가지고 있습니다.</p>\n<p>$x(t+1) = 0.7x(t)+u(t)+\\epsilon(t)$</p>\n<p>$u(t)$는 control 신호로서 에이전트의 행동입니다. $\\epsilon$은 noise distribution으로 환경에 가해지는 노이즈입니다. 에이전트의 목표는 적절한 $u(t)$를 통해 $x(t)$를 0으로 유지하는 것입니다. $x(t)$를 0으로 유지하기 위해서 필요한 소모값(cost)는 $x(t)^2$로 정의하며 cost를 최소화하도록 학습합니다. 이 논문에서는 실험할 때 복잡성을 더 해주기 위해 noise distribution인 $\\epsilon$을 dynamics에 추가하였습니다.</p>\n<p>이 실험에서 policy는 다음과 같이 설정하였습니다. 파라미터가 2개 밖에 없는 간단한 policy입니다.</p>\n<p>$\\pi(u;x,\\theta)\\propto \\exp(\\theta_1s_1x^2+\\theta_2s_2x)$</p>\n<p>이 policy를 간단히 그래프로 그려보면 다음과 같습니다. 가로축은 $x$, 세로축은 cost입니다. $\\theta_1$과 $\\theta_2$를 (0.5, 0.5), (1, 0), (0, 1)로 설정하고 $s_1$, $s_2$는, 1로 두었습니다. $x$는 -1~1사이의 범위로 그렸습니다. </p>\n<center> <img src=\"https://www.dropbox.com/s/v69qyrwn7zurk8c/Screenshot%202018-06-08%2014.57.07.png?dl=1\" width=\"500px\"> </center>\n\n<p>아래의 그림은 LQR을 학습한 그래프입니다. cost가 $x^2$이기 때문에 cost가 0으로 갈수록 agent는 0에서 안정적으로 머무른다고 볼 수 있습니다. 6개의 선 중에서 오른쪽 세 개가 일반적인 gradient 방법을 사용해서 학습한 결과입니다. 그리고 왼쪽의 세 개의 선이 natural policy gradient를 통해 학습한 곡선입니다. 일반 gradient 방법보다 natural gradient가 훨씬 빠르게 학습합니다(time 축이 log scale인 것을 감안하면 차이가 많이 납니다.). </p>\n<p>하지만 문제가 있습니다. NPG를 학습한 세 개의 곡선은 $\\theta$를 rescale 한 것입니다. $\\theta$앞에 곱해지는 숫자에 따라 학습의 과정이 다릅니다. 이 것은 coordinate에 따라 steepest gradient가 다르게 측정된다는 것입니다. 즉, covariant gradient가 아니라는 뜻입니다. 이 논문에서는 natural gradient를 통해 gradient가 covariant하도록 만들고 싶었는데 실패한 것입니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/fhn8cgje0rdws0i/Screenshot%202018-06-08%2023.13.37.png?dl=1\" width=\"300px\"> </center>\n\n<p>natural gradient가 covariant하지 않은 이유는 Fisher Information Matrix가 예상했던 바와는 달리 invariant metric이 아니기 때문입니다. 또한 FIM이 invariant metric이 아닌 이유는 FIM을 계산할 때 $\\rho_s$가 곱해지기 때문입니다. 하지만 여전히 의의가 있는 것은 기존 gradient 방법들보다 훨씬 빠르게 학습한다는 것입니다!</p>\n<p><br></p>\n<h2 id=\"6-2-Simple-2-state-MDP\"><a href=\"#6-2-Simple-2-state-MDP\" class=\"headerlink\" title=\"6.2 Simple 2-state MDP\"></a>6.2 Simple 2-state MDP</h2><p>이제 다른 예제에서 NPG를 테스트합니다. 2개의 state만 가지는 MDP를 고려해봅시다. <a href=\"http://repository.cmu.edu/cgi/viewcontent.cgi?article=1080&amp;context=robotics\" target=\"_blank\" rel=\"noopener\">그림출처</a>. 그림으로보면 다음과 같습니다. $x=0$ 상태와 $x=1$ 상태 두 개가 존재합니다. 에이전트는 각 상태에서 다시 자신의 상태로 되돌아오는 행동을 하거나 다른 상태로 가는 행동을 할 수 있습니다. 상태 $x=0$에서 다시 자기 자신으로 돌아오면 1의 보상을 받고 상태 $x=1$에서 자기 자신으로 돌아오면 2의 보상을 받습니다. 결국 optimal policy는 상태 $x=1$에서 계속 자기 자신으로 돌아오는 행동을 취하는 것입니다. </p>\n<p><img src=\"https://www.dropbox.com/s/g1x9yknzsrip59i/Screenshot%202018-06-08%2023.06.50.png?dl=1\"></p>\n<p>문제를 좀 어렵게 만들기 위해 state distribution을 다음과 같이 설정합니다. 즉, 대부분의 경우에 상태 x=0에서 에이전트가 시작하는 것입니다.</p>\n<p>$$\\rho(x=0)=0.8,  \\rho(x=1)=0.2$$</p>\n<p>일반적인 gradient 방법을 사용하면 다음과 같은 policy gradient 식에 따라서 업데이트를 하게 됩니다. 이 때, $\\rho(s)$가 gradient에 곱해지므로 상대적으로 상태 0에서의 gradient 값이 커지게 됩니다. 따라서 에이전트는 상태 0에서의 gradient(상태 0에서 스스로에게 돌아오는 행동을 취하도록 정책을 업데이트하는 gradient)를 따라 parameterized policy를 update합니다. 따라서 아래 그림의 첫번째 그림에서처럼 reward가 1에서 오랜 시간동안 머무릅니다. 즉, 에이전트가 상태 0에서 self-loop를 계속 돌고 있다는 것입니다.</p>\n<p>$$\\nabla\\eta(\\theta)=\\sum_{s,a}\\rho^{\\pi}(s)\\nabla\\pi(a;s,\\theta)Q^{\\pi}(s,a)$$</p>\n<center><img src=\"https://www.dropbox.com/s/xtb77mfazbppnss/Screenshot%202018-06-08%2023.14.24.png?dl=1\" width=\"300px\"></center>\n\n<p>하지만 NPG를 사용할 경우에는 훨씬 빠르게 average reward가 2에 도달합니다. gradient 방법이 $1.7\\times 10^7$정도의 시간만에 2에 도달한 반면 NPG는 2의 시간만에 도달합니다. 또한 $\\rho(x=0)$가 $10^{-5}$이하로 떨어지지 않습니다.</p>\n<p>한 가지 그래프를 더 살펴봅시다. 다음 그래프는 parameter $\\theta$가 업데이트 되는 과정을 보여줍니다. 이 그래프에서는 parameter가 2개 있습니다. 일반적인 gradient가 아래 그래프에서 실선에 해당합니다. 이 실선의 그래프는 보면 처음부터 중반까지 $\\theta_i$만 거의 업데이트하는 것을 볼 수 있습니다. 그에 비해 NPG는 두 개의 parameter를 균등하게 업데이트하는 것을 볼 수 있습니다. </p>\n<center><img src=\"https://www.dropbox.com/s/g7pazozw2k6rd7x/Screenshot%202018-06-08%2023.23.25.png?dl=1\" width=\"300px\"></center>\n\n<p>policy가 $\\pi(a;s,\\theta)\\propto \\exp(\\theta_{sa})$일 때, 다음과 같이 $F_{-1}$이 gradient 앞에 weight로 곱해지는데 이게 $\\rho$와는 달리 각 parameter에 대해 균등합니다. 따라서 위 그래프에서와 같이 각 parameter는 비슷한 비율로 업데이트가 되는 것입니다.</p>\n<p>$$\\bar{\\nabla}\\eta(\\theta) = F^{-1}\\nabla\\eta(\\theta)$$</p>\n<p><br></p>\n<h2 id=\"6-3-Tetris\"><a href=\"#6-3-Tetris\" class=\"headerlink\" title=\"6.3 Tetris\"></a>6.3 Tetris</h2><p>NPG를 테스트할 tetris 예제는 Neuro Dynamic Programming 책에 소개되어 있습니다. 다음 그림은 tetris 예제를 보여줍니다. 보통 그림에서와 같이 state의 feature를 정해줍니다. <a href=\"http://slideplayer.com/slide/5215520/\" target=\"_blank\" rel=\"noopener\">그림 출처</a></p>\n<p><img src=\"https://www.dropbox.com/s/y1halso9yermy8s/Screenshot%202018-06-08%2023.44.34.png?dl=1\"></p>\n<p>이 예제에서도 exponential family로 policy를 표현합니다. $\\pi(a;s,\\theta) \\propto \\exp(\\theta^T\\phi_{sa})$ 로 표현합니다.</p>\n<p>tetris는 linear function approximator와 greedy policy iteration을 사용할 경우 performance가 갑자기 떨어지는 현상이 있습니다. 밑의 그림에서 A의 spike가 있는 그래프가 이 경우입니다. 그 밑에 낮게 누워있는 그래프는 일반적인 policy gradient 방법입니다. 하지만 Natural policy gradient를 사용할 경우 B 그림에서 오른쪽 그래프와 같이 성능개선이 뚜렷합니다. Policy Iteration 처럼 성능이 뚝 떨어지지 않고 안정적으로 유지합니다. 또한 그림 C에서 보는 것처럼 오른쪽 그래프인 일반적인 gradient 방법보다 훨씬 빠르게 학습하는 것을 볼 수 있습니다.</p>\n<p><img src=\"https://www.dropbox.com/s/pr6s2qrqaic0wyj/Screenshot%202018-06-08%2023.40.16.png?dl=1\"></p>\n<p><br><br></p>\n<h1 id=\"7-Discussion\"><a href=\"#7-Discussion\" class=\"headerlink\" title=\"7. Discussion\"></a>7. Discussion</h1><p>Natural Gradient Method는  Policy Iteration에서와 같이 Greedy Action을 선택하도록 학습합니다. Line search 기법과 함께 사용하면 더 Policy Iteration과 비슷해집니다. Greedy Policy Iteration에서와 비교하면 Performance Improvement가 보장됩니다. 하지만 FIM이 asymtotically Hessian으로 수렴하지 않습니다. 그렇기 때문에 Conjugate Gradient Method로 구하는 방법이 더 좋을수 있습니다.</p>\n<p>살펴봤듯이 본 논문의 NPG는 완벽하지 않습니다. 위의 몇가지 문제점을 극복하기 위한 추가적인 연구가 필요하다고 할 수 있습니다.</p>\n<p><br><br></p>\n<h1 id=\"처음으로\"><a href=\"#처음으로\" class=\"headerlink\" title=\"처음으로\"></a>처음으로</h1><h2 id=\"PG-Travel-Guide\"><a href=\"#PG-Travel-Guide\" class=\"headerlink\" title=\"PG Travel Guide\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/\">PG Travel Guide</a></h2><p><br></p>\n<h1 id=\"이전으로\"><a href=\"#이전으로\" class=\"headerlink\" title=\"이전으로\"></a>이전으로</h1><h2 id=\"DDPG-여행하기\"><a href=\"#DDPG-여행하기\" class=\"headerlink\" title=\"DDPG 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/26/3_ddpg/\">DDPG 여행하기</a></h2><p><br></p>\n<h1 id=\"다음으로\"><a href=\"#다음으로\" class=\"headerlink\" title=\"다음으로\"></a>다음으로</h1><h2 id=\"NPG-Code\"><a href=\"#NPG-Code\" class=\"headerlink\" title=\"NPG Code\"></a><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/tnpg.py\" target=\"_blank\" rel=\"noopener\">NPG Code</a></h2><h2 id=\"TRPO-여행하기\"><a href=\"#TRPO-여행하기\" class=\"headerlink\" title=\"TRPO 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/24/5_trpo/\">TRPO 여행하기</a></h2>"},{"title":"Proximal Policy Optimization","date":"2018-06-22T07:53:12.000Z","author":"이동민, 장수영, 차금강","subtitle":"피지여행 7번째 논문","_content":"\n<center> <img src=\"https://www.dropbox.com/s/145van5kldfvvd5/Screen%20Shot%202018-07-18%20at%201.19.30%20AM.png?dl=1\" width=\"700\"> </center>\n\n논문 저자 : John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov\n논문 링크 : https://arxiv.org/pdf/1707.06347.pdf\nProceeding : unpublished.\n정리 : 이동민, 장수영, 차금강\n\n---\n\n# 1. 들어가며...\n\nSutton_PG부터 시작하여 TRPO, GAE를 거쳐 PPO까지 대단히 고생많으셨습니다. 먼저 이 논문은 TRPO보다는 쉽습니다. cliping이라는 새로운 개념이 나오지만 크게 어렵진 않습니다.\n\n이 논문에서는 Reinforcement Learning에서 Policy Gradient Method의 새로운 방법인 PPO를 제안합니다. 이 방법은 agent가 환경과의 상호작용을 통해 data를 sampling하는 것과 stochastic gradient ascent를 사용하여 \"surrogate\" objective function을 optimizing하는 것을 번갈아가면서 하는 방법입니다. data sample마다 one gradient update를 수행하는 기존의 방법과는 달리, minibatch update의 multiple epochs를 가능하게 하는 새로운 objective function을 말합니다.\n\n이 알고리즘의 장점으로는\n\n- Trust Region Policy Optimization(TRPO)의 장점만을 가집니다. 다시 말해 알고리즘으로 학습하기에 훨씬 더 간단하고, 더 일반적이고, 더 좋은 sample complexity (empirically)를 가집니다. \n- 또한 다른 online policy gradient method들을 능가했고, 전반적으로 sample complexity(특히 computational complexity), simplicity, wall-time가 좋다고 합니다.\n\n<br><br>\n\n# 2. Introduction\n\n<br>\n## 2.1 대표적인 방법들\n\n- DQN\n    - Discrete action space를 가지는 문제들에는 효과적으로 적용 가능하지만, continuous control에도 잘 작동하는지는 검증되지 않았습니다.\n- A3C - \"Vanilla\" policy gradient methods\n    - Data efficiency와 robustness 측면이 좋지 않습니다. A3C의 data efficiency의 경우 on-policy로서 한 번 쓴 data는 바로 버리기 때문에 data efficiency가 좋지 않다는 것입니다.\n- TRPO\n    - 간단히 말해 복잡합니다.\n    - 또한 noise(ex. dropout)나 parameter sharing(policy와 value function 간 혹은 auxiliary tasks와의)를 포함하는 architecture와의 호환성 없습니다.\n\n<br>\n## 2.2 대표적인 방법들 대비 개선사항\n\n- Scalability\n    - large models and parallel implementations\n- Data Efficiency\n- Robustness\n    - hyperparameter tuning없이 다양한 문제들에 적용되어 해결\n\n<br>\n## 2.3 제안하는 알고리즘\n\n이 논문에는 Clipped probability ratios를 포함하는 objective function 제안하였습니다.\n\n- TRPO의 data efficiency와 robustness를 유지하면서도 first-order approximation만 사용합니다.\n- Policy 성능에 대한 lower bound를 제공합니다.\n\n따라서 Policy로부터의 data sampling과 sampled data를 이용한 최적화를 번갈아가면서 수행합니다.\n\n<br>\n## 2.4 실험 결과\n\n다양한 버전의 surrogate objectives 중에는 clipped probability ratio가 가장 성능이 좋았습니다.\n\n- Continuous control tasks에서 기존 알고리즘 대비 성능이 좋습니다.\n- Atari에서는 A2C 대비 sampling efficiency 측면에서는 성능이 월등히 좋으며, ACER 대비 훨씬 간단하지만 성능은 비슷합니다.\n\n<br><br>\n\n# 3. Backgroud: Policy Optimization\n\n<br>\n## 3.1 Policy Gradient Methods\n\n일반적인 PG Method들은 실험적으로 destructive large policy updates가 발생합니다.\n\n- 더 자세히 말하자면, PG Method가 수행하는 parameter space에서의 gradual update가 policy space에서는 큰 변화를 유발할 수 있다는 의미입니다.\n\n<br>\n## 3.2 Trust Region Methods\n\nPolicy update 크기에 대한 contraint하에 objective function(\"surrogate\" function)을 최대화하는 것이 목표입니다. 수식은 아래와 같습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/gx6udoz5upswyf9/Screen%20Shot%202018-07-31%20at%2011.11.49%20PM.png?dl=1\" width=\"400\"> </center>\n\n위의 수식은 contraint로 인해 excessive large policy update가 방지됩니다.\n\nTRPO에서는 constrained optimization problem을 풀기 위해서는 다음과 같은 방법들이 필요합니다.\n\n1. Fisher Information Matrix인 second-order derivative of KL divergence를 사용하거나,\n    - 여기서 second-order matrixes를 구하기 위해서는 많은 계산량 필요합니다.\n2. Conjugate Gradient를 사용합니다.\n    - Conjugate Gradient는 구현하기가 어렵습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/6xpw9igndl3dmb9/Screen%20Shot%202018-07-31%20at%2011.15.13%20PM.png?dl=1\" width=\"500\"> </center>\n\n원래 이론적으로 위의 수식과 같이 \"contraint\"가 아니라 objective에 \"penalty\"를 부여하는 형태입니다. 하지만 다양한 문제들(혹은 학습 중에 특성이 변하는 문제)에서 모두 잘 동작하는 single value $\\beta$를 찾는 것(robustness)이 어렵기 때문에, TRPO에서는 penalty대신 contraint를 취하는 방식을 택한 것입니다.\n\n<br><br>\n\n# 4. Clipped Surrogate Objective\n\n이번 section에서는 TRPO의 surrogate obejctive function을 강제적으로 clipping하는 방법에 대하여 말합니다.\n\n먼저 기존의 TRPO의 surrogate function을 다음과 같이 표현합니다. \n$$r_t(\\theta)=\\dfrac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta old}(a_t|s_t)}, \\, \\, r_t(\\theta old) = 1$$\n\n위의 수식을 이용하여 TRPO의 surrogate object를 최대화합니다.\n<center> <img src=\"https://www.dropbox.com/s/e524vwsyolp6h9n/Screen%20Shot%202018-07-26%20at%2010.12.22%20AM.png?dl=1\" width=\"350\"> </center>\n\n위의 수식을 그대로 사용한다면 excessively large policy update가 됩니다. 따라서 penalty를 이용하여 필요 이상의 policy update를 방지합니다. TRPO에서는 KL-Divergence를 이용하여 penalty를 적용하지만 PPO에서는 computation적으로 효율적인 penalty를 적용하고 excessively large policy update를 방지하기 위해 아래와 같은 clipping 기법을 사용합니다.\n\n$$L^{CLIP}(\\theta) = \\hat{E}_t [min(r_t(\\theta) \\, \\hat{A}_t,  clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\, \\hat{A}_t)]$$\n\n- 위의 수식에 대한 추가적인 설명\n    - $\\epsilon$은 hyperparameter로서 continuous control에서는 0.2일 때 성능이 가장 좋았으며, Atari Game에서는 0.1 x $\\alpha$ 값을 사용합니다. (여기서 $\\alpha$ 는 학습률로 1 에서 시작하여 학습이 진행됨에 따라 0 으로 감소합니다.)\n    - Clipped 와 unclipped objective 중 min 값을 택함으로써 $L^{CLIP} (\\theta)$는 unclipped objective에 대한 lowerbound가 됩니다.\n\n이어서 아래의 그림을 보겠습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/3wcuf2tq7q24fy6/Screen%20Shot%202018-07-26%20at%2010.25.08%20AM.png?dl=1\" width=\"600\"> </center>\n\n위의 그림을 보면 두 가지의 그래프(Advantage Function $\\hat{A}_t$의 부호에 따라)로 clip에 대해서 설명하고 있습니다.\n\n- Advantage Function $\\hat{A}_t$가 양수일 때\n    - Advantage가 현재보다 높다라는 뜻이며 파라미터를 +의 방향으로 업데이트 하여야 합니다. 다시 말해 어떠한 상태 $s$에서 행동 $a$가 평균보다 좋다는 의미입니다. 따라서 이를 취할 확률이 증가하게 되고, $r_t (\\theta)$를 clip하여 $\\epsilon$보다 커지지 않도록 유도하는 것입니다.\n    - 추가적으로, TRPO에서 다뤘던 constraint가 아니라 단순히 clip하는 것이기 때문에 $\\pi_\\theta (a_t|s_t)$의 증가량은 $\\epsilon$보다 더 커질 수도 있지만, 더 커져봐야 objective function에 도움이 되지 않기에 대부분 $\\epsilon$ 이하로 유지합니다.\n    - 또한 만약 $r_t (\\theta)$가 objective function의 값을 감소시키는 방향으로 움직이는 경우더라도 $1-\\epsilon$보다 작아져도 됩니다. 여기서의 목적은 최대한 lowerbound를 구하는 것이 목적이기 때문입니다. 그러니까 쉽게 말해서 왼쪽 그림에서도 볼 수 있듯이 $1+\\epsilon$만 구하는 데에 포커스를 맞추고 있고, $1-\\epsilon$은 신경쓰지 않고 있는 것입니다. (이 부분은 Advantage Function $\\hat{A}_t$가 음수일 때도 동일합니다.)\n- Advantage Function $\\hat{A}_t$가 음수일 때 \n    - Advantage가 현재보다 좋지 않다라는 뜻이며 그의 반대 방향으로 업데이트 하여야 합니다. 다시 말해 어떠한 상태 $s$에서 행동 $a$가 평균보다 좋지 않다는 의미입니다. 따라서 이를 취할 확률이 감소하게 되고, $r_t (\\theta)$를 clip하여 $\\epsilon$보다 작아지지 않도록 유도하는 것입니다.  \n    - 또한 $r_t(\\theta)$은 확률을 뜻하는 두 개의 함수를 분자 분모로 가지고 있으며, 분수로 구성되어 있기 때문에 무조건 양수로 이루어져 있습니다. Advantage function인 $\\hat{A}_t$와 곱해져 Objective function인 $L^{CLIP}$은 Advantage function과 방향이 같아집니다.\n\n위의 설명으로 인하여 나오는 그림이 아래의 그림입니다.\n\n<center> <img src=\"https://www.dropbox.com/s/f7i97geiligfej9/Screen%20Shot%202018-07-26%20at%2011.02.17%20AM.png?dl=1\" width=\"700\"> </center>\n\n위의 그림에서 빨간색 그래프를 보면,\n\n- $L^{CLIP}$은 min 값들의 평균이기 때문에 평균들의 min 값보다는 더 작아집니다. 즉, 주황색 그래프와 초록색 그래프 중 작은값보다 더 작아지는 것을 볼 수 있습니다.\n- $L^{CLIP}$을 최대화하는 $\\theta$가 다음 $\\pi_\\theta$가 됩니다. 다시 말해 PPO는 기존 PG 방법들처럼 parameter space에서 parameter $\\theta$를 점진적으로 업데이트하는 것이 아니라, 매번 policy를 maximize하는 방향으로 업데이트하는 것으로 볼 수 있는 것입니다.\n\n\n<br><br>\n\n# 5. Adaptive KL Penalty Coefficient\n\n이전까지 설명했던 Clipped Surrogate Objective 방법과 달리 기존의 TRPO에서 Adaptive한 파라미터를 적용한 또 다른 방법에 대해서 알아보겠습니다. 사실 이 방법은 앞서본 clip을 사용한 방법보다는 성능이 좋지 않습니다. 하지만 baseline으로써 알아볼 필요가 있습니다.\n\nclip에서 다뤘던 Probability ratio $r_t(\\theta)$ 대신 KL divergence를 이용하여 penalty를 줍니다. 그 결과 각각의 policy update에 대해 KL divergence $d_{targ}$의 target value를 얻습니다.\n\n- clipped surrogate objective 대신하여 or 추가적으로 사용할 수 있다고 합니다. \n- 자체 실험 결과에서는 clipped surrogate objective 보다는 성능이 안 좋았다고 합니다.\n- 둘 다 사용한 실험 결과는 없습니다.\n\n이 알고리즘에서 각각의 policy update에 적용하는 step은 다음과 같습니다. \n\n1. KL-penalized objective 최적화를 합니다. 기존의 TRPO의 Objective function에 $\\beta$를 적용하여 다음과 같이 $\\beta$를 Adaptive하게 조절하고 있습니다. 수식은 아래와 같습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/ojq7kfobf0f8x9n/Screen%20Shot%202018-07-26%20at%2011.22.44%20AM.png?dl=1\" width=\"500\"> </center>\n\n2. <img src=\"https://www.dropbox.com/s/j9phkalrbgf45k0/Screen%20Shot%202018-07-26%20at%2011.34.25%20AM.png?dl=1\" width=\"230\">을 계산합니다.\n    - 만약 $d < d_{targ} \\, / \\, 1.5$라면, 그 때 $\\beta \\leftarrow \\beta \\, / 2$\n    - 만약 $d > d_{targ} \\, x \\, 1.5$라면, 그 때 $\\beta \\leftarrow \\beta \\, x 2$\n    - 즉, KL-divergence 값이 일정 이상 커지게 되면 objective function에 penalty를 더 크게 부과합니다.\n    - 갱신된 $\\beta$ 값은 다음 policy update 때 사용합니다. \n\n$\\beta$를 조절하는 방법은 만약 $\\theta old$와 $\\theta$간의 파라미터 차이가 크다면 penalty를 강하게 부여하여 차이를 작게하고, 파라미터 차이가 작다면 penalty를 완화시켜 주어 차이를 더 크게하는 것입니다.\n\n정리하자면, KL-divergence를 constraint로 둔 것이 아니기 때문에, 간혹 excessive large policy update가 발생할 수도 있지만, KL Penaly coefficienct인 $\\beta$가 KL-divergence에 따라 adpative 하게 조정됨으로써 excessive large policy update가 지속적으로 발생되는 것을 방지하는 것입니다.\n\n<br><br>\n\n# 6. Algorithm\n\n앞서 다뤘던 section들은 어떻게 policy만을 업데이트 하는지에 대해 설명하고 있습니다. 이번 section에서는 value, entropy-exploration과 합쳐서 어떻게 통합적으로 업데이트 하는지에 대해서 알아보겠습니다.\n\n먼저 Variance-reduced advantage function estimator을 계산하기 위해 learned state-value fucntion $V(s)$을 사용합니다. 여기에는 두 가지 방법이 있습니다.\n\n- Generalized advantage estimation\n- Finite horizontal estimators\n\n만약 policy와 value function 간 parameter sharing하는 neural network architecture를 사용한다면, policy surrogate와 value function error term을 combine한 loss function을 사용해야 합니다. 또한 이 loss function에 entropy bonus term을 추가하여, 충분한 exploration이 될 수 있도록 합니다.(exploration하는 부분은 [A3C 논문](https://arxiv.org/pdf/1602.01783.pdf)에 나와있습니다.)\n\n그래서 앞써 다뤘던 objective function을 $L^{CLIP}$라고 표현한다면 PPO에서 제시하는 통합적으로 최대화해야하는 objective function은 다음과 같이 표현할 수 있습니다.\n\n$$L_t^{CLIP+VF+S} (\\theta) = \\hat{E}_t [L^{CLIP} (\\theta) - c_1 L_t^{VF} (\\theta) + c_2 S[\\pi_\\theta] (s_t)]$$\n\n위의 수식에 대한 추가적인 설명은 다음과 같습니다.\n\n- $c_1, c_2$ : coefficients\n- $S$ : entropy bonus\n- $L_t^{VF}$ : squared-error loss $(V_\\theta (s_t) - V_t^{targ})^2$\n\n위의 objective function을 최대화하면 Reinforcement Learning을 통한 policy 학습, state-value function의 학습, exploration을 할 수 있습니다.\n\n추가적으로 A3C와 같은 요즘 인기있는 PG의 style에서는 T time steps(T는 episode length 보다 훨씬 작은 크기) 동안 policy에 따라서 sample들을 얻고, 이 sample들을 업데이트에 사용합니다. 이러한 방식은 time step T 까지만 고려하는 advantage estimator가 필요하며, A3C에서 다음과 같이 사용합니다.\n\n$$\\hat{A}_t = - V(s_t) + r_t + \\gamma r_{t+1} + \\cdots + \\gamma^{T-t+1} r_{T-1} + \\gamma^{T-t}V(s_T)$$\n\n- 여기서 $t$는 주어진 length T trajectory segment 내에서 $[0, T]$에 있는 time index입니다.\n\n위의 수식에 더하여 generalized version인 GAE의 truncated version(generalized advantage estimation)을 사용합니다.($\\lambda$가 1이면 위 식과 같아집니다.) 수식은 아래와 같습니다. \n\n$$\\hat{A}_t = \\delta_t + (\\gamma\\lambda)\\delta_{t+1} + \\cdots + (\\gamma\\lambda)^{T-t+1}\\delta_{T-1},$$$$where \\,\\,\\,\\, \\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$$\n\n그래서 고정된 length T trajectory segment를 사용한 PPO algorithm은 다음과 같은 pseudo code로 나타낼 수 있습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/q00y8x7eryl1ncd/Screen%20Shot%202018-07-26%20at%202.28.03%20PM.png?dl=1\" width=\"800\"> </center>\n\n- 여기서 actor는 A3C처럼 병렬로 두어도 상관없습니다.\n- advantage estimate 계산을 통해서 surrogate loss를 계산합니다.\n\n<br><br>\n\n# 7. Experimnet\n\n<br>\n## 7.1 Surrogate Objectives의 비교\n\n이번 section에서는 3가지 Surrogate Objectives를 비교분석 하고 있습니다.\n\n* No clipping or penalty: $L_t(\\theta) = r_t(\\theta)\\hat{A}_t$\n* Clipping: $L_t(\\theta) = min(r_t(\\theta) \\, \\hat{A}_t, \\, clip(r_t(\\theta), 1-\\epsilon,1+\\epsilon) \\, \\hat{A}_t)$\n* KL penalty(fixed or adaptive): <img src=\"https://www.dropbox.com/s/ksnhlxz2riuns1p/Screen%20Shot%202018-07-26%20at%202.42.50%20PM.png?dl=1\" width=\"270\">\n\n그리고 환경은 MuJoCo를 사용하며 OpenAI Gym 환경에서 실험을 진행하였습니다. 그리고 네트워크들은 2개의 hidden layer를 가지며 각각 64개의 unit들을 가지고 있습니다. 그리고 tanh를 활성화 함수로써 사용하고 있으며 네트워크는 Gaussian distribution을 출력하는 것으로 구성되어 있습니다. 그리고 Policy Network와 Value Network는 파라미터들을 공유하고 있지 않으며, 위에서 설명한 Exploration을 위한 entropy는 사용하지 않습니다.\n\n7개의 환경에서 실험하였으며 각 환경들은 HalfCheetah, Hopper, InvertedDoublePendulum, InvertedPerdulum, Reacher, Swimmer, Walker2d를 사용하였으며 모든 환경들은 v1 버전을 사용하였습니다.\n\n아래의 표는 실험 결과입니다. 세 가지 Surrogate Objective에 대해 각 파라미터들을 표에 표기된 방식대로 변화하며 실험하였습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/4xbkxh4m82mu1vo/Screen%20Shot%202018-07-26%20at%202.45.42%20PM.png?dl=1\" width=\"450\"> </center>\n\n<br>\n## 7.2 Comparison to other Algorithms in the Continuous Domain\n\n이번 section에서는 Clipping 버전의 PPO와 다른 알고리즘들 간의 비교를 연속적인 Action을 가지는 환경에서 실험을 한 결과를 보여주고 있습니다. 다른 알고리즘은 TRPO, cross-entropy method, vanilla policy gradient with adaptive stepsize, A2C, A2C with trust region를 사용하였습니다. 환경과 사용한 네트워크의 구성은 이전 section과 동일합니다.\n\n<center> <img src=\"https://www.dropbox.com/s/tf7djngioxnxtef/Screen%20Shot%202018-07-26%20at%202.50.23%20PM.png?dl=1\" width=\"900\"> </center>\n\n여타 다른 알고리즘보다 좋은 성능을 보이는 것을 알 수 있습니다.\n\n<br>\n## 7.3 Showcase in the Continuous Domain: Humanoid Running and Steering\n\n이번 section에서는 Humanoid-v0, HumanoidFlagrun-v0, HumanoidFlagrunHarder-v0의 환경에서 Roboschool을 사용하여 실험하였습니다.\n\n- Humanoid-v0는 단순히 앞으로 걸어나가는 환경\n- HumanoidFlagrun-v0은 200스텝마다 혹은 목적지에 도착할 때 마다 위치가 바뀌는 목적지에 걸어서 도달하는 환경입니다. \n- HumanoidFlagrunHarder-v0은 환경이 초기화될 때 마다 특정 경계안에 Humanoid가 위치하게 되며 특정 영역 밖으로 걸어나가는 것을 수행하는 환경입니다.\n\n\n<center> <img src=\"https://www.dropbox.com/s/y112ua7il6l0296/Screen%20Shot%202018-07-26%20at%202.53.01%20PM.png?dl=1\" width=\"900\"> </center>\n\n- Roboschool을 사용하여 3D humanoid control task에서 PPO 알고리즘으로 학습시킨 Learning curve입니다.\n\n<center> <img src=\"https://www.dropbox.com/s/09scyk6zeviyswt/Screen%20Shot%202018-07-26%20at%202.53.10%20PM.png?dl=1\" width=\"900\"> </center>\n\n- Roboschool Humanoid Flagrun에서 학습된 policy의 frame들입니다. \n\n<br>\n## 7.4 Comparison to Other Algorithms on the Atari Domain\n\n이번 section에서는 Atari Domain에서 PPO, A2C, ACER(Actor Critic with Experience Replay)의 3가지 알고리즘을 비교합니다.\n\n<center> <img src=\"https://www.dropbox.com/s/jxuqdncxpnoyqgi/Screen%20Shot%202018-07-26%20at%203.00.00%20PM.png?dl=1\" width=\"600\"> </center>\n\n전체 training에 대해서는 PPO가 ACER보다 좋은 성능을 내고 있습니다. 하지만 마지막 100 에피소드만을 비교했을 때는 PPO보다 ACER이 더 좋은 성능을 보이고 있습니다. 이는 PPO가 더 빨리 최종 능력에 도달하지만, ACER이 가진 potential이 더 높다는 것입니다.\n\n<br><br>\n\n# 8. Conclusion\n\n이 논문에서는 policy update를 하기 위한 방법으로 stochastic gradient ascent의 multiple epchs를 사용하는 policy optimization method들의 하나의 알고리즘은 Proximal Policy Optimization(PPO)를 소개합니다.\n\n이 알고리즘은 trust region method의 stability와 reliability를 가집니다. 여기에 더하여 학습하기에 훨씬 더 간단하고, 더 일반적인 setting으로 적용하기에 편한 A3C로서 code를 구성하기에도 편하고, 계산량도 훨씬 덜합니다. 그리고 전반적으로 더 좋은 성능을 가집니다.\n\n<br><br>\n\n# 처음으로\n\n## [PG Travel Guide](https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/)\n\n<br>\n\n# 이전으로\n\n## [GAE 여행하기](https://reinforcement-learning-kr.github.io/2018/06/23/6_gae/)\n\n<br>\n\n# 다음으로\n\n## [PPO Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/ppo_gae.py)","source":"_posts/7_ppo.md","raw":"---\ntitle: Proximal Policy Optimization\ndate: 2018-06-22 16:53:12\ntags: [\"프로젝트\", \"피지여행\"]\ncategories: 프로젝트\nauthor: 이동민, 장수영, 차금강\nsubtitle: 피지여행 7번째 논문\n---\n\n<center> <img src=\"https://www.dropbox.com/s/145van5kldfvvd5/Screen%20Shot%202018-07-18%20at%201.19.30%20AM.png?dl=1\" width=\"700\"> </center>\n\n논문 저자 : John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov\n논문 링크 : https://arxiv.org/pdf/1707.06347.pdf\nProceeding : unpublished.\n정리 : 이동민, 장수영, 차금강\n\n---\n\n# 1. 들어가며...\n\nSutton_PG부터 시작하여 TRPO, GAE를 거쳐 PPO까지 대단히 고생많으셨습니다. 먼저 이 논문은 TRPO보다는 쉽습니다. cliping이라는 새로운 개념이 나오지만 크게 어렵진 않습니다.\n\n이 논문에서는 Reinforcement Learning에서 Policy Gradient Method의 새로운 방법인 PPO를 제안합니다. 이 방법은 agent가 환경과의 상호작용을 통해 data를 sampling하는 것과 stochastic gradient ascent를 사용하여 \"surrogate\" objective function을 optimizing하는 것을 번갈아가면서 하는 방법입니다. data sample마다 one gradient update를 수행하는 기존의 방법과는 달리, minibatch update의 multiple epochs를 가능하게 하는 새로운 objective function을 말합니다.\n\n이 알고리즘의 장점으로는\n\n- Trust Region Policy Optimization(TRPO)의 장점만을 가집니다. 다시 말해 알고리즘으로 학습하기에 훨씬 더 간단하고, 더 일반적이고, 더 좋은 sample complexity (empirically)를 가집니다. \n- 또한 다른 online policy gradient method들을 능가했고, 전반적으로 sample complexity(특히 computational complexity), simplicity, wall-time가 좋다고 합니다.\n\n<br><br>\n\n# 2. Introduction\n\n<br>\n## 2.1 대표적인 방법들\n\n- DQN\n    - Discrete action space를 가지는 문제들에는 효과적으로 적용 가능하지만, continuous control에도 잘 작동하는지는 검증되지 않았습니다.\n- A3C - \"Vanilla\" policy gradient methods\n    - Data efficiency와 robustness 측면이 좋지 않습니다. A3C의 data efficiency의 경우 on-policy로서 한 번 쓴 data는 바로 버리기 때문에 data efficiency가 좋지 않다는 것입니다.\n- TRPO\n    - 간단히 말해 복잡합니다.\n    - 또한 noise(ex. dropout)나 parameter sharing(policy와 value function 간 혹은 auxiliary tasks와의)를 포함하는 architecture와의 호환성 없습니다.\n\n<br>\n## 2.2 대표적인 방법들 대비 개선사항\n\n- Scalability\n    - large models and parallel implementations\n- Data Efficiency\n- Robustness\n    - hyperparameter tuning없이 다양한 문제들에 적용되어 해결\n\n<br>\n## 2.3 제안하는 알고리즘\n\n이 논문에는 Clipped probability ratios를 포함하는 objective function 제안하였습니다.\n\n- TRPO의 data efficiency와 robustness를 유지하면서도 first-order approximation만 사용합니다.\n- Policy 성능에 대한 lower bound를 제공합니다.\n\n따라서 Policy로부터의 data sampling과 sampled data를 이용한 최적화를 번갈아가면서 수행합니다.\n\n<br>\n## 2.4 실험 결과\n\n다양한 버전의 surrogate objectives 중에는 clipped probability ratio가 가장 성능이 좋았습니다.\n\n- Continuous control tasks에서 기존 알고리즘 대비 성능이 좋습니다.\n- Atari에서는 A2C 대비 sampling efficiency 측면에서는 성능이 월등히 좋으며, ACER 대비 훨씬 간단하지만 성능은 비슷합니다.\n\n<br><br>\n\n# 3. Backgroud: Policy Optimization\n\n<br>\n## 3.1 Policy Gradient Methods\n\n일반적인 PG Method들은 실험적으로 destructive large policy updates가 발생합니다.\n\n- 더 자세히 말하자면, PG Method가 수행하는 parameter space에서의 gradual update가 policy space에서는 큰 변화를 유발할 수 있다는 의미입니다.\n\n<br>\n## 3.2 Trust Region Methods\n\nPolicy update 크기에 대한 contraint하에 objective function(\"surrogate\" function)을 최대화하는 것이 목표입니다. 수식은 아래와 같습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/gx6udoz5upswyf9/Screen%20Shot%202018-07-31%20at%2011.11.49%20PM.png?dl=1\" width=\"400\"> </center>\n\n위의 수식은 contraint로 인해 excessive large policy update가 방지됩니다.\n\nTRPO에서는 constrained optimization problem을 풀기 위해서는 다음과 같은 방법들이 필요합니다.\n\n1. Fisher Information Matrix인 second-order derivative of KL divergence를 사용하거나,\n    - 여기서 second-order matrixes를 구하기 위해서는 많은 계산량 필요합니다.\n2. Conjugate Gradient를 사용합니다.\n    - Conjugate Gradient는 구현하기가 어렵습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/6xpw9igndl3dmb9/Screen%20Shot%202018-07-31%20at%2011.15.13%20PM.png?dl=1\" width=\"500\"> </center>\n\n원래 이론적으로 위의 수식과 같이 \"contraint\"가 아니라 objective에 \"penalty\"를 부여하는 형태입니다. 하지만 다양한 문제들(혹은 학습 중에 특성이 변하는 문제)에서 모두 잘 동작하는 single value $\\beta$를 찾는 것(robustness)이 어렵기 때문에, TRPO에서는 penalty대신 contraint를 취하는 방식을 택한 것입니다.\n\n<br><br>\n\n# 4. Clipped Surrogate Objective\n\n이번 section에서는 TRPO의 surrogate obejctive function을 강제적으로 clipping하는 방법에 대하여 말합니다.\n\n먼저 기존의 TRPO의 surrogate function을 다음과 같이 표현합니다. \n$$r_t(\\theta)=\\dfrac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta old}(a_t|s_t)}, \\, \\, r_t(\\theta old) = 1$$\n\n위의 수식을 이용하여 TRPO의 surrogate object를 최대화합니다.\n<center> <img src=\"https://www.dropbox.com/s/e524vwsyolp6h9n/Screen%20Shot%202018-07-26%20at%2010.12.22%20AM.png?dl=1\" width=\"350\"> </center>\n\n위의 수식을 그대로 사용한다면 excessively large policy update가 됩니다. 따라서 penalty를 이용하여 필요 이상의 policy update를 방지합니다. TRPO에서는 KL-Divergence를 이용하여 penalty를 적용하지만 PPO에서는 computation적으로 효율적인 penalty를 적용하고 excessively large policy update를 방지하기 위해 아래와 같은 clipping 기법을 사용합니다.\n\n$$L^{CLIP}(\\theta) = \\hat{E}_t [min(r_t(\\theta) \\, \\hat{A}_t,  clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\, \\hat{A}_t)]$$\n\n- 위의 수식에 대한 추가적인 설명\n    - $\\epsilon$은 hyperparameter로서 continuous control에서는 0.2일 때 성능이 가장 좋았으며, Atari Game에서는 0.1 x $\\alpha$ 값을 사용합니다. (여기서 $\\alpha$ 는 학습률로 1 에서 시작하여 학습이 진행됨에 따라 0 으로 감소합니다.)\n    - Clipped 와 unclipped objective 중 min 값을 택함으로써 $L^{CLIP} (\\theta)$는 unclipped objective에 대한 lowerbound가 됩니다.\n\n이어서 아래의 그림을 보겠습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/3wcuf2tq7q24fy6/Screen%20Shot%202018-07-26%20at%2010.25.08%20AM.png?dl=1\" width=\"600\"> </center>\n\n위의 그림을 보면 두 가지의 그래프(Advantage Function $\\hat{A}_t$의 부호에 따라)로 clip에 대해서 설명하고 있습니다.\n\n- Advantage Function $\\hat{A}_t$가 양수일 때\n    - Advantage가 현재보다 높다라는 뜻이며 파라미터를 +의 방향으로 업데이트 하여야 합니다. 다시 말해 어떠한 상태 $s$에서 행동 $a$가 평균보다 좋다는 의미입니다. 따라서 이를 취할 확률이 증가하게 되고, $r_t (\\theta)$를 clip하여 $\\epsilon$보다 커지지 않도록 유도하는 것입니다.\n    - 추가적으로, TRPO에서 다뤘던 constraint가 아니라 단순히 clip하는 것이기 때문에 $\\pi_\\theta (a_t|s_t)$의 증가량은 $\\epsilon$보다 더 커질 수도 있지만, 더 커져봐야 objective function에 도움이 되지 않기에 대부분 $\\epsilon$ 이하로 유지합니다.\n    - 또한 만약 $r_t (\\theta)$가 objective function의 값을 감소시키는 방향으로 움직이는 경우더라도 $1-\\epsilon$보다 작아져도 됩니다. 여기서의 목적은 최대한 lowerbound를 구하는 것이 목적이기 때문입니다. 그러니까 쉽게 말해서 왼쪽 그림에서도 볼 수 있듯이 $1+\\epsilon$만 구하는 데에 포커스를 맞추고 있고, $1-\\epsilon$은 신경쓰지 않고 있는 것입니다. (이 부분은 Advantage Function $\\hat{A}_t$가 음수일 때도 동일합니다.)\n- Advantage Function $\\hat{A}_t$가 음수일 때 \n    - Advantage가 현재보다 좋지 않다라는 뜻이며 그의 반대 방향으로 업데이트 하여야 합니다. 다시 말해 어떠한 상태 $s$에서 행동 $a$가 평균보다 좋지 않다는 의미입니다. 따라서 이를 취할 확률이 감소하게 되고, $r_t (\\theta)$를 clip하여 $\\epsilon$보다 작아지지 않도록 유도하는 것입니다.  \n    - 또한 $r_t(\\theta)$은 확률을 뜻하는 두 개의 함수를 분자 분모로 가지고 있으며, 분수로 구성되어 있기 때문에 무조건 양수로 이루어져 있습니다. Advantage function인 $\\hat{A}_t$와 곱해져 Objective function인 $L^{CLIP}$은 Advantage function과 방향이 같아집니다.\n\n위의 설명으로 인하여 나오는 그림이 아래의 그림입니다.\n\n<center> <img src=\"https://www.dropbox.com/s/f7i97geiligfej9/Screen%20Shot%202018-07-26%20at%2011.02.17%20AM.png?dl=1\" width=\"700\"> </center>\n\n위의 그림에서 빨간색 그래프를 보면,\n\n- $L^{CLIP}$은 min 값들의 평균이기 때문에 평균들의 min 값보다는 더 작아집니다. 즉, 주황색 그래프와 초록색 그래프 중 작은값보다 더 작아지는 것을 볼 수 있습니다.\n- $L^{CLIP}$을 최대화하는 $\\theta$가 다음 $\\pi_\\theta$가 됩니다. 다시 말해 PPO는 기존 PG 방법들처럼 parameter space에서 parameter $\\theta$를 점진적으로 업데이트하는 것이 아니라, 매번 policy를 maximize하는 방향으로 업데이트하는 것으로 볼 수 있는 것입니다.\n\n\n<br><br>\n\n# 5. Adaptive KL Penalty Coefficient\n\n이전까지 설명했던 Clipped Surrogate Objective 방법과 달리 기존의 TRPO에서 Adaptive한 파라미터를 적용한 또 다른 방법에 대해서 알아보겠습니다. 사실 이 방법은 앞서본 clip을 사용한 방법보다는 성능이 좋지 않습니다. 하지만 baseline으로써 알아볼 필요가 있습니다.\n\nclip에서 다뤘던 Probability ratio $r_t(\\theta)$ 대신 KL divergence를 이용하여 penalty를 줍니다. 그 결과 각각의 policy update에 대해 KL divergence $d_{targ}$의 target value를 얻습니다.\n\n- clipped surrogate objective 대신하여 or 추가적으로 사용할 수 있다고 합니다. \n- 자체 실험 결과에서는 clipped surrogate objective 보다는 성능이 안 좋았다고 합니다.\n- 둘 다 사용한 실험 결과는 없습니다.\n\n이 알고리즘에서 각각의 policy update에 적용하는 step은 다음과 같습니다. \n\n1. KL-penalized objective 최적화를 합니다. 기존의 TRPO의 Objective function에 $\\beta$를 적용하여 다음과 같이 $\\beta$를 Adaptive하게 조절하고 있습니다. 수식은 아래와 같습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/ojq7kfobf0f8x9n/Screen%20Shot%202018-07-26%20at%2011.22.44%20AM.png?dl=1\" width=\"500\"> </center>\n\n2. <img src=\"https://www.dropbox.com/s/j9phkalrbgf45k0/Screen%20Shot%202018-07-26%20at%2011.34.25%20AM.png?dl=1\" width=\"230\">을 계산합니다.\n    - 만약 $d < d_{targ} \\, / \\, 1.5$라면, 그 때 $\\beta \\leftarrow \\beta \\, / 2$\n    - 만약 $d > d_{targ} \\, x \\, 1.5$라면, 그 때 $\\beta \\leftarrow \\beta \\, x 2$\n    - 즉, KL-divergence 값이 일정 이상 커지게 되면 objective function에 penalty를 더 크게 부과합니다.\n    - 갱신된 $\\beta$ 값은 다음 policy update 때 사용합니다. \n\n$\\beta$를 조절하는 방법은 만약 $\\theta old$와 $\\theta$간의 파라미터 차이가 크다면 penalty를 강하게 부여하여 차이를 작게하고, 파라미터 차이가 작다면 penalty를 완화시켜 주어 차이를 더 크게하는 것입니다.\n\n정리하자면, KL-divergence를 constraint로 둔 것이 아니기 때문에, 간혹 excessive large policy update가 발생할 수도 있지만, KL Penaly coefficienct인 $\\beta$가 KL-divergence에 따라 adpative 하게 조정됨으로써 excessive large policy update가 지속적으로 발생되는 것을 방지하는 것입니다.\n\n<br><br>\n\n# 6. Algorithm\n\n앞서 다뤘던 section들은 어떻게 policy만을 업데이트 하는지에 대해 설명하고 있습니다. 이번 section에서는 value, entropy-exploration과 합쳐서 어떻게 통합적으로 업데이트 하는지에 대해서 알아보겠습니다.\n\n먼저 Variance-reduced advantage function estimator을 계산하기 위해 learned state-value fucntion $V(s)$을 사용합니다. 여기에는 두 가지 방법이 있습니다.\n\n- Generalized advantage estimation\n- Finite horizontal estimators\n\n만약 policy와 value function 간 parameter sharing하는 neural network architecture를 사용한다면, policy surrogate와 value function error term을 combine한 loss function을 사용해야 합니다. 또한 이 loss function에 entropy bonus term을 추가하여, 충분한 exploration이 될 수 있도록 합니다.(exploration하는 부분은 [A3C 논문](https://arxiv.org/pdf/1602.01783.pdf)에 나와있습니다.)\n\n그래서 앞써 다뤘던 objective function을 $L^{CLIP}$라고 표현한다면 PPO에서 제시하는 통합적으로 최대화해야하는 objective function은 다음과 같이 표현할 수 있습니다.\n\n$$L_t^{CLIP+VF+S} (\\theta) = \\hat{E}_t [L^{CLIP} (\\theta) - c_1 L_t^{VF} (\\theta) + c_2 S[\\pi_\\theta] (s_t)]$$\n\n위의 수식에 대한 추가적인 설명은 다음과 같습니다.\n\n- $c_1, c_2$ : coefficients\n- $S$ : entropy bonus\n- $L_t^{VF}$ : squared-error loss $(V_\\theta (s_t) - V_t^{targ})^2$\n\n위의 objective function을 최대화하면 Reinforcement Learning을 통한 policy 학습, state-value function의 학습, exploration을 할 수 있습니다.\n\n추가적으로 A3C와 같은 요즘 인기있는 PG의 style에서는 T time steps(T는 episode length 보다 훨씬 작은 크기) 동안 policy에 따라서 sample들을 얻고, 이 sample들을 업데이트에 사용합니다. 이러한 방식은 time step T 까지만 고려하는 advantage estimator가 필요하며, A3C에서 다음과 같이 사용합니다.\n\n$$\\hat{A}_t = - V(s_t) + r_t + \\gamma r_{t+1} + \\cdots + \\gamma^{T-t+1} r_{T-1} + \\gamma^{T-t}V(s_T)$$\n\n- 여기서 $t$는 주어진 length T trajectory segment 내에서 $[0, T]$에 있는 time index입니다.\n\n위의 수식에 더하여 generalized version인 GAE의 truncated version(generalized advantage estimation)을 사용합니다.($\\lambda$가 1이면 위 식과 같아집니다.) 수식은 아래와 같습니다. \n\n$$\\hat{A}_t = \\delta_t + (\\gamma\\lambda)\\delta_{t+1} + \\cdots + (\\gamma\\lambda)^{T-t+1}\\delta_{T-1},$$$$where \\,\\,\\,\\, \\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$$\n\n그래서 고정된 length T trajectory segment를 사용한 PPO algorithm은 다음과 같은 pseudo code로 나타낼 수 있습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/q00y8x7eryl1ncd/Screen%20Shot%202018-07-26%20at%202.28.03%20PM.png?dl=1\" width=\"800\"> </center>\n\n- 여기서 actor는 A3C처럼 병렬로 두어도 상관없습니다.\n- advantage estimate 계산을 통해서 surrogate loss를 계산합니다.\n\n<br><br>\n\n# 7. Experimnet\n\n<br>\n## 7.1 Surrogate Objectives의 비교\n\n이번 section에서는 3가지 Surrogate Objectives를 비교분석 하고 있습니다.\n\n* No clipping or penalty: $L_t(\\theta) = r_t(\\theta)\\hat{A}_t$\n* Clipping: $L_t(\\theta) = min(r_t(\\theta) \\, \\hat{A}_t, \\, clip(r_t(\\theta), 1-\\epsilon,1+\\epsilon) \\, \\hat{A}_t)$\n* KL penalty(fixed or adaptive): <img src=\"https://www.dropbox.com/s/ksnhlxz2riuns1p/Screen%20Shot%202018-07-26%20at%202.42.50%20PM.png?dl=1\" width=\"270\">\n\n그리고 환경은 MuJoCo를 사용하며 OpenAI Gym 환경에서 실험을 진행하였습니다. 그리고 네트워크들은 2개의 hidden layer를 가지며 각각 64개의 unit들을 가지고 있습니다. 그리고 tanh를 활성화 함수로써 사용하고 있으며 네트워크는 Gaussian distribution을 출력하는 것으로 구성되어 있습니다. 그리고 Policy Network와 Value Network는 파라미터들을 공유하고 있지 않으며, 위에서 설명한 Exploration을 위한 entropy는 사용하지 않습니다.\n\n7개의 환경에서 실험하였으며 각 환경들은 HalfCheetah, Hopper, InvertedDoublePendulum, InvertedPerdulum, Reacher, Swimmer, Walker2d를 사용하였으며 모든 환경들은 v1 버전을 사용하였습니다.\n\n아래의 표는 실험 결과입니다. 세 가지 Surrogate Objective에 대해 각 파라미터들을 표에 표기된 방식대로 변화하며 실험하였습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/4xbkxh4m82mu1vo/Screen%20Shot%202018-07-26%20at%202.45.42%20PM.png?dl=1\" width=\"450\"> </center>\n\n<br>\n## 7.2 Comparison to other Algorithms in the Continuous Domain\n\n이번 section에서는 Clipping 버전의 PPO와 다른 알고리즘들 간의 비교를 연속적인 Action을 가지는 환경에서 실험을 한 결과를 보여주고 있습니다. 다른 알고리즘은 TRPO, cross-entropy method, vanilla policy gradient with adaptive stepsize, A2C, A2C with trust region를 사용하였습니다. 환경과 사용한 네트워크의 구성은 이전 section과 동일합니다.\n\n<center> <img src=\"https://www.dropbox.com/s/tf7djngioxnxtef/Screen%20Shot%202018-07-26%20at%202.50.23%20PM.png?dl=1\" width=\"900\"> </center>\n\n여타 다른 알고리즘보다 좋은 성능을 보이는 것을 알 수 있습니다.\n\n<br>\n## 7.3 Showcase in the Continuous Domain: Humanoid Running and Steering\n\n이번 section에서는 Humanoid-v0, HumanoidFlagrun-v0, HumanoidFlagrunHarder-v0의 환경에서 Roboschool을 사용하여 실험하였습니다.\n\n- Humanoid-v0는 단순히 앞으로 걸어나가는 환경\n- HumanoidFlagrun-v0은 200스텝마다 혹은 목적지에 도착할 때 마다 위치가 바뀌는 목적지에 걸어서 도달하는 환경입니다. \n- HumanoidFlagrunHarder-v0은 환경이 초기화될 때 마다 특정 경계안에 Humanoid가 위치하게 되며 특정 영역 밖으로 걸어나가는 것을 수행하는 환경입니다.\n\n\n<center> <img src=\"https://www.dropbox.com/s/y112ua7il6l0296/Screen%20Shot%202018-07-26%20at%202.53.01%20PM.png?dl=1\" width=\"900\"> </center>\n\n- Roboschool을 사용하여 3D humanoid control task에서 PPO 알고리즘으로 학습시킨 Learning curve입니다.\n\n<center> <img src=\"https://www.dropbox.com/s/09scyk6zeviyswt/Screen%20Shot%202018-07-26%20at%202.53.10%20PM.png?dl=1\" width=\"900\"> </center>\n\n- Roboschool Humanoid Flagrun에서 학습된 policy의 frame들입니다. \n\n<br>\n## 7.4 Comparison to Other Algorithms on the Atari Domain\n\n이번 section에서는 Atari Domain에서 PPO, A2C, ACER(Actor Critic with Experience Replay)의 3가지 알고리즘을 비교합니다.\n\n<center> <img src=\"https://www.dropbox.com/s/jxuqdncxpnoyqgi/Screen%20Shot%202018-07-26%20at%203.00.00%20PM.png?dl=1\" width=\"600\"> </center>\n\n전체 training에 대해서는 PPO가 ACER보다 좋은 성능을 내고 있습니다. 하지만 마지막 100 에피소드만을 비교했을 때는 PPO보다 ACER이 더 좋은 성능을 보이고 있습니다. 이는 PPO가 더 빨리 최종 능력에 도달하지만, ACER이 가진 potential이 더 높다는 것입니다.\n\n<br><br>\n\n# 8. Conclusion\n\n이 논문에서는 policy update를 하기 위한 방법으로 stochastic gradient ascent의 multiple epchs를 사용하는 policy optimization method들의 하나의 알고리즘은 Proximal Policy Optimization(PPO)를 소개합니다.\n\n이 알고리즘은 trust region method의 stability와 reliability를 가집니다. 여기에 더하여 학습하기에 훨씬 더 간단하고, 더 일반적인 setting으로 적용하기에 편한 A3C로서 code를 구성하기에도 편하고, 계산량도 훨씬 덜합니다. 그리고 전반적으로 더 좋은 성능을 가집니다.\n\n<br><br>\n\n# 처음으로\n\n## [PG Travel Guide](https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/)\n\n<br>\n\n# 이전으로\n\n## [GAE 여행하기](https://reinforcement-learning-kr.github.io/2018/06/23/6_gae/)\n\n<br>\n\n# 다음으로\n\n## [PPO Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/ppo_gae.py)","slug":"7_ppo","published":1,"updated":"2018-09-08T11:00:20.612Z","_id":"cjkj6tyrl0007j715h3xhmg1v","comments":1,"layout":"post","photos":[],"link":"","content":"<center> <img src=\"https://www.dropbox.com/s/145van5kldfvvd5/Screen%20Shot%202018-07-18%20at%201.19.30%20AM.png?dl=1\" width=\"700\"> </center>\n\n<p>논문 저자 : John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov<br>논문 링크 : <a href=\"https://arxiv.org/pdf/1707.06347.pdf\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/pdf/1707.06347.pdf</a><br>Proceeding : unpublished.<br>정리 : 이동민, 장수영, 차금강</p>\n<hr>\n<h1 id=\"1-들어가며…\"><a href=\"#1-들어가며…\" class=\"headerlink\" title=\"1. 들어가며…\"></a>1. 들어가며…</h1><p>Sutton_PG부터 시작하여 TRPO, GAE를 거쳐 PPO까지 대단히 고생많으셨습니다. 먼저 이 논문은 TRPO보다는 쉽습니다. cliping이라는 새로운 개념이 나오지만 크게 어렵진 않습니다.</p>\n<p>이 논문에서는 Reinforcement Learning에서 Policy Gradient Method의 새로운 방법인 PPO를 제안합니다. 이 방법은 agent가 환경과의 상호작용을 통해 data를 sampling하는 것과 stochastic gradient ascent를 사용하여 “surrogate” objective function을 optimizing하는 것을 번갈아가면서 하는 방법입니다. data sample마다 one gradient update를 수행하는 기존의 방법과는 달리, minibatch update의 multiple epochs를 가능하게 하는 새로운 objective function을 말합니다.</p>\n<p>이 알고리즘의 장점으로는</p>\n<ul>\n<li>Trust Region Policy Optimization(TRPO)의 장점만을 가집니다. 다시 말해 알고리즘으로 학습하기에 훨씬 더 간단하고, 더 일반적이고, 더 좋은 sample complexity (empirically)를 가집니다. </li>\n<li>또한 다른 online policy gradient method들을 능가했고, 전반적으로 sample complexity(특히 computational complexity), simplicity, wall-time가 좋다고 합니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"2-Introduction\"><a href=\"#2-Introduction\" class=\"headerlink\" title=\"2. Introduction\"></a>2. Introduction</h1><p><br></p>\n<h2 id=\"2-1-대표적인-방법들\"><a href=\"#2-1-대표적인-방법들\" class=\"headerlink\" title=\"2.1 대표적인 방법들\"></a>2.1 대표적인 방법들</h2><ul>\n<li>DQN<ul>\n<li>Discrete action space를 가지는 문제들에는 효과적으로 적용 가능하지만, continuous control에도 잘 작동하는지는 검증되지 않았습니다.</li>\n</ul>\n</li>\n<li>A3C - “Vanilla” policy gradient methods<ul>\n<li>Data efficiency와 robustness 측면이 좋지 않습니다. A3C의 data efficiency의 경우 on-policy로서 한 번 쓴 data는 바로 버리기 때문에 data efficiency가 좋지 않다는 것입니다.</li>\n</ul>\n</li>\n<li>TRPO<ul>\n<li>간단히 말해 복잡합니다.</li>\n<li>또한 noise(ex. dropout)나 parameter sharing(policy와 value function 간 혹은 auxiliary tasks와의)를 포함하는 architecture와의 호환성 없습니다.</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-2-대표적인-방법들-대비-개선사항\"><a href=\"#2-2-대표적인-방법들-대비-개선사항\" class=\"headerlink\" title=\"2.2 대표적인 방법들 대비 개선사항\"></a>2.2 대표적인 방법들 대비 개선사항</h2><ul>\n<li>Scalability<ul>\n<li>large models and parallel implementations</li>\n</ul>\n</li>\n<li>Data Efficiency</li>\n<li>Robustness<ul>\n<li>hyperparameter tuning없이 다양한 문제들에 적용되어 해결</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-3-제안하는-알고리즘\"><a href=\"#2-3-제안하는-알고리즘\" class=\"headerlink\" title=\"2.3 제안하는 알고리즘\"></a>2.3 제안하는 알고리즘</h2><p>이 논문에는 Clipped probability ratios를 포함하는 objective function 제안하였습니다.</p>\n<ul>\n<li>TRPO의 data efficiency와 robustness를 유지하면서도 first-order approximation만 사용합니다.</li>\n<li>Policy 성능에 대한 lower bound를 제공합니다.</li>\n</ul>\n<p>따라서 Policy로부터의 data sampling과 sampled data를 이용한 최적화를 번갈아가면서 수행합니다.</p>\n<p><br></p>\n<h2 id=\"2-4-실험-결과\"><a href=\"#2-4-실험-결과\" class=\"headerlink\" title=\"2.4 실험 결과\"></a>2.4 실험 결과</h2><p>다양한 버전의 surrogate objectives 중에는 clipped probability ratio가 가장 성능이 좋았습니다.</p>\n<ul>\n<li>Continuous control tasks에서 기존 알고리즘 대비 성능이 좋습니다.</li>\n<li>Atari에서는 A2C 대비 sampling efficiency 측면에서는 성능이 월등히 좋으며, ACER 대비 훨씬 간단하지만 성능은 비슷합니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"3-Backgroud-Policy-Optimization\"><a href=\"#3-Backgroud-Policy-Optimization\" class=\"headerlink\" title=\"3. Backgroud: Policy Optimization\"></a>3. Backgroud: Policy Optimization</h1><p><br></p>\n<h2 id=\"3-1-Policy-Gradient-Methods\"><a href=\"#3-1-Policy-Gradient-Methods\" class=\"headerlink\" title=\"3.1 Policy Gradient Methods\"></a>3.1 Policy Gradient Methods</h2><p>일반적인 PG Method들은 실험적으로 destructive large policy updates가 발생합니다.</p>\n<ul>\n<li>더 자세히 말하자면, PG Method가 수행하는 parameter space에서의 gradual update가 policy space에서는 큰 변화를 유발할 수 있다는 의미입니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-2-Trust-Region-Methods\"><a href=\"#3-2-Trust-Region-Methods\" class=\"headerlink\" title=\"3.2 Trust Region Methods\"></a>3.2 Trust Region Methods</h2><p>Policy update 크기에 대한 contraint하에 objective function(“surrogate” function)을 최대화하는 것이 목표입니다. 수식은 아래와 같습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/gx6udoz5upswyf9/Screen%20Shot%202018-07-31%20at%2011.11.49%20PM.png?dl=1\" width=\"400\"> </center>\n\n<p>위의 수식은 contraint로 인해 excessive large policy update가 방지됩니다.</p>\n<p>TRPO에서는 constrained optimization problem을 풀기 위해서는 다음과 같은 방법들이 필요합니다.</p>\n<ol>\n<li>Fisher Information Matrix인 second-order derivative of KL divergence를 사용하거나,<ul>\n<li>여기서 second-order matrixes를 구하기 위해서는 많은 계산량 필요합니다.</li>\n</ul>\n</li>\n<li>Conjugate Gradient를 사용합니다.<ul>\n<li>Conjugate Gradient는 구현하기가 어렵습니다.</li>\n</ul>\n</li>\n</ol>\n<center> <img src=\"https://www.dropbox.com/s/6xpw9igndl3dmb9/Screen%20Shot%202018-07-31%20at%2011.15.13%20PM.png?dl=1\" width=\"500\"> </center>\n\n<p>원래 이론적으로 위의 수식과 같이 “contraint”가 아니라 objective에 “penalty”를 부여하는 형태입니다. 하지만 다양한 문제들(혹은 학습 중에 특성이 변하는 문제)에서 모두 잘 동작하는 single value $\\beta$를 찾는 것(robustness)이 어렵기 때문에, TRPO에서는 penalty대신 contraint를 취하는 방식을 택한 것입니다.</p>\n<p><br><br></p>\n<h1 id=\"4-Clipped-Surrogate-Objective\"><a href=\"#4-Clipped-Surrogate-Objective\" class=\"headerlink\" title=\"4. Clipped Surrogate Objective\"></a>4. Clipped Surrogate Objective</h1><p>이번 section에서는 TRPO의 surrogate obejctive function을 강제적으로 clipping하는 방법에 대하여 말합니다.</p>\n<p>먼저 기존의 TRPO의 surrogate function을 다음과 같이 표현합니다.<br>$$r_t(\\theta)=\\dfrac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta old}(a_t|s_t)}, \\, \\, r_t(\\theta old) = 1$$</p>\n<p>위의 수식을 이용하여 TRPO의 surrogate object를 최대화합니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/e524vwsyolp6h9n/Screen%20Shot%202018-07-26%20at%2010.12.22%20AM.png?dl=1\" width=\"350\"> </center>\n\n<p>위의 수식을 그대로 사용한다면 excessively large policy update가 됩니다. 따라서 penalty를 이용하여 필요 이상의 policy update를 방지합니다. TRPO에서는 KL-Divergence를 이용하여 penalty를 적용하지만 PPO에서는 computation적으로 효율적인 penalty를 적용하고 excessively large policy update를 방지하기 위해 아래와 같은 clipping 기법을 사용합니다.</p>\n<p>$$L^{CLIP}(\\theta) = \\hat{E}_t [min(r_t(\\theta) \\, \\hat{A}_t,  clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\, \\hat{A}_t)]$$</p>\n<ul>\n<li>위의 수식에 대한 추가적인 설명<ul>\n<li>$\\epsilon$은 hyperparameter로서 continuous control에서는 0.2일 때 성능이 가장 좋았으며, Atari Game에서는 0.1 x $\\alpha$ 값을 사용합니다. (여기서 $\\alpha$ 는 학습률로 1 에서 시작하여 학습이 진행됨에 따라 0 으로 감소합니다.)</li>\n<li>Clipped 와 unclipped objective 중 min 값을 택함으로써 $L^{CLIP} (\\theta)$는 unclipped objective에 대한 lowerbound가 됩니다.</li>\n</ul>\n</li>\n</ul>\n<p>이어서 아래의 그림을 보겠습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/3wcuf2tq7q24fy6/Screen%20Shot%202018-07-26%20at%2010.25.08%20AM.png?dl=1\" width=\"600\"> </center>\n\n<p>위의 그림을 보면 두 가지의 그래프(Advantage Function $\\hat{A}_t$의 부호에 따라)로 clip에 대해서 설명하고 있습니다.</p>\n<ul>\n<li>Advantage Function $\\hat{A}_t$가 양수일 때<ul>\n<li>Advantage가 현재보다 높다라는 뜻이며 파라미터를 +의 방향으로 업데이트 하여야 합니다. 다시 말해 어떠한 상태 $s$에서 행동 $a$가 평균보다 좋다는 의미입니다. 따라서 이를 취할 확률이 증가하게 되고, $r_t (\\theta)$를 clip하여 $\\epsilon$보다 커지지 않도록 유도하는 것입니다.</li>\n<li>추가적으로, TRPO에서 다뤘던 constraint가 아니라 단순히 clip하는 것이기 때문에 $\\pi_\\theta (a_t|s_t)$의 증가량은 $\\epsilon$보다 더 커질 수도 있지만, 더 커져봐야 objective function에 도움이 되지 않기에 대부분 $\\epsilon$ 이하로 유지합니다.</li>\n<li>또한 만약 $r_t (\\theta)$가 objective function의 값을 감소시키는 방향으로 움직이는 경우더라도 $1-\\epsilon$보다 작아져도 됩니다. 여기서의 목적은 최대한 lowerbound를 구하는 것이 목적이기 때문입니다. 그러니까 쉽게 말해서 왼쪽 그림에서도 볼 수 있듯이 $1+\\epsilon$만 구하는 데에 포커스를 맞추고 있고, $1-\\epsilon$은 신경쓰지 않고 있는 것입니다. (이 부분은 Advantage Function $\\hat{A}_t$가 음수일 때도 동일합니다.)</li>\n</ul>\n</li>\n<li>Advantage Function $\\hat{A}_t$가 음수일 때 <ul>\n<li>Advantage가 현재보다 좋지 않다라는 뜻이며 그의 반대 방향으로 업데이트 하여야 합니다. 다시 말해 어떠한 상태 $s$에서 행동 $a$가 평균보다 좋지 않다는 의미입니다. 따라서 이를 취할 확률이 감소하게 되고, $r_t (\\theta)$를 clip하여 $\\epsilon$보다 작아지지 않도록 유도하는 것입니다.  </li>\n<li>또한 $r_t(\\theta)$은 확률을 뜻하는 두 개의 함수를 분자 분모로 가지고 있으며, 분수로 구성되어 있기 때문에 무조건 양수로 이루어져 있습니다. Advantage function인 $\\hat{A}_t$와 곱해져 Objective function인 $L^{CLIP}$은 Advantage function과 방향이 같아집니다.</li>\n</ul>\n</li>\n</ul>\n<p>위의 설명으로 인하여 나오는 그림이 아래의 그림입니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/f7i97geiligfej9/Screen%20Shot%202018-07-26%20at%2011.02.17%20AM.png?dl=1\" width=\"700\"> </center>\n\n<p>위의 그림에서 빨간색 그래프를 보면,</p>\n<ul>\n<li>$L^{CLIP}$은 min 값들의 평균이기 때문에 평균들의 min 값보다는 더 작아집니다. 즉, 주황색 그래프와 초록색 그래프 중 작은값보다 더 작아지는 것을 볼 수 있습니다.</li>\n<li>$L^{CLIP}$을 최대화하는 $\\theta$가 다음 $\\pi_\\theta$가 됩니다. 다시 말해 PPO는 기존 PG 방법들처럼 parameter space에서 parameter $\\theta$를 점진적으로 업데이트하는 것이 아니라, 매번 policy를 maximize하는 방향으로 업데이트하는 것으로 볼 수 있는 것입니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"5-Adaptive-KL-Penalty-Coefficient\"><a href=\"#5-Adaptive-KL-Penalty-Coefficient\" class=\"headerlink\" title=\"5. Adaptive KL Penalty Coefficient\"></a>5. Adaptive KL Penalty Coefficient</h1><p>이전까지 설명했던 Clipped Surrogate Objective 방법과 달리 기존의 TRPO에서 Adaptive한 파라미터를 적용한 또 다른 방법에 대해서 알아보겠습니다. 사실 이 방법은 앞서본 clip을 사용한 방법보다는 성능이 좋지 않습니다. 하지만 baseline으로써 알아볼 필요가 있습니다.</p>\n<p>clip에서 다뤘던 Probability ratio $r_t(\\theta)$ 대신 KL divergence를 이용하여 penalty를 줍니다. 그 결과 각각의 policy update에 대해 KL divergence $d_{targ}$의 target value를 얻습니다.</p>\n<ul>\n<li>clipped surrogate objective 대신하여 or 추가적으로 사용할 수 있다고 합니다. </li>\n<li>자체 실험 결과에서는 clipped surrogate objective 보다는 성능이 안 좋았다고 합니다.</li>\n<li>둘 다 사용한 실험 결과는 없습니다.</li>\n</ul>\n<p>이 알고리즘에서 각각의 policy update에 적용하는 step은 다음과 같습니다. </p>\n<ol>\n<li>KL-penalized objective 최적화를 합니다. 기존의 TRPO의 Objective function에 $\\beta$를 적용하여 다음과 같이 $\\beta$를 Adaptive하게 조절하고 있습니다. 수식은 아래와 같습니다.</li>\n</ol>\n<center> <img src=\"https://www.dropbox.com/s/ojq7kfobf0f8x9n/Screen%20Shot%202018-07-26%20at%2011.22.44%20AM.png?dl=1\" width=\"500\"> </center>\n\n<ol start=\"2\">\n<li><img src=\"https://www.dropbox.com/s/j9phkalrbgf45k0/Screen%20Shot%202018-07-26%20at%2011.34.25%20AM.png?dl=1\" width=\"230\">을 계산합니다.<ul>\n<li>만약 $d &lt; d_{targ} \\, / \\, 1.5$라면, 그 때 $\\beta \\leftarrow \\beta \\, / 2$</li>\n<li>만약 $d &gt; d_{targ} \\, x \\, 1.5$라면, 그 때 $\\beta \\leftarrow \\beta \\, x 2$</li>\n<li>즉, KL-divergence 값이 일정 이상 커지게 되면 objective function에 penalty를 더 크게 부과합니다.</li>\n<li>갱신된 $\\beta$ 값은 다음 policy update 때 사용합니다. </li>\n</ul>\n</li>\n</ol>\n<p>$\\beta$를 조절하는 방법은 만약 $\\theta old$와 $\\theta$간의 파라미터 차이가 크다면 penalty를 강하게 부여하여 차이를 작게하고, 파라미터 차이가 작다면 penalty를 완화시켜 주어 차이를 더 크게하는 것입니다.</p>\n<p>정리하자면, KL-divergence를 constraint로 둔 것이 아니기 때문에, 간혹 excessive large policy update가 발생할 수도 있지만, KL Penaly coefficienct인 $\\beta$가 KL-divergence에 따라 adpative 하게 조정됨으로써 excessive large policy update가 지속적으로 발생되는 것을 방지하는 것입니다.</p>\n<p><br><br></p>\n<h1 id=\"6-Algorithm\"><a href=\"#6-Algorithm\" class=\"headerlink\" title=\"6. Algorithm\"></a>6. Algorithm</h1><p>앞서 다뤘던 section들은 어떻게 policy만을 업데이트 하는지에 대해 설명하고 있습니다. 이번 section에서는 value, entropy-exploration과 합쳐서 어떻게 통합적으로 업데이트 하는지에 대해서 알아보겠습니다.</p>\n<p>먼저 Variance-reduced advantage function estimator을 계산하기 위해 learned state-value fucntion $V(s)$을 사용합니다. 여기에는 두 가지 방법이 있습니다.</p>\n<ul>\n<li>Generalized advantage estimation</li>\n<li>Finite horizontal estimators</li>\n</ul>\n<p>만약 policy와 value function 간 parameter sharing하는 neural network architecture를 사용한다면, policy surrogate와 value function error term을 combine한 loss function을 사용해야 합니다. 또한 이 loss function에 entropy bonus term을 추가하여, 충분한 exploration이 될 수 있도록 합니다.(exploration하는 부분은 <a href=\"https://arxiv.org/pdf/1602.01783.pdf\" target=\"_blank\" rel=\"noopener\">A3C 논문</a>에 나와있습니다.)</p>\n<p>그래서 앞써 다뤘던 objective function을 $L^{CLIP}$라고 표현한다면 PPO에서 제시하는 통합적으로 최대화해야하는 objective function은 다음과 같이 표현할 수 있습니다.</p>\n<p>$$L_t^{CLIP+VF+S} (\\theta) = \\hat{E}_t [L^{CLIP} (\\theta) - c_1 L_t^{VF} (\\theta) + c_2 S[\\pi_\\theta] (s_t)]$$</p>\n<p>위의 수식에 대한 추가적인 설명은 다음과 같습니다.</p>\n<ul>\n<li>$c_1, c_2$ : coefficients</li>\n<li>$S$ : entropy bonus</li>\n<li>$L_t^{VF}$ : squared-error loss $(V_\\theta (s_t) - V_t^{targ})^2$</li>\n</ul>\n<p>위의 objective function을 최대화하면 Reinforcement Learning을 통한 policy 학습, state-value function의 학습, exploration을 할 수 있습니다.</p>\n<p>추가적으로 A3C와 같은 요즘 인기있는 PG의 style에서는 T time steps(T는 episode length 보다 훨씬 작은 크기) 동안 policy에 따라서 sample들을 얻고, 이 sample들을 업데이트에 사용합니다. 이러한 방식은 time step T 까지만 고려하는 advantage estimator가 필요하며, A3C에서 다음과 같이 사용합니다.</p>\n<p>$$\\hat{A}_t = - V(s_t) + r_t + \\gamma r_{t+1} + \\cdots + \\gamma^{T-t+1} r_{T-1} + \\gamma^{T-t}V(s_T)$$</p>\n<ul>\n<li>여기서 $t$는 주어진 length T trajectory segment 내에서 $[0, T]$에 있는 time index입니다.</li>\n</ul>\n<p>위의 수식에 더하여 generalized version인 GAE의 truncated version(generalized advantage estimation)을 사용합니다.($\\lambda$가 1이면 위 식과 같아집니다.) 수식은 아래와 같습니다. </p>\n<p>$$\\hat{A}_t = \\delta_t + (\\gamma\\lambda)\\delta_{t+1} + \\cdots + (\\gamma\\lambda)^{T-t+1}\\delta_{T-1},$$$$where \\,\\,\\,\\, \\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$$</p>\n<p>그래서 고정된 length T trajectory segment를 사용한 PPO algorithm은 다음과 같은 pseudo code로 나타낼 수 있습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/q00y8x7eryl1ncd/Screen%20Shot%202018-07-26%20at%202.28.03%20PM.png?dl=1\" width=\"800\"> </center>\n\n<ul>\n<li>여기서 actor는 A3C처럼 병렬로 두어도 상관없습니다.</li>\n<li>advantage estimate 계산을 통해서 surrogate loss를 계산합니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"7-Experimnet\"><a href=\"#7-Experimnet\" class=\"headerlink\" title=\"7. Experimnet\"></a>7. Experimnet</h1><p><br></p>\n<h2 id=\"7-1-Surrogate-Objectives의-비교\"><a href=\"#7-1-Surrogate-Objectives의-비교\" class=\"headerlink\" title=\"7.1 Surrogate Objectives의 비교\"></a>7.1 Surrogate Objectives의 비교</h2><p>이번 section에서는 3가지 Surrogate Objectives를 비교분석 하고 있습니다.</p>\n<ul>\n<li>No clipping or penalty: $L_t(\\theta) = r_t(\\theta)\\hat{A}_t$</li>\n<li>Clipping: $L_t(\\theta) = min(r_t(\\theta) \\, \\hat{A}_t, \\, clip(r_t(\\theta), 1-\\epsilon,1+\\epsilon) \\, \\hat{A}_t)$</li>\n<li>KL penalty(fixed or adaptive): <img src=\"https://www.dropbox.com/s/ksnhlxz2riuns1p/Screen%20Shot%202018-07-26%20at%202.42.50%20PM.png?dl=1\" width=\"270\"></li>\n</ul>\n<p>그리고 환경은 MuJoCo를 사용하며 OpenAI Gym 환경에서 실험을 진행하였습니다. 그리고 네트워크들은 2개의 hidden layer를 가지며 각각 64개의 unit들을 가지고 있습니다. 그리고 tanh를 활성화 함수로써 사용하고 있으며 네트워크는 Gaussian distribution을 출력하는 것으로 구성되어 있습니다. 그리고 Policy Network와 Value Network는 파라미터들을 공유하고 있지 않으며, 위에서 설명한 Exploration을 위한 entropy는 사용하지 않습니다.</p>\n<p>7개의 환경에서 실험하였으며 각 환경들은 HalfCheetah, Hopper, InvertedDoublePendulum, InvertedPerdulum, Reacher, Swimmer, Walker2d를 사용하였으며 모든 환경들은 v1 버전을 사용하였습니다.</p>\n<p>아래의 표는 실험 결과입니다. 세 가지 Surrogate Objective에 대해 각 파라미터들을 표에 표기된 방식대로 변화하며 실험하였습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/4xbkxh4m82mu1vo/Screen%20Shot%202018-07-26%20at%202.45.42%20PM.png?dl=1\" width=\"450\"> </center>\n\n<p><br></p>\n<h2 id=\"7-2-Comparison-to-other-Algorithms-in-the-Continuous-Domain\"><a href=\"#7-2-Comparison-to-other-Algorithms-in-the-Continuous-Domain\" class=\"headerlink\" title=\"7.2 Comparison to other Algorithms in the Continuous Domain\"></a>7.2 Comparison to other Algorithms in the Continuous Domain</h2><p>이번 section에서는 Clipping 버전의 PPO와 다른 알고리즘들 간의 비교를 연속적인 Action을 가지는 환경에서 실험을 한 결과를 보여주고 있습니다. 다른 알고리즘은 TRPO, cross-entropy method, vanilla policy gradient with adaptive stepsize, A2C, A2C with trust region를 사용하였습니다. 환경과 사용한 네트워크의 구성은 이전 section과 동일합니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/tf7djngioxnxtef/Screen%20Shot%202018-07-26%20at%202.50.23%20PM.png?dl=1\" width=\"900\"> </center>\n\n<p>여타 다른 알고리즘보다 좋은 성능을 보이는 것을 알 수 있습니다.</p>\n<p><br></p>\n<h2 id=\"7-3-Showcase-in-the-Continuous-Domain-Humanoid-Running-and-Steering\"><a href=\"#7-3-Showcase-in-the-Continuous-Domain-Humanoid-Running-and-Steering\" class=\"headerlink\" title=\"7.3 Showcase in the Continuous Domain: Humanoid Running and Steering\"></a>7.3 Showcase in the Continuous Domain: Humanoid Running and Steering</h2><p>이번 section에서는 Humanoid-v0, HumanoidFlagrun-v0, HumanoidFlagrunHarder-v0의 환경에서 Roboschool을 사용하여 실험하였습니다.</p>\n<ul>\n<li>Humanoid-v0는 단순히 앞으로 걸어나가는 환경</li>\n<li>HumanoidFlagrun-v0은 200스텝마다 혹은 목적지에 도착할 때 마다 위치가 바뀌는 목적지에 걸어서 도달하는 환경입니다. </li>\n<li>HumanoidFlagrunHarder-v0은 환경이 초기화될 때 마다 특정 경계안에 Humanoid가 위치하게 되며 특정 영역 밖으로 걸어나가는 것을 수행하는 환경입니다.</li>\n</ul>\n<center> <img src=\"https://www.dropbox.com/s/y112ua7il6l0296/Screen%20Shot%202018-07-26%20at%202.53.01%20PM.png?dl=1\" width=\"900\"> </center>\n\n<ul>\n<li>Roboschool을 사용하여 3D humanoid control task에서 PPO 알고리즘으로 학습시킨 Learning curve입니다.</li>\n</ul>\n<center> <img src=\"https://www.dropbox.com/s/09scyk6zeviyswt/Screen%20Shot%202018-07-26%20at%202.53.10%20PM.png?dl=1\" width=\"900\"> </center>\n\n<ul>\n<li>Roboschool Humanoid Flagrun에서 학습된 policy의 frame들입니다. </li>\n</ul>\n<p><br></p>\n<h2 id=\"7-4-Comparison-to-Other-Algorithms-on-the-Atari-Domain\"><a href=\"#7-4-Comparison-to-Other-Algorithms-on-the-Atari-Domain\" class=\"headerlink\" title=\"7.4 Comparison to Other Algorithms on the Atari Domain\"></a>7.4 Comparison to Other Algorithms on the Atari Domain</h2><p>이번 section에서는 Atari Domain에서 PPO, A2C, ACER(Actor Critic with Experience Replay)의 3가지 알고리즘을 비교합니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/jxuqdncxpnoyqgi/Screen%20Shot%202018-07-26%20at%203.00.00%20PM.png?dl=1\" width=\"600\"> </center>\n\n<p>전체 training에 대해서는 PPO가 ACER보다 좋은 성능을 내고 있습니다. 하지만 마지막 100 에피소드만을 비교했을 때는 PPO보다 ACER이 더 좋은 성능을 보이고 있습니다. 이는 PPO가 더 빨리 최종 능력에 도달하지만, ACER이 가진 potential이 더 높다는 것입니다.</p>\n<p><br><br></p>\n<h1 id=\"8-Conclusion\"><a href=\"#8-Conclusion\" class=\"headerlink\" title=\"8. Conclusion\"></a>8. Conclusion</h1><p>이 논문에서는 policy update를 하기 위한 방법으로 stochastic gradient ascent의 multiple epchs를 사용하는 policy optimization method들의 하나의 알고리즘은 Proximal Policy Optimization(PPO)를 소개합니다.</p>\n<p>이 알고리즘은 trust region method의 stability와 reliability를 가집니다. 여기에 더하여 학습하기에 훨씬 더 간단하고, 더 일반적인 setting으로 적용하기에 편한 A3C로서 code를 구성하기에도 편하고, 계산량도 훨씬 덜합니다. 그리고 전반적으로 더 좋은 성능을 가집니다.</p>\n<p><br><br></p>\n<h1 id=\"처음으로\"><a href=\"#처음으로\" class=\"headerlink\" title=\"처음으로\"></a>처음으로</h1><h2 id=\"PG-Travel-Guide\"><a href=\"#PG-Travel-Guide\" class=\"headerlink\" title=\"PG Travel Guide\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/\">PG Travel Guide</a></h2><p><br></p>\n<h1 id=\"이전으로\"><a href=\"#이전으로\" class=\"headerlink\" title=\"이전으로\"></a>이전으로</h1><h2 id=\"GAE-여행하기\"><a href=\"#GAE-여행하기\" class=\"headerlink\" title=\"GAE 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/23/6_gae/\">GAE 여행하기</a></h2><p><br></p>\n<h1 id=\"다음으로\"><a href=\"#다음으로\" class=\"headerlink\" title=\"다음으로\"></a>다음으로</h1><h2 id=\"PPO-Code\"><a href=\"#PPO-Code\" class=\"headerlink\" title=\"PPO Code\"></a><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/ppo_gae.py\" target=\"_blank\" rel=\"noopener\">PPO Code</a></h2>","site":{"data":{}},"excerpt":"","more":"<center> <img src=\"https://www.dropbox.com/s/145van5kldfvvd5/Screen%20Shot%202018-07-18%20at%201.19.30%20AM.png?dl=1\" width=\"700\"> </center>\n\n<p>논문 저자 : John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov<br>논문 링크 : <a href=\"https://arxiv.org/pdf/1707.06347.pdf\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/pdf/1707.06347.pdf</a><br>Proceeding : unpublished.<br>정리 : 이동민, 장수영, 차금강</p>\n<hr>\n<h1 id=\"1-들어가며…\"><a href=\"#1-들어가며…\" class=\"headerlink\" title=\"1. 들어가며…\"></a>1. 들어가며…</h1><p>Sutton_PG부터 시작하여 TRPO, GAE를 거쳐 PPO까지 대단히 고생많으셨습니다. 먼저 이 논문은 TRPO보다는 쉽습니다. cliping이라는 새로운 개념이 나오지만 크게 어렵진 않습니다.</p>\n<p>이 논문에서는 Reinforcement Learning에서 Policy Gradient Method의 새로운 방법인 PPO를 제안합니다. 이 방법은 agent가 환경과의 상호작용을 통해 data를 sampling하는 것과 stochastic gradient ascent를 사용하여 “surrogate” objective function을 optimizing하는 것을 번갈아가면서 하는 방법입니다. data sample마다 one gradient update를 수행하는 기존의 방법과는 달리, minibatch update의 multiple epochs를 가능하게 하는 새로운 objective function을 말합니다.</p>\n<p>이 알고리즘의 장점으로는</p>\n<ul>\n<li>Trust Region Policy Optimization(TRPO)의 장점만을 가집니다. 다시 말해 알고리즘으로 학습하기에 훨씬 더 간단하고, 더 일반적이고, 더 좋은 sample complexity (empirically)를 가집니다. </li>\n<li>또한 다른 online policy gradient method들을 능가했고, 전반적으로 sample complexity(특히 computational complexity), simplicity, wall-time가 좋다고 합니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"2-Introduction\"><a href=\"#2-Introduction\" class=\"headerlink\" title=\"2. Introduction\"></a>2. Introduction</h1><p><br></p>\n<h2 id=\"2-1-대표적인-방법들\"><a href=\"#2-1-대표적인-방법들\" class=\"headerlink\" title=\"2.1 대표적인 방법들\"></a>2.1 대표적인 방법들</h2><ul>\n<li>DQN<ul>\n<li>Discrete action space를 가지는 문제들에는 효과적으로 적용 가능하지만, continuous control에도 잘 작동하는지는 검증되지 않았습니다.</li>\n</ul>\n</li>\n<li>A3C - “Vanilla” policy gradient methods<ul>\n<li>Data efficiency와 robustness 측면이 좋지 않습니다. A3C의 data efficiency의 경우 on-policy로서 한 번 쓴 data는 바로 버리기 때문에 data efficiency가 좋지 않다는 것입니다.</li>\n</ul>\n</li>\n<li>TRPO<ul>\n<li>간단히 말해 복잡합니다.</li>\n<li>또한 noise(ex. dropout)나 parameter sharing(policy와 value function 간 혹은 auxiliary tasks와의)를 포함하는 architecture와의 호환성 없습니다.</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-2-대표적인-방법들-대비-개선사항\"><a href=\"#2-2-대표적인-방법들-대비-개선사항\" class=\"headerlink\" title=\"2.2 대표적인 방법들 대비 개선사항\"></a>2.2 대표적인 방법들 대비 개선사항</h2><ul>\n<li>Scalability<ul>\n<li>large models and parallel implementations</li>\n</ul>\n</li>\n<li>Data Efficiency</li>\n<li>Robustness<ul>\n<li>hyperparameter tuning없이 다양한 문제들에 적용되어 해결</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-3-제안하는-알고리즘\"><a href=\"#2-3-제안하는-알고리즘\" class=\"headerlink\" title=\"2.3 제안하는 알고리즘\"></a>2.3 제안하는 알고리즘</h2><p>이 논문에는 Clipped probability ratios를 포함하는 objective function 제안하였습니다.</p>\n<ul>\n<li>TRPO의 data efficiency와 robustness를 유지하면서도 first-order approximation만 사용합니다.</li>\n<li>Policy 성능에 대한 lower bound를 제공합니다.</li>\n</ul>\n<p>따라서 Policy로부터의 data sampling과 sampled data를 이용한 최적화를 번갈아가면서 수행합니다.</p>\n<p><br></p>\n<h2 id=\"2-4-실험-결과\"><a href=\"#2-4-실험-결과\" class=\"headerlink\" title=\"2.4 실험 결과\"></a>2.4 실험 결과</h2><p>다양한 버전의 surrogate objectives 중에는 clipped probability ratio가 가장 성능이 좋았습니다.</p>\n<ul>\n<li>Continuous control tasks에서 기존 알고리즘 대비 성능이 좋습니다.</li>\n<li>Atari에서는 A2C 대비 sampling efficiency 측면에서는 성능이 월등히 좋으며, ACER 대비 훨씬 간단하지만 성능은 비슷합니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"3-Backgroud-Policy-Optimization\"><a href=\"#3-Backgroud-Policy-Optimization\" class=\"headerlink\" title=\"3. Backgroud: Policy Optimization\"></a>3. Backgroud: Policy Optimization</h1><p><br></p>\n<h2 id=\"3-1-Policy-Gradient-Methods\"><a href=\"#3-1-Policy-Gradient-Methods\" class=\"headerlink\" title=\"3.1 Policy Gradient Methods\"></a>3.1 Policy Gradient Methods</h2><p>일반적인 PG Method들은 실험적으로 destructive large policy updates가 발생합니다.</p>\n<ul>\n<li>더 자세히 말하자면, PG Method가 수행하는 parameter space에서의 gradual update가 policy space에서는 큰 변화를 유발할 수 있다는 의미입니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-2-Trust-Region-Methods\"><a href=\"#3-2-Trust-Region-Methods\" class=\"headerlink\" title=\"3.2 Trust Region Methods\"></a>3.2 Trust Region Methods</h2><p>Policy update 크기에 대한 contraint하에 objective function(“surrogate” function)을 최대화하는 것이 목표입니다. 수식은 아래와 같습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/gx6udoz5upswyf9/Screen%20Shot%202018-07-31%20at%2011.11.49%20PM.png?dl=1\" width=\"400\"> </center>\n\n<p>위의 수식은 contraint로 인해 excessive large policy update가 방지됩니다.</p>\n<p>TRPO에서는 constrained optimization problem을 풀기 위해서는 다음과 같은 방법들이 필요합니다.</p>\n<ol>\n<li>Fisher Information Matrix인 second-order derivative of KL divergence를 사용하거나,<ul>\n<li>여기서 second-order matrixes를 구하기 위해서는 많은 계산량 필요합니다.</li>\n</ul>\n</li>\n<li>Conjugate Gradient를 사용합니다.<ul>\n<li>Conjugate Gradient는 구현하기가 어렵습니다.</li>\n</ul>\n</li>\n</ol>\n<center> <img src=\"https://www.dropbox.com/s/6xpw9igndl3dmb9/Screen%20Shot%202018-07-31%20at%2011.15.13%20PM.png?dl=1\" width=\"500\"> </center>\n\n<p>원래 이론적으로 위의 수식과 같이 “contraint”가 아니라 objective에 “penalty”를 부여하는 형태입니다. 하지만 다양한 문제들(혹은 학습 중에 특성이 변하는 문제)에서 모두 잘 동작하는 single value $\\beta$를 찾는 것(robustness)이 어렵기 때문에, TRPO에서는 penalty대신 contraint를 취하는 방식을 택한 것입니다.</p>\n<p><br><br></p>\n<h1 id=\"4-Clipped-Surrogate-Objective\"><a href=\"#4-Clipped-Surrogate-Objective\" class=\"headerlink\" title=\"4. Clipped Surrogate Objective\"></a>4. Clipped Surrogate Objective</h1><p>이번 section에서는 TRPO의 surrogate obejctive function을 강제적으로 clipping하는 방법에 대하여 말합니다.</p>\n<p>먼저 기존의 TRPO의 surrogate function을 다음과 같이 표현합니다.<br>$$r_t(\\theta)=\\dfrac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta old}(a_t|s_t)}, \\, \\, r_t(\\theta old) = 1$$</p>\n<p>위의 수식을 이용하여 TRPO의 surrogate object를 최대화합니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/e524vwsyolp6h9n/Screen%20Shot%202018-07-26%20at%2010.12.22%20AM.png?dl=1\" width=\"350\"> </center>\n\n<p>위의 수식을 그대로 사용한다면 excessively large policy update가 됩니다. 따라서 penalty를 이용하여 필요 이상의 policy update를 방지합니다. TRPO에서는 KL-Divergence를 이용하여 penalty를 적용하지만 PPO에서는 computation적으로 효율적인 penalty를 적용하고 excessively large policy update를 방지하기 위해 아래와 같은 clipping 기법을 사용합니다.</p>\n<p>$$L^{CLIP}(\\theta) = \\hat{E}_t [min(r_t(\\theta) \\, \\hat{A}_t,  clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\, \\hat{A}_t)]$$</p>\n<ul>\n<li>위의 수식에 대한 추가적인 설명<ul>\n<li>$\\epsilon$은 hyperparameter로서 continuous control에서는 0.2일 때 성능이 가장 좋았으며, Atari Game에서는 0.1 x $\\alpha$ 값을 사용합니다. (여기서 $\\alpha$ 는 학습률로 1 에서 시작하여 학습이 진행됨에 따라 0 으로 감소합니다.)</li>\n<li>Clipped 와 unclipped objective 중 min 값을 택함으로써 $L^{CLIP} (\\theta)$는 unclipped objective에 대한 lowerbound가 됩니다.</li>\n</ul>\n</li>\n</ul>\n<p>이어서 아래의 그림을 보겠습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/3wcuf2tq7q24fy6/Screen%20Shot%202018-07-26%20at%2010.25.08%20AM.png?dl=1\" width=\"600\"> </center>\n\n<p>위의 그림을 보면 두 가지의 그래프(Advantage Function $\\hat{A}_t$의 부호에 따라)로 clip에 대해서 설명하고 있습니다.</p>\n<ul>\n<li>Advantage Function $\\hat{A}_t$가 양수일 때<ul>\n<li>Advantage가 현재보다 높다라는 뜻이며 파라미터를 +의 방향으로 업데이트 하여야 합니다. 다시 말해 어떠한 상태 $s$에서 행동 $a$가 평균보다 좋다는 의미입니다. 따라서 이를 취할 확률이 증가하게 되고, $r_t (\\theta)$를 clip하여 $\\epsilon$보다 커지지 않도록 유도하는 것입니다.</li>\n<li>추가적으로, TRPO에서 다뤘던 constraint가 아니라 단순히 clip하는 것이기 때문에 $\\pi_\\theta (a_t|s_t)$의 증가량은 $\\epsilon$보다 더 커질 수도 있지만, 더 커져봐야 objective function에 도움이 되지 않기에 대부분 $\\epsilon$ 이하로 유지합니다.</li>\n<li>또한 만약 $r_t (\\theta)$가 objective function의 값을 감소시키는 방향으로 움직이는 경우더라도 $1-\\epsilon$보다 작아져도 됩니다. 여기서의 목적은 최대한 lowerbound를 구하는 것이 목적이기 때문입니다. 그러니까 쉽게 말해서 왼쪽 그림에서도 볼 수 있듯이 $1+\\epsilon$만 구하는 데에 포커스를 맞추고 있고, $1-\\epsilon$은 신경쓰지 않고 있는 것입니다. (이 부분은 Advantage Function $\\hat{A}_t$가 음수일 때도 동일합니다.)</li>\n</ul>\n</li>\n<li>Advantage Function $\\hat{A}_t$가 음수일 때 <ul>\n<li>Advantage가 현재보다 좋지 않다라는 뜻이며 그의 반대 방향으로 업데이트 하여야 합니다. 다시 말해 어떠한 상태 $s$에서 행동 $a$가 평균보다 좋지 않다는 의미입니다. 따라서 이를 취할 확률이 감소하게 되고, $r_t (\\theta)$를 clip하여 $\\epsilon$보다 작아지지 않도록 유도하는 것입니다.  </li>\n<li>또한 $r_t(\\theta)$은 확률을 뜻하는 두 개의 함수를 분자 분모로 가지고 있으며, 분수로 구성되어 있기 때문에 무조건 양수로 이루어져 있습니다. Advantage function인 $\\hat{A}_t$와 곱해져 Objective function인 $L^{CLIP}$은 Advantage function과 방향이 같아집니다.</li>\n</ul>\n</li>\n</ul>\n<p>위의 설명으로 인하여 나오는 그림이 아래의 그림입니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/f7i97geiligfej9/Screen%20Shot%202018-07-26%20at%2011.02.17%20AM.png?dl=1\" width=\"700\"> </center>\n\n<p>위의 그림에서 빨간색 그래프를 보면,</p>\n<ul>\n<li>$L^{CLIP}$은 min 값들의 평균이기 때문에 평균들의 min 값보다는 더 작아집니다. 즉, 주황색 그래프와 초록색 그래프 중 작은값보다 더 작아지는 것을 볼 수 있습니다.</li>\n<li>$L^{CLIP}$을 최대화하는 $\\theta$가 다음 $\\pi_\\theta$가 됩니다. 다시 말해 PPO는 기존 PG 방법들처럼 parameter space에서 parameter $\\theta$를 점진적으로 업데이트하는 것이 아니라, 매번 policy를 maximize하는 방향으로 업데이트하는 것으로 볼 수 있는 것입니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"5-Adaptive-KL-Penalty-Coefficient\"><a href=\"#5-Adaptive-KL-Penalty-Coefficient\" class=\"headerlink\" title=\"5. Adaptive KL Penalty Coefficient\"></a>5. Adaptive KL Penalty Coefficient</h1><p>이전까지 설명했던 Clipped Surrogate Objective 방법과 달리 기존의 TRPO에서 Adaptive한 파라미터를 적용한 또 다른 방법에 대해서 알아보겠습니다. 사실 이 방법은 앞서본 clip을 사용한 방법보다는 성능이 좋지 않습니다. 하지만 baseline으로써 알아볼 필요가 있습니다.</p>\n<p>clip에서 다뤘던 Probability ratio $r_t(\\theta)$ 대신 KL divergence를 이용하여 penalty를 줍니다. 그 결과 각각의 policy update에 대해 KL divergence $d_{targ}$의 target value를 얻습니다.</p>\n<ul>\n<li>clipped surrogate objective 대신하여 or 추가적으로 사용할 수 있다고 합니다. </li>\n<li>자체 실험 결과에서는 clipped surrogate objective 보다는 성능이 안 좋았다고 합니다.</li>\n<li>둘 다 사용한 실험 결과는 없습니다.</li>\n</ul>\n<p>이 알고리즘에서 각각의 policy update에 적용하는 step은 다음과 같습니다. </p>\n<ol>\n<li>KL-penalized objective 최적화를 합니다. 기존의 TRPO의 Objective function에 $\\beta$를 적용하여 다음과 같이 $\\beta$를 Adaptive하게 조절하고 있습니다. 수식은 아래와 같습니다.</li>\n</ol>\n<center> <img src=\"https://www.dropbox.com/s/ojq7kfobf0f8x9n/Screen%20Shot%202018-07-26%20at%2011.22.44%20AM.png?dl=1\" width=\"500\"> </center>\n\n<ol start=\"2\">\n<li><img src=\"https://www.dropbox.com/s/j9phkalrbgf45k0/Screen%20Shot%202018-07-26%20at%2011.34.25%20AM.png?dl=1\" width=\"230\">을 계산합니다.<ul>\n<li>만약 $d &lt; d_{targ} \\, / \\, 1.5$라면, 그 때 $\\beta \\leftarrow \\beta \\, / 2$</li>\n<li>만약 $d &gt; d_{targ} \\, x \\, 1.5$라면, 그 때 $\\beta \\leftarrow \\beta \\, x 2$</li>\n<li>즉, KL-divergence 값이 일정 이상 커지게 되면 objective function에 penalty를 더 크게 부과합니다.</li>\n<li>갱신된 $\\beta$ 값은 다음 policy update 때 사용합니다. </li>\n</ul>\n</li>\n</ol>\n<p>$\\beta$를 조절하는 방법은 만약 $\\theta old$와 $\\theta$간의 파라미터 차이가 크다면 penalty를 강하게 부여하여 차이를 작게하고, 파라미터 차이가 작다면 penalty를 완화시켜 주어 차이를 더 크게하는 것입니다.</p>\n<p>정리하자면, KL-divergence를 constraint로 둔 것이 아니기 때문에, 간혹 excessive large policy update가 발생할 수도 있지만, KL Penaly coefficienct인 $\\beta$가 KL-divergence에 따라 adpative 하게 조정됨으로써 excessive large policy update가 지속적으로 발생되는 것을 방지하는 것입니다.</p>\n<p><br><br></p>\n<h1 id=\"6-Algorithm\"><a href=\"#6-Algorithm\" class=\"headerlink\" title=\"6. Algorithm\"></a>6. Algorithm</h1><p>앞서 다뤘던 section들은 어떻게 policy만을 업데이트 하는지에 대해 설명하고 있습니다. 이번 section에서는 value, entropy-exploration과 합쳐서 어떻게 통합적으로 업데이트 하는지에 대해서 알아보겠습니다.</p>\n<p>먼저 Variance-reduced advantage function estimator을 계산하기 위해 learned state-value fucntion $V(s)$을 사용합니다. 여기에는 두 가지 방법이 있습니다.</p>\n<ul>\n<li>Generalized advantage estimation</li>\n<li>Finite horizontal estimators</li>\n</ul>\n<p>만약 policy와 value function 간 parameter sharing하는 neural network architecture를 사용한다면, policy surrogate와 value function error term을 combine한 loss function을 사용해야 합니다. 또한 이 loss function에 entropy bonus term을 추가하여, 충분한 exploration이 될 수 있도록 합니다.(exploration하는 부분은 <a href=\"https://arxiv.org/pdf/1602.01783.pdf\" target=\"_blank\" rel=\"noopener\">A3C 논문</a>에 나와있습니다.)</p>\n<p>그래서 앞써 다뤘던 objective function을 $L^{CLIP}$라고 표현한다면 PPO에서 제시하는 통합적으로 최대화해야하는 objective function은 다음과 같이 표현할 수 있습니다.</p>\n<p>$$L_t^{CLIP+VF+S} (\\theta) = \\hat{E}_t [L^{CLIP} (\\theta) - c_1 L_t^{VF} (\\theta) + c_2 S[\\pi_\\theta] (s_t)]$$</p>\n<p>위의 수식에 대한 추가적인 설명은 다음과 같습니다.</p>\n<ul>\n<li>$c_1, c_2$ : coefficients</li>\n<li>$S$ : entropy bonus</li>\n<li>$L_t^{VF}$ : squared-error loss $(V_\\theta (s_t) - V_t^{targ})^2$</li>\n</ul>\n<p>위의 objective function을 최대화하면 Reinforcement Learning을 통한 policy 학습, state-value function의 학습, exploration을 할 수 있습니다.</p>\n<p>추가적으로 A3C와 같은 요즘 인기있는 PG의 style에서는 T time steps(T는 episode length 보다 훨씬 작은 크기) 동안 policy에 따라서 sample들을 얻고, 이 sample들을 업데이트에 사용합니다. 이러한 방식은 time step T 까지만 고려하는 advantage estimator가 필요하며, A3C에서 다음과 같이 사용합니다.</p>\n<p>$$\\hat{A}_t = - V(s_t) + r_t + \\gamma r_{t+1} + \\cdots + \\gamma^{T-t+1} r_{T-1} + \\gamma^{T-t}V(s_T)$$</p>\n<ul>\n<li>여기서 $t$는 주어진 length T trajectory segment 내에서 $[0, T]$에 있는 time index입니다.</li>\n</ul>\n<p>위의 수식에 더하여 generalized version인 GAE의 truncated version(generalized advantage estimation)을 사용합니다.($\\lambda$가 1이면 위 식과 같아집니다.) 수식은 아래와 같습니다. </p>\n<p>$$\\hat{A}_t = \\delta_t + (\\gamma\\lambda)\\delta_{t+1} + \\cdots + (\\gamma\\lambda)^{T-t+1}\\delta_{T-1},$$$$where \\,\\,\\,\\, \\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$$</p>\n<p>그래서 고정된 length T trajectory segment를 사용한 PPO algorithm은 다음과 같은 pseudo code로 나타낼 수 있습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/q00y8x7eryl1ncd/Screen%20Shot%202018-07-26%20at%202.28.03%20PM.png?dl=1\" width=\"800\"> </center>\n\n<ul>\n<li>여기서 actor는 A3C처럼 병렬로 두어도 상관없습니다.</li>\n<li>advantage estimate 계산을 통해서 surrogate loss를 계산합니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"7-Experimnet\"><a href=\"#7-Experimnet\" class=\"headerlink\" title=\"7. Experimnet\"></a>7. Experimnet</h1><p><br></p>\n<h2 id=\"7-1-Surrogate-Objectives의-비교\"><a href=\"#7-1-Surrogate-Objectives의-비교\" class=\"headerlink\" title=\"7.1 Surrogate Objectives의 비교\"></a>7.1 Surrogate Objectives의 비교</h2><p>이번 section에서는 3가지 Surrogate Objectives를 비교분석 하고 있습니다.</p>\n<ul>\n<li>No clipping or penalty: $L_t(\\theta) = r_t(\\theta)\\hat{A}_t$</li>\n<li>Clipping: $L_t(\\theta) = min(r_t(\\theta) \\, \\hat{A}_t, \\, clip(r_t(\\theta), 1-\\epsilon,1+\\epsilon) \\, \\hat{A}_t)$</li>\n<li>KL penalty(fixed or adaptive): <img src=\"https://www.dropbox.com/s/ksnhlxz2riuns1p/Screen%20Shot%202018-07-26%20at%202.42.50%20PM.png?dl=1\" width=\"270\"></li>\n</ul>\n<p>그리고 환경은 MuJoCo를 사용하며 OpenAI Gym 환경에서 실험을 진행하였습니다. 그리고 네트워크들은 2개의 hidden layer를 가지며 각각 64개의 unit들을 가지고 있습니다. 그리고 tanh를 활성화 함수로써 사용하고 있으며 네트워크는 Gaussian distribution을 출력하는 것으로 구성되어 있습니다. 그리고 Policy Network와 Value Network는 파라미터들을 공유하고 있지 않으며, 위에서 설명한 Exploration을 위한 entropy는 사용하지 않습니다.</p>\n<p>7개의 환경에서 실험하였으며 각 환경들은 HalfCheetah, Hopper, InvertedDoublePendulum, InvertedPerdulum, Reacher, Swimmer, Walker2d를 사용하였으며 모든 환경들은 v1 버전을 사용하였습니다.</p>\n<p>아래의 표는 실험 결과입니다. 세 가지 Surrogate Objective에 대해 각 파라미터들을 표에 표기된 방식대로 변화하며 실험하였습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/4xbkxh4m82mu1vo/Screen%20Shot%202018-07-26%20at%202.45.42%20PM.png?dl=1\" width=\"450\"> </center>\n\n<p><br></p>\n<h2 id=\"7-2-Comparison-to-other-Algorithms-in-the-Continuous-Domain\"><a href=\"#7-2-Comparison-to-other-Algorithms-in-the-Continuous-Domain\" class=\"headerlink\" title=\"7.2 Comparison to other Algorithms in the Continuous Domain\"></a>7.2 Comparison to other Algorithms in the Continuous Domain</h2><p>이번 section에서는 Clipping 버전의 PPO와 다른 알고리즘들 간의 비교를 연속적인 Action을 가지는 환경에서 실험을 한 결과를 보여주고 있습니다. 다른 알고리즘은 TRPO, cross-entropy method, vanilla policy gradient with adaptive stepsize, A2C, A2C with trust region를 사용하였습니다. 환경과 사용한 네트워크의 구성은 이전 section과 동일합니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/tf7djngioxnxtef/Screen%20Shot%202018-07-26%20at%202.50.23%20PM.png?dl=1\" width=\"900\"> </center>\n\n<p>여타 다른 알고리즘보다 좋은 성능을 보이는 것을 알 수 있습니다.</p>\n<p><br></p>\n<h2 id=\"7-3-Showcase-in-the-Continuous-Domain-Humanoid-Running-and-Steering\"><a href=\"#7-3-Showcase-in-the-Continuous-Domain-Humanoid-Running-and-Steering\" class=\"headerlink\" title=\"7.3 Showcase in the Continuous Domain: Humanoid Running and Steering\"></a>7.3 Showcase in the Continuous Domain: Humanoid Running and Steering</h2><p>이번 section에서는 Humanoid-v0, HumanoidFlagrun-v0, HumanoidFlagrunHarder-v0의 환경에서 Roboschool을 사용하여 실험하였습니다.</p>\n<ul>\n<li>Humanoid-v0는 단순히 앞으로 걸어나가는 환경</li>\n<li>HumanoidFlagrun-v0은 200스텝마다 혹은 목적지에 도착할 때 마다 위치가 바뀌는 목적지에 걸어서 도달하는 환경입니다. </li>\n<li>HumanoidFlagrunHarder-v0은 환경이 초기화될 때 마다 특정 경계안에 Humanoid가 위치하게 되며 특정 영역 밖으로 걸어나가는 것을 수행하는 환경입니다.</li>\n</ul>\n<center> <img src=\"https://www.dropbox.com/s/y112ua7il6l0296/Screen%20Shot%202018-07-26%20at%202.53.01%20PM.png?dl=1\" width=\"900\"> </center>\n\n<ul>\n<li>Roboschool을 사용하여 3D humanoid control task에서 PPO 알고리즘으로 학습시킨 Learning curve입니다.</li>\n</ul>\n<center> <img src=\"https://www.dropbox.com/s/09scyk6zeviyswt/Screen%20Shot%202018-07-26%20at%202.53.10%20PM.png?dl=1\" width=\"900\"> </center>\n\n<ul>\n<li>Roboschool Humanoid Flagrun에서 학습된 policy의 frame들입니다. </li>\n</ul>\n<p><br></p>\n<h2 id=\"7-4-Comparison-to-Other-Algorithms-on-the-Atari-Domain\"><a href=\"#7-4-Comparison-to-Other-Algorithms-on-the-Atari-Domain\" class=\"headerlink\" title=\"7.4 Comparison to Other Algorithms on the Atari Domain\"></a>7.4 Comparison to Other Algorithms on the Atari Domain</h2><p>이번 section에서는 Atari Domain에서 PPO, A2C, ACER(Actor Critic with Experience Replay)의 3가지 알고리즘을 비교합니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/jxuqdncxpnoyqgi/Screen%20Shot%202018-07-26%20at%203.00.00%20PM.png?dl=1\" width=\"600\"> </center>\n\n<p>전체 training에 대해서는 PPO가 ACER보다 좋은 성능을 내고 있습니다. 하지만 마지막 100 에피소드만을 비교했을 때는 PPO보다 ACER이 더 좋은 성능을 보이고 있습니다. 이는 PPO가 더 빨리 최종 능력에 도달하지만, ACER이 가진 potential이 더 높다는 것입니다.</p>\n<p><br><br></p>\n<h1 id=\"8-Conclusion\"><a href=\"#8-Conclusion\" class=\"headerlink\" title=\"8. Conclusion\"></a>8. Conclusion</h1><p>이 논문에서는 policy update를 하기 위한 방법으로 stochastic gradient ascent의 multiple epchs를 사용하는 policy optimization method들의 하나의 알고리즘은 Proximal Policy Optimization(PPO)를 소개합니다.</p>\n<p>이 알고리즘은 trust region method의 stability와 reliability를 가집니다. 여기에 더하여 학습하기에 훨씬 더 간단하고, 더 일반적인 setting으로 적용하기에 편한 A3C로서 code를 구성하기에도 편하고, 계산량도 훨씬 덜합니다. 그리고 전반적으로 더 좋은 성능을 가집니다.</p>\n<p><br><br></p>\n<h1 id=\"처음으로\"><a href=\"#처음으로\" class=\"headerlink\" title=\"처음으로\"></a>처음으로</h1><h2 id=\"PG-Travel-Guide\"><a href=\"#PG-Travel-Guide\" class=\"headerlink\" title=\"PG Travel Guide\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/\">PG Travel Guide</a></h2><p><br></p>\n<h1 id=\"이전으로\"><a href=\"#이전으로\" class=\"headerlink\" title=\"이전으로\"></a>이전으로</h1><h2 id=\"GAE-여행하기\"><a href=\"#GAE-여행하기\" class=\"headerlink\" title=\"GAE 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/23/6_gae/\">GAE 여행하기</a></h2><p><br></p>\n<h1 id=\"다음으로\"><a href=\"#다음으로\" class=\"headerlink\" title=\"다음으로\"></a>다음으로</h1><h2 id=\"PPO-Code\"><a href=\"#PPO-Code\" class=\"headerlink\" title=\"PPO Code\"></a><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/ppo_gae.py\" target=\"_blank\" rel=\"noopener\">PPO Code</a></h2>"},{"title":"Deterministic Policy Gradient Algorithms","date":"2018-06-27T08:21:48.000Z","author":"김동민, 공민서, 장수영, 차금강","subtitle":"피지여행 2번째 논문","_content":"\n<center> <img src=\"https://www.dropbox.com/s/etwa3hn5c0pegdv/Screen%20Shot%202018-07-18%20at%2012.51.35%20AM.png?dl=1\" width=\"800\"> </center>\n\n논문 저자 : David Silver, Guy Lever, Nicloas Heess, Thomas Degris, Daan Wierstra, Martin Riedmiller\n논문 링크 : [main text](http://proceedings.mlr.press/v32/silver14.pdf), [supplementary material](http://proceedings.mlr.press/v32/silver14-supp.pdf)\nProceeding : International Conference on Machine Learning (ICML) 2014\n정리 : 김동민, 공민서, 장수영, 차금강\n\n---\n\n# 1. 들어가며...\n\n- Deterministic Policy Gradient (DPG) Theorem을 제안합니다.\n    - 중요한 점은 DPG는 Expected gradient of the action-value function의 형태라는 것입니다.\n- Policy variance가 0에 수렴할 경우, DPG는 Stochastic Policy Gradient (SPG)와 동일해집니다. (Theorem 2)\n    - Theorem 2로 인해 기존 Policy Gradient (PG) 와 관련된 기법들을 DPG에 적용할 수 있게 됩니다.\n        - 예. Sutton PG, natural gradients, actor-critic, episodic/batch methods\n- 적절한 exploration 을 위해 model-free, off-policy actor-critic algorithm 을 제안합니다\n    - action-value function approximator 사용으로 인해 policy gradient가 bias되는 것을 방지하기 위해 compatibility condition을 제공합니다. (Theorem 3)\n- DPG 는 SPG 보다 성능이 좋습니다.\n    - 특히 high dimensional action spaces를 가지는 tasks에서의 성능 향상이 큽니다.\n        - SPG의 policy gradient는 state와 action spaces 모두에 대해서, DPG의 policy gradient는 state spaces에 대해서만 평균을 취합니다.\n        - 결과적으로, action spaces의 dimension이 커질수록 data efficiency가 높은 DPG의 학습이 더 잘 이뤄지게 됩니다.\n        - 무한정 학습을 시키면, SPG도 최적으로 수렴할 것으로 예상되기에 위 성능 비교는 일정 iteration 내로 한정합니다.\n    - 기존 기법들에 비해 computation 양이 많지 않습니다.\n        - Computation 은 action dimensionality 와 policy parameters 수에 비례합니다.\n\n<br><br>\n\n# 2. Background\n\n<br>\n## 2.1 Performance objective function\n\n$$\n\\begin{align}\nJ(\\pi_{\\theta}) &= \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\pi_{\\theta}(s,a)r(s,a)da ds = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[r(s,a)]\n\\end{align}\n$$\n\n<br>\n## 2.2 SPG Theorem\n- State distribution $ \\rho^{\\pi}(s) $ 은 policy parameters에 영향을 받지만, policy gradient 를 계산할 때는 state distribution 의 gradient 를 고려할 필요가 없습니다.\n$$\\begin{eqnarray}\\nabla_{\\theta}J(\\pi_{\\theta}) &=& \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\\\ &=& E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]\n\\end{eqnarray}$$\n\n<br>\n## 2.3 Stochastic Actor-Critic Algorithms\n- Actor와 Critic이 번갈아가면서 동작하며 stochastic policy를 최적화하는 기법입니다.\n- Actor: $ Q^{\\pi}(s,a) $ 를 근사한 $ Q^w(s,a) $를 이용해 stochastic policy gradient를 ascent하는 방향으로 policy parameter $ \\theta $를 업데이트함으로써 stochastic policy를 발전시킵니다.\n    - $ \\nabla_{\\theta}J(\\pi_{\\theta}) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q^{w}(s,a)] $\n- Critic: SARSA나 Q-learning같은 Temporal-difference (TD) learning을 이용해 action-value function의 parameter, $ w $를 업데이트함으로써 $ Q^w(s,a) $가 $ Q^{\\pi}(s,a) $과 유사해지도록 합니다.\n- 실제 값인 $ Q^{\\pi}(s,a) $ 대신 이를 근사한 $ Q^w(s,a) $를 사용하게 되면, 일반적으로 bias가 발생합니다. 하지만, compatible condition에 부합하는 $ Q^w(s,a) $를 사용하게 되면, bias가 발생하지 않습니다.\n\n<br>\n## 2.4 Off-policy Actor-Critic\n- Distinct behavior policy $ \\beta(a|s) ( \\neq \\pi_{\\theta}(a|s) ) $ 로부터 샘플링된 trajectories 를 이용한 Actor-Critic\n- Performance objective function\n    - $\\begin{eqnarray}\n        J_{\\beta}(\\pi_{\\theta}) \n        &=& \\int_{S}\\rho^{\\beta}(s)V^{\\pi}(s)ds \\nonumber \\\\\\\\\n        &=& \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads\n        \\end{eqnarray} $\n- off-policy policy gradient\n    - $ \\begin{eqnarray}\n        \\nabla_{\\theta}J_{\\beta}(\\pi_{\\theta}) &\\approx& \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\end{eqnarray} $\n        $=E_{s \\sim \\rho^{\\beta}, a \\sim \\beta}[\\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)}\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]$\n    - off-policy policy gradient 식에서의 물결 표시는 [Degris, 2012b](https://arxiv.org/abs/1205.4839) 논문에 근거합니다.\n        - Exact off-policy policy gradient 와 이를 approximate 한 policy gradient 는 아래와 같습니다. (빨간색 상자에 있는 항목을 삭제함으로써 근사합니다.)\n            - <img src=\"https://www.dropbox.com/s/xzpv3okc139c1fs/Screenshot%202018-06-16%2017.48.51.png?dl=1\" width=500px>\n        - [Degris, 2012b] Theorem 1 에 의해 policy parameter 가 approximated policy gradient ( $\\nabla_{u}𝑄^{\\pi,\\gamma}(𝑠,𝑎)$ term 제거)에 따라 업데이트되어도 policy 는 improve 가 됨이 보장되기에 exact off-policy policy gradient 대신 approximated off-policy policy gradient 를 사용해도 괜찮습니다.\n            - <img src=\"https://www.dropbox.com/s/mk13931r4scjngo/Screenshot%202018-06-16%2017.49.24.png?dl=1\" width=500px>\n    - off-policy policy gradient 식에서 $ \\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)} $는 importance sampling ratio 입니다.\n        - off-policy actor-critic에서는 $ \\beta $에 의해 샘플링된 trajectory를 이용해서 stochastic policy $ \\pi $를 예측하는 것이기 때문에 importance sampling이 필요합니다.\n\n\n<br><br>\n\n# 3. Gradient of Deterministic Policies\n\n<br>\n## 3.1 Regularity Conditions\n- 어떠한 이론이 성립하기 위한 전제 조건\n- Regularity conditions A.1\n    - $ p(s'|s,a), \\nabla_{a}p(s'|s,a), \\mu_{\\theta}(s), \\nabla_{\\theta}\\mu_{\\theta}(s), r(s,a), \\nabla_{a}r(s,a), p_{1}(s) $ are continuous in all parameters and variables $ s, a, s' $ and $ x $.\n- regularity conditions A.2\n    - There exists a $ b $ and $ L $ such that $ \\sup_{s}p_{1}(s) < b $, $ \\sup_{a,s,s'}p(s'|s,a) < b $, $ \\sup_{a,s}r(s,a) < b $, $ \\sup_{a,s,s'}\\|\\nabla_{a}p(s'|s,a)\\| < L $, and $ \\sup_{a,s}\\|\\nabla_{a}r(s,a)\\| < L $.\n\n<br>\n## 3.2 Deterministic Policy Gradient Theorem\n- Deterministic policy\n    - $ \\mu_{\\theta} : S \\to A $ with parameter vector $ \\theta \\in \\mathbb{R}^n $\n- Probability distribution\n    - $ p(s \\to s', t, \\mu) $\n- Discounted state distribution\n    - $ \\rho^{\\mu}(s) $\n- Performance objective\n\n$$\nJ(\\mu_{\\theta}) = E[r^{\\gamma}_{1} | \\mu] \n$$\n\n$$\n= \\int_{S}\\rho^{\\mu}(s)r(s,\\mu_{\\theta}(s))ds \n= E_{s \\sim \\rho^{\\mu}}[r(s,\\mu_{\\theta}(s))]\n$$\n\n- DPG Theorem\n    - MDP 가 A.1 만족한다면, 아래 식이 성립합니다.\n    $\\nabla_{\\theta}J(\\mu_{\\theta}) = \\int_{S}\\rho^{\\mu}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}ds \\nonumber$\n    $= E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]   \\nonumber $ \n    \n\t- DPG는 state space에 대해서만 평균을 취하면 되기에, state와 action space 모두에 대해 평균을 취해야 하는 SPG에 비해 data efficiency가 좋습니다. 즉, 더 적은 양의 데이터로도 학습이 잘 이뤄지게 됩니다.\n\n<br>    \n## 3.3 DPG 형태에 대한 informal intuition\n- Generalized policy iteration\n    - 정책 평가와 정책 발전을 한 번 씩 번갈아 가면서 실행하는 정책 iteration\n        - 위와 같이 해도 정책 평가에서 예측한 가치함수가 최적 가치함수에 수렴합니다.\n- 정책 평가\n    - action-value function $ Q^{\\pi}(s,a) $ or $ Q^{\\mu}(s,a) $을 estimate 하는 것 입니다.\n- 정책 발전\n    - 위 estimated action-value function에 따라 정책을 update하는 것 입니다.\n    - 주로 action-value function에 대한 greedy maximisation을 사용합니다.\n        - $ \\mu^{k+1}(s) = \\arg\\max\\limits_{a}Q^{\\mu^{k}}(s,a) $\n        - greedy 정책 발전은 매 단계마다 global maximization을 해야하는데, 이로 인해 continuous action spaces에서 계산량이 급격히 늘어납니다.\n    - 그래서 policy gradient 방법이 나옵니다.\n        - policy 를 $ \\theta $에 대해서 parameterize 합니다.\n        - 매 단계마다 global maximisation 수행하는 대신, 방문하는 state $ s $마다 policy parameter를 action-value function $ Q $의 $ \\theta $에 대한 gradient $ \\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s)) $ 방향으로 proportional하게 update 합니다.\n        - 하지만 각 state는 다른 방향을 제시할 수 있기에, state distribution $ \\rho^{\\mu}(s) $에 대한 기대값을 취해 policy parameter를 update 할 수도 있습니다.\n            - $ \\theta^{k+1} = \\theta^{k} + \\alpha E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s))] $\n        - 이는 chain-rule에 따라 아래와 같이 분리될 수 있습니다.\n            - $ \\theta^{k+1} = \\theta^{k} + \\alpha E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu^{k}}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $ (7)\n            - chain rule: $ \\frac{\\partial Q}{\\partial \\theta} = \\frac{\\partial a}{\\partial \\theta} \\frac{\\partial Q}{\\partial a} $\n        - 하지만 state distribution $ \\rho^{\\mu} $은 정책에 dependent 합니다.\n            - 정책이 바꾸게 되면, 바뀐 정책에 따라 방문하게 되는 state가 변하기 때문에 state distribution이 변하게 됩니다.\n        - 그렇기에 정책 update 시 state distribution에 대한 gradient를 고려하지 않는데 정책 발전이 이뤄진다는 것은 직관적으로 와닿지 않을 수 있습니다.\n        - deterministic policy gradient theorem은 state distribution에 대한 gradient 계산없이 위 식(7) 대로만 update해도 performance objective의 gradient를 정확하게 따름을 의미합니다.\n\n<br>\n## 3.4 DPG는 SPG의 limiting case\n- stochastic policy parameterization\n    - $ \\pi_{\\mu_{\\theta},\\sigma} $ by a deterministic policy $ \\mu_{\\theta} : S \\to A $ and a variance parameter $ \\sigma $\n    - $ \\sigma = 0 $ 이면, $ \\pi_{\\mu_{\\theta},\\sigma} \\equiv \\mu_{\\theta} $\n- Theorem 2. Policy의 variance가 0에 수렴하면, 즉, $ \\sigma \\to 0 $, stochastic policy gradient와 deterministic policy gradient는 동일해집니다.\n    - 조건: stochastic policy $ \\pi_{\\mu_{\\theta},\\sigma} = \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $\n        - $ \\sigma $는 variance입니다.\n        - $ \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $는 condition B.1을 만족합니다.\n        - MDP는 conditions A.1과 A.2를 만족합니다.\n    - 결과:\n        - $ \\lim\\limits_{\\sigma\\downarrow0}\\nabla_{\\theta}J(\\pi_{\\mu_{\\theta},\\sigma}) = \\nabla_{\\theta}J(\\mu_{\\theta})  $\n            - 좌변은 standard stochastic gradient이며, 우변은 deterministic gradient입니다.\n    - 의미:\n        - deterministic policy gradient는 stochastic policy gradient의 특수 case 입니다.\n        - 기존 유명한 policy gradients 기법들에 deterministic policy gradients 를 적용할 수 있습니다.\n            - 기존 기법들 예: compatible function approximation (Sutton, 1999), natural gradients (Kakade, 2001), actor-critic (Bhatnagar, 2007) or episodic/batch methods (Peters, 2005)\n\n<br><br>\n\n# 4. Deterministic Actor-Critic Algorithms\n1. SARSA critic를 이용한 on-policy actor-critic\n    - 단점\n        - deterministic policy에 의해 행동하면 exploration이 잘 되지 않기에, sub-optimal에 빠지기 쉽습니다.\n    - 목적\n        - 교훈/정보제공\n        - 환경에서 충분한 noise를 제공하여 exploration을 시킬 수 있다면, deterministic policy를 사용한다고 하여도 좋은 학습 결과를 얻을 수도 있습니다.\n            - 예. 바람이 agent의 행동에 영향(noise)을 줌\n    - Remind: 살사(SARSA) update rule\n        - $ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma Q(s_{t+1},a_{t+1}) - Q(s_{t},a_{t})) $\n    - Algorithm\n        - Critic은 MSE를 $ \\bf minimize $하는 방향, 즉, action-value function을 stochastic gradient $ \\bf descent $ 방법으로 update합니다.\n            - $ MSE = [Q^{\\mu}(s,a) - Q^{w}(s,a)]^2 $\n                - critic은 실제 $ Q^{\\mu}(s,a) $ 대신 미분 가능한 $ Q^{w}(s,a) $로 대체하여 action-value function을 estimate하며, 이 둘 간 mean square error를 최소화하는 것이 목표입니다.\n            - $ \\nabla_{w}MSE \\approx -2 * [r + \\gamma Q^{w}(s',a') - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $\n                - $ \\nabla_{w}MSE = -2 * [Q^{\\mu}(s,a) - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $\n                - $ Q^{\\mu}(s,a) $ 를 $ r + \\gamma Q^{w}(s',a') $로 대체\n                    - $ Q^{\\mu}(s,a) = r + \\gamma Q^{\\mu}(s',a') $\n            - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $\n                - $w_{t+1} = w_{t} - \\alpha_{w}\\nabla_{w}MSE  \\nonumber$\n                $ \\approx w_{t} - \\alpha_{w} * (-2 * [r + \\gamma Q^{w}(s',a') - Q^{w}(s,a)] \\nabla_{w}Q^{w}(s,a)$\n                - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},a_{t+1}) - Q^{w}(s_{t},a_{t}) $\n        - Actor는 식(9)에 따라 보상이 $ \\bf maximize $되는 방향, 즉, deterministic policy를 stochastic gradient $ \\bf ascent $ 방법으로 update합니다.\n            - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $\n2. Q-learning 을 이용한 off-policy actor-critic\n    - stochastic behavior policy $ \\beta(a|s) $에 의해 생성된 trajectories로부터 deterministic target policy $ \\mu_{\\theta}(s) $를 학습하는 off-policy actor-critic입니다\n    - performance objective\n        - $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)V^{\\mu}(s)ds \\nonumber \\\\\\\\$\n          $= \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds \\nonumber \\\\\\\\$\n          $= E_{s \\sim \\rho^{\\beta}}[Q^{\\mu}(s,\\mu_{\\theta}(s))]$\n    - off-policy deterministic policy gradient\n        - $ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $\n            - 논문에는 아래와 같이 나와있는데, 물결 표시 부분은 오류로 판단됩니다.\n            - $ \\begin{eqnarray}\n                \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) &\\approx& \\int_{S}\\rho^{\\beta}(s)\\nabla_{\\theta}\\mu_{\\theta}(a|s)Q^{\\mu}(s,a)ds \\nonumber \\\\\n                &=& E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]\n                \\end{eqnarray} $\n            - 근거: Action이 deterministic하기에 stochastic 경우와는 다르게 performance objective에서 action에 대해 평균을 구할 필요가 없습니다. 그렇기에, 곱의 미분이 있을 필요가 없고, [Degris, 2012b]에서 처럼 곱의 미분을 통해 생기는 action-value function에 대한 gradient term을 생략할 필요가 사라집니다.\n    - Remind: 큐러닝(Q-learning) update rule\n        - $ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma \\max\\limits_{a}Q(s_{t+1},a) - Q(s_{t},a_{t})) $\n    - algorithm: OPDAC (Off-Policy Deterministic Actor-Critic)\n        - 살사를 이용한 on-policy deterministic actor-critic과 아래 부분을 제외하고는 같습니다.\n            - target policy는 $ \\beta(a|s) $에 의해 생성된 trajectories를 통해 학습합니다.\n            - 업데이트 목표 부분에 실제 행동 값 $ a_{t+1} $이 아니라 정책으로부터 나온 행동 값 $ \\mu_{\\theta}(s_{t+1}) $을 사용합니다.\n                - $ \\mu_{\\theta}(s_{t+1}) $ 은 가장 높은 Q 값을 가지는 행동. 즉, Q-learning.\n        - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $\n        - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $\n        - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $\n    - Stochastic off-policy actor-critic은 대개 actor와 critic 모두 importance sampling을 필요로 하지만, deterministic policy gradient에선 importance sampling이 필요없습니다.\n        - Actor 는 deterministic 이기에 sampling 자체가 필요없습니다.\n            - Stochastic policy인 경우, Actor에서 importance sampling이 필요한 이유는 상태 $ s $에서의 가치 함수 값 $ V^{\\pi}(s) $을 estimate하기 위해 $ \\pi $가 아니라 $ \\beta $에 따라 sampling을 한 후, 평균을 내기 때문입니다.\n            - 하지만 deterministic policy인 경우, 상태 $ s $에서의 가치 함수 값 $ V^{\\pi}(s) = Q^{\\pi}(s,\\mu_{\\theta}) $ 즉, action이 상태 s에 대해 deterministic이기에 sampling을 통해 estimate할 필요가 없고, 따라서 importance sampling도 필요없어집니다.\n            - stochastic vs. deterministic performance objective\n                - stochastic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads $\n                - deterministic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds $\n        - Critic이 사용하는 Q-learning은 importance sampling이 필요없는 off policy 알고리즘입니다.\n            - Q-learning도 업데이트 목표를 특정 분포에서 샘플링을 통해 estimate 하는 것이 아니라 Q 함수를 최대화하는 action을 선택하는 것이기에 위 actor 에서의 deterministic 경우와 비슷하게 볼 수 있습니다.\n3. compatible function approximation 및 gradient temporal-difference learning 을 이용한 actor-critic\n    - 위 살사/Q-learning 기반 on/off-policy는 아래와 같은 문제가 존재합니다.\n        - function approximator에 의한 bias\n            - 일반적으로 $ Q^{\\mu}(s,a) $ 를 $ Q^{w}(s,a) $로 대체하여 deterministic policy gradient를 구하면, 그 gradient는 ascent하는 방향이 아닐 수도 있습니다.\n        - off-policy learning에 의한 instabilities\n    - 그래서 stochastic처럼 $ \\nabla_{a}Q^{\\mu}(s,a) $를 $ \\nabla_{a}Q^{w}(s,a) $로 대체해도 deterministic policy gradient에 영향을 미치지 않을 compatible function approximator $ Q^{w}(s,a) $를 찾아야 합니다.\n    - Theorem 3. 아래 두 조건을 만족하면, $ Q^{w}(s,a) $는 deterministic policy $ \\mu_{\\theta}(s) $와 compatible 합니다. 즉, $ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $\n        - $ \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $입니다.\n        - $ w $는 $ MSE(\\theta, w) = E[\\epsilon(s;\\theta,w)^{\\top}\\epsilon(s;\\theta,w)] $를 최소화합니다.\n            - $ \\epsilon(s;\\theta,w) = \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} - \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}  $\n    - Theorem 3은 on-policy 뿐만 아니라 off-policy에도 적용 가능합니다.\n    - $ Q^{w}(s,a) = (a-\\mu_{\\theta}(s))^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top} w + V^{v}(s) $\n        - 어떠한 deterministic policy에 대해서도 위 형태와 같은 compatible function approximator가 존재합니다.\n        - 앞의 term은 advantage를, 뒤의 term은 value로 볼 수 있습니다.\n    - $ Q^{w}(s,a) = \\phi(s,a)^{\\top} w + v^{\\top}\\phi(s) $\n        - 정의 : $ \\phi(s,a) \\overset{\\underset{\\mathrm{def}}{}}{=} \\nabla_{\\theta}\\mu_{\\theta}(s)(a-\\mu_{\\theta}(s)) $\n        - 일례 : $ V^{v}(s) = v^{\\top}\\phi(s) $\n        - Theorem 3 만족 여부\n            - 첫 번째 조건 만족합니다.\n            - 두 번째 조건은 대강 만족합니다.\n                - $ \\nabla_{a}Q^{\\mu}(s,a) $에 대한 unbiased sample을 획득하기는 매우 어렵기에, 일반적인 정책 평가 방법들로 $ w $를 학습합니다.\n                - 이 정책 평가 방법들을 이용하면 $ Q^{w}(s,a) \\approx Q^{\\mu}(s,a) $인 reasonable solution을 찾을 수 있기에 대강 $ \\nabla_{a}Q^{w}(s,a) \\approx \\nabla_{a}Q^{\\mu}(s,a) $이 될 것입니다.\n        - action-value function에 대한 linear function approximator는 큰 값을 가지는 actions에 대해선 diverge할 수 있어 global하게 action-values 예측하기에는 좋지 않지만, local critic에 사용할 때는 매우 유용합니다.\n            - 즉, 절대값이 아니라 작은 변화량을 다루는 gradient method 경우엔 $ A^{w}(s,\\mu_{\\theta}(s)+\\delta) = \\delta^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $로, diverge하지 않고, 값을 얻을 수 있습니다.\n    - COPDAC-Q algorithm (Compatible Off-Policy Deterministic Actor-Critic Q-learning critic)\n        - Critic: 실제 action-value function에 대한 linear function approximator인 $ Q^{w}(s,a) = \\phi(s,a)^{\\top}w $를 estimate합니다.\n            - $ \\phi(s,a) = a^{\\top}\\nabla_{\\theta}\\mu_{\\theta} $\n            - Behavior policy $ \\beta(a|s) $로부터 얻은 samples를 이용하여 Q-learning이나 gradient Q-learning과 같은 off-policy algorithm으로 학습 가능합니다.\n        - Actor: estimated action-value function에 대한 gradient를 $ \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $로 치환 후, 정책을 업데이트합니다.\n        - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $\n        - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) $\n        - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $\n    - off-policy Q-learning은 linear function approximation을 이용하면 diverge 할 수도 있습니다.\n        - $ \\mu_{\\theta}(s_{t+1}) $이 diverge할 수도 있기 때문으로 판단됩니다.\n        - 그렇기에 simple Q-learning 대신 다른 기법이 필요합니다.\n    - 그렇기에 critic 에 gradient Q-learning 사용한 COPDAC-GQ (Gradient Q-learning critic) algorithm을 제안합니다.\n        - gradient temporal-difference learning에 기반한 기법들은 true gradient descent algorithm이며, converge가 보장됩니다. (Sutton, 2009)\n            - 기본 아이디어는 stochastic gradient descent로 mean-squared projected Bellman error (MSPBE)를 최소화하는 것입니다.\n            - critic이 actor보다 빠른 time-scale로 update되도록 step size들을 잘 조절하면, critic은 MSPBE를 최소화하는 parameters로 converge하게 됩니다.\n            - critic에 gradient temporal-difference learning의 일종인 gradient Q-learning을 사용한 논문입니다. (Maei, 2010)\n    - COPDAC-GQ algorithm\n        - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $\n        - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $\n        - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) - \\alpha_{w}\\gamma\\phi(s_{t+1}, \\mu_{\\theta}(s_{t+1}))(\\phi(s_{t},a_{t})^{\\top} u_{t}) $\n        - $ v_{t+1} = v_{t} + \\alpha_{v}\\delta_{t}\\phi(s_{t}) - \\alpha_{v}\\gamma\\phi(s_{t+1})(\\phi(s_{t},a_{t})^{\\top} u_{t}) $\n        - $ u_{t+1} = u_{t} + \\alpha_{u}(\\delta_{t}-\\phi(s_{t}, a_{t})^{\\top} u_{t})\\phi(s_{t}, a_{t}) $\n    - stochastic actor-critic과 같이 매 time-step 마다 update 시 필요한 계산의 복잡도는 $ O(mn) $입니다.\n        - m은 action dimensions, n은 number of policy parameters\n    - Natural policy gradient를 이용해 deterministic policies를 찾을 수 있습니다.\n        - $ M(\\theta)^{-1}\\nabla_{\\theta}J(\\mu_{\\theta}) $은 any metric $ M(\\theta) $에 대한 performance objective (식(14))의 steepest ascent direction 입니다. (Toussaint, 2012)\n        - Natural gradient는 Fisher information metric $ M_{\\pi}(\\theta) $에 대한 steepest ascent direction 입니다.\n            -  $ M_{\\pi}(\\theta) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)^{\\top}] $\n            - Fisher information metric은 policy reparameterization에 대해 불변입니다. (Bagnell, 2003)\n        - deterministic policies에 대한 metric으로 $ M_{\\mu}(\\theta) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}] $을 사용합니다.\n        \t- 이는 variance가 0인 policy에 대한 Fisher information metric으로 볼 수 있습니다.\n        \t- $ \\frac{\\nabla_{\\theta}\\pi_{\\theta}(a\\vert s)}{\\pi_{\\theta}(a\\vert s)}$에서 policy variance가 0이면, 특정 s에 대한 $ \\pi_{\\theta}(a|s)$만 1이 되고, 나머지는 0입니다.\n        - deterministic policy gradient theorem과 compatible function approximation을 결합하면 $ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] $이 됩니다.\n            - $ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $\n            - $ \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)} \\approx \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $\n        - 그렇기에 steepest ascent direction은 $ M_{\\mu}(\\theta)^{-1}\\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = w $이 됩니다.\n            - $ E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}]^{-1}E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] = w $\n        - 이 알고리즘은 COPDAC-Q 혹은 COPDAC-GQ에서 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $ 식을 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta}w_{t} $로 바꿔주기만 하면 됩니다.\n\n<br><br>\n\n# 5. Experiments\n\n<br>\n## 5.1. Continuous Bandit\n- Stochastic Actor-Critic (SAC)과 COPDAC 간 성능 비교 수행합니다.\n    - Action dimension이 커질수록 성능 차이가 심합니다.\n    - 빠르게 수렴하는 것을 통해 DPG의 data efficiency가 SPG에 비해 좋다는 것을 확인할 수 있지만, 반면, time-step이 증가할수록 SAC와 COPDAC 간 성능 차이가 줄어드는 것을 통해 성능 차이가 심하다는 것은 일정 time step 내에서만 해당하는 것이라고 유추해볼 수 있습니다.\n    - <img src=\"https://www.dropbox.com/s/hrkyq0s2f24z66r/Screenshot%202018-06-16%2017.47.38.png?dl=1\">\n\n<br>\n## 5.2. Continuous Reinforcement Learning\n- COPDAC-Q, SAC, off-policy stochastic actor-critic(OffPAC-TD) 간 성능 비교 수행합니다.\n    - COPDAC-Q의 성능이 약간 더 좋습니다.\n    - COPDAC-Q의 학습이 더 빨리 이뤄집니다.\n    - <img src=\"https://www.dropbox.com/s/qdca4augapmzsxi/Screenshot%202018-06-16%2017.47.07.png?dl=1\">\n\n<br>\n## 5.3. Octopus Arm\n- 목표: 6 segments octopus arm (20 action dimensions & 50 state dimensions)을 control하여 target을 맞추는 것입니다.\n    - COPDAC-Q 사용 시, action space dimension이 큰 octopus arm을 잘 control하여 target을 맞춤입니다.\n    - <img src=\"https://www.dropbox.com/s/xrxb0a52wntekld/Screenshot%202018-06-16%2017.46.28.png?dl=1\" width=600px>\n    - 기존 기법들은 action spaces 혹은 action과 state spaces 둘 다 작은 경우들에 대해서만 실험했다고 하며, 비교하고 있지 않습니다.\n        - 기존 기법들이 6 segments octopus arm에서 동작을 잘 안 했을 것 같긴한데, 그래도 실험해서 보여줬으면 하지만 실험을 하지 않았습니다.\n    - 8 segment arm 동영상이 저자 홈페이지에 있다고 하는데, 안 보입니다.\n- [참고] Octopus Arm 이란?\n    - [OctopusArm Youtube Link](https://www.youtube.com/watch?v=AxeeHif0euY)\n    - <img src=\"https://www.dropbox.com/s/950ycj06sudakjx/Screenshot%202018-06-16%2017.45.52.png?dl=1\">\n\n<br><br>\n\n# 처음으로\n\n## [PG Travel Guide](https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/)\n\n<br>\n\n# 이전으로\n\n## [Sutton PG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/28/1_sutton-pg/)\n\n<br>\n\n# 다음으로\n\n## [DDPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/26/3_ddpg/)","source":"_posts/2_dpg.md","raw":"---\ntitle: Deterministic Policy Gradient Algorithms\ndate: 2018-06-27 17:21:48\ntags: [\"프로젝트\", \"피지여행\"]\ncategories: 프로젝트\nauthor: 김동민, 공민서, 장수영, 차금강\nsubtitle: 피지여행 2번째 논문\n---\n\n<center> <img src=\"https://www.dropbox.com/s/etwa3hn5c0pegdv/Screen%20Shot%202018-07-18%20at%2012.51.35%20AM.png?dl=1\" width=\"800\"> </center>\n\n논문 저자 : David Silver, Guy Lever, Nicloas Heess, Thomas Degris, Daan Wierstra, Martin Riedmiller\n논문 링크 : [main text](http://proceedings.mlr.press/v32/silver14.pdf), [supplementary material](http://proceedings.mlr.press/v32/silver14-supp.pdf)\nProceeding : International Conference on Machine Learning (ICML) 2014\n정리 : 김동민, 공민서, 장수영, 차금강\n\n---\n\n# 1. 들어가며...\n\n- Deterministic Policy Gradient (DPG) Theorem을 제안합니다.\n    - 중요한 점은 DPG는 Expected gradient of the action-value function의 형태라는 것입니다.\n- Policy variance가 0에 수렴할 경우, DPG는 Stochastic Policy Gradient (SPG)와 동일해집니다. (Theorem 2)\n    - Theorem 2로 인해 기존 Policy Gradient (PG) 와 관련된 기법들을 DPG에 적용할 수 있게 됩니다.\n        - 예. Sutton PG, natural gradients, actor-critic, episodic/batch methods\n- 적절한 exploration 을 위해 model-free, off-policy actor-critic algorithm 을 제안합니다\n    - action-value function approximator 사용으로 인해 policy gradient가 bias되는 것을 방지하기 위해 compatibility condition을 제공합니다. (Theorem 3)\n- DPG 는 SPG 보다 성능이 좋습니다.\n    - 특히 high dimensional action spaces를 가지는 tasks에서의 성능 향상이 큽니다.\n        - SPG의 policy gradient는 state와 action spaces 모두에 대해서, DPG의 policy gradient는 state spaces에 대해서만 평균을 취합니다.\n        - 결과적으로, action spaces의 dimension이 커질수록 data efficiency가 높은 DPG의 학습이 더 잘 이뤄지게 됩니다.\n        - 무한정 학습을 시키면, SPG도 최적으로 수렴할 것으로 예상되기에 위 성능 비교는 일정 iteration 내로 한정합니다.\n    - 기존 기법들에 비해 computation 양이 많지 않습니다.\n        - Computation 은 action dimensionality 와 policy parameters 수에 비례합니다.\n\n<br><br>\n\n# 2. Background\n\n<br>\n## 2.1 Performance objective function\n\n$$\n\\begin{align}\nJ(\\pi_{\\theta}) &= \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\pi_{\\theta}(s,a)r(s,a)da ds = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[r(s,a)]\n\\end{align}\n$$\n\n<br>\n## 2.2 SPG Theorem\n- State distribution $ \\rho^{\\pi}(s) $ 은 policy parameters에 영향을 받지만, policy gradient 를 계산할 때는 state distribution 의 gradient 를 고려할 필요가 없습니다.\n$$\\begin{eqnarray}\\nabla_{\\theta}J(\\pi_{\\theta}) &=& \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\\\ &=& E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]\n\\end{eqnarray}$$\n\n<br>\n## 2.3 Stochastic Actor-Critic Algorithms\n- Actor와 Critic이 번갈아가면서 동작하며 stochastic policy를 최적화하는 기법입니다.\n- Actor: $ Q^{\\pi}(s,a) $ 를 근사한 $ Q^w(s,a) $를 이용해 stochastic policy gradient를 ascent하는 방향으로 policy parameter $ \\theta $를 업데이트함으로써 stochastic policy를 발전시킵니다.\n    - $ \\nabla_{\\theta}J(\\pi_{\\theta}) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q^{w}(s,a)] $\n- Critic: SARSA나 Q-learning같은 Temporal-difference (TD) learning을 이용해 action-value function의 parameter, $ w $를 업데이트함으로써 $ Q^w(s,a) $가 $ Q^{\\pi}(s,a) $과 유사해지도록 합니다.\n- 실제 값인 $ Q^{\\pi}(s,a) $ 대신 이를 근사한 $ Q^w(s,a) $를 사용하게 되면, 일반적으로 bias가 발생합니다. 하지만, compatible condition에 부합하는 $ Q^w(s,a) $를 사용하게 되면, bias가 발생하지 않습니다.\n\n<br>\n## 2.4 Off-policy Actor-Critic\n- Distinct behavior policy $ \\beta(a|s) ( \\neq \\pi_{\\theta}(a|s) ) $ 로부터 샘플링된 trajectories 를 이용한 Actor-Critic\n- Performance objective function\n    - $\\begin{eqnarray}\n        J_{\\beta}(\\pi_{\\theta}) \n        &=& \\int_{S}\\rho^{\\beta}(s)V^{\\pi}(s)ds \\nonumber \\\\\\\\\n        &=& \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads\n        \\end{eqnarray} $\n- off-policy policy gradient\n    - $ \\begin{eqnarray}\n        \\nabla_{\\theta}J_{\\beta}(\\pi_{\\theta}) &\\approx& \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\end{eqnarray} $\n        $=E_{s \\sim \\rho^{\\beta}, a \\sim \\beta}[\\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)}\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]$\n    - off-policy policy gradient 식에서의 물결 표시는 [Degris, 2012b](https://arxiv.org/abs/1205.4839) 논문에 근거합니다.\n        - Exact off-policy policy gradient 와 이를 approximate 한 policy gradient 는 아래와 같습니다. (빨간색 상자에 있는 항목을 삭제함으로써 근사합니다.)\n            - <img src=\"https://www.dropbox.com/s/xzpv3okc139c1fs/Screenshot%202018-06-16%2017.48.51.png?dl=1\" width=500px>\n        - [Degris, 2012b] Theorem 1 에 의해 policy parameter 가 approximated policy gradient ( $\\nabla_{u}𝑄^{\\pi,\\gamma}(𝑠,𝑎)$ term 제거)에 따라 업데이트되어도 policy 는 improve 가 됨이 보장되기에 exact off-policy policy gradient 대신 approximated off-policy policy gradient 를 사용해도 괜찮습니다.\n            - <img src=\"https://www.dropbox.com/s/mk13931r4scjngo/Screenshot%202018-06-16%2017.49.24.png?dl=1\" width=500px>\n    - off-policy policy gradient 식에서 $ \\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)} $는 importance sampling ratio 입니다.\n        - off-policy actor-critic에서는 $ \\beta $에 의해 샘플링된 trajectory를 이용해서 stochastic policy $ \\pi $를 예측하는 것이기 때문에 importance sampling이 필요합니다.\n\n\n<br><br>\n\n# 3. Gradient of Deterministic Policies\n\n<br>\n## 3.1 Regularity Conditions\n- 어떠한 이론이 성립하기 위한 전제 조건\n- Regularity conditions A.1\n    - $ p(s'|s,a), \\nabla_{a}p(s'|s,a), \\mu_{\\theta}(s), \\nabla_{\\theta}\\mu_{\\theta}(s), r(s,a), \\nabla_{a}r(s,a), p_{1}(s) $ are continuous in all parameters and variables $ s, a, s' $ and $ x $.\n- regularity conditions A.2\n    - There exists a $ b $ and $ L $ such that $ \\sup_{s}p_{1}(s) < b $, $ \\sup_{a,s,s'}p(s'|s,a) < b $, $ \\sup_{a,s}r(s,a) < b $, $ \\sup_{a,s,s'}\\|\\nabla_{a}p(s'|s,a)\\| < L $, and $ \\sup_{a,s}\\|\\nabla_{a}r(s,a)\\| < L $.\n\n<br>\n## 3.2 Deterministic Policy Gradient Theorem\n- Deterministic policy\n    - $ \\mu_{\\theta} : S \\to A $ with parameter vector $ \\theta \\in \\mathbb{R}^n $\n- Probability distribution\n    - $ p(s \\to s', t, \\mu) $\n- Discounted state distribution\n    - $ \\rho^{\\mu}(s) $\n- Performance objective\n\n$$\nJ(\\mu_{\\theta}) = E[r^{\\gamma}_{1} | \\mu] \n$$\n\n$$\n= \\int_{S}\\rho^{\\mu}(s)r(s,\\mu_{\\theta}(s))ds \n= E_{s \\sim \\rho^{\\mu}}[r(s,\\mu_{\\theta}(s))]\n$$\n\n- DPG Theorem\n    - MDP 가 A.1 만족한다면, 아래 식이 성립합니다.\n    $\\nabla_{\\theta}J(\\mu_{\\theta}) = \\int_{S}\\rho^{\\mu}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}ds \\nonumber$\n    $= E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]   \\nonumber $ \n    \n\t- DPG는 state space에 대해서만 평균을 취하면 되기에, state와 action space 모두에 대해 평균을 취해야 하는 SPG에 비해 data efficiency가 좋습니다. 즉, 더 적은 양의 데이터로도 학습이 잘 이뤄지게 됩니다.\n\n<br>    \n## 3.3 DPG 형태에 대한 informal intuition\n- Generalized policy iteration\n    - 정책 평가와 정책 발전을 한 번 씩 번갈아 가면서 실행하는 정책 iteration\n        - 위와 같이 해도 정책 평가에서 예측한 가치함수가 최적 가치함수에 수렴합니다.\n- 정책 평가\n    - action-value function $ Q^{\\pi}(s,a) $ or $ Q^{\\mu}(s,a) $을 estimate 하는 것 입니다.\n- 정책 발전\n    - 위 estimated action-value function에 따라 정책을 update하는 것 입니다.\n    - 주로 action-value function에 대한 greedy maximisation을 사용합니다.\n        - $ \\mu^{k+1}(s) = \\arg\\max\\limits_{a}Q^{\\mu^{k}}(s,a) $\n        - greedy 정책 발전은 매 단계마다 global maximization을 해야하는데, 이로 인해 continuous action spaces에서 계산량이 급격히 늘어납니다.\n    - 그래서 policy gradient 방법이 나옵니다.\n        - policy 를 $ \\theta $에 대해서 parameterize 합니다.\n        - 매 단계마다 global maximisation 수행하는 대신, 방문하는 state $ s $마다 policy parameter를 action-value function $ Q $의 $ \\theta $에 대한 gradient $ \\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s)) $ 방향으로 proportional하게 update 합니다.\n        - 하지만 각 state는 다른 방향을 제시할 수 있기에, state distribution $ \\rho^{\\mu}(s) $에 대한 기대값을 취해 policy parameter를 update 할 수도 있습니다.\n            - $ \\theta^{k+1} = \\theta^{k} + \\alpha E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s))] $\n        - 이는 chain-rule에 따라 아래와 같이 분리될 수 있습니다.\n            - $ \\theta^{k+1} = \\theta^{k} + \\alpha E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu^{k}}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $ (7)\n            - chain rule: $ \\frac{\\partial Q}{\\partial \\theta} = \\frac{\\partial a}{\\partial \\theta} \\frac{\\partial Q}{\\partial a} $\n        - 하지만 state distribution $ \\rho^{\\mu} $은 정책에 dependent 합니다.\n            - 정책이 바꾸게 되면, 바뀐 정책에 따라 방문하게 되는 state가 변하기 때문에 state distribution이 변하게 됩니다.\n        - 그렇기에 정책 update 시 state distribution에 대한 gradient를 고려하지 않는데 정책 발전이 이뤄진다는 것은 직관적으로 와닿지 않을 수 있습니다.\n        - deterministic policy gradient theorem은 state distribution에 대한 gradient 계산없이 위 식(7) 대로만 update해도 performance objective의 gradient를 정확하게 따름을 의미합니다.\n\n<br>\n## 3.4 DPG는 SPG의 limiting case\n- stochastic policy parameterization\n    - $ \\pi_{\\mu_{\\theta},\\sigma} $ by a deterministic policy $ \\mu_{\\theta} : S \\to A $ and a variance parameter $ \\sigma $\n    - $ \\sigma = 0 $ 이면, $ \\pi_{\\mu_{\\theta},\\sigma} \\equiv \\mu_{\\theta} $\n- Theorem 2. Policy의 variance가 0에 수렴하면, 즉, $ \\sigma \\to 0 $, stochastic policy gradient와 deterministic policy gradient는 동일해집니다.\n    - 조건: stochastic policy $ \\pi_{\\mu_{\\theta},\\sigma} = \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $\n        - $ \\sigma $는 variance입니다.\n        - $ \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $는 condition B.1을 만족합니다.\n        - MDP는 conditions A.1과 A.2를 만족합니다.\n    - 결과:\n        - $ \\lim\\limits_{\\sigma\\downarrow0}\\nabla_{\\theta}J(\\pi_{\\mu_{\\theta},\\sigma}) = \\nabla_{\\theta}J(\\mu_{\\theta})  $\n            - 좌변은 standard stochastic gradient이며, 우변은 deterministic gradient입니다.\n    - 의미:\n        - deterministic policy gradient는 stochastic policy gradient의 특수 case 입니다.\n        - 기존 유명한 policy gradients 기법들에 deterministic policy gradients 를 적용할 수 있습니다.\n            - 기존 기법들 예: compatible function approximation (Sutton, 1999), natural gradients (Kakade, 2001), actor-critic (Bhatnagar, 2007) or episodic/batch methods (Peters, 2005)\n\n<br><br>\n\n# 4. Deterministic Actor-Critic Algorithms\n1. SARSA critic를 이용한 on-policy actor-critic\n    - 단점\n        - deterministic policy에 의해 행동하면 exploration이 잘 되지 않기에, sub-optimal에 빠지기 쉽습니다.\n    - 목적\n        - 교훈/정보제공\n        - 환경에서 충분한 noise를 제공하여 exploration을 시킬 수 있다면, deterministic policy를 사용한다고 하여도 좋은 학습 결과를 얻을 수도 있습니다.\n            - 예. 바람이 agent의 행동에 영향(noise)을 줌\n    - Remind: 살사(SARSA) update rule\n        - $ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma Q(s_{t+1},a_{t+1}) - Q(s_{t},a_{t})) $\n    - Algorithm\n        - Critic은 MSE를 $ \\bf minimize $하는 방향, 즉, action-value function을 stochastic gradient $ \\bf descent $ 방법으로 update합니다.\n            - $ MSE = [Q^{\\mu}(s,a) - Q^{w}(s,a)]^2 $\n                - critic은 실제 $ Q^{\\mu}(s,a) $ 대신 미분 가능한 $ Q^{w}(s,a) $로 대체하여 action-value function을 estimate하며, 이 둘 간 mean square error를 최소화하는 것이 목표입니다.\n            - $ \\nabla_{w}MSE \\approx -2 * [r + \\gamma Q^{w}(s',a') - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $\n                - $ \\nabla_{w}MSE = -2 * [Q^{\\mu}(s,a) - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $\n                - $ Q^{\\mu}(s,a) $ 를 $ r + \\gamma Q^{w}(s',a') $로 대체\n                    - $ Q^{\\mu}(s,a) = r + \\gamma Q^{\\mu}(s',a') $\n            - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $\n                - $w_{t+1} = w_{t} - \\alpha_{w}\\nabla_{w}MSE  \\nonumber$\n                $ \\approx w_{t} - \\alpha_{w} * (-2 * [r + \\gamma Q^{w}(s',a') - Q^{w}(s,a)] \\nabla_{w}Q^{w}(s,a)$\n                - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},a_{t+1}) - Q^{w}(s_{t},a_{t}) $\n        - Actor는 식(9)에 따라 보상이 $ \\bf maximize $되는 방향, 즉, deterministic policy를 stochastic gradient $ \\bf ascent $ 방법으로 update합니다.\n            - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $\n2. Q-learning 을 이용한 off-policy actor-critic\n    - stochastic behavior policy $ \\beta(a|s) $에 의해 생성된 trajectories로부터 deterministic target policy $ \\mu_{\\theta}(s) $를 학습하는 off-policy actor-critic입니다\n    - performance objective\n        - $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)V^{\\mu}(s)ds \\nonumber \\\\\\\\$\n          $= \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds \\nonumber \\\\\\\\$\n          $= E_{s \\sim \\rho^{\\beta}}[Q^{\\mu}(s,\\mu_{\\theta}(s))]$\n    - off-policy deterministic policy gradient\n        - $ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $\n            - 논문에는 아래와 같이 나와있는데, 물결 표시 부분은 오류로 판단됩니다.\n            - $ \\begin{eqnarray}\n                \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) &\\approx& \\int_{S}\\rho^{\\beta}(s)\\nabla_{\\theta}\\mu_{\\theta}(a|s)Q^{\\mu}(s,a)ds \\nonumber \\\\\n                &=& E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]\n                \\end{eqnarray} $\n            - 근거: Action이 deterministic하기에 stochastic 경우와는 다르게 performance objective에서 action에 대해 평균을 구할 필요가 없습니다. 그렇기에, 곱의 미분이 있을 필요가 없고, [Degris, 2012b]에서 처럼 곱의 미분을 통해 생기는 action-value function에 대한 gradient term을 생략할 필요가 사라집니다.\n    - Remind: 큐러닝(Q-learning) update rule\n        - $ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma \\max\\limits_{a}Q(s_{t+1},a) - Q(s_{t},a_{t})) $\n    - algorithm: OPDAC (Off-Policy Deterministic Actor-Critic)\n        - 살사를 이용한 on-policy deterministic actor-critic과 아래 부분을 제외하고는 같습니다.\n            - target policy는 $ \\beta(a|s) $에 의해 생성된 trajectories를 통해 학습합니다.\n            - 업데이트 목표 부분에 실제 행동 값 $ a_{t+1} $이 아니라 정책으로부터 나온 행동 값 $ \\mu_{\\theta}(s_{t+1}) $을 사용합니다.\n                - $ \\mu_{\\theta}(s_{t+1}) $ 은 가장 높은 Q 값을 가지는 행동. 즉, Q-learning.\n        - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $\n        - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $\n        - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $\n    - Stochastic off-policy actor-critic은 대개 actor와 critic 모두 importance sampling을 필요로 하지만, deterministic policy gradient에선 importance sampling이 필요없습니다.\n        - Actor 는 deterministic 이기에 sampling 자체가 필요없습니다.\n            - Stochastic policy인 경우, Actor에서 importance sampling이 필요한 이유는 상태 $ s $에서의 가치 함수 값 $ V^{\\pi}(s) $을 estimate하기 위해 $ \\pi $가 아니라 $ \\beta $에 따라 sampling을 한 후, 평균을 내기 때문입니다.\n            - 하지만 deterministic policy인 경우, 상태 $ s $에서의 가치 함수 값 $ V^{\\pi}(s) = Q^{\\pi}(s,\\mu_{\\theta}) $ 즉, action이 상태 s에 대해 deterministic이기에 sampling을 통해 estimate할 필요가 없고, 따라서 importance sampling도 필요없어집니다.\n            - stochastic vs. deterministic performance objective\n                - stochastic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads $\n                - deterministic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds $\n        - Critic이 사용하는 Q-learning은 importance sampling이 필요없는 off policy 알고리즘입니다.\n            - Q-learning도 업데이트 목표를 특정 분포에서 샘플링을 통해 estimate 하는 것이 아니라 Q 함수를 최대화하는 action을 선택하는 것이기에 위 actor 에서의 deterministic 경우와 비슷하게 볼 수 있습니다.\n3. compatible function approximation 및 gradient temporal-difference learning 을 이용한 actor-critic\n    - 위 살사/Q-learning 기반 on/off-policy는 아래와 같은 문제가 존재합니다.\n        - function approximator에 의한 bias\n            - 일반적으로 $ Q^{\\mu}(s,a) $ 를 $ Q^{w}(s,a) $로 대체하여 deterministic policy gradient를 구하면, 그 gradient는 ascent하는 방향이 아닐 수도 있습니다.\n        - off-policy learning에 의한 instabilities\n    - 그래서 stochastic처럼 $ \\nabla_{a}Q^{\\mu}(s,a) $를 $ \\nabla_{a}Q^{w}(s,a) $로 대체해도 deterministic policy gradient에 영향을 미치지 않을 compatible function approximator $ Q^{w}(s,a) $를 찾아야 합니다.\n    - Theorem 3. 아래 두 조건을 만족하면, $ Q^{w}(s,a) $는 deterministic policy $ \\mu_{\\theta}(s) $와 compatible 합니다. 즉, $ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $\n        - $ \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $입니다.\n        - $ w $는 $ MSE(\\theta, w) = E[\\epsilon(s;\\theta,w)^{\\top}\\epsilon(s;\\theta,w)] $를 최소화합니다.\n            - $ \\epsilon(s;\\theta,w) = \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} - \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}  $\n    - Theorem 3은 on-policy 뿐만 아니라 off-policy에도 적용 가능합니다.\n    - $ Q^{w}(s,a) = (a-\\mu_{\\theta}(s))^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top} w + V^{v}(s) $\n        - 어떠한 deterministic policy에 대해서도 위 형태와 같은 compatible function approximator가 존재합니다.\n        - 앞의 term은 advantage를, 뒤의 term은 value로 볼 수 있습니다.\n    - $ Q^{w}(s,a) = \\phi(s,a)^{\\top} w + v^{\\top}\\phi(s) $\n        - 정의 : $ \\phi(s,a) \\overset{\\underset{\\mathrm{def}}{}}{=} \\nabla_{\\theta}\\mu_{\\theta}(s)(a-\\mu_{\\theta}(s)) $\n        - 일례 : $ V^{v}(s) = v^{\\top}\\phi(s) $\n        - Theorem 3 만족 여부\n            - 첫 번째 조건 만족합니다.\n            - 두 번째 조건은 대강 만족합니다.\n                - $ \\nabla_{a}Q^{\\mu}(s,a) $에 대한 unbiased sample을 획득하기는 매우 어렵기에, 일반적인 정책 평가 방법들로 $ w $를 학습합니다.\n                - 이 정책 평가 방법들을 이용하면 $ Q^{w}(s,a) \\approx Q^{\\mu}(s,a) $인 reasonable solution을 찾을 수 있기에 대강 $ \\nabla_{a}Q^{w}(s,a) \\approx \\nabla_{a}Q^{\\mu}(s,a) $이 될 것입니다.\n        - action-value function에 대한 linear function approximator는 큰 값을 가지는 actions에 대해선 diverge할 수 있어 global하게 action-values 예측하기에는 좋지 않지만, local critic에 사용할 때는 매우 유용합니다.\n            - 즉, 절대값이 아니라 작은 변화량을 다루는 gradient method 경우엔 $ A^{w}(s,\\mu_{\\theta}(s)+\\delta) = \\delta^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $로, diverge하지 않고, 값을 얻을 수 있습니다.\n    - COPDAC-Q algorithm (Compatible Off-Policy Deterministic Actor-Critic Q-learning critic)\n        - Critic: 실제 action-value function에 대한 linear function approximator인 $ Q^{w}(s,a) = \\phi(s,a)^{\\top}w $를 estimate합니다.\n            - $ \\phi(s,a) = a^{\\top}\\nabla_{\\theta}\\mu_{\\theta} $\n            - Behavior policy $ \\beta(a|s) $로부터 얻은 samples를 이용하여 Q-learning이나 gradient Q-learning과 같은 off-policy algorithm으로 학습 가능합니다.\n        - Actor: estimated action-value function에 대한 gradient를 $ \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $로 치환 후, 정책을 업데이트합니다.\n        - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $\n        - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) $\n        - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $\n    - off-policy Q-learning은 linear function approximation을 이용하면 diverge 할 수도 있습니다.\n        - $ \\mu_{\\theta}(s_{t+1}) $이 diverge할 수도 있기 때문으로 판단됩니다.\n        - 그렇기에 simple Q-learning 대신 다른 기법이 필요합니다.\n    - 그렇기에 critic 에 gradient Q-learning 사용한 COPDAC-GQ (Gradient Q-learning critic) algorithm을 제안합니다.\n        - gradient temporal-difference learning에 기반한 기법들은 true gradient descent algorithm이며, converge가 보장됩니다. (Sutton, 2009)\n            - 기본 아이디어는 stochastic gradient descent로 mean-squared projected Bellman error (MSPBE)를 최소화하는 것입니다.\n            - critic이 actor보다 빠른 time-scale로 update되도록 step size들을 잘 조절하면, critic은 MSPBE를 최소화하는 parameters로 converge하게 됩니다.\n            - critic에 gradient temporal-difference learning의 일종인 gradient Q-learning을 사용한 논문입니다. (Maei, 2010)\n    - COPDAC-GQ algorithm\n        - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $\n        - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $\n        - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) - \\alpha_{w}\\gamma\\phi(s_{t+1}, \\mu_{\\theta}(s_{t+1}))(\\phi(s_{t},a_{t})^{\\top} u_{t}) $\n        - $ v_{t+1} = v_{t} + \\alpha_{v}\\delta_{t}\\phi(s_{t}) - \\alpha_{v}\\gamma\\phi(s_{t+1})(\\phi(s_{t},a_{t})^{\\top} u_{t}) $\n        - $ u_{t+1} = u_{t} + \\alpha_{u}(\\delta_{t}-\\phi(s_{t}, a_{t})^{\\top} u_{t})\\phi(s_{t}, a_{t}) $\n    - stochastic actor-critic과 같이 매 time-step 마다 update 시 필요한 계산의 복잡도는 $ O(mn) $입니다.\n        - m은 action dimensions, n은 number of policy parameters\n    - Natural policy gradient를 이용해 deterministic policies를 찾을 수 있습니다.\n        - $ M(\\theta)^{-1}\\nabla_{\\theta}J(\\mu_{\\theta}) $은 any metric $ M(\\theta) $에 대한 performance objective (식(14))의 steepest ascent direction 입니다. (Toussaint, 2012)\n        - Natural gradient는 Fisher information metric $ M_{\\pi}(\\theta) $에 대한 steepest ascent direction 입니다.\n            -  $ M_{\\pi}(\\theta) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)^{\\top}] $\n            - Fisher information metric은 policy reparameterization에 대해 불변입니다. (Bagnell, 2003)\n        - deterministic policies에 대한 metric으로 $ M_{\\mu}(\\theta) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}] $을 사용합니다.\n        \t- 이는 variance가 0인 policy에 대한 Fisher information metric으로 볼 수 있습니다.\n        \t- $ \\frac{\\nabla_{\\theta}\\pi_{\\theta}(a\\vert s)}{\\pi_{\\theta}(a\\vert s)}$에서 policy variance가 0이면, 특정 s에 대한 $ \\pi_{\\theta}(a|s)$만 1이 되고, 나머지는 0입니다.\n        - deterministic policy gradient theorem과 compatible function approximation을 결합하면 $ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] $이 됩니다.\n            - $ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $\n            - $ \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)} \\approx \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $\n        - 그렇기에 steepest ascent direction은 $ M_{\\mu}(\\theta)^{-1}\\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = w $이 됩니다.\n            - $ E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}]^{-1}E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] = w $\n        - 이 알고리즘은 COPDAC-Q 혹은 COPDAC-GQ에서 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $ 식을 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta}w_{t} $로 바꿔주기만 하면 됩니다.\n\n<br><br>\n\n# 5. Experiments\n\n<br>\n## 5.1. Continuous Bandit\n- Stochastic Actor-Critic (SAC)과 COPDAC 간 성능 비교 수행합니다.\n    - Action dimension이 커질수록 성능 차이가 심합니다.\n    - 빠르게 수렴하는 것을 통해 DPG의 data efficiency가 SPG에 비해 좋다는 것을 확인할 수 있지만, 반면, time-step이 증가할수록 SAC와 COPDAC 간 성능 차이가 줄어드는 것을 통해 성능 차이가 심하다는 것은 일정 time step 내에서만 해당하는 것이라고 유추해볼 수 있습니다.\n    - <img src=\"https://www.dropbox.com/s/hrkyq0s2f24z66r/Screenshot%202018-06-16%2017.47.38.png?dl=1\">\n\n<br>\n## 5.2. Continuous Reinforcement Learning\n- COPDAC-Q, SAC, off-policy stochastic actor-critic(OffPAC-TD) 간 성능 비교 수행합니다.\n    - COPDAC-Q의 성능이 약간 더 좋습니다.\n    - COPDAC-Q의 학습이 더 빨리 이뤄집니다.\n    - <img src=\"https://www.dropbox.com/s/qdca4augapmzsxi/Screenshot%202018-06-16%2017.47.07.png?dl=1\">\n\n<br>\n## 5.3. Octopus Arm\n- 목표: 6 segments octopus arm (20 action dimensions & 50 state dimensions)을 control하여 target을 맞추는 것입니다.\n    - COPDAC-Q 사용 시, action space dimension이 큰 octopus arm을 잘 control하여 target을 맞춤입니다.\n    - <img src=\"https://www.dropbox.com/s/xrxb0a52wntekld/Screenshot%202018-06-16%2017.46.28.png?dl=1\" width=600px>\n    - 기존 기법들은 action spaces 혹은 action과 state spaces 둘 다 작은 경우들에 대해서만 실험했다고 하며, 비교하고 있지 않습니다.\n        - 기존 기법들이 6 segments octopus arm에서 동작을 잘 안 했을 것 같긴한데, 그래도 실험해서 보여줬으면 하지만 실험을 하지 않았습니다.\n    - 8 segment arm 동영상이 저자 홈페이지에 있다고 하는데, 안 보입니다.\n- [참고] Octopus Arm 이란?\n    - [OctopusArm Youtube Link](https://www.youtube.com/watch?v=AxeeHif0euY)\n    - <img src=\"https://www.dropbox.com/s/950ycj06sudakjx/Screenshot%202018-06-16%2017.45.52.png?dl=1\">\n\n<br><br>\n\n# 처음으로\n\n## [PG Travel Guide](https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/)\n\n<br>\n\n# 이전으로\n\n## [Sutton PG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/28/1_sutton-pg/)\n\n<br>\n\n# 다음으로\n\n## [DDPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/26/3_ddpg/)","slug":"2_dpg","published":1,"updated":"2018-09-08T11:00:20.605Z","_id":"cjkj6tyrt000gj715smm2b4il","comments":1,"layout":"post","photos":[],"link":"","content":"<center> <img src=\"https://www.dropbox.com/s/etwa3hn5c0pegdv/Screen%20Shot%202018-07-18%20at%2012.51.35%20AM.png?dl=1\" width=\"800\"> </center>\n\n<p>논문 저자 : David Silver, Guy Lever, Nicloas Heess, Thomas Degris, Daan Wierstra, Martin Riedmiller<br>논문 링크 : <a href=\"http://proceedings.mlr.press/v32/silver14.pdf\" target=\"_blank\" rel=\"noopener\">main text</a>, <a href=\"http://proceedings.mlr.press/v32/silver14-supp.pdf\" target=\"_blank\" rel=\"noopener\">supplementary material</a><br>Proceeding : International Conference on Machine Learning (ICML) 2014<br>정리 : 김동민, 공민서, 장수영, 차금강</p>\n<hr>\n<h1 id=\"1-들어가며…\"><a href=\"#1-들어가며…\" class=\"headerlink\" title=\"1. 들어가며…\"></a>1. 들어가며…</h1><ul>\n<li>Deterministic Policy Gradient (DPG) Theorem을 제안합니다.<ul>\n<li>중요한 점은 DPG는 Expected gradient of the action-value function의 형태라는 것입니다.</li>\n</ul>\n</li>\n<li>Policy variance가 0에 수렴할 경우, DPG는 Stochastic Policy Gradient (SPG)와 동일해집니다. (Theorem 2)<ul>\n<li>Theorem 2로 인해 기존 Policy Gradient (PG) 와 관련된 기법들을 DPG에 적용할 수 있게 됩니다.<ul>\n<li>예. Sutton PG, natural gradients, actor-critic, episodic/batch methods</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>적절한 exploration 을 위해 model-free, off-policy actor-critic algorithm 을 제안합니다<ul>\n<li>action-value function approximator 사용으로 인해 policy gradient가 bias되는 것을 방지하기 위해 compatibility condition을 제공합니다. (Theorem 3)</li>\n</ul>\n</li>\n<li>DPG 는 SPG 보다 성능이 좋습니다.<ul>\n<li>특히 high dimensional action spaces를 가지는 tasks에서의 성능 향상이 큽니다.<ul>\n<li>SPG의 policy gradient는 state와 action spaces 모두에 대해서, DPG의 policy gradient는 state spaces에 대해서만 평균을 취합니다.</li>\n<li>결과적으로, action spaces의 dimension이 커질수록 data efficiency가 높은 DPG의 학습이 더 잘 이뤄지게 됩니다.</li>\n<li>무한정 학습을 시키면, SPG도 최적으로 수렴할 것으로 예상되기에 위 성능 비교는 일정 iteration 내로 한정합니다.</li>\n</ul>\n</li>\n<li>기존 기법들에 비해 computation 양이 많지 않습니다.<ul>\n<li>Computation 은 action dimensionality 와 policy parameters 수에 비례합니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"2-Background\"><a href=\"#2-Background\" class=\"headerlink\" title=\"2. Background\"></a>2. Background</h1><p><br></p>\n<h2 id=\"2-1-Performance-objective-function\"><a href=\"#2-1-Performance-objective-function\" class=\"headerlink\" title=\"2.1 Performance objective function\"></a>2.1 Performance objective function</h2><p>$$<br>\\begin{align}<br>J(\\pi_{\\theta}) &amp;= \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\pi_{\\theta}(s,a)r(s,a)da ds = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[r(s,a)]<br>\\end{align}<br>$$</p>\n<p><br></p>\n<h2 id=\"2-2-SPG-Theorem\"><a href=\"#2-2-SPG-Theorem\" class=\"headerlink\" title=\"2.2 SPG Theorem\"></a>2.2 SPG Theorem</h2><ul>\n<li>State distribution $ \\rho^{\\pi}(s) $ 은 policy parameters에 영향을 받지만, policy gradient 를 계산할 때는 state distribution 의 gradient 를 고려할 필요가 없습니다.<br>$$\\begin{eqnarray}\\nabla_{\\theta}J(\\pi_{\\theta}) &amp;=&amp; \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\ &amp;=&amp; E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]<br>\\end{eqnarray}$$</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-3-Stochastic-Actor-Critic-Algorithms\"><a href=\"#2-3-Stochastic-Actor-Critic-Algorithms\" class=\"headerlink\" title=\"2.3 Stochastic Actor-Critic Algorithms\"></a>2.3 Stochastic Actor-Critic Algorithms</h2><ul>\n<li>Actor와 Critic이 번갈아가면서 동작하며 stochastic policy를 최적화하는 기법입니다.</li>\n<li>Actor: $ Q^{\\pi}(s,a) $ 를 근사한 $ Q^w(s,a) $를 이용해 stochastic policy gradient를 ascent하는 방향으로 policy parameter $ \\theta $를 업데이트함으로써 stochastic policy를 발전시킵니다.<ul>\n<li>$ \\nabla_{\\theta}J(\\pi_{\\theta}) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q^{w}(s,a)] $</li>\n</ul>\n</li>\n<li>Critic: SARSA나 Q-learning같은 Temporal-difference (TD) learning을 이용해 action-value function의 parameter, $ w $를 업데이트함으로써 $ Q^w(s,a) $가 $ Q^{\\pi}(s,a) $과 유사해지도록 합니다.</li>\n<li>실제 값인 $ Q^{\\pi}(s,a) $ 대신 이를 근사한 $ Q^w(s,a) $를 사용하게 되면, 일반적으로 bias가 발생합니다. 하지만, compatible condition에 부합하는 $ Q^w(s,a) $를 사용하게 되면, bias가 발생하지 않습니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-4-Off-policy-Actor-Critic\"><a href=\"#2-4-Off-policy-Actor-Critic\" class=\"headerlink\" title=\"2.4 Off-policy Actor-Critic\"></a>2.4 Off-policy Actor-Critic</h2><ul>\n<li>Distinct behavior policy $ \\beta(a|s) ( \\neq \\pi_{\\theta}(a|s) ) $ 로부터 샘플링된 trajectories 를 이용한 Actor-Critic</li>\n<li>Performance objective function<ul>\n<li>$\\begin{eqnarray}<br>  J_{\\beta}(\\pi_{\\theta})<br>  &amp;=&amp; \\int_{S}\\rho^{\\beta}(s)V^{\\pi}(s)ds \\nonumber \\\\<br>  &amp;=&amp; \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads<br>  \\end{eqnarray} $</li>\n</ul>\n</li>\n<li>off-policy policy gradient<ul>\n<li>$ \\begin{eqnarray}<br>  \\nabla_{\\theta}J_{\\beta}(\\pi_{\\theta}) &amp;\\approx&amp; \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\end{eqnarray} $<br>  $=E_{s \\sim \\rho^{\\beta}, a \\sim \\beta}[\\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)}\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]$</li>\n<li>off-policy policy gradient 식에서의 물결 표시는 <a href=\"https://arxiv.org/abs/1205.4839\" target=\"_blank\" rel=\"noopener\">Degris, 2012b</a> 논문에 근거합니다.<ul>\n<li>Exact off-policy policy gradient 와 이를 approximate 한 policy gradient 는 아래와 같습니다. (빨간색 상자에 있는 항목을 삭제함으로써 근사합니다.)<ul>\n<li><img src=\"https://www.dropbox.com/s/xzpv3okc139c1fs/Screenshot%202018-06-16%2017.48.51.png?dl=1\" width=\"500px\"></li>\n</ul>\n</li>\n<li>[Degris, 2012b] Theorem 1 에 의해 policy parameter 가 approximated policy gradient ( $\\nabla_{u}𝑄^{\\pi,\\gamma}(𝑠,𝑎)$ term 제거)에 따라 업데이트되어도 policy 는 improve 가 됨이 보장되기에 exact off-policy policy gradient 대신 approximated off-policy policy gradient 를 사용해도 괜찮습니다.<ul>\n<li><img src=\"https://www.dropbox.com/s/mk13931r4scjngo/Screenshot%202018-06-16%2017.49.24.png?dl=1\" width=\"500px\"></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>off-policy policy gradient 식에서 $ \\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)} $는 importance sampling ratio 입니다.<ul>\n<li>off-policy actor-critic에서는 $ \\beta $에 의해 샘플링된 trajectory를 이용해서 stochastic policy $ \\pi $를 예측하는 것이기 때문에 importance sampling이 필요합니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"3-Gradient-of-Deterministic-Policies\"><a href=\"#3-Gradient-of-Deterministic-Policies\" class=\"headerlink\" title=\"3. Gradient of Deterministic Policies\"></a>3. Gradient of Deterministic Policies</h1><p><br></p>\n<h2 id=\"3-1-Regularity-Conditions\"><a href=\"#3-1-Regularity-Conditions\" class=\"headerlink\" title=\"3.1 Regularity Conditions\"></a>3.1 Regularity Conditions</h2><ul>\n<li>어떠한 이론이 성립하기 위한 전제 조건</li>\n<li>Regularity conditions A.1<ul>\n<li>$ p(s’|s,a), \\nabla_{a}p(s’|s,a), \\mu_{\\theta}(s), \\nabla_{\\theta}\\mu_{\\theta}(s), r(s,a), \\nabla_{a}r(s,a), p_{1}(s) $ are continuous in all parameters and variables $ s, a, s’ $ and $ x $.</li>\n</ul>\n</li>\n<li>regularity conditions A.2<ul>\n<li>There exists a $ b $ and $ L $ such that $ \\sup_{s}p_{1}(s) &lt; b $, $ \\sup_{a,s,s’}p(s’|s,a) &lt; b $, $ \\sup_{a,s}r(s,a) &lt; b $, $ \\sup_{a,s,s’}|\\nabla_{a}p(s’|s,a)| &lt; L $, and $ \\sup_{a,s}|\\nabla_{a}r(s,a)| &lt; L $.</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-2-Deterministic-Policy-Gradient-Theorem\"><a href=\"#3-2-Deterministic-Policy-Gradient-Theorem\" class=\"headerlink\" title=\"3.2 Deterministic Policy Gradient Theorem\"></a>3.2 Deterministic Policy Gradient Theorem</h2><ul>\n<li>Deterministic policy<ul>\n<li>$ \\mu_{\\theta} : S \\to A $ with parameter vector $ \\theta \\in \\mathbb{R}^n $</li>\n</ul>\n</li>\n<li>Probability distribution<ul>\n<li>$ p(s \\to s’, t, \\mu) $</li>\n</ul>\n</li>\n<li>Discounted state distribution<ul>\n<li>$ \\rho^{\\mu}(s) $</li>\n</ul>\n</li>\n<li>Performance objective</li>\n</ul>\n<p>$$<br>J(\\mu_{\\theta}) = E[r^{\\gamma}_{1} | \\mu]<br>$$</p>\n<p>$$<br>= \\int_{S}\\rho^{\\mu}(s)r(s,\\mu_{\\theta}(s))ds<br>= E_{s \\sim \\rho^{\\mu}}[r(s,\\mu_{\\theta}(s))]<br>$$</p>\n<ul>\n<li><p>DPG Theorem</p>\n<ul>\n<li><p>MDP 가 A.1 만족한다면, 아래 식이 성립합니다.<br>$\\nabla_{\\theta}J(\\mu_{\\theta}) = \\int_{S}\\rho^{\\mu}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}ds \\nonumber$<br>$= E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]   \\nonumber $ </p>\n</li>\n<li><p>DPG는 state space에 대해서만 평균을 취하면 되기에, state와 action space 모두에 대해 평균을 취해야 하는 SPG에 비해 data efficiency가 좋습니다. 즉, 더 적은 양의 데이터로도 학습이 잘 이뤄지게 됩니다.</p>\n</li>\n</ul>\n</li>\n</ul>\n<p><br>    </p>\n<h2 id=\"3-3-DPG-형태에-대한-informal-intuition\"><a href=\"#3-3-DPG-형태에-대한-informal-intuition\" class=\"headerlink\" title=\"3.3 DPG 형태에 대한 informal intuition\"></a>3.3 DPG 형태에 대한 informal intuition</h2><ul>\n<li>Generalized policy iteration<ul>\n<li>정책 평가와 정책 발전을 한 번 씩 번갈아 가면서 실행하는 정책 iteration<ul>\n<li>위와 같이 해도 정책 평가에서 예측한 가치함수가 최적 가치함수에 수렴합니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>정책 평가<ul>\n<li>action-value function $ Q^{\\pi}(s,a) $ or $ Q^{\\mu}(s,a) $을 estimate 하는 것 입니다.</li>\n</ul>\n</li>\n<li>정책 발전<ul>\n<li>위 estimated action-value function에 따라 정책을 update하는 것 입니다.</li>\n<li>주로 action-value function에 대한 greedy maximisation을 사용합니다.<ul>\n<li>$ \\mu^{k+1}(s) = \\arg\\max\\limits_{a}Q^{\\mu^{k}}(s,a) $</li>\n<li>greedy 정책 발전은 매 단계마다 global maximization을 해야하는데, 이로 인해 continuous action spaces에서 계산량이 급격히 늘어납니다.</li>\n</ul>\n</li>\n<li>그래서 policy gradient 방법이 나옵니다.<ul>\n<li>policy 를 $ \\theta $에 대해서 parameterize 합니다.</li>\n<li>매 단계마다 global maximisation 수행하는 대신, 방문하는 state $ s $마다 policy parameter를 action-value function $ Q $의 $ \\theta $에 대한 gradient $ \\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s)) $ 방향으로 proportional하게 update 합니다.</li>\n<li>하지만 각 state는 다른 방향을 제시할 수 있기에, state distribution $ \\rho^{\\mu}(s) $에 대한 기대값을 취해 policy parameter를 update 할 수도 있습니다.<ul>\n<li>$ \\theta^{k+1} = \\theta^{k} + \\alpha E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s))] $</li>\n</ul>\n</li>\n<li>이는 chain-rule에 따라 아래와 같이 분리될 수 있습니다.<ul>\n<li>$ \\theta^{k+1} = \\theta^{k} + \\alpha E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu^{k}}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $ (7)</li>\n<li>chain rule: $ \\frac{\\partial Q}{\\partial \\theta} = \\frac{\\partial a}{\\partial \\theta} \\frac{\\partial Q}{\\partial a} $</li>\n</ul>\n</li>\n<li>하지만 state distribution $ \\rho^{\\mu} $은 정책에 dependent 합니다.<ul>\n<li>정책이 바꾸게 되면, 바뀐 정책에 따라 방문하게 되는 state가 변하기 때문에 state distribution이 변하게 됩니다.</li>\n</ul>\n</li>\n<li>그렇기에 정책 update 시 state distribution에 대한 gradient를 고려하지 않는데 정책 발전이 이뤄진다는 것은 직관적으로 와닿지 않을 수 있습니다.</li>\n<li>deterministic policy gradient theorem은 state distribution에 대한 gradient 계산없이 위 식(7) 대로만 update해도 performance objective의 gradient를 정확하게 따름을 의미합니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-4-DPG는-SPG의-limiting-case\"><a href=\"#3-4-DPG는-SPG의-limiting-case\" class=\"headerlink\" title=\"3.4 DPG는 SPG의 limiting case\"></a>3.4 DPG는 SPG의 limiting case</h2><ul>\n<li>stochastic policy parameterization<ul>\n<li>$ \\pi_{\\mu_{\\theta},\\sigma} $ by a deterministic policy $ \\mu_{\\theta} : S \\to A $ and a variance parameter $ \\sigma $</li>\n<li>$ \\sigma = 0 $ 이면, $ \\pi_{\\mu_{\\theta},\\sigma} \\equiv \\mu_{\\theta} $</li>\n</ul>\n</li>\n<li>Theorem 2. Policy의 variance가 0에 수렴하면, 즉, $ \\sigma \\to 0 $, stochastic policy gradient와 deterministic policy gradient는 동일해집니다.<ul>\n<li>조건: stochastic policy $ \\pi_{\\mu_{\\theta},\\sigma} = \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $<ul>\n<li>$ \\sigma $는 variance입니다.</li>\n<li>$ \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $는 condition B.1을 만족합니다.</li>\n<li>MDP는 conditions A.1과 A.2를 만족합니다.</li>\n</ul>\n</li>\n<li>결과:<ul>\n<li>$ \\lim\\limits_{\\sigma\\downarrow0}\\nabla_{\\theta}J(\\pi_{\\mu_{\\theta},\\sigma}) = \\nabla_{\\theta}J(\\mu_{\\theta})  $<ul>\n<li>좌변은 standard stochastic gradient이며, 우변은 deterministic gradient입니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>의미:<ul>\n<li>deterministic policy gradient는 stochastic policy gradient의 특수 case 입니다.</li>\n<li>기존 유명한 policy gradients 기법들에 deterministic policy gradients 를 적용할 수 있습니다.<ul>\n<li>기존 기법들 예: compatible function approximation (Sutton, 1999), natural gradients (Kakade, 2001), actor-critic (Bhatnagar, 2007) or episodic/batch methods (Peters, 2005)</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"4-Deterministic-Actor-Critic-Algorithms\"><a href=\"#4-Deterministic-Actor-Critic-Algorithms\" class=\"headerlink\" title=\"4. Deterministic Actor-Critic Algorithms\"></a>4. Deterministic Actor-Critic Algorithms</h1><ol>\n<li>SARSA critic를 이용한 on-policy actor-critic<ul>\n<li>단점<ul>\n<li>deterministic policy에 의해 행동하면 exploration이 잘 되지 않기에, sub-optimal에 빠지기 쉽습니다.</li>\n</ul>\n</li>\n<li>목적<ul>\n<li>교훈/정보제공</li>\n<li>환경에서 충분한 noise를 제공하여 exploration을 시킬 수 있다면, deterministic policy를 사용한다고 하여도 좋은 학습 결과를 얻을 수도 있습니다.<ul>\n<li>예. 바람이 agent의 행동에 영향(noise)을 줌</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Remind: 살사(SARSA) update rule<ul>\n<li>$ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma Q(s_{t+1},a_{t+1}) - Q(s_{t},a_{t})) $</li>\n</ul>\n</li>\n<li>Algorithm<ul>\n<li>Critic은 MSE를 $ \\bf minimize $하는 방향, 즉, action-value function을 stochastic gradient $ \\bf descent $ 방법으로 update합니다.<ul>\n<li>$ MSE = [Q^{\\mu}(s,a) - Q^{w}(s,a)]^2 $<ul>\n<li>critic은 실제 $ Q^{\\mu}(s,a) $ 대신 미분 가능한 $ Q^{w}(s,a) $로 대체하여 action-value function을 estimate하며, 이 둘 간 mean square error를 최소화하는 것이 목표입니다.</li>\n</ul>\n</li>\n<li>$ \\nabla_{w}MSE \\approx -2 * [r + \\gamma Q^{w}(s’,a’) - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $<ul>\n<li>$ \\nabla_{w}MSE = -2 * [Q^{\\mu}(s,a) - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $</li>\n<li>$ Q^{\\mu}(s,a) $ 를 $ r + \\gamma Q^{w}(s’,a’) $로 대체<ul>\n<li>$ Q^{\\mu}(s,a) = r + \\gamma Q^{\\mu}(s’,a’) $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $<ul>\n<li>$w_{t+1} = w_{t} - \\alpha_{w}\\nabla_{w}MSE  \\nonumber$<br>$ \\approx w_{t} - \\alpha_{w} <em> (-2 </em> [r + \\gamma Q^{w}(s’,a’) - Q^{w}(s,a)] \\nabla_{w}Q^{w}(s,a)$</li>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},a_{t+1}) - Q^{w}(s_{t},a_{t}) $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Actor는 식(9)에 따라 보상이 $ \\bf maximize $되는 방향, 즉, deterministic policy를 stochastic gradient $ \\bf ascent $ 방법으로 update합니다.<ul>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Q-learning 을 이용한 off-policy actor-critic<ul>\n<li>stochastic behavior policy $ \\beta(a|s) $에 의해 생성된 trajectories로부터 deterministic target policy $ \\mu_{\\theta}(s) $를 학습하는 off-policy actor-critic입니다</li>\n<li>performance objective<ul>\n<li>$ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)V^{\\mu}(s)ds \\nonumber \\\\$<br>$= \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds \\nonumber \\\\$<br>$= E_{s \\sim \\rho^{\\beta}}[Q^{\\mu}(s,\\mu_{\\theta}(s))]$</li>\n</ul>\n</li>\n<li>off-policy deterministic policy gradient<ul>\n<li>$ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $<ul>\n<li>논문에는 아래와 같이 나와있는데, 물결 표시 부분은 오류로 판단됩니다.</li>\n<li>$ \\begin{eqnarray}<br>  \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) &amp;\\approx&amp; \\int_{S}\\rho^{\\beta}(s)\\nabla_{\\theta}\\mu_{\\theta}(a|s)Q^{\\mu}(s,a)ds \\nonumber \\<br>  &amp;=&amp; E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]<br>  \\end{eqnarray} $</li>\n<li>근거: Action이 deterministic하기에 stochastic 경우와는 다르게 performance objective에서 action에 대해 평균을 구할 필요가 없습니다. 그렇기에, 곱의 미분이 있을 필요가 없고, [Degris, 2012b]에서 처럼 곱의 미분을 통해 생기는 action-value function에 대한 gradient term을 생략할 필요가 사라집니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Remind: 큐러닝(Q-learning) update rule<ul>\n<li>$ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma \\max\\limits_{a}Q(s_{t+1},a) - Q(s_{t},a_{t})) $</li>\n</ul>\n</li>\n<li>algorithm: OPDAC (Off-Policy Deterministic Actor-Critic)<ul>\n<li>살사를 이용한 on-policy deterministic actor-critic과 아래 부분을 제외하고는 같습니다.<ul>\n<li>target policy는 $ \\beta(a|s) $에 의해 생성된 trajectories를 통해 학습합니다.</li>\n<li>업데이트 목표 부분에 실제 행동 값 $ a_{t+1} $이 아니라 정책으로부터 나온 행동 값 $ \\mu_{\\theta}(s_{t+1}) $을 사용합니다.<ul>\n<li>$ \\mu_{\\theta}(s_{t+1}) $ 은 가장 높은 Q 값을 가지는 행동. 즉, Q-learning.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $</li>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $</li>\n</ul>\n</li>\n<li>Stochastic off-policy actor-critic은 대개 actor와 critic 모두 importance sampling을 필요로 하지만, deterministic policy gradient에선 importance sampling이 필요없습니다.<ul>\n<li>Actor 는 deterministic 이기에 sampling 자체가 필요없습니다.<ul>\n<li>Stochastic policy인 경우, Actor에서 importance sampling이 필요한 이유는 상태 $ s $에서의 가치 함수 값 $ V^{\\pi}(s) $을 estimate하기 위해 $ \\pi $가 아니라 $ \\beta $에 따라 sampling을 한 후, 평균을 내기 때문입니다.</li>\n<li>하지만 deterministic policy인 경우, 상태 $ s $에서의 가치 함수 값 $ V^{\\pi}(s) = Q^{\\pi}(s,\\mu_{\\theta}) $ 즉, action이 상태 s에 대해 deterministic이기에 sampling을 통해 estimate할 필요가 없고, 따라서 importance sampling도 필요없어집니다.</li>\n<li>stochastic vs. deterministic performance objective<ul>\n<li>stochastic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads $</li>\n<li>deterministic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Critic이 사용하는 Q-learning은 importance sampling이 필요없는 off policy 알고리즘입니다.<ul>\n<li>Q-learning도 업데이트 목표를 특정 분포에서 샘플링을 통해 estimate 하는 것이 아니라 Q 함수를 최대화하는 action을 선택하는 것이기에 위 actor 에서의 deterministic 경우와 비슷하게 볼 수 있습니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>compatible function approximation 및 gradient temporal-difference learning 을 이용한 actor-critic<ul>\n<li>위 살사/Q-learning 기반 on/off-policy는 아래와 같은 문제가 존재합니다.<ul>\n<li>function approximator에 의한 bias<ul>\n<li>일반적으로 $ Q^{\\mu}(s,a) $ 를 $ Q^{w}(s,a) $로 대체하여 deterministic policy gradient를 구하면, 그 gradient는 ascent하는 방향이 아닐 수도 있습니다.</li>\n</ul>\n</li>\n<li>off-policy learning에 의한 instabilities</li>\n</ul>\n</li>\n<li>그래서 stochastic처럼 $ \\nabla_{a}Q^{\\mu}(s,a) $를 $ \\nabla_{a}Q^{w}(s,a) $로 대체해도 deterministic policy gradient에 영향을 미치지 않을 compatible function approximator $ Q^{w}(s,a) $를 찾아야 합니다.</li>\n<li>Theorem 3. 아래 두 조건을 만족하면, $ Q^{w}(s,a) $는 deterministic policy $ \\mu_{\\theta}(s) $와 compatible 합니다. 즉, $ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $<ul>\n<li>$ \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $입니다.</li>\n<li>$ w $는 $ MSE(\\theta, w) = E[\\epsilon(s;\\theta,w)^{\\top}\\epsilon(s;\\theta,w)] $를 최소화합니다.<ul>\n<li>$ \\epsilon(s;\\theta,w) = \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} - \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}  $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Theorem 3은 on-policy 뿐만 아니라 off-policy에도 적용 가능합니다.</li>\n<li>$ Q^{w}(s,a) = (a-\\mu_{\\theta}(s))^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top} w + V^{v}(s) $<ul>\n<li>어떠한 deterministic policy에 대해서도 위 형태와 같은 compatible function approximator가 존재합니다.</li>\n<li>앞의 term은 advantage를, 뒤의 term은 value로 볼 수 있습니다.</li>\n</ul>\n</li>\n<li>$ Q^{w}(s,a) = \\phi(s,a)^{\\top} w + v^{\\top}\\phi(s) $<ul>\n<li>정의 : $ \\phi(s,a) \\overset{\\underset{\\mathrm{def}}{}}{=} \\nabla_{\\theta}\\mu_{\\theta}(s)(a-\\mu_{\\theta}(s)) $</li>\n<li>일례 : $ V^{v}(s) = v^{\\top}\\phi(s) $</li>\n<li>Theorem 3 만족 여부<ul>\n<li>첫 번째 조건 만족합니다.</li>\n<li>두 번째 조건은 대강 만족합니다.<ul>\n<li>$ \\nabla_{a}Q^{\\mu}(s,a) $에 대한 unbiased sample을 획득하기는 매우 어렵기에, 일반적인 정책 평가 방법들로 $ w $를 학습합니다.</li>\n<li>이 정책 평가 방법들을 이용하면 $ Q^{w}(s,a) \\approx Q^{\\mu}(s,a) $인 reasonable solution을 찾을 수 있기에 대강 $ \\nabla_{a}Q^{w}(s,a) \\approx \\nabla_{a}Q^{\\mu}(s,a) $이 될 것입니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>action-value function에 대한 linear function approximator는 큰 값을 가지는 actions에 대해선 diverge할 수 있어 global하게 action-values 예측하기에는 좋지 않지만, local critic에 사용할 때는 매우 유용합니다.<ul>\n<li>즉, 절대값이 아니라 작은 변화량을 다루는 gradient method 경우엔 $ A^{w}(s,\\mu_{\\theta}(s)+\\delta) = \\delta^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $로, diverge하지 않고, 값을 얻을 수 있습니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>COPDAC-Q algorithm (Compatible Off-Policy Deterministic Actor-Critic Q-learning critic)<ul>\n<li>Critic: 실제 action-value function에 대한 linear function approximator인 $ Q^{w}(s,a) = \\phi(s,a)^{\\top}w $를 estimate합니다.<ul>\n<li>$ \\phi(s,a) = a^{\\top}\\nabla_{\\theta}\\mu_{\\theta} $</li>\n<li>Behavior policy $ \\beta(a|s) $로부터 얻은 samples를 이용하여 Q-learning이나 gradient Q-learning과 같은 off-policy algorithm으로 학습 가능합니다.</li>\n</ul>\n</li>\n<li>Actor: estimated action-value function에 대한 gradient를 $ \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $로 치환 후, 정책을 업데이트합니다.</li>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) $</li>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $</li>\n</ul>\n</li>\n<li>off-policy Q-learning은 linear function approximation을 이용하면 diverge 할 수도 있습니다.<ul>\n<li>$ \\mu_{\\theta}(s_{t+1}) $이 diverge할 수도 있기 때문으로 판단됩니다.</li>\n<li>그렇기에 simple Q-learning 대신 다른 기법이 필요합니다.</li>\n</ul>\n</li>\n<li>그렇기에 critic 에 gradient Q-learning 사용한 COPDAC-GQ (Gradient Q-learning critic) algorithm을 제안합니다.<ul>\n<li>gradient temporal-difference learning에 기반한 기법들은 true gradient descent algorithm이며, converge가 보장됩니다. (Sutton, 2009)<ul>\n<li>기본 아이디어는 stochastic gradient descent로 mean-squared projected Bellman error (MSPBE)를 최소화하는 것입니다.</li>\n<li>critic이 actor보다 빠른 time-scale로 update되도록 step size들을 잘 조절하면, critic은 MSPBE를 최소화하는 parameters로 converge하게 됩니다.</li>\n<li>critic에 gradient temporal-difference learning의 일종인 gradient Q-learning을 사용한 논문입니다. (Maei, 2010)</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>COPDAC-GQ algorithm<ul>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $</li>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) - \\alpha_{w}\\gamma\\phi(s_{t+1}, \\mu_{\\theta}(s_{t+1}))(\\phi(s_{t},a_{t})^{\\top} u_{t}) $</li>\n<li>$ v_{t+1} = v_{t} + \\alpha_{v}\\delta_{t}\\phi(s_{t}) - \\alpha_{v}\\gamma\\phi(s_{t+1})(\\phi(s_{t},a_{t})^{\\top} u_{t}) $</li>\n<li>$ u_{t+1} = u_{t} + \\alpha_{u}(\\delta_{t}-\\phi(s_{t}, a_{t})^{\\top} u_{t})\\phi(s_{t}, a_{t}) $</li>\n</ul>\n</li>\n<li>stochastic actor-critic과 같이 매 time-step 마다 update 시 필요한 계산의 복잡도는 $ O(mn) $입니다.<ul>\n<li>m은 action dimensions, n은 number of policy parameters</li>\n</ul>\n</li>\n<li>Natural policy gradient를 이용해 deterministic policies를 찾을 수 있습니다.<ul>\n<li>$ M(\\theta)^{-1}\\nabla_{\\theta}J(\\mu_{\\theta}) $은 any metric $ M(\\theta) $에 대한 performance objective (식(14))의 steepest ascent direction 입니다. (Toussaint, 2012)</li>\n<li>Natural gradient는 Fisher information metric $ M_{\\pi}(\\theta) $에 대한 steepest ascent direction 입니다.<ul>\n<li>$ M_{\\pi}(\\theta) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)^{\\top}] $</li>\n<li>Fisher information metric은 policy reparameterization에 대해 불변입니다. (Bagnell, 2003)</li>\n</ul>\n</li>\n<li>deterministic policies에 대한 metric으로 $ M_{\\mu}(\\theta) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}] $을 사용합니다.<ul>\n<li>이는 variance가 0인 policy에 대한 Fisher information metric으로 볼 수 있습니다.</li>\n<li>$ \\frac{\\nabla_{\\theta}\\pi_{\\theta}(a\\vert s)}{\\pi_{\\theta}(a\\vert s)}$에서 policy variance가 0이면, 특정 s에 대한 $ \\pi_{\\theta}(a|s)$만 1이 되고, 나머지는 0입니다.</li>\n</ul>\n</li>\n<li>deterministic policy gradient theorem과 compatible function approximation을 결합하면 $ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] $이 됩니다.<ul>\n<li>$ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $</li>\n<li>$ \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)} \\approx \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $</li>\n</ul>\n</li>\n<li>그렇기에 steepest ascent direction은 $ M_{\\mu}(\\theta)^{-1}\\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = w $이 됩니다.<ul>\n<li>$ E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}]^{-1}E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] = w $</li>\n</ul>\n</li>\n<li>이 알고리즘은 COPDAC-Q 혹은 COPDAC-GQ에서 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $ 식을 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta}w_{t} $로 바꿔주기만 하면 됩니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<p><br><br></p>\n<h1 id=\"5-Experiments\"><a href=\"#5-Experiments\" class=\"headerlink\" title=\"5. Experiments\"></a>5. Experiments</h1><p><br></p>\n<h2 id=\"5-1-Continuous-Bandit\"><a href=\"#5-1-Continuous-Bandit\" class=\"headerlink\" title=\"5.1. Continuous Bandit\"></a>5.1. Continuous Bandit</h2><ul>\n<li>Stochastic Actor-Critic (SAC)과 COPDAC 간 성능 비교 수행합니다.<ul>\n<li>Action dimension이 커질수록 성능 차이가 심합니다.</li>\n<li>빠르게 수렴하는 것을 통해 DPG의 data efficiency가 SPG에 비해 좋다는 것을 확인할 수 있지만, 반면, time-step이 증가할수록 SAC와 COPDAC 간 성능 차이가 줄어드는 것을 통해 성능 차이가 심하다는 것은 일정 time step 내에서만 해당하는 것이라고 유추해볼 수 있습니다.</li>\n<li><img src=\"https://www.dropbox.com/s/hrkyq0s2f24z66r/Screenshot%202018-06-16%2017.47.38.png?dl=1\"></li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"5-2-Continuous-Reinforcement-Learning\"><a href=\"#5-2-Continuous-Reinforcement-Learning\" class=\"headerlink\" title=\"5.2. Continuous Reinforcement Learning\"></a>5.2. Continuous Reinforcement Learning</h2><ul>\n<li>COPDAC-Q, SAC, off-policy stochastic actor-critic(OffPAC-TD) 간 성능 비교 수행합니다.<ul>\n<li>COPDAC-Q의 성능이 약간 더 좋습니다.</li>\n<li>COPDAC-Q의 학습이 더 빨리 이뤄집니다.</li>\n<li><img src=\"https://www.dropbox.com/s/qdca4augapmzsxi/Screenshot%202018-06-16%2017.47.07.png?dl=1\"></li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"5-3-Octopus-Arm\"><a href=\"#5-3-Octopus-Arm\" class=\"headerlink\" title=\"5.3. Octopus Arm\"></a>5.3. Octopus Arm</h2><ul>\n<li>목표: 6 segments octopus arm (20 action dimensions &amp; 50 state dimensions)을 control하여 target을 맞추는 것입니다.<ul>\n<li>COPDAC-Q 사용 시, action space dimension이 큰 octopus arm을 잘 control하여 target을 맞춤입니다.</li>\n<li><img src=\"https://www.dropbox.com/s/xrxb0a52wntekld/Screenshot%202018-06-16%2017.46.28.png?dl=1\" width=\"600px\"></li>\n<li>기존 기법들은 action spaces 혹은 action과 state spaces 둘 다 작은 경우들에 대해서만 실험했다고 하며, 비교하고 있지 않습니다.<ul>\n<li>기존 기법들이 6 segments octopus arm에서 동작을 잘 안 했을 것 같긴한데, 그래도 실험해서 보여줬으면 하지만 실험을 하지 않았습니다.</li>\n</ul>\n</li>\n<li>8 segment arm 동영상이 저자 홈페이지에 있다고 하는데, 안 보입니다.</li>\n</ul>\n</li>\n<li>[참고] Octopus Arm 이란?<ul>\n<li><a href=\"https://www.youtube.com/watch?v=AxeeHif0euY\" target=\"_blank\" rel=\"noopener\">OctopusArm Youtube Link</a></li>\n<li><img src=\"https://www.dropbox.com/s/950ycj06sudakjx/Screenshot%202018-06-16%2017.45.52.png?dl=1\"></li>\n</ul>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"처음으로\"><a href=\"#처음으로\" class=\"headerlink\" title=\"처음으로\"></a>처음으로</h1><h2 id=\"PG-Travel-Guide\"><a href=\"#PG-Travel-Guide\" class=\"headerlink\" title=\"PG Travel Guide\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/\">PG Travel Guide</a></h2><p><br></p>\n<h1 id=\"이전으로\"><a href=\"#이전으로\" class=\"headerlink\" title=\"이전으로\"></a>이전으로</h1><h2 id=\"Sutton-PG-여행하기\"><a href=\"#Sutton-PG-여행하기\" class=\"headerlink\" title=\"Sutton PG 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/28/1_sutton-pg/\">Sutton PG 여행하기</a></h2><p><br></p>\n<h1 id=\"다음으로\"><a href=\"#다음으로\" class=\"headerlink\" title=\"다음으로\"></a>다음으로</h1><h2 id=\"DDPG-여행하기\"><a href=\"#DDPG-여행하기\" class=\"headerlink\" title=\"DDPG 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/26/3_ddpg/\">DDPG 여행하기</a></h2>","site":{"data":{}},"excerpt":"","more":"<center> <img src=\"https://www.dropbox.com/s/etwa3hn5c0pegdv/Screen%20Shot%202018-07-18%20at%2012.51.35%20AM.png?dl=1\" width=\"800\"> </center>\n\n<p>논문 저자 : David Silver, Guy Lever, Nicloas Heess, Thomas Degris, Daan Wierstra, Martin Riedmiller<br>논문 링크 : <a href=\"http://proceedings.mlr.press/v32/silver14.pdf\" target=\"_blank\" rel=\"noopener\">main text</a>, <a href=\"http://proceedings.mlr.press/v32/silver14-supp.pdf\" target=\"_blank\" rel=\"noopener\">supplementary material</a><br>Proceeding : International Conference on Machine Learning (ICML) 2014<br>정리 : 김동민, 공민서, 장수영, 차금강</p>\n<hr>\n<h1 id=\"1-들어가며…\"><a href=\"#1-들어가며…\" class=\"headerlink\" title=\"1. 들어가며…\"></a>1. 들어가며…</h1><ul>\n<li>Deterministic Policy Gradient (DPG) Theorem을 제안합니다.<ul>\n<li>중요한 점은 DPG는 Expected gradient of the action-value function의 형태라는 것입니다.</li>\n</ul>\n</li>\n<li>Policy variance가 0에 수렴할 경우, DPG는 Stochastic Policy Gradient (SPG)와 동일해집니다. (Theorem 2)<ul>\n<li>Theorem 2로 인해 기존 Policy Gradient (PG) 와 관련된 기법들을 DPG에 적용할 수 있게 됩니다.<ul>\n<li>예. Sutton PG, natural gradients, actor-critic, episodic/batch methods</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>적절한 exploration 을 위해 model-free, off-policy actor-critic algorithm 을 제안합니다<ul>\n<li>action-value function approximator 사용으로 인해 policy gradient가 bias되는 것을 방지하기 위해 compatibility condition을 제공합니다. (Theorem 3)</li>\n</ul>\n</li>\n<li>DPG 는 SPG 보다 성능이 좋습니다.<ul>\n<li>특히 high dimensional action spaces를 가지는 tasks에서의 성능 향상이 큽니다.<ul>\n<li>SPG의 policy gradient는 state와 action spaces 모두에 대해서, DPG의 policy gradient는 state spaces에 대해서만 평균을 취합니다.</li>\n<li>결과적으로, action spaces의 dimension이 커질수록 data efficiency가 높은 DPG의 학습이 더 잘 이뤄지게 됩니다.</li>\n<li>무한정 학습을 시키면, SPG도 최적으로 수렴할 것으로 예상되기에 위 성능 비교는 일정 iteration 내로 한정합니다.</li>\n</ul>\n</li>\n<li>기존 기법들에 비해 computation 양이 많지 않습니다.<ul>\n<li>Computation 은 action dimensionality 와 policy parameters 수에 비례합니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"2-Background\"><a href=\"#2-Background\" class=\"headerlink\" title=\"2. Background\"></a>2. Background</h1><p><br></p>\n<h2 id=\"2-1-Performance-objective-function\"><a href=\"#2-1-Performance-objective-function\" class=\"headerlink\" title=\"2.1 Performance objective function\"></a>2.1 Performance objective function</h2><p>$$<br>\\begin{align}<br>J(\\pi_{\\theta}) &amp;= \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\pi_{\\theta}(s,a)r(s,a)da ds = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[r(s,a)]<br>\\end{align}<br>$$</p>\n<p><br></p>\n<h2 id=\"2-2-SPG-Theorem\"><a href=\"#2-2-SPG-Theorem\" class=\"headerlink\" title=\"2.2 SPG Theorem\"></a>2.2 SPG Theorem</h2><ul>\n<li>State distribution $ \\rho^{\\pi}(s) $ 은 policy parameters에 영향을 받지만, policy gradient 를 계산할 때는 state distribution 의 gradient 를 고려할 필요가 없습니다.<br>$$\\begin{eqnarray}\\nabla_{\\theta}J(\\pi_{\\theta}) &amp;=&amp; \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\ &amp;=&amp; E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]<br>\\end{eqnarray}$$</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-3-Stochastic-Actor-Critic-Algorithms\"><a href=\"#2-3-Stochastic-Actor-Critic-Algorithms\" class=\"headerlink\" title=\"2.3 Stochastic Actor-Critic Algorithms\"></a>2.3 Stochastic Actor-Critic Algorithms</h2><ul>\n<li>Actor와 Critic이 번갈아가면서 동작하며 stochastic policy를 최적화하는 기법입니다.</li>\n<li>Actor: $ Q^{\\pi}(s,a) $ 를 근사한 $ Q^w(s,a) $를 이용해 stochastic policy gradient를 ascent하는 방향으로 policy parameter $ \\theta $를 업데이트함으로써 stochastic policy를 발전시킵니다.<ul>\n<li>$ \\nabla_{\\theta}J(\\pi_{\\theta}) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q^{w}(s,a)] $</li>\n</ul>\n</li>\n<li>Critic: SARSA나 Q-learning같은 Temporal-difference (TD) learning을 이용해 action-value function의 parameter, $ w $를 업데이트함으로써 $ Q^w(s,a) $가 $ Q^{\\pi}(s,a) $과 유사해지도록 합니다.</li>\n<li>실제 값인 $ Q^{\\pi}(s,a) $ 대신 이를 근사한 $ Q^w(s,a) $를 사용하게 되면, 일반적으로 bias가 발생합니다. 하지만, compatible condition에 부합하는 $ Q^w(s,a) $를 사용하게 되면, bias가 발생하지 않습니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-4-Off-policy-Actor-Critic\"><a href=\"#2-4-Off-policy-Actor-Critic\" class=\"headerlink\" title=\"2.4 Off-policy Actor-Critic\"></a>2.4 Off-policy Actor-Critic</h2><ul>\n<li>Distinct behavior policy $ \\beta(a|s) ( \\neq \\pi_{\\theta}(a|s) ) $ 로부터 샘플링된 trajectories 를 이용한 Actor-Critic</li>\n<li>Performance objective function<ul>\n<li>$\\begin{eqnarray}<br>  J_{\\beta}(\\pi_{\\theta})<br>  &amp;=&amp; \\int_{S}\\rho^{\\beta}(s)V^{\\pi}(s)ds \\nonumber \\\\<br>  &amp;=&amp; \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads<br>  \\end{eqnarray} $</li>\n</ul>\n</li>\n<li>off-policy policy gradient<ul>\n<li>$ \\begin{eqnarray}<br>  \\nabla_{\\theta}J_{\\beta}(\\pi_{\\theta}) &amp;\\approx&amp; \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\end{eqnarray} $<br>  $=E_{s \\sim \\rho^{\\beta}, a \\sim \\beta}[\\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)}\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]$</li>\n<li>off-policy policy gradient 식에서의 물결 표시는 <a href=\"https://arxiv.org/abs/1205.4839\" target=\"_blank\" rel=\"noopener\">Degris, 2012b</a> 논문에 근거합니다.<ul>\n<li>Exact off-policy policy gradient 와 이를 approximate 한 policy gradient 는 아래와 같습니다. (빨간색 상자에 있는 항목을 삭제함으로써 근사합니다.)<ul>\n<li><img src=\"https://www.dropbox.com/s/xzpv3okc139c1fs/Screenshot%202018-06-16%2017.48.51.png?dl=1\" width=\"500px\"></li>\n</ul>\n</li>\n<li>[Degris, 2012b] Theorem 1 에 의해 policy parameter 가 approximated policy gradient ( $\\nabla_{u}𝑄^{\\pi,\\gamma}(𝑠,𝑎)$ term 제거)에 따라 업데이트되어도 policy 는 improve 가 됨이 보장되기에 exact off-policy policy gradient 대신 approximated off-policy policy gradient 를 사용해도 괜찮습니다.<ul>\n<li><img src=\"https://www.dropbox.com/s/mk13931r4scjngo/Screenshot%202018-06-16%2017.49.24.png?dl=1\" width=\"500px\"></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>off-policy policy gradient 식에서 $ \\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)} $는 importance sampling ratio 입니다.<ul>\n<li>off-policy actor-critic에서는 $ \\beta $에 의해 샘플링된 trajectory를 이용해서 stochastic policy $ \\pi $를 예측하는 것이기 때문에 importance sampling이 필요합니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"3-Gradient-of-Deterministic-Policies\"><a href=\"#3-Gradient-of-Deterministic-Policies\" class=\"headerlink\" title=\"3. Gradient of Deterministic Policies\"></a>3. Gradient of Deterministic Policies</h1><p><br></p>\n<h2 id=\"3-1-Regularity-Conditions\"><a href=\"#3-1-Regularity-Conditions\" class=\"headerlink\" title=\"3.1 Regularity Conditions\"></a>3.1 Regularity Conditions</h2><ul>\n<li>어떠한 이론이 성립하기 위한 전제 조건</li>\n<li>Regularity conditions A.1<ul>\n<li>$ p(s’|s,a), \\nabla_{a}p(s’|s,a), \\mu_{\\theta}(s), \\nabla_{\\theta}\\mu_{\\theta}(s), r(s,a), \\nabla_{a}r(s,a), p_{1}(s) $ are continuous in all parameters and variables $ s, a, s’ $ and $ x $.</li>\n</ul>\n</li>\n<li>regularity conditions A.2<ul>\n<li>There exists a $ b $ and $ L $ such that $ \\sup_{s}p_{1}(s) &lt; b $, $ \\sup_{a,s,s’}p(s’|s,a) &lt; b $, $ \\sup_{a,s}r(s,a) &lt; b $, $ \\sup_{a,s,s’}|\\nabla_{a}p(s’|s,a)| &lt; L $, and $ \\sup_{a,s}|\\nabla_{a}r(s,a)| &lt; L $.</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-2-Deterministic-Policy-Gradient-Theorem\"><a href=\"#3-2-Deterministic-Policy-Gradient-Theorem\" class=\"headerlink\" title=\"3.2 Deterministic Policy Gradient Theorem\"></a>3.2 Deterministic Policy Gradient Theorem</h2><ul>\n<li>Deterministic policy<ul>\n<li>$ \\mu_{\\theta} : S \\to A $ with parameter vector $ \\theta \\in \\mathbb{R}^n $</li>\n</ul>\n</li>\n<li>Probability distribution<ul>\n<li>$ p(s \\to s’, t, \\mu) $</li>\n</ul>\n</li>\n<li>Discounted state distribution<ul>\n<li>$ \\rho^{\\mu}(s) $</li>\n</ul>\n</li>\n<li>Performance objective</li>\n</ul>\n<p>$$<br>J(\\mu_{\\theta}) = E[r^{\\gamma}_{1} | \\mu]<br>$$</p>\n<p>$$<br>= \\int_{S}\\rho^{\\mu}(s)r(s,\\mu_{\\theta}(s))ds<br>= E_{s \\sim \\rho^{\\mu}}[r(s,\\mu_{\\theta}(s))]<br>$$</p>\n<ul>\n<li><p>DPG Theorem</p>\n<ul>\n<li><p>MDP 가 A.1 만족한다면, 아래 식이 성립합니다.<br>$\\nabla_{\\theta}J(\\mu_{\\theta}) = \\int_{S}\\rho^{\\mu}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}ds \\nonumber$<br>$= E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]   \\nonumber $ </p>\n</li>\n<li><p>DPG는 state space에 대해서만 평균을 취하면 되기에, state와 action space 모두에 대해 평균을 취해야 하는 SPG에 비해 data efficiency가 좋습니다. 즉, 더 적은 양의 데이터로도 학습이 잘 이뤄지게 됩니다.</p>\n</li>\n</ul>\n</li>\n</ul>\n<p><br>    </p>\n<h2 id=\"3-3-DPG-형태에-대한-informal-intuition\"><a href=\"#3-3-DPG-형태에-대한-informal-intuition\" class=\"headerlink\" title=\"3.3 DPG 형태에 대한 informal intuition\"></a>3.3 DPG 형태에 대한 informal intuition</h2><ul>\n<li>Generalized policy iteration<ul>\n<li>정책 평가와 정책 발전을 한 번 씩 번갈아 가면서 실행하는 정책 iteration<ul>\n<li>위와 같이 해도 정책 평가에서 예측한 가치함수가 최적 가치함수에 수렴합니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>정책 평가<ul>\n<li>action-value function $ Q^{\\pi}(s,a) $ or $ Q^{\\mu}(s,a) $을 estimate 하는 것 입니다.</li>\n</ul>\n</li>\n<li>정책 발전<ul>\n<li>위 estimated action-value function에 따라 정책을 update하는 것 입니다.</li>\n<li>주로 action-value function에 대한 greedy maximisation을 사용합니다.<ul>\n<li>$ \\mu^{k+1}(s) = \\arg\\max\\limits_{a}Q^{\\mu^{k}}(s,a) $</li>\n<li>greedy 정책 발전은 매 단계마다 global maximization을 해야하는데, 이로 인해 continuous action spaces에서 계산량이 급격히 늘어납니다.</li>\n</ul>\n</li>\n<li>그래서 policy gradient 방법이 나옵니다.<ul>\n<li>policy 를 $ \\theta $에 대해서 parameterize 합니다.</li>\n<li>매 단계마다 global maximisation 수행하는 대신, 방문하는 state $ s $마다 policy parameter를 action-value function $ Q $의 $ \\theta $에 대한 gradient $ \\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s)) $ 방향으로 proportional하게 update 합니다.</li>\n<li>하지만 각 state는 다른 방향을 제시할 수 있기에, state distribution $ \\rho^{\\mu}(s) $에 대한 기대값을 취해 policy parameter를 update 할 수도 있습니다.<ul>\n<li>$ \\theta^{k+1} = \\theta^{k} + \\alpha E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s))] $</li>\n</ul>\n</li>\n<li>이는 chain-rule에 따라 아래와 같이 분리될 수 있습니다.<ul>\n<li>$ \\theta^{k+1} = \\theta^{k} + \\alpha E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu^{k}}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $ (7)</li>\n<li>chain rule: $ \\frac{\\partial Q}{\\partial \\theta} = \\frac{\\partial a}{\\partial \\theta} \\frac{\\partial Q}{\\partial a} $</li>\n</ul>\n</li>\n<li>하지만 state distribution $ \\rho^{\\mu} $은 정책에 dependent 합니다.<ul>\n<li>정책이 바꾸게 되면, 바뀐 정책에 따라 방문하게 되는 state가 변하기 때문에 state distribution이 변하게 됩니다.</li>\n</ul>\n</li>\n<li>그렇기에 정책 update 시 state distribution에 대한 gradient를 고려하지 않는데 정책 발전이 이뤄진다는 것은 직관적으로 와닿지 않을 수 있습니다.</li>\n<li>deterministic policy gradient theorem은 state distribution에 대한 gradient 계산없이 위 식(7) 대로만 update해도 performance objective의 gradient를 정확하게 따름을 의미합니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-4-DPG는-SPG의-limiting-case\"><a href=\"#3-4-DPG는-SPG의-limiting-case\" class=\"headerlink\" title=\"3.4 DPG는 SPG의 limiting case\"></a>3.4 DPG는 SPG의 limiting case</h2><ul>\n<li>stochastic policy parameterization<ul>\n<li>$ \\pi_{\\mu_{\\theta},\\sigma} $ by a deterministic policy $ \\mu_{\\theta} : S \\to A $ and a variance parameter $ \\sigma $</li>\n<li>$ \\sigma = 0 $ 이면, $ \\pi_{\\mu_{\\theta},\\sigma} \\equiv \\mu_{\\theta} $</li>\n</ul>\n</li>\n<li>Theorem 2. Policy의 variance가 0에 수렴하면, 즉, $ \\sigma \\to 0 $, stochastic policy gradient와 deterministic policy gradient는 동일해집니다.<ul>\n<li>조건: stochastic policy $ \\pi_{\\mu_{\\theta},\\sigma} = \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $<ul>\n<li>$ \\sigma $는 variance입니다.</li>\n<li>$ \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $는 condition B.1을 만족합니다.</li>\n<li>MDP는 conditions A.1과 A.2를 만족합니다.</li>\n</ul>\n</li>\n<li>결과:<ul>\n<li>$ \\lim\\limits_{\\sigma\\downarrow0}\\nabla_{\\theta}J(\\pi_{\\mu_{\\theta},\\sigma}) = \\nabla_{\\theta}J(\\mu_{\\theta})  $<ul>\n<li>좌변은 standard stochastic gradient이며, 우변은 deterministic gradient입니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>의미:<ul>\n<li>deterministic policy gradient는 stochastic policy gradient의 특수 case 입니다.</li>\n<li>기존 유명한 policy gradients 기법들에 deterministic policy gradients 를 적용할 수 있습니다.<ul>\n<li>기존 기법들 예: compatible function approximation (Sutton, 1999), natural gradients (Kakade, 2001), actor-critic (Bhatnagar, 2007) or episodic/batch methods (Peters, 2005)</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"4-Deterministic-Actor-Critic-Algorithms\"><a href=\"#4-Deterministic-Actor-Critic-Algorithms\" class=\"headerlink\" title=\"4. Deterministic Actor-Critic Algorithms\"></a>4. Deterministic Actor-Critic Algorithms</h1><ol>\n<li>SARSA critic를 이용한 on-policy actor-critic<ul>\n<li>단점<ul>\n<li>deterministic policy에 의해 행동하면 exploration이 잘 되지 않기에, sub-optimal에 빠지기 쉽습니다.</li>\n</ul>\n</li>\n<li>목적<ul>\n<li>교훈/정보제공</li>\n<li>환경에서 충분한 noise를 제공하여 exploration을 시킬 수 있다면, deterministic policy를 사용한다고 하여도 좋은 학습 결과를 얻을 수도 있습니다.<ul>\n<li>예. 바람이 agent의 행동에 영향(noise)을 줌</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Remind: 살사(SARSA) update rule<ul>\n<li>$ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma Q(s_{t+1},a_{t+1}) - Q(s_{t},a_{t})) $</li>\n</ul>\n</li>\n<li>Algorithm<ul>\n<li>Critic은 MSE를 $ \\bf minimize $하는 방향, 즉, action-value function을 stochastic gradient $ \\bf descent $ 방법으로 update합니다.<ul>\n<li>$ MSE = [Q^{\\mu}(s,a) - Q^{w}(s,a)]^2 $<ul>\n<li>critic은 실제 $ Q^{\\mu}(s,a) $ 대신 미분 가능한 $ Q^{w}(s,a) $로 대체하여 action-value function을 estimate하며, 이 둘 간 mean square error를 최소화하는 것이 목표입니다.</li>\n</ul>\n</li>\n<li>$ \\nabla_{w}MSE \\approx -2 * [r + \\gamma Q^{w}(s’,a’) - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $<ul>\n<li>$ \\nabla_{w}MSE = -2 * [Q^{\\mu}(s,a) - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $</li>\n<li>$ Q^{\\mu}(s,a) $ 를 $ r + \\gamma Q^{w}(s’,a’) $로 대체<ul>\n<li>$ Q^{\\mu}(s,a) = r + \\gamma Q^{\\mu}(s’,a’) $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $<ul>\n<li>$w_{t+1} = w_{t} - \\alpha_{w}\\nabla_{w}MSE  \\nonumber$<br>$ \\approx w_{t} - \\alpha_{w} <em> (-2 </em> [r + \\gamma Q^{w}(s’,a’) - Q^{w}(s,a)] \\nabla_{w}Q^{w}(s,a)$</li>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},a_{t+1}) - Q^{w}(s_{t},a_{t}) $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Actor는 식(9)에 따라 보상이 $ \\bf maximize $되는 방향, 즉, deterministic policy를 stochastic gradient $ \\bf ascent $ 방법으로 update합니다.<ul>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Q-learning 을 이용한 off-policy actor-critic<ul>\n<li>stochastic behavior policy $ \\beta(a|s) $에 의해 생성된 trajectories로부터 deterministic target policy $ \\mu_{\\theta}(s) $를 학습하는 off-policy actor-critic입니다</li>\n<li>performance objective<ul>\n<li>$ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)V^{\\mu}(s)ds \\nonumber \\\\$<br>$= \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds \\nonumber \\\\$<br>$= E_{s \\sim \\rho^{\\beta}}[Q^{\\mu}(s,\\mu_{\\theta}(s))]$</li>\n</ul>\n</li>\n<li>off-policy deterministic policy gradient<ul>\n<li>$ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $<ul>\n<li>논문에는 아래와 같이 나와있는데, 물결 표시 부분은 오류로 판단됩니다.</li>\n<li>$ \\begin{eqnarray}<br>  \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) &amp;\\approx&amp; \\int_{S}\\rho^{\\beta}(s)\\nabla_{\\theta}\\mu_{\\theta}(a|s)Q^{\\mu}(s,a)ds \\nonumber \\<br>  &amp;=&amp; E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]<br>  \\end{eqnarray} $</li>\n<li>근거: Action이 deterministic하기에 stochastic 경우와는 다르게 performance objective에서 action에 대해 평균을 구할 필요가 없습니다. 그렇기에, 곱의 미분이 있을 필요가 없고, [Degris, 2012b]에서 처럼 곱의 미분을 통해 생기는 action-value function에 대한 gradient term을 생략할 필요가 사라집니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Remind: 큐러닝(Q-learning) update rule<ul>\n<li>$ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma \\max\\limits_{a}Q(s_{t+1},a) - Q(s_{t},a_{t})) $</li>\n</ul>\n</li>\n<li>algorithm: OPDAC (Off-Policy Deterministic Actor-Critic)<ul>\n<li>살사를 이용한 on-policy deterministic actor-critic과 아래 부분을 제외하고는 같습니다.<ul>\n<li>target policy는 $ \\beta(a|s) $에 의해 생성된 trajectories를 통해 학습합니다.</li>\n<li>업데이트 목표 부분에 실제 행동 값 $ a_{t+1} $이 아니라 정책으로부터 나온 행동 값 $ \\mu_{\\theta}(s_{t+1}) $을 사용합니다.<ul>\n<li>$ \\mu_{\\theta}(s_{t+1}) $ 은 가장 높은 Q 값을 가지는 행동. 즉, Q-learning.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $</li>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $</li>\n</ul>\n</li>\n<li>Stochastic off-policy actor-critic은 대개 actor와 critic 모두 importance sampling을 필요로 하지만, deterministic policy gradient에선 importance sampling이 필요없습니다.<ul>\n<li>Actor 는 deterministic 이기에 sampling 자체가 필요없습니다.<ul>\n<li>Stochastic policy인 경우, Actor에서 importance sampling이 필요한 이유는 상태 $ s $에서의 가치 함수 값 $ V^{\\pi}(s) $을 estimate하기 위해 $ \\pi $가 아니라 $ \\beta $에 따라 sampling을 한 후, 평균을 내기 때문입니다.</li>\n<li>하지만 deterministic policy인 경우, 상태 $ s $에서의 가치 함수 값 $ V^{\\pi}(s) = Q^{\\pi}(s,\\mu_{\\theta}) $ 즉, action이 상태 s에 대해 deterministic이기에 sampling을 통해 estimate할 필요가 없고, 따라서 importance sampling도 필요없어집니다.</li>\n<li>stochastic vs. deterministic performance objective<ul>\n<li>stochastic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads $</li>\n<li>deterministic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Critic이 사용하는 Q-learning은 importance sampling이 필요없는 off policy 알고리즘입니다.<ul>\n<li>Q-learning도 업데이트 목표를 특정 분포에서 샘플링을 통해 estimate 하는 것이 아니라 Q 함수를 최대화하는 action을 선택하는 것이기에 위 actor 에서의 deterministic 경우와 비슷하게 볼 수 있습니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>compatible function approximation 및 gradient temporal-difference learning 을 이용한 actor-critic<ul>\n<li>위 살사/Q-learning 기반 on/off-policy는 아래와 같은 문제가 존재합니다.<ul>\n<li>function approximator에 의한 bias<ul>\n<li>일반적으로 $ Q^{\\mu}(s,a) $ 를 $ Q^{w}(s,a) $로 대체하여 deterministic policy gradient를 구하면, 그 gradient는 ascent하는 방향이 아닐 수도 있습니다.</li>\n</ul>\n</li>\n<li>off-policy learning에 의한 instabilities</li>\n</ul>\n</li>\n<li>그래서 stochastic처럼 $ \\nabla_{a}Q^{\\mu}(s,a) $를 $ \\nabla_{a}Q^{w}(s,a) $로 대체해도 deterministic policy gradient에 영향을 미치지 않을 compatible function approximator $ Q^{w}(s,a) $를 찾아야 합니다.</li>\n<li>Theorem 3. 아래 두 조건을 만족하면, $ Q^{w}(s,a) $는 deterministic policy $ \\mu_{\\theta}(s) $와 compatible 합니다. 즉, $ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $<ul>\n<li>$ \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $입니다.</li>\n<li>$ w $는 $ MSE(\\theta, w) = E[\\epsilon(s;\\theta,w)^{\\top}\\epsilon(s;\\theta,w)] $를 최소화합니다.<ul>\n<li>$ \\epsilon(s;\\theta,w) = \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} - \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}  $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Theorem 3은 on-policy 뿐만 아니라 off-policy에도 적용 가능합니다.</li>\n<li>$ Q^{w}(s,a) = (a-\\mu_{\\theta}(s))^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top} w + V^{v}(s) $<ul>\n<li>어떠한 deterministic policy에 대해서도 위 형태와 같은 compatible function approximator가 존재합니다.</li>\n<li>앞의 term은 advantage를, 뒤의 term은 value로 볼 수 있습니다.</li>\n</ul>\n</li>\n<li>$ Q^{w}(s,a) = \\phi(s,a)^{\\top} w + v^{\\top}\\phi(s) $<ul>\n<li>정의 : $ \\phi(s,a) \\overset{\\underset{\\mathrm{def}}{}}{=} \\nabla_{\\theta}\\mu_{\\theta}(s)(a-\\mu_{\\theta}(s)) $</li>\n<li>일례 : $ V^{v}(s) = v^{\\top}\\phi(s) $</li>\n<li>Theorem 3 만족 여부<ul>\n<li>첫 번째 조건 만족합니다.</li>\n<li>두 번째 조건은 대강 만족합니다.<ul>\n<li>$ \\nabla_{a}Q^{\\mu}(s,a) $에 대한 unbiased sample을 획득하기는 매우 어렵기에, 일반적인 정책 평가 방법들로 $ w $를 학습합니다.</li>\n<li>이 정책 평가 방법들을 이용하면 $ Q^{w}(s,a) \\approx Q^{\\mu}(s,a) $인 reasonable solution을 찾을 수 있기에 대강 $ \\nabla_{a}Q^{w}(s,a) \\approx \\nabla_{a}Q^{\\mu}(s,a) $이 될 것입니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>action-value function에 대한 linear function approximator는 큰 값을 가지는 actions에 대해선 diverge할 수 있어 global하게 action-values 예측하기에는 좋지 않지만, local critic에 사용할 때는 매우 유용합니다.<ul>\n<li>즉, 절대값이 아니라 작은 변화량을 다루는 gradient method 경우엔 $ A^{w}(s,\\mu_{\\theta}(s)+\\delta) = \\delta^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $로, diverge하지 않고, 값을 얻을 수 있습니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>COPDAC-Q algorithm (Compatible Off-Policy Deterministic Actor-Critic Q-learning critic)<ul>\n<li>Critic: 실제 action-value function에 대한 linear function approximator인 $ Q^{w}(s,a) = \\phi(s,a)^{\\top}w $를 estimate합니다.<ul>\n<li>$ \\phi(s,a) = a^{\\top}\\nabla_{\\theta}\\mu_{\\theta} $</li>\n<li>Behavior policy $ \\beta(a|s) $로부터 얻은 samples를 이용하여 Q-learning이나 gradient Q-learning과 같은 off-policy algorithm으로 학습 가능합니다.</li>\n</ul>\n</li>\n<li>Actor: estimated action-value function에 대한 gradient를 $ \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $로 치환 후, 정책을 업데이트합니다.</li>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) $</li>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $</li>\n</ul>\n</li>\n<li>off-policy Q-learning은 linear function approximation을 이용하면 diverge 할 수도 있습니다.<ul>\n<li>$ \\mu_{\\theta}(s_{t+1}) $이 diverge할 수도 있기 때문으로 판단됩니다.</li>\n<li>그렇기에 simple Q-learning 대신 다른 기법이 필요합니다.</li>\n</ul>\n</li>\n<li>그렇기에 critic 에 gradient Q-learning 사용한 COPDAC-GQ (Gradient Q-learning critic) algorithm을 제안합니다.<ul>\n<li>gradient temporal-difference learning에 기반한 기법들은 true gradient descent algorithm이며, converge가 보장됩니다. (Sutton, 2009)<ul>\n<li>기본 아이디어는 stochastic gradient descent로 mean-squared projected Bellman error (MSPBE)를 최소화하는 것입니다.</li>\n<li>critic이 actor보다 빠른 time-scale로 update되도록 step size들을 잘 조절하면, critic은 MSPBE를 최소화하는 parameters로 converge하게 됩니다.</li>\n<li>critic에 gradient temporal-difference learning의 일종인 gradient Q-learning을 사용한 논문입니다. (Maei, 2010)</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>COPDAC-GQ algorithm<ul>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $</li>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) - \\alpha_{w}\\gamma\\phi(s_{t+1}, \\mu_{\\theta}(s_{t+1}))(\\phi(s_{t},a_{t})^{\\top} u_{t}) $</li>\n<li>$ v_{t+1} = v_{t} + \\alpha_{v}\\delta_{t}\\phi(s_{t}) - \\alpha_{v}\\gamma\\phi(s_{t+1})(\\phi(s_{t},a_{t})^{\\top} u_{t}) $</li>\n<li>$ u_{t+1} = u_{t} + \\alpha_{u}(\\delta_{t}-\\phi(s_{t}, a_{t})^{\\top} u_{t})\\phi(s_{t}, a_{t}) $</li>\n</ul>\n</li>\n<li>stochastic actor-critic과 같이 매 time-step 마다 update 시 필요한 계산의 복잡도는 $ O(mn) $입니다.<ul>\n<li>m은 action dimensions, n은 number of policy parameters</li>\n</ul>\n</li>\n<li>Natural policy gradient를 이용해 deterministic policies를 찾을 수 있습니다.<ul>\n<li>$ M(\\theta)^{-1}\\nabla_{\\theta}J(\\mu_{\\theta}) $은 any metric $ M(\\theta) $에 대한 performance objective (식(14))의 steepest ascent direction 입니다. (Toussaint, 2012)</li>\n<li>Natural gradient는 Fisher information metric $ M_{\\pi}(\\theta) $에 대한 steepest ascent direction 입니다.<ul>\n<li>$ M_{\\pi}(\\theta) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)^{\\top}] $</li>\n<li>Fisher information metric은 policy reparameterization에 대해 불변입니다. (Bagnell, 2003)</li>\n</ul>\n</li>\n<li>deterministic policies에 대한 metric으로 $ M_{\\mu}(\\theta) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}] $을 사용합니다.<ul>\n<li>이는 variance가 0인 policy에 대한 Fisher information metric으로 볼 수 있습니다.</li>\n<li>$ \\frac{\\nabla_{\\theta}\\pi_{\\theta}(a\\vert s)}{\\pi_{\\theta}(a\\vert s)}$에서 policy variance가 0이면, 특정 s에 대한 $ \\pi_{\\theta}(a|s)$만 1이 되고, 나머지는 0입니다.</li>\n</ul>\n</li>\n<li>deterministic policy gradient theorem과 compatible function approximation을 결합하면 $ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] $이 됩니다.<ul>\n<li>$ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $</li>\n<li>$ \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)} \\approx \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $</li>\n</ul>\n</li>\n<li>그렇기에 steepest ascent direction은 $ M_{\\mu}(\\theta)^{-1}\\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = w $이 됩니다.<ul>\n<li>$ E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}]^{-1}E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] = w $</li>\n</ul>\n</li>\n<li>이 알고리즘은 COPDAC-Q 혹은 COPDAC-GQ에서 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $ 식을 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta}w_{t} $로 바꿔주기만 하면 됩니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<p><br><br></p>\n<h1 id=\"5-Experiments\"><a href=\"#5-Experiments\" class=\"headerlink\" title=\"5. Experiments\"></a>5. Experiments</h1><p><br></p>\n<h2 id=\"5-1-Continuous-Bandit\"><a href=\"#5-1-Continuous-Bandit\" class=\"headerlink\" title=\"5.1. Continuous Bandit\"></a>5.1. Continuous Bandit</h2><ul>\n<li>Stochastic Actor-Critic (SAC)과 COPDAC 간 성능 비교 수행합니다.<ul>\n<li>Action dimension이 커질수록 성능 차이가 심합니다.</li>\n<li>빠르게 수렴하는 것을 통해 DPG의 data efficiency가 SPG에 비해 좋다는 것을 확인할 수 있지만, 반면, time-step이 증가할수록 SAC와 COPDAC 간 성능 차이가 줄어드는 것을 통해 성능 차이가 심하다는 것은 일정 time step 내에서만 해당하는 것이라고 유추해볼 수 있습니다.</li>\n<li><img src=\"https://www.dropbox.com/s/hrkyq0s2f24z66r/Screenshot%202018-06-16%2017.47.38.png?dl=1\"></li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"5-2-Continuous-Reinforcement-Learning\"><a href=\"#5-2-Continuous-Reinforcement-Learning\" class=\"headerlink\" title=\"5.2. Continuous Reinforcement Learning\"></a>5.2. Continuous Reinforcement Learning</h2><ul>\n<li>COPDAC-Q, SAC, off-policy stochastic actor-critic(OffPAC-TD) 간 성능 비교 수행합니다.<ul>\n<li>COPDAC-Q의 성능이 약간 더 좋습니다.</li>\n<li>COPDAC-Q의 학습이 더 빨리 이뤄집니다.</li>\n<li><img src=\"https://www.dropbox.com/s/qdca4augapmzsxi/Screenshot%202018-06-16%2017.47.07.png?dl=1\"></li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"5-3-Octopus-Arm\"><a href=\"#5-3-Octopus-Arm\" class=\"headerlink\" title=\"5.3. Octopus Arm\"></a>5.3. Octopus Arm</h2><ul>\n<li>목표: 6 segments octopus arm (20 action dimensions &amp; 50 state dimensions)을 control하여 target을 맞추는 것입니다.<ul>\n<li>COPDAC-Q 사용 시, action space dimension이 큰 octopus arm을 잘 control하여 target을 맞춤입니다.</li>\n<li><img src=\"https://www.dropbox.com/s/xrxb0a52wntekld/Screenshot%202018-06-16%2017.46.28.png?dl=1\" width=\"600px\"></li>\n<li>기존 기법들은 action spaces 혹은 action과 state spaces 둘 다 작은 경우들에 대해서만 실험했다고 하며, 비교하고 있지 않습니다.<ul>\n<li>기존 기법들이 6 segments octopus arm에서 동작을 잘 안 했을 것 같긴한데, 그래도 실험해서 보여줬으면 하지만 실험을 하지 않았습니다.</li>\n</ul>\n</li>\n<li>8 segment arm 동영상이 저자 홈페이지에 있다고 하는데, 안 보입니다.</li>\n</ul>\n</li>\n<li>[참고] Octopus Arm 이란?<ul>\n<li><a href=\"https://www.youtube.com/watch?v=AxeeHif0euY\" target=\"_blank\" rel=\"noopener\">OctopusArm Youtube Link</a></li>\n<li><img src=\"https://www.dropbox.com/s/950ycj06sudakjx/Screenshot%202018-06-16%2017.45.52.png?dl=1\"></li>\n</ul>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"처음으로\"><a href=\"#처음으로\" class=\"headerlink\" title=\"처음으로\"></a>처음으로</h1><h2 id=\"PG-Travel-Guide\"><a href=\"#PG-Travel-Guide\" class=\"headerlink\" title=\"PG Travel Guide\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/\">PG Travel Guide</a></h2><p><br></p>\n<h1 id=\"이전으로\"><a href=\"#이전으로\" class=\"headerlink\" title=\"이전으로\"></a>이전으로</h1><h2 id=\"Sutton-PG-여행하기\"><a href=\"#Sutton-PG-여행하기\" class=\"headerlink\" title=\"Sutton PG 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/28/1_sutton-pg/\">Sutton PG 여행하기</a></h2><p><br></p>\n<h1 id=\"다음으로\"><a href=\"#다음으로\" class=\"headerlink\" title=\"다음으로\"></a>다음으로</h1><h2 id=\"DDPG-여행하기\"><a href=\"#DDPG-여행하기\" class=\"headerlink\" title=\"DDPG 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/26/3_ddpg/\">DDPG 여행하기</a></h2>"},{"title":"Policy Gradient Methods for Reinforcement Learning with Function Approximation","date":"2018-06-28T05:18:32.000Z","author":"김동민, 이동민","subtitle":"피지여행 1번째 논문","_content":"\n<center> <img src=\"https://www.dropbox.com/s/83i4atz733ali0w/Screen%20Shot%202018-07-18%20at%2012.10.52%20AM.png?dl=1\" width=\"600\"> </center>\n\n논문 저자 : Richard S. Sutton, David McAllester, Satinder Singh, Yishay Mansour    \n논문 링크 : [NIPS](http://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf)\nProceeding : Advances in Neural Information Processing Systems (NIPS) 2000        \n정리 : 김동민, 이동민\n\n---\n\n# 1. Intro to Policy Gradient\n\n이 논문은 policy gradient (PG) 기법의 효시와도 같으며 향후 많은 파생연구를 낳은 중요한 논문입니다. 7페이지의 짧은 논문이지만 읽기에 만만한 논문은 아닙니다. 이 논문을 이해하기 위해 필요한 배경지식을 먼저 설명하고 논문을 차근차근 살펴보도록 하겠습니다.\n\n<br>\n## 1.1 Value Function Approach\n\n전통적으로 강화학습 기법은 value function을 기반으로 동작하였습니다. 특정 state에서 vaue function 또는 value function을 근사하는 함수(function approximation)를 최대화하는 action을 찾는 greedy action-selection policy가  대표적입니다.\n\n논문에서는 이러한 방법은 deterministic한 policy를 찾는 쪽으로 나아가게 되지만 종종 최적의 policy는 stochastic한 성질을 가지기 때문에 이 방법으로는 최적의 policy를 찾을 수 없다고 언급하고 있습니다. (그러나 이 논문이 나오고 나서 한참 후 David Silver를 필두로한 DeepMind의 연구진들은 high-dimensional action space를 가지는 application에서 보다 빠르게 동작하는 [deterministic policy gradient](../../../06/27/2_dpg/)을 개발하였습니다.) 아마도 이러한 부분은 $\\epsilon$-greedy action-selection method로 개선될 수 있을 것입니다. 또 하나의 문제는 value function의 작은 변화로 인해서 action이 크게 변할 수도 있다는 것입니다. 이것은 알고리듬의 수렴성에 문제를 야기할 수 있습니다. 이러한 문제점을 해결하기 위하여 policy search라는 새로운 기법이 고안됩니다.\n\n<br>\n## 1.2 Policy Search\n\npolicy search는 최적의 policy $\\pi^*$를 reward로부터 직접 찾습니다. policy search 방법은 크게 두 가지로 분류할 수 있습니다. 첫 번째는 gradient-based optimization으로 policy gradient method가 이에 속합니다. 두 번째는 gradient-free optimization으로 진화(evolutionary) 연산을 이용하는 것이 이에 속합니다. 이 논문에서는 제목에서 알 수 있듯이 gradient-based optimization을 다룹니다. gradient는 변화량을 의미합니다. 특정 policy x와 또 다른 policy y가 있을 때 policy가 얼마나 많이 변화했는지 어떻게 모델링할 수 있을까요? policy의 변화를 제어하는 어떤 파라미터가 있다면 이 파라미터를 조정하여 policy를 변화시킬 수 있습니다. policy x에 해당하는 파라미터값과 policy y에 해당하는 파라미터값의 차이가 policy의 변화량이라고 할 수 있습니다. 이와 같은 방식으로 policy의 변화를 모델링하기 위하여 파라미터 $\\theta$를 이용하여 policy를 $\\pi_{\\theta}$로 표현할 수 있습니다. 최적의 policy를 찾기 위하여 expected return $E\\left[R|\\theta\\right]$이 최대화되도록 parameter를 조정합니다. 이를 간단한 수식으로 정리하면 다음과 같습니다.\n\n$$\n\\pi^* = \\arg \\max\\limits_\\pi  E\\left[ {\\left. R \\right|\\pi } \\right] \\to {\\text{original problem} } \n\\\\\\\\  {\\Downarrow} \\\\\\\\\n{\\text{policy parameterization by } } \\pi_{\\theta}: \\Theta  \\to \\Pi \\\\\\\\ {\\Downarrow} $$\n\n$$\\pi^* = \\arg \\max\\limits_{\\theta}  E\\left[ {\\left. R \\right|\\theta } \\right] \\to {\\text{policy search problem} }\n$$\n\n<br>\n## 1.3 How to Obtain the Expected Return\nexpected return을 최대화하는 방향으로 policy를 update한다고 하였습니다. 그렇다면 expected return은 어떻게 구할 수 있을까요? 여기에도 크게 두 가지 방법이 있습니다. 첫 번째는 deterministic approximation으로 Markov decision process의 dynamics를 모델링한 후 수식을 통해 구하는 것입니다. 두 번째 방법은 monte carlo estimation으로 dynamics에 대한 모델을 하지 않고 많은 sample들을 얻은 후 empirical하게 expected return을 계산하는 방법입니다. 어느 방법이 더 좋을지는 풀고자 하는 문제의 특성에 따라 다를 것입니다. dynamics에 대한 모델이 어렵거나 변화가 큰 경우에는 두 번째 방법이 좀 더 현실적이지만 gradient를 구하는 것은 더 어렵습니다. 결국 gradient를 esimate하는방법을 고안해야 합니다. 가장 유명한 gradient estimation 방법이 1992년 R. J. Williams에 의해서 제안된 [REINFORCE](https://link.springer.com/content/pdf/10.1007/BF00992696.pdf) 기법입니다. REINFORCE는 Monte Carlo estimate 또는 likelihood-ratio estimate라고 부르는 방법을 이용합니다. 이 방법에 대해서 좀 더 알아볼까요?\n\n### 1.3.1 Monte Carlo Gradient Estimation\n다음과 같은 parameter $\\theta$를 가지는 random variable $X$가 있습니다: $X:\\Omega\\mapsto\\mathcal{X}$. 그리고 이 $x$에 대한 함수 $f$가 있습니다: $f:\\mathcal{X}\\mapsto\\mathbb{R}$. expected return처럼 $E[f(x)]$를 최대화하고자 합니다. 이를 위해서는 $\\nabla_{\\theta}E[f(x)]$를 구해야 합니다. 이 때 log derivate trick을 이용하여 다음과 같이 수식을 변형시킬 수 있습니다.\n\n$$\n\\begin{align}\n\\nabla_{\\theta} E_{p(x;\\theta)}\\left[ {f(x)} \\right] \n&= \\nabla_\\theta\\int{f\\left( x \\right)p\\left( {x;\\theta} \\right) dx}\n\\\\\\\\\n&= \\int { {\\nabla_\\theta }p\\left( {x;\\theta } \\right)f\\left( x \\right)dx} \n\\\\\\\\\n&= \\int {\\frac{ {p\\left( {x;\\theta } \\right)} }{ {p\\left( {x;\\theta } \\right)} }{\\nabla_\\theta }p\\left( {x;\\theta } \\right)f\\left( x \\right)dx} \n\\\\\\\\\n&= \\int {p\\left( {x;\\theta } \\right){\\nabla_\\theta }\\log p\\left( {x;\\theta } \\right)f\\left( x \\right)dx} \n\\\\\\\\\n&= E_{p(x;\\theta)}[f(x)\\nabla_\\theta \\log p({x;\\theta})]\n\\end{align}\n$$\n\n이제 expectation은 많은 샘플들을 모아서 평균을 취하는 Monte Carlo 기법으로 근사화할 수 있습니다.\n\n$${\\nabla_{\\theta} }{E_{p\\left( {x;\\theta } \\right)} }\\left[ {f\\left( x \\right)} \\right] = \\frac{1}{N}\\sum_{n=1}^N f\\left(X_n\\right)\\nabla_\\theta\\log p\\left(X_n;\\theta\\right)$$\n\n이 방법을 사용하기 위해서 필요한 조건은 $\\log p\\left(X_n;\\theta\\right)$가 미분가능해야 한다는 것 뿐입니다. 그러나 이 방법은 얻은 샘플들에 의존하기 때문에 경우에 따라서 큰 variance를 가질 수 있습니다.\n\n<br><br>\n\n# 2. Policy Gradient Methods\n\n이제 본격적으로 policy gradient 기법에 대해서 알아보겠습니다.\n\n<br>\n## 2.1 System Model\n\n논문에서 사용하는 수학기호(notation)와 가정들을 설명하는 시스템모델은 다음과 같습니다.\n\n* Markov decision process (MDP) at each time $t\\in{0,1,2,...}$\n* $S_t\\in\\mathcal{S}$: state\n* $A_t\\in\\mathcal{A}$: action\n* $r_t\\in \\mathcal{R}$: reward\n* $\\theta\\in\\mathbb{R}^l$: vector of policy parameters, for $l\\ll\\left|\\mathcal{S}\\right|$\n* $\\mathcal{P} _{s s'}^a = \\Pr[S _{t+1}=s' \\vert S_t=s,A_t=a]$: state transition probabilities\n* $\\mathcal{R} _{s s'}^a = E[R _{t+1}\\vert S_t=s,A_t=a]$: expected reward\n* $\\pi(s,a,\\theta)=\\Pr[A_t=a|S_t=s,\\theta]$: policy, shortened as $\\pi(s,a)$ or $\\pi(a|s)$\n* $\\theta$: policy parameter들의 vector  \n* $\\rho$: 해당 policy들의 성능을 나타내는 척도 (예. average reward per step)  \n* $\\alpha$: positive-definite한 step size \n\n<br>\n## 2.2 Policy Gradent Approach\npolicy gradient는 stochastic policy를 자체적인 파리미터를 가진 function approximator를 이용해서 근사화시킵니다.\n\n\npolicy parameter는 gradient에 비례하여 업데이트됩니다.\n\n$$\\Delta\\theta \\approx \\alpha\\frac{\\partial\\rho}{\\partial\\theta}$$\n\n$\\rho$는 $\\theta$에 대하여 미분가능해야겠죠? 위와 같이 업데이트하면 local optimal policy로 수렴합니다. 논문의 가장 중요한 contribution은 특정 조건을 만족하는 function approximator를 이용하여 경험(experience, sample)을 축적하고 이것들을 이용하여 위의 gradient를 unbiased estimate할 수 있음을 증명한 것입니다.\n\n먼저 $\\rho$, 즉, reward를 표현하는 두 가지 방법에 대해서 알아보겠습니다.\n\n<br>\n## 2.3 Average Reward Formulation\nAverage reward formulation은 시간의 흐름에 따른 reward를 표현한다기보다는 모든 시간의 reward를 평균을 내서 표현하는 방법입니다.\n\n* Long‐term expected reward per step\n$$\\rho(\\pi)=\\lim_{n\\to\\infty}\\frac{1}{n}E[r_1+r_2+\\cdots+r_n|\\pi] = \\sum_s d^\\pi(s)\\sum_a\\pi(s,a)\\mathcal{R}_s^a$$\n\n    - $d^\\pi(s): \\lim_{t\\to\\infty}\\Pr[S_t=s|s_0,\\pi]$: stationary distribution of states under $\\pi$\n    - $d^\\pi(s)$가 존재한다고 가정합니다. 그리고 모든 policy에 대해서 $d^\\pi(s)$는 $s_0$와 independent합니다.\n    - 위의 (limit수식과 summation수식 사이의) 등식은 왜 성립할까요? Time average와 Ensemble average가 같다는 뜻으로 [ergodic](https://en.wikipedia.org/wiki/Ergodicity)한 system에서 성립합니다.\n\n* Value of a state-action pair given a policy\n$$Q^{\\pi(s,a)} = \\sum_{t=1}^\\infty E[r_t - \\rho(\\pi)|S_0=s, A_0=a, \\pi]$$\n    - 위의 Q-function의 표현식이 정의인지 유도된 것인지는 이 포스트를 작성하는 저희도 아직 확실히 모르겠습니다. 다만, $r_t$들을 더하면 무한대로 발산할 가능성이 매우 높은데 $\\rho$를 빼줌으로해서 어찌보면 평균 reward로부터의 차이를 더하는 것이라고 할 수 있고 이것은 bound된 값일 가능성이 더 높기 때문에 Q-function을 이렇게 표현하는 것이 더 안전하다고 할 수 있습니다. 이 부분에 대해서 좋은 의견이 있으시면 피드백을 주시면 감사하겠습니다!\n\n* State-value function\n$$V^\\pi(s) = \\sum_{a} \\pi(s,a)Q^\\pi(s,a)$$\n\n<br>\n## 2.4 Start-State Formulation\nstart-state formulation은 시간의 흐름에 따라 감소하는 reward들을 표현합니다.\n\n* Long-term expected reward per step with a designated start state $S_0$\n$$\\rho(\\pi) = E\\left[\\left.\\sum_{t=1}^\\infty \\gamma^{t-1}r_t\\right|S_0,\\pi\\right]$$\n    - $\\gamma\\in\\left[0,1\\right]$: discount rate\n\n* Value of a state‐action pair given a policy\n$$Q^\\pi(s,a) = E\\left[\\left.\\sum_{k=1}^\\infty \\gamma^{k-1} r_{t+k}\\right|S_t=s, A_t=a, \\pi\\right]$$\n\n* Alternative form of $Q^\\pi(s,a)$\n$$\nQ^\\pi(s,a) = R_s^a + \\gamma\\sum _{s'} \\mathcal{P} _{s s'}^a V^{\\pi}(s')\n$$\n\n<br>\n## 2.5 Policy Gradient Theorem\n논문의 중요한 결과인 Theorem 1은 다음과 같습니다.\n\n**Theorem 1 (Policy Gradient)** *For any MDP, in either the average‐reward or start‐state formulations,*\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)$$\n\n놀라운 부분은 expected return의 gradient를 취할 때 $\\frac{\\partial d^\\pi(s)}{\\partial\\theta}$를 구하지 않아도 된다는 점입니다. 즉, policy의 변화가 state distribution에 영향을 주지 않는다는 것입니다. 이것은 다시 말하면 아래 수식과 같이 우리는 $\\frac{\\partial\\rho}{\\partial\\theta}$에 대한 unbiased estimator를 구할 수 있다는 뜻입니다.\n\n$$E_s\\left[\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)\\right] = \\frac{\\partial\\rho}{\\partial\\theta}$$\n\n이로인해서 우리는 gradient를 sampling을 통해서 추정할 수 있습니다. 그렇지만 이것은 sample이 아주 많을 때만 성립합니다!\n\n또 한 가지 문제는 우리는 $Q^\\pi(s,a)$의 정확한 값을 알 수 없다는 것 입니다. 이것을 estimate하기 위해서 현재의 return값 $R_t$를 이용할 수 있습니다.\n\n<br>\n## 2.6 Proof of Policy Gradient Theorem\n증명을 살펴보겠습니다.\n\n* average-reward formulation을 이용할 때의 증명은 아래와 같습니다.\n\n$$\n\\begin{align}\n\\frac{\\partial V^\\pi(s)}{\\partial\\theta} &\\overset{\\underset{\\mathrm{def} }{} }{=}\\frac{\\partial}{\\partial\\theta}\\sum_a \\pi(s,a)Q^\\pi(s,a)\n\\\\\\\\\n&=\\sum_a \\frac{\\partial}{\\partial\\theta}\\left[\\pi(s,a)Q^\\pi(s,a)\\right]\\\\\\\\\n&=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\frac{\\partial Q^\\pi(s,a)}{\\partial\\theta}\\right]\\\\\\\\\n&=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\frac{\\partial}{\\partial\\theta}\\left[R_s^a -\\rho(\\pi) + \\sum_{s'}\\mathcal{P}_{s s'}^a V^{\\pi}(s')\\right]\\right]\\\\\\\\\n&=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\left[-\\frac{\\partial\\rho}{\\partial\\theta} + \\sum_{s'}\\mathcal{P}_{s s'}^a \\frac{\\partial V^{\\pi}(s')}{\\partial\\theta}\\right]\\right]\\\\\\\\\n\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\sum_{s'}\\mathcal{P}_{s s'}^a \\frac{\\partial V^{\\pi}(s')}{\\partial\\theta}\\right] - \\frac{\\partial V^\\pi(s)}{\\partial\\theta}\n\\end{align}\n$$\n\n양변에 stationary distribution에 대한 평균을 취합니다.\n$$\n\\begin{align}\n\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+ \\sum_s d^\\pi(s)\\sum_a\\pi(s,a)\\sum_{s'}\\mathcal{P}_{s s'}^a \\frac{\\partial V^{\\pi}(s')}{\\partial\\theta} \n\\\\\\\\\n&- \\sum_s d^\\pi(s)\\frac{\\partial V^\\pi(s)}{\\partial\\theta}\n\\end{align}\n$$\n\n모든 state $s$에서 모든 state $s'$으로 이동하는 것은 state $s'$에 존재할 확률이므로 다음과 같이 표현할 수 있습니다.\n\n$$\n\\begin{align}\n\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+ \\sum_s d^\\pi(s') \\frac{\\partial V^{\\pi}(s')}{\\partial\\theta} - \\sum_s d^\\pi(s)\\frac{\\partial V^\\pi(s)}{\\partial\\theta}\n\\end{align}\n$$\n\n뒤의 두 항은 같은 식이므로 0이 됩니다. 따라서 다음과 같이 표현됩니다.\n$$\n\\begin{align}\n\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)\n\\end{align}\n$$\n\n$\\frac{\\partial\\rho}{\\partial\\theta}$는 reward의 파라미터에 대한 미분값입니다. reward 자체가 이미 모든 state에 대한 평균값이므로 $s$의 함수가 아닙니다.  $s$에 대한 함수가 아니므로 $\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}=\\frac{\\partial\\rho}{\\partial\\theta}\\sum_s d^\\pi(s)=\\frac{\\partial\\rho}{\\partial\\theta}$입니다. 여기서 $d^\\pi(s)$는 state에 대한 확률값이죠. 모든 state에 대해서 이 값을 다 더하면 1이 됩니다.\n이를 이용해서 위의 식을 다시 표현하면 아래와 같고 이를 통해 증명이 완성됩니다.\n\n$$\n\\begin{align}\n\\therefore\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)\n\\end{align}\n$$\n\n* 다음으로 start-state formulation에 대한 증명입니다.\n\nstart-state formulation은 이 논문에서도 나오고 서튼책에서도 나오는데, 서튼책의 증명이 더 설명이 자세합니다. 그래서 논문에 있는 증명이 아닌 서튼책에 있는 증명으로 설명드리겠습니다. 다만, notation이 약간 다른데, 큰 문제 없이 이해하실 수 있으므로 별도의 설명은 하지 않겠습니다. (서튼책 [Link](https://drive.google.com/file/d/1xeUDVGWGUUv1-ccUMAZHJLej2C7aAFWY/view))\n\n$$\\nabla{v_\\pi}(s)=\\nabla\\big[\\sum_a\\pi(a|s)q_\\pi(s,a)\\big]$$ \n\n다음으로 $(f\\cdot g)'=f'\\cdot g+f\\cdot g'$ (product rule of calculus)에 의해 다음과 같이 나타낼 수 있습니다.\n$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\nabla q_\\pi(s,a)\\big]$$  \n\n이어서 $p(s',r|s,a) := Pr[S_t=s', R_t=r|S_{t-1}=s,A_{t-1}=a ]$ (서튼책 3.2 공식) 에 의해 다음과 같이 나타낼 수 있습니다.\n\n$$=\\sum_a\\big[\\nabla\\pi(a|s)q_{\\pi}(s,a)+\\pi(a|s)\\nabla\\big[\\sum_{s', r}p(s',r|s,a)(r+v_{\\pi}(s'))\\big]\\big]$$  \n\n계속해서 $p(s'|s,a)=\\sum_{r\\in R}p(s',r|s,a)$ (서튼책 3.4 공식) 에 의해 다음과 같이 나타낼 수 있습니다.\n$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\sum_{s'}p(s'|s,a)\\nabla v_\\pi(s') \\big]$$ \n\n여기서 $\\nabla v_\\pi(s')$ 부분을 unrolling을 하면 다음과 같이 나타낼 수 있습니다.\n$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\sum_{s'}p(s'|s,a)\\sum_{a'}\\big[\\nabla\\pi(a'|s')q_\\pi(s',a')\\\\\\\\+\\pi(a'|s')\\sum_{s''}p(s''|s',a')\\nabla v_\\pi(s''))\\big] \\big]$$ \n\n$\\nabla v_\\pi(s'')$에 대해서 unrolling을 하고 또 나오는 다른 항에 대해서도 계속 반복하다보면 다음과 같이 나타낼 수 있습니다.\n$$=\\sum_{x\\in S}\\sum_{k=0}^{\\infty}\\Pr(s\\rightarrow x,k,\\pi)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$\n\n여기에서 $\\Pr(s\\rightarrow x,k,\\pi)$ 는 policy $\\pi$ 에 대해 state $s$에서 state $x$까지 $k$ step만큼 움직일 때의 변환 확률입니다. 따라서 위의 수식을 아래처럼 다시 나타낼 수 있습니다.\n <!-- \\begin{align}  -->\n$$\\nabla J(\\theta)=\\nabla v_\\pi (s_0)$$ \n$$=\\sum_s(\\sum_{k=0}^{\\infty}\\Pr(s_0\\rightarrow s,k,\\pi))\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$\n$$=\\sum_s \\eta(s) \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$ \n<!-- \\end{align} -->\n- 서튼책에서 $\\eta(s)$는 다음과 같이 표현할 수 있습니다.\n    - $\\eta(s)$ = 평균적으로 하나의 episode마다 state에서 머무른 time step의 수 (page 199)\n- 논문에서는 $\\eta(s)$를 다음과 같이 표현합니다.\n    - $d^\\pi(s)=\\sum_{t=0}^{\\infty}\\gamma^t Pr(s_t=s|s_0,\\pi)$ \n    ($\\gamma=1$ is allowed only in episodic tasks = discounted weighting of states)\n\n여기서부터가 논문에는 없는 내용입니다. 위의 최종 수식을 다시 한 번 적으면 아래와 같습니다.\n$$\\sum_s \\eta(s) \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$\n\n위의 수식을 아래와 같이 바꿀 수 있습니다.\n$$\\sum_{s'} \\eta(s') \\cdot\\sum_s \\frac{\\eta(s)}{\\sum_{s'}\\eta(s')} \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$\n\n그러면 이것을 새로운 기호로 다시 나타낼 수 있습니다.\n$$\\sum_{s'} \\eta(s') \\cdot\\sum_s \\mu(s)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$ \n- 여기서 $\\mu(s)$ 는 state distribution이라고 하며, weight를 state distribution으로 바꿔주는 과정이라고 생각할 수 있습니다. 쉽게 말해 어떠한 state에 agent가 머무르는 확률입니다.\n\n위의 수식은 아래의 수식과 비례합니다. 따라서 최종 형태는 다음과 같습니다.\n$$\\propto \\sum_s \\mu(s)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$\n\n<br><br>\n\n# 3. Policy Gradient with Approximation\n이 장에서는 앞서 다뤘던 Theorem 1 중에 $Q^\\pi$에 대해서 중점적으로 다룹니다. 어떠한 $Q^\\pi$가 학습된 function approximator로 근사된다고 합시다. \n\n$f_w$ ($f_w$는 $S × A \\rightarrow \\Re$)는 parameter $w$를 가지는 어떠한 $Q^\\pi$ 에 대한 근사치입니다. 학습된 function approximator에 대해서 생각하기 때문에, 학습된 $f_w$는 error를 최소화하는 방향으로 parameter $w$를 업데이트합니다. 그러면 다음과 같은 관계식을 생각할 수 있습니다.\n$$\\Delta w_t \\propto \\frac{\\partial}{\\partial w}\\big[ {\\hat Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]^2 \\propto \\big[{\\hat Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s_t,a_t)}{\\partial w}$$ \n(여기서 ${\\hat Q}^\\pi(s_t,a_t)$ 는 $Q^\\pi(s_t,a_t)$의 unbiased estimator $R_t$입니다.)\n\n그러면 위와 같은 수식이 local optimum에 수렴을 했을 때, 다음과 같이 나타낼 수 있습니다.\n$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$\n\n- 위의 수식에 대한 추가 설명\n    - local optimum으로 수렴을 하기 때문에 당연히 위의 수식은 0이 됩니다.\n    - 또한 stochastic policy이기 때문에 local optimum으로 수렴하려면 모든 state, action에 대해 expectation을 해야합니다. 따라서 $\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)$이 붙게 됩니다.\n\n<br>\n## 3.1 Theorem 2: Policy Gradient with Function Approximation\n\n만약 $f_w$가 아래의 등식을 만족한다고 합시다.\n$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$\n\n또한 $f_w$가 policy parameterization과 양립할 수 있다고 합시다. policy parameterization에 대한 여러가지 의미가 있는데 이 논문에서는 다음을 의미합니다.\n$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}$$\n- 위의 수식에 대한 추가 설명\n    - 이 수식은 Monte-Calro Estimation을 이용하기 위한 조건입니다.\n    - Compatibility Condition이라고 부릅니다.\n    - 우변(= $\\nabla\\log\\pi(s,a)$)으로 유도함으로써 아래의 수식과 같이 sampling을 이용한 추정이 가능해집니다.\n\n따라서 위의 두 수식을 이용하여 아래와 같이 나타낼 수 있습니다.\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}f_w(s,a)$$\n\n<br>\n## 3.2 Proof of Theorem 2\n\n$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$\n\n$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}$$\n\n위의 두 수식을 합치면 다음과 같이 나타낼 수 있습니다.\n$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}=0$$\n\n분자와 분모에 있는 $\\pi(s,a)$를 지우면 아래와 같습니다.\n$$\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]=0$$\n\n이어서 위의 수식이 0이기 때문에 policy gradient theorem(앞서 다뤘던 Theorem 1)에서 위의 수식을 뺄 수 있습니다.\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)-\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ Q^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]$$\n\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ Q^\\pi(s_t,a_t)-Q^\\pi(s_t,a_t)+f_w(s_t,a_t) \\big]$$\n\n$$\\therefore\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}f_w(s_t,a_t)$$\n\n<br><br>\n\n# 4. Application to Deriving Algorithms and Advantages\n\n<br>\n## 4.1 Application to Deriving Algorithms\nfeature의 linear combination에서 Gibbs distribution (softmax)를 하나의 policy로 생각해볼 수 있습니다.\n$$\\pi(s,a)=\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_b e^{\\theta^{T}\\phi_{sb} }}$$\n- $\\phi_{sa}$: state-action pair $s$, $a$를 나타내는 $l$-dimensional feature vector\n\ncompatible condition을 적용하면 다음과 같습니다. (4.2 Proof of application of compatible condition 참고)\n$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\frac{1}{\\pi(s,a)}=\\phi_{sa}-\\sum_b \\pi(s,b)\\phi_{sb}$$ \n\n이어서 $f_w$을 적분한 natural parameterization은 다음과 같습니다.\n$$f_w(s,a)=w^T\\big[ \\phi_{sa}-\\sum_b \\pi(s,b)\\phi_{sb}\\big]$$\n\n- 위의 수식에 대한 추가 설명\n    - $f_w$는 policy로서 같은 feature들에 대해 linear합니다.\n    - $f_w$는 각각의 state에 대해서 평균이 0입니다. ($\\sum_a\\pi(s,a)f_w(s,a)=0$)\n    - advantage function $A^\\pi(s,a)=Q^\\pi(s,a)-V^\\pi(s)$의 하나의 근사치로서 $f_w$를 생각해도 좋습니다.\n\n<br>\n## 4.2 Proof of application of compatible condition\n증명을 하기 전에 다음을 가정합니다.\n1. $(\\frac{f(x)}{g(x)})'=f'(x)\\cdot \\frac{1}{g(x)}-f(x)\\cdot \\frac{g'(x)}{g(x)^{2} }=\\frac{f'(x)\\cdot g(x)-f(x)\\cdot g'(x)}{g(x)^{2} }$\n2. $f(\\theta)=e^{\\theta^{T}\\phi_{sa} }$, $g(\\theta)=\\sum_be^{\\theta^{T}\\phi_{sb} }$\n\n$$\n\\begin{align}\n\\frac{\\partial f_w(s,a)}{\\partial w}&=\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\frac{1}{\\pi(s,a)}\n\\\\\\\\\n&=\\big(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }}\\big)'\\frac{1}{\\pi(s,a)}\n\\\\\\\\\n&=\\big[ \\phi_{sa}(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }})-(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }})\\frac{\\sum_b\\phi_{sb}e^{ {\\theta^T}\\phi_{sb} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }}\\big]\\frac{1}{\\pi(s,a)}\\\\\\\\\n&=\\big[ \\phi_{sa}\\pi(s,a)-\\pi(s,a)\\sum_b\\phi_{sb}\\pi(s,b)\\big]\\frac{1}{\\pi(s,a)}\n\\\\\\\\\n&=\\pi(s,a)\\big[ \\phi_{sa}-\\sum_b\\pi(s,b)\\phi_{sb}\\big]\\frac{1}{\\pi(s,a)}\n\\\\\\\\\n&\\therefore\\phi_{sa}-\\sum_b\\pi(s,b)\\phi_{sb}\n\\end{align}\n$$\n\n<br>\n## 4.3 Application to Advantages\nPolicy Gradient with Function Approximation Theorem (Theorem 2)는 advantage function으로 확장될 수 있습니다.\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_sd^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ f_w(s,a)+v(s) \\big]$$\n- 위의 수식에 대한 추가설명\n    - $v$ ($v$는 $S\\rightarrow\\Re$) 는 arbitrary function입니다.\n    - 이 수식은 $\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}=0$이기 때문에 가능해집니다.\n    - 즉, 이 수식은 $\\pi(s,a)$의 gradient에만 dependent하기 때문에 advantage 역할을 하는 함수들을 넣어도 아무런 상관이 없습니다.\n    - $v$의 선택은 Theorem들에 영향을 미치지 못하지만, 실질적으로 gradient estimator의 variance에 영향을 미칩니다.\n    - 이러한 문제는 전체적으로 이전의 연구에 reinforcement baseline의 사용에 있어서 유사합니다.\n    - (comment) 위의 수식에서 $f_w(s,a)+v(s)$와 Application to Deriving Algorithms의 $f_w(s,a)$와는 다른 것입니다. Application to Deriving Algorithms의 $f_w(s,a)$은 softmax에 의해 스스로 advantage function의 역할을 할 수 있습니다. 하지만 보통의 경우에는 그러지 못할 수도 있기 때문에 위의 수식처럼 $f_w(s,a)+v(s)$을 추가하여 zero mean만들어서 variance를 줄일 수 있습니다.\n\n<br><br>\n\n# 5. Convergence of Policy Iteration with Function Approximation\n\n<br>\n## 5.1 Theorem 3: Convergence of Policy Iteration with Function Approximation\n\npolicy iteration with function approximation은 locally optimal policy에 수렴합니다. $\\pi$와 $f_w$를 \n\n1. compatibility condition을 만족하는 policy와 value function에 대한\n2. 그리고 $\\max_{\\theta, s, a, i, j}\\big| \\frac{\\partial^2\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}\\big|<B<\\infty$에 대한\n\n어떠한 미분가능한 function approximator라고 합시다.\n\n- (comment) $\\max_{\\theta, s, a, i, j}\\big| \\frac{\\partial^2\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}\\big|<B<\\infty$에서 $\\pi(s,a)$라는 function은 이계미분값이 존재하고, 임의의 상수인 (Bound) B에 bound되어 있기 때문에 function의 그래프는 smooth하다고 볼 수 있습니다. (아래의 그림 중 빨간색 그래프 참고)\n\n<center> <img src=\"https://www.dropbox.com/s/xx02ejfg5ao19ps/Screen%20Shot%202018-07-10%20at%203.41.06%20PM.png?dl=1\" width=\"200\"> </center>\n\n이어서 ${\\alpha_k}$는 $\\lim_{k\\rightarrow\\infty}\\alpha_k=0$이며 $\\sum_k\\alpha_k=\\infty$를 만족하는 step-size sequence라고 합시다.\n\n그 때, bounded reward를 가진 MDP에 대해\n1. 어떠한 $\\theta_0, \\pi_k=\\pi(\\cdot,\\cdot,\\theta_k)$\n2. 그리고 $\\sum_sd^{\\pi_{k} }(s)\\sum_a\\pi_k(s,a)\\big[ Q^{\\pi_{k} }(s,a)-f_w(s,a)\\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$로 인하여 $w_k=w$, $\\theta_{k+1}=\\theta_k+\\alpha_k\\sum_sd^{\\pi_{k} }(s)\\sum_a\\frac{\\partial\\pi_k(s,a)}{\\partial\\theta}f_{w_{k} }(s,a)$\n\n으로 정의된 sequence $\\rho(\\pi_k)$는 $\\lim_{k\\rightarrow\\infty}\\frac{\\partial\\rho(\\pi_k)}{\\partial\\theta}=0$이기 때문에 수렴합니다.\n- sequence $\\rho(\\pi_k)_{k=0}^\\infty$에 대한 추가 설명\n    - 2번의 식을 통해 자연스럽게 actor-critic으로 연결됩니다.\n    - $\\theta_{k+1}=\\theta_k+\\alpha_k\\sum_sd^{\\pi_{k} }(s)\\sum_a\\frac{\\partial\\pi_k(s,a)}{\\partial\\theta}f_{w_{k} }(s,a)$ 에 따라 $\\theta$가 1, 2, ..., $\\infty$로 갈텐데, 거기에 따른 objective function or performance의 sequence입니다.\n    - (comment) 굳이 sequence라는 표현이 없어도 될 것 같습니다. 어짜피 k가 $\\infty$로 가면 $\\rho(\\pi_k)$가 수렴한다는 의미이기 때문에 불필요해보입니다.\n\n<br>\n## 5.2 Proof of Theorem 3\n- Theorem 2는 $\\theta_k$ update가 gradient의 error를 최소화한다는 것을 증명했습니다.\n- $\\frac{\\partial^{2}\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}$와 MDP의 reward에서의 bound는, $\\frac{\\partial^{2}\\rho}{\\partial\\theta_i\\partial\\theta_j}$ 또한 bound된다는 것을 증명합니다.\n- step-size 필요조건 때문에 이러한 bound된 것들은 Proposition 3.5 from page 96 of Bertsekas and Tsitsiklis (1996)에 적용하기 위해 필요한 조건입니다.\n- Proposition 3.5 from page 96 of Bertsekas and Tsitsiklis (1996)은 local optimum으로 수렴한다는 것을 증명했습니다.\n- Proposition 3.5를 자세하게 설명하지는 않겠습니다. 일반적인 gradient method의 수렴성을 증명한 것이며, 이 논문의 policy gradient method도 gradient method의 일종이므로 특정 조건을 만족할 때 수렴한다는 것입니다. 수렴하는 원리는 소위 말하는 Lipschitz continuity condition을 만족하기 때문인데 다음과 같습니다. $L>0$인 임의의 상수입니다.\n$$\n\\parallel \\nabla f(r) - \\nabla f(\\bar{r}) \\parallel \\leq L \\parallel r - \\bar{r} \\parallel\n$$\n즉, 업데이트가 진행되어 나감에 따라서 gradient의 차이가 점점 줄어들게 되므로 언젠가는 0으로 수렴하게 됩니다.\n\n<br><br>\n\n# 6. Summary \n\n논문에서 설명한 policy gradient 기법을 요약하면 다음과 같습니다.\n\n* Original maximization problem: $\\max\\rho_{\\theta}=E\\left[R\\right]$\n* gradient ascent method를 이용한 parameter update: $\\theta_{t+1} = \\theta_{t} + \\alpha\\left.\\frac{\\partial E[R]}{\\partial\\theta}\\right|_{\\theta_t}$\n    - 그러나 우리는 gradient를 모르고, 추정하기도 어렵습니다. 왜냐하면 expectation이 안에 있기 때문입니다.\n    - 그렇다면 이것을 추정 가능한 형태로 바꿔야합니다. 다시 말해 expectation이 밖에 있는 형태로 바꾸는 것입니다.\n    - Expectation이 밖에 있으면 왜 추정이 유리할까요? 바로 Sample mean을 취하면 되기 때문입니다.\n* Approximate gradient\n    - 방법1: 논문의 Theorem 1을 이용하면 gradient와 expectation의 위치를 바꿀 수 있습니다.\n    $$\\frac{\\partial E[R]}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)$$\n    - 방법2: Log derivative trick을 이용하면 gradient와 expectation의 위치를 바꿀 수 있습니다. (Theorem 2)\n    $$\\theta_{t+1} = \\theta_{t} + \\alpha E\\left[ \\left. R\\frac{\\partial}{\\partial\\theta}\\log p_{\\theta} \\right\\vert \\theta_t\\right]$$\n    - 그리고 특정 trajectory를 따라가면서 return값을 구하고 이것을 여러 번 수행하여 sample mean을 취하면 gradient를 추정하는 것이 가능합니다.\n    - 이러한 기법을 제시한 기존의 연구가 REINFORCE입니다.\n    - 여러 Trajectory를 이용하므로 variance가 높을 수 밖에 없습니다.\n    - Advantage를 활용하거나 하는 방식으로 이후 여러 연구가 진행되었습니다.\n\n<br><br>\n\n# 처음으로\n\n## [PG Travel Guide](https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/)\n\n<br>\n\n# 다음으로\n\n## [DPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/27/2_dpg/)","source":"_posts/1_sutton-pg.md","raw":"---\ntitle: Policy Gradient Methods for Reinforcement Learning with Function Approximation\ndate: 2018-06-28 14:18:32\ntags: [\"프로젝트\", \"피지여행\"]\ncategories: 프로젝트\nauthor: 김동민, 이동민\nsubtitle: 피지여행 1번째 논문\n---\n\n<center> <img src=\"https://www.dropbox.com/s/83i4atz733ali0w/Screen%20Shot%202018-07-18%20at%2012.10.52%20AM.png?dl=1\" width=\"600\"> </center>\n\n논문 저자 : Richard S. Sutton, David McAllester, Satinder Singh, Yishay Mansour    \n논문 링크 : [NIPS](http://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf)\nProceeding : Advances in Neural Information Processing Systems (NIPS) 2000        \n정리 : 김동민, 이동민\n\n---\n\n# 1. Intro to Policy Gradient\n\n이 논문은 policy gradient (PG) 기법의 효시와도 같으며 향후 많은 파생연구를 낳은 중요한 논문입니다. 7페이지의 짧은 논문이지만 읽기에 만만한 논문은 아닙니다. 이 논문을 이해하기 위해 필요한 배경지식을 먼저 설명하고 논문을 차근차근 살펴보도록 하겠습니다.\n\n<br>\n## 1.1 Value Function Approach\n\n전통적으로 강화학습 기법은 value function을 기반으로 동작하였습니다. 특정 state에서 vaue function 또는 value function을 근사하는 함수(function approximation)를 최대화하는 action을 찾는 greedy action-selection policy가  대표적입니다.\n\n논문에서는 이러한 방법은 deterministic한 policy를 찾는 쪽으로 나아가게 되지만 종종 최적의 policy는 stochastic한 성질을 가지기 때문에 이 방법으로는 최적의 policy를 찾을 수 없다고 언급하고 있습니다. (그러나 이 논문이 나오고 나서 한참 후 David Silver를 필두로한 DeepMind의 연구진들은 high-dimensional action space를 가지는 application에서 보다 빠르게 동작하는 [deterministic policy gradient](../../../06/27/2_dpg/)을 개발하였습니다.) 아마도 이러한 부분은 $\\epsilon$-greedy action-selection method로 개선될 수 있을 것입니다. 또 하나의 문제는 value function의 작은 변화로 인해서 action이 크게 변할 수도 있다는 것입니다. 이것은 알고리듬의 수렴성에 문제를 야기할 수 있습니다. 이러한 문제점을 해결하기 위하여 policy search라는 새로운 기법이 고안됩니다.\n\n<br>\n## 1.2 Policy Search\n\npolicy search는 최적의 policy $\\pi^*$를 reward로부터 직접 찾습니다. policy search 방법은 크게 두 가지로 분류할 수 있습니다. 첫 번째는 gradient-based optimization으로 policy gradient method가 이에 속합니다. 두 번째는 gradient-free optimization으로 진화(evolutionary) 연산을 이용하는 것이 이에 속합니다. 이 논문에서는 제목에서 알 수 있듯이 gradient-based optimization을 다룹니다. gradient는 변화량을 의미합니다. 특정 policy x와 또 다른 policy y가 있을 때 policy가 얼마나 많이 변화했는지 어떻게 모델링할 수 있을까요? policy의 변화를 제어하는 어떤 파라미터가 있다면 이 파라미터를 조정하여 policy를 변화시킬 수 있습니다. policy x에 해당하는 파라미터값과 policy y에 해당하는 파라미터값의 차이가 policy의 변화량이라고 할 수 있습니다. 이와 같은 방식으로 policy의 변화를 모델링하기 위하여 파라미터 $\\theta$를 이용하여 policy를 $\\pi_{\\theta}$로 표현할 수 있습니다. 최적의 policy를 찾기 위하여 expected return $E\\left[R|\\theta\\right]$이 최대화되도록 parameter를 조정합니다. 이를 간단한 수식으로 정리하면 다음과 같습니다.\n\n$$\n\\pi^* = \\arg \\max\\limits_\\pi  E\\left[ {\\left. R \\right|\\pi } \\right] \\to {\\text{original problem} } \n\\\\\\\\  {\\Downarrow} \\\\\\\\\n{\\text{policy parameterization by } } \\pi_{\\theta}: \\Theta  \\to \\Pi \\\\\\\\ {\\Downarrow} $$\n\n$$\\pi^* = \\arg \\max\\limits_{\\theta}  E\\left[ {\\left. R \\right|\\theta } \\right] \\to {\\text{policy search problem} }\n$$\n\n<br>\n## 1.3 How to Obtain the Expected Return\nexpected return을 최대화하는 방향으로 policy를 update한다고 하였습니다. 그렇다면 expected return은 어떻게 구할 수 있을까요? 여기에도 크게 두 가지 방법이 있습니다. 첫 번째는 deterministic approximation으로 Markov decision process의 dynamics를 모델링한 후 수식을 통해 구하는 것입니다. 두 번째 방법은 monte carlo estimation으로 dynamics에 대한 모델을 하지 않고 많은 sample들을 얻은 후 empirical하게 expected return을 계산하는 방법입니다. 어느 방법이 더 좋을지는 풀고자 하는 문제의 특성에 따라 다를 것입니다. dynamics에 대한 모델이 어렵거나 변화가 큰 경우에는 두 번째 방법이 좀 더 현실적이지만 gradient를 구하는 것은 더 어렵습니다. 결국 gradient를 esimate하는방법을 고안해야 합니다. 가장 유명한 gradient estimation 방법이 1992년 R. J. Williams에 의해서 제안된 [REINFORCE](https://link.springer.com/content/pdf/10.1007/BF00992696.pdf) 기법입니다. REINFORCE는 Monte Carlo estimate 또는 likelihood-ratio estimate라고 부르는 방법을 이용합니다. 이 방법에 대해서 좀 더 알아볼까요?\n\n### 1.3.1 Monte Carlo Gradient Estimation\n다음과 같은 parameter $\\theta$를 가지는 random variable $X$가 있습니다: $X:\\Omega\\mapsto\\mathcal{X}$. 그리고 이 $x$에 대한 함수 $f$가 있습니다: $f:\\mathcal{X}\\mapsto\\mathbb{R}$. expected return처럼 $E[f(x)]$를 최대화하고자 합니다. 이를 위해서는 $\\nabla_{\\theta}E[f(x)]$를 구해야 합니다. 이 때 log derivate trick을 이용하여 다음과 같이 수식을 변형시킬 수 있습니다.\n\n$$\n\\begin{align}\n\\nabla_{\\theta} E_{p(x;\\theta)}\\left[ {f(x)} \\right] \n&= \\nabla_\\theta\\int{f\\left( x \\right)p\\left( {x;\\theta} \\right) dx}\n\\\\\\\\\n&= \\int { {\\nabla_\\theta }p\\left( {x;\\theta } \\right)f\\left( x \\right)dx} \n\\\\\\\\\n&= \\int {\\frac{ {p\\left( {x;\\theta } \\right)} }{ {p\\left( {x;\\theta } \\right)} }{\\nabla_\\theta }p\\left( {x;\\theta } \\right)f\\left( x \\right)dx} \n\\\\\\\\\n&= \\int {p\\left( {x;\\theta } \\right){\\nabla_\\theta }\\log p\\left( {x;\\theta } \\right)f\\left( x \\right)dx} \n\\\\\\\\\n&= E_{p(x;\\theta)}[f(x)\\nabla_\\theta \\log p({x;\\theta})]\n\\end{align}\n$$\n\n이제 expectation은 많은 샘플들을 모아서 평균을 취하는 Monte Carlo 기법으로 근사화할 수 있습니다.\n\n$${\\nabla_{\\theta} }{E_{p\\left( {x;\\theta } \\right)} }\\left[ {f\\left( x \\right)} \\right] = \\frac{1}{N}\\sum_{n=1}^N f\\left(X_n\\right)\\nabla_\\theta\\log p\\left(X_n;\\theta\\right)$$\n\n이 방법을 사용하기 위해서 필요한 조건은 $\\log p\\left(X_n;\\theta\\right)$가 미분가능해야 한다는 것 뿐입니다. 그러나 이 방법은 얻은 샘플들에 의존하기 때문에 경우에 따라서 큰 variance를 가질 수 있습니다.\n\n<br><br>\n\n# 2. Policy Gradient Methods\n\n이제 본격적으로 policy gradient 기법에 대해서 알아보겠습니다.\n\n<br>\n## 2.1 System Model\n\n논문에서 사용하는 수학기호(notation)와 가정들을 설명하는 시스템모델은 다음과 같습니다.\n\n* Markov decision process (MDP) at each time $t\\in{0,1,2,...}$\n* $S_t\\in\\mathcal{S}$: state\n* $A_t\\in\\mathcal{A}$: action\n* $r_t\\in \\mathcal{R}$: reward\n* $\\theta\\in\\mathbb{R}^l$: vector of policy parameters, for $l\\ll\\left|\\mathcal{S}\\right|$\n* $\\mathcal{P} _{s s'}^a = \\Pr[S _{t+1}=s' \\vert S_t=s,A_t=a]$: state transition probabilities\n* $\\mathcal{R} _{s s'}^a = E[R _{t+1}\\vert S_t=s,A_t=a]$: expected reward\n* $\\pi(s,a,\\theta)=\\Pr[A_t=a|S_t=s,\\theta]$: policy, shortened as $\\pi(s,a)$ or $\\pi(a|s)$\n* $\\theta$: policy parameter들의 vector  \n* $\\rho$: 해당 policy들의 성능을 나타내는 척도 (예. average reward per step)  \n* $\\alpha$: positive-definite한 step size \n\n<br>\n## 2.2 Policy Gradent Approach\npolicy gradient는 stochastic policy를 자체적인 파리미터를 가진 function approximator를 이용해서 근사화시킵니다.\n\n\npolicy parameter는 gradient에 비례하여 업데이트됩니다.\n\n$$\\Delta\\theta \\approx \\alpha\\frac{\\partial\\rho}{\\partial\\theta}$$\n\n$\\rho$는 $\\theta$에 대하여 미분가능해야겠죠? 위와 같이 업데이트하면 local optimal policy로 수렴합니다. 논문의 가장 중요한 contribution은 특정 조건을 만족하는 function approximator를 이용하여 경험(experience, sample)을 축적하고 이것들을 이용하여 위의 gradient를 unbiased estimate할 수 있음을 증명한 것입니다.\n\n먼저 $\\rho$, 즉, reward를 표현하는 두 가지 방법에 대해서 알아보겠습니다.\n\n<br>\n## 2.3 Average Reward Formulation\nAverage reward formulation은 시간의 흐름에 따른 reward를 표현한다기보다는 모든 시간의 reward를 평균을 내서 표현하는 방법입니다.\n\n* Long‐term expected reward per step\n$$\\rho(\\pi)=\\lim_{n\\to\\infty}\\frac{1}{n}E[r_1+r_2+\\cdots+r_n|\\pi] = \\sum_s d^\\pi(s)\\sum_a\\pi(s,a)\\mathcal{R}_s^a$$\n\n    - $d^\\pi(s): \\lim_{t\\to\\infty}\\Pr[S_t=s|s_0,\\pi]$: stationary distribution of states under $\\pi$\n    - $d^\\pi(s)$가 존재한다고 가정합니다. 그리고 모든 policy에 대해서 $d^\\pi(s)$는 $s_0$와 independent합니다.\n    - 위의 (limit수식과 summation수식 사이의) 등식은 왜 성립할까요? Time average와 Ensemble average가 같다는 뜻으로 [ergodic](https://en.wikipedia.org/wiki/Ergodicity)한 system에서 성립합니다.\n\n* Value of a state-action pair given a policy\n$$Q^{\\pi(s,a)} = \\sum_{t=1}^\\infty E[r_t - \\rho(\\pi)|S_0=s, A_0=a, \\pi]$$\n    - 위의 Q-function의 표현식이 정의인지 유도된 것인지는 이 포스트를 작성하는 저희도 아직 확실히 모르겠습니다. 다만, $r_t$들을 더하면 무한대로 발산할 가능성이 매우 높은데 $\\rho$를 빼줌으로해서 어찌보면 평균 reward로부터의 차이를 더하는 것이라고 할 수 있고 이것은 bound된 값일 가능성이 더 높기 때문에 Q-function을 이렇게 표현하는 것이 더 안전하다고 할 수 있습니다. 이 부분에 대해서 좋은 의견이 있으시면 피드백을 주시면 감사하겠습니다!\n\n* State-value function\n$$V^\\pi(s) = \\sum_{a} \\pi(s,a)Q^\\pi(s,a)$$\n\n<br>\n## 2.4 Start-State Formulation\nstart-state formulation은 시간의 흐름에 따라 감소하는 reward들을 표현합니다.\n\n* Long-term expected reward per step with a designated start state $S_0$\n$$\\rho(\\pi) = E\\left[\\left.\\sum_{t=1}^\\infty \\gamma^{t-1}r_t\\right|S_0,\\pi\\right]$$\n    - $\\gamma\\in\\left[0,1\\right]$: discount rate\n\n* Value of a state‐action pair given a policy\n$$Q^\\pi(s,a) = E\\left[\\left.\\sum_{k=1}^\\infty \\gamma^{k-1} r_{t+k}\\right|S_t=s, A_t=a, \\pi\\right]$$\n\n* Alternative form of $Q^\\pi(s,a)$\n$$\nQ^\\pi(s,a) = R_s^a + \\gamma\\sum _{s'} \\mathcal{P} _{s s'}^a V^{\\pi}(s')\n$$\n\n<br>\n## 2.5 Policy Gradient Theorem\n논문의 중요한 결과인 Theorem 1은 다음과 같습니다.\n\n**Theorem 1 (Policy Gradient)** *For any MDP, in either the average‐reward or start‐state formulations,*\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)$$\n\n놀라운 부분은 expected return의 gradient를 취할 때 $\\frac{\\partial d^\\pi(s)}{\\partial\\theta}$를 구하지 않아도 된다는 점입니다. 즉, policy의 변화가 state distribution에 영향을 주지 않는다는 것입니다. 이것은 다시 말하면 아래 수식과 같이 우리는 $\\frac{\\partial\\rho}{\\partial\\theta}$에 대한 unbiased estimator를 구할 수 있다는 뜻입니다.\n\n$$E_s\\left[\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)\\right] = \\frac{\\partial\\rho}{\\partial\\theta}$$\n\n이로인해서 우리는 gradient를 sampling을 통해서 추정할 수 있습니다. 그렇지만 이것은 sample이 아주 많을 때만 성립합니다!\n\n또 한 가지 문제는 우리는 $Q^\\pi(s,a)$의 정확한 값을 알 수 없다는 것 입니다. 이것을 estimate하기 위해서 현재의 return값 $R_t$를 이용할 수 있습니다.\n\n<br>\n## 2.6 Proof of Policy Gradient Theorem\n증명을 살펴보겠습니다.\n\n* average-reward formulation을 이용할 때의 증명은 아래와 같습니다.\n\n$$\n\\begin{align}\n\\frac{\\partial V^\\pi(s)}{\\partial\\theta} &\\overset{\\underset{\\mathrm{def} }{} }{=}\\frac{\\partial}{\\partial\\theta}\\sum_a \\pi(s,a)Q^\\pi(s,a)\n\\\\\\\\\n&=\\sum_a \\frac{\\partial}{\\partial\\theta}\\left[\\pi(s,a)Q^\\pi(s,a)\\right]\\\\\\\\\n&=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\frac{\\partial Q^\\pi(s,a)}{\\partial\\theta}\\right]\\\\\\\\\n&=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\frac{\\partial}{\\partial\\theta}\\left[R_s^a -\\rho(\\pi) + \\sum_{s'}\\mathcal{P}_{s s'}^a V^{\\pi}(s')\\right]\\right]\\\\\\\\\n&=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\left[-\\frac{\\partial\\rho}{\\partial\\theta} + \\sum_{s'}\\mathcal{P}_{s s'}^a \\frac{\\partial V^{\\pi}(s')}{\\partial\\theta}\\right]\\right]\\\\\\\\\n\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\sum_{s'}\\mathcal{P}_{s s'}^a \\frac{\\partial V^{\\pi}(s')}{\\partial\\theta}\\right] - \\frac{\\partial V^\\pi(s)}{\\partial\\theta}\n\\end{align}\n$$\n\n양변에 stationary distribution에 대한 평균을 취합니다.\n$$\n\\begin{align}\n\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+ \\sum_s d^\\pi(s)\\sum_a\\pi(s,a)\\sum_{s'}\\mathcal{P}_{s s'}^a \\frac{\\partial V^{\\pi}(s')}{\\partial\\theta} \n\\\\\\\\\n&- \\sum_s d^\\pi(s)\\frac{\\partial V^\\pi(s)}{\\partial\\theta}\n\\end{align}\n$$\n\n모든 state $s$에서 모든 state $s'$으로 이동하는 것은 state $s'$에 존재할 확률이므로 다음과 같이 표현할 수 있습니다.\n\n$$\n\\begin{align}\n\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+ \\sum_s d^\\pi(s') \\frac{\\partial V^{\\pi}(s')}{\\partial\\theta} - \\sum_s d^\\pi(s)\\frac{\\partial V^\\pi(s)}{\\partial\\theta}\n\\end{align}\n$$\n\n뒤의 두 항은 같은 식이므로 0이 됩니다. 따라서 다음과 같이 표현됩니다.\n$$\n\\begin{align}\n\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)\n\\end{align}\n$$\n\n$\\frac{\\partial\\rho}{\\partial\\theta}$는 reward의 파라미터에 대한 미분값입니다. reward 자체가 이미 모든 state에 대한 평균값이므로 $s$의 함수가 아닙니다.  $s$에 대한 함수가 아니므로 $\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}=\\frac{\\partial\\rho}{\\partial\\theta}\\sum_s d^\\pi(s)=\\frac{\\partial\\rho}{\\partial\\theta}$입니다. 여기서 $d^\\pi(s)$는 state에 대한 확률값이죠. 모든 state에 대해서 이 값을 다 더하면 1이 됩니다.\n이를 이용해서 위의 식을 다시 표현하면 아래와 같고 이를 통해 증명이 완성됩니다.\n\n$$\n\\begin{align}\n\\therefore\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)\n\\end{align}\n$$\n\n* 다음으로 start-state formulation에 대한 증명입니다.\n\nstart-state formulation은 이 논문에서도 나오고 서튼책에서도 나오는데, 서튼책의 증명이 더 설명이 자세합니다. 그래서 논문에 있는 증명이 아닌 서튼책에 있는 증명으로 설명드리겠습니다. 다만, notation이 약간 다른데, 큰 문제 없이 이해하실 수 있으므로 별도의 설명은 하지 않겠습니다. (서튼책 [Link](https://drive.google.com/file/d/1xeUDVGWGUUv1-ccUMAZHJLej2C7aAFWY/view))\n\n$$\\nabla{v_\\pi}(s)=\\nabla\\big[\\sum_a\\pi(a|s)q_\\pi(s,a)\\big]$$ \n\n다음으로 $(f\\cdot g)'=f'\\cdot g+f\\cdot g'$ (product rule of calculus)에 의해 다음과 같이 나타낼 수 있습니다.\n$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\nabla q_\\pi(s,a)\\big]$$  \n\n이어서 $p(s',r|s,a) := Pr[S_t=s', R_t=r|S_{t-1}=s,A_{t-1}=a ]$ (서튼책 3.2 공식) 에 의해 다음과 같이 나타낼 수 있습니다.\n\n$$=\\sum_a\\big[\\nabla\\pi(a|s)q_{\\pi}(s,a)+\\pi(a|s)\\nabla\\big[\\sum_{s', r}p(s',r|s,a)(r+v_{\\pi}(s'))\\big]\\big]$$  \n\n계속해서 $p(s'|s,a)=\\sum_{r\\in R}p(s',r|s,a)$ (서튼책 3.4 공식) 에 의해 다음과 같이 나타낼 수 있습니다.\n$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\sum_{s'}p(s'|s,a)\\nabla v_\\pi(s') \\big]$$ \n\n여기서 $\\nabla v_\\pi(s')$ 부분을 unrolling을 하면 다음과 같이 나타낼 수 있습니다.\n$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\sum_{s'}p(s'|s,a)\\sum_{a'}\\big[\\nabla\\pi(a'|s')q_\\pi(s',a')\\\\\\\\+\\pi(a'|s')\\sum_{s''}p(s''|s',a')\\nabla v_\\pi(s''))\\big] \\big]$$ \n\n$\\nabla v_\\pi(s'')$에 대해서 unrolling을 하고 또 나오는 다른 항에 대해서도 계속 반복하다보면 다음과 같이 나타낼 수 있습니다.\n$$=\\sum_{x\\in S}\\sum_{k=0}^{\\infty}\\Pr(s\\rightarrow x,k,\\pi)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$\n\n여기에서 $\\Pr(s\\rightarrow x,k,\\pi)$ 는 policy $\\pi$ 에 대해 state $s$에서 state $x$까지 $k$ step만큼 움직일 때의 변환 확률입니다. 따라서 위의 수식을 아래처럼 다시 나타낼 수 있습니다.\n <!-- \\begin{align}  -->\n$$\\nabla J(\\theta)=\\nabla v_\\pi (s_0)$$ \n$$=\\sum_s(\\sum_{k=0}^{\\infty}\\Pr(s_0\\rightarrow s,k,\\pi))\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$\n$$=\\sum_s \\eta(s) \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$ \n<!-- \\end{align} -->\n- 서튼책에서 $\\eta(s)$는 다음과 같이 표현할 수 있습니다.\n    - $\\eta(s)$ = 평균적으로 하나의 episode마다 state에서 머무른 time step의 수 (page 199)\n- 논문에서는 $\\eta(s)$를 다음과 같이 표현합니다.\n    - $d^\\pi(s)=\\sum_{t=0}^{\\infty}\\gamma^t Pr(s_t=s|s_0,\\pi)$ \n    ($\\gamma=1$ is allowed only in episodic tasks = discounted weighting of states)\n\n여기서부터가 논문에는 없는 내용입니다. 위의 최종 수식을 다시 한 번 적으면 아래와 같습니다.\n$$\\sum_s \\eta(s) \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$\n\n위의 수식을 아래와 같이 바꿀 수 있습니다.\n$$\\sum_{s'} \\eta(s') \\cdot\\sum_s \\frac{\\eta(s)}{\\sum_{s'}\\eta(s')} \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$\n\n그러면 이것을 새로운 기호로 다시 나타낼 수 있습니다.\n$$\\sum_{s'} \\eta(s') \\cdot\\sum_s \\mu(s)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$ \n- 여기서 $\\mu(s)$ 는 state distribution이라고 하며, weight를 state distribution으로 바꿔주는 과정이라고 생각할 수 있습니다. 쉽게 말해 어떠한 state에 agent가 머무르는 확률입니다.\n\n위의 수식은 아래의 수식과 비례합니다. 따라서 최종 형태는 다음과 같습니다.\n$$\\propto \\sum_s \\mu(s)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$\n\n<br><br>\n\n# 3. Policy Gradient with Approximation\n이 장에서는 앞서 다뤘던 Theorem 1 중에 $Q^\\pi$에 대해서 중점적으로 다룹니다. 어떠한 $Q^\\pi$가 학습된 function approximator로 근사된다고 합시다. \n\n$f_w$ ($f_w$는 $S × A \\rightarrow \\Re$)는 parameter $w$를 가지는 어떠한 $Q^\\pi$ 에 대한 근사치입니다. 학습된 function approximator에 대해서 생각하기 때문에, 학습된 $f_w$는 error를 최소화하는 방향으로 parameter $w$를 업데이트합니다. 그러면 다음과 같은 관계식을 생각할 수 있습니다.\n$$\\Delta w_t \\propto \\frac{\\partial}{\\partial w}\\big[ {\\hat Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]^2 \\propto \\big[{\\hat Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s_t,a_t)}{\\partial w}$$ \n(여기서 ${\\hat Q}^\\pi(s_t,a_t)$ 는 $Q^\\pi(s_t,a_t)$의 unbiased estimator $R_t$입니다.)\n\n그러면 위와 같은 수식이 local optimum에 수렴을 했을 때, 다음과 같이 나타낼 수 있습니다.\n$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$\n\n- 위의 수식에 대한 추가 설명\n    - local optimum으로 수렴을 하기 때문에 당연히 위의 수식은 0이 됩니다.\n    - 또한 stochastic policy이기 때문에 local optimum으로 수렴하려면 모든 state, action에 대해 expectation을 해야합니다. 따라서 $\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)$이 붙게 됩니다.\n\n<br>\n## 3.1 Theorem 2: Policy Gradient with Function Approximation\n\n만약 $f_w$가 아래의 등식을 만족한다고 합시다.\n$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$\n\n또한 $f_w$가 policy parameterization과 양립할 수 있다고 합시다. policy parameterization에 대한 여러가지 의미가 있는데 이 논문에서는 다음을 의미합니다.\n$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}$$\n- 위의 수식에 대한 추가 설명\n    - 이 수식은 Monte-Calro Estimation을 이용하기 위한 조건입니다.\n    - Compatibility Condition이라고 부릅니다.\n    - 우변(= $\\nabla\\log\\pi(s,a)$)으로 유도함으로써 아래의 수식과 같이 sampling을 이용한 추정이 가능해집니다.\n\n따라서 위의 두 수식을 이용하여 아래와 같이 나타낼 수 있습니다.\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}f_w(s,a)$$\n\n<br>\n## 3.2 Proof of Theorem 2\n\n$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$\n\n$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}$$\n\n위의 두 수식을 합치면 다음과 같이 나타낼 수 있습니다.\n$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}=0$$\n\n분자와 분모에 있는 $\\pi(s,a)$를 지우면 아래와 같습니다.\n$$\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]=0$$\n\n이어서 위의 수식이 0이기 때문에 policy gradient theorem(앞서 다뤘던 Theorem 1)에서 위의 수식을 뺄 수 있습니다.\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)-\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ Q^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]$$\n\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ Q^\\pi(s_t,a_t)-Q^\\pi(s_t,a_t)+f_w(s_t,a_t) \\big]$$\n\n$$\\therefore\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}f_w(s_t,a_t)$$\n\n<br><br>\n\n# 4. Application to Deriving Algorithms and Advantages\n\n<br>\n## 4.1 Application to Deriving Algorithms\nfeature의 linear combination에서 Gibbs distribution (softmax)를 하나의 policy로 생각해볼 수 있습니다.\n$$\\pi(s,a)=\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_b e^{\\theta^{T}\\phi_{sb} }}$$\n- $\\phi_{sa}$: state-action pair $s$, $a$를 나타내는 $l$-dimensional feature vector\n\ncompatible condition을 적용하면 다음과 같습니다. (4.2 Proof of application of compatible condition 참고)\n$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\frac{1}{\\pi(s,a)}=\\phi_{sa}-\\sum_b \\pi(s,b)\\phi_{sb}$$ \n\n이어서 $f_w$을 적분한 natural parameterization은 다음과 같습니다.\n$$f_w(s,a)=w^T\\big[ \\phi_{sa}-\\sum_b \\pi(s,b)\\phi_{sb}\\big]$$\n\n- 위의 수식에 대한 추가 설명\n    - $f_w$는 policy로서 같은 feature들에 대해 linear합니다.\n    - $f_w$는 각각의 state에 대해서 평균이 0입니다. ($\\sum_a\\pi(s,a)f_w(s,a)=0$)\n    - advantage function $A^\\pi(s,a)=Q^\\pi(s,a)-V^\\pi(s)$의 하나의 근사치로서 $f_w$를 생각해도 좋습니다.\n\n<br>\n## 4.2 Proof of application of compatible condition\n증명을 하기 전에 다음을 가정합니다.\n1. $(\\frac{f(x)}{g(x)})'=f'(x)\\cdot \\frac{1}{g(x)}-f(x)\\cdot \\frac{g'(x)}{g(x)^{2} }=\\frac{f'(x)\\cdot g(x)-f(x)\\cdot g'(x)}{g(x)^{2} }$\n2. $f(\\theta)=e^{\\theta^{T}\\phi_{sa} }$, $g(\\theta)=\\sum_be^{\\theta^{T}\\phi_{sb} }$\n\n$$\n\\begin{align}\n\\frac{\\partial f_w(s,a)}{\\partial w}&=\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\frac{1}{\\pi(s,a)}\n\\\\\\\\\n&=\\big(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }}\\big)'\\frac{1}{\\pi(s,a)}\n\\\\\\\\\n&=\\big[ \\phi_{sa}(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }})-(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }})\\frac{\\sum_b\\phi_{sb}e^{ {\\theta^T}\\phi_{sb} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }}\\big]\\frac{1}{\\pi(s,a)}\\\\\\\\\n&=\\big[ \\phi_{sa}\\pi(s,a)-\\pi(s,a)\\sum_b\\phi_{sb}\\pi(s,b)\\big]\\frac{1}{\\pi(s,a)}\n\\\\\\\\\n&=\\pi(s,a)\\big[ \\phi_{sa}-\\sum_b\\pi(s,b)\\phi_{sb}\\big]\\frac{1}{\\pi(s,a)}\n\\\\\\\\\n&\\therefore\\phi_{sa}-\\sum_b\\pi(s,b)\\phi_{sb}\n\\end{align}\n$$\n\n<br>\n## 4.3 Application to Advantages\nPolicy Gradient with Function Approximation Theorem (Theorem 2)는 advantage function으로 확장될 수 있습니다.\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_sd^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ f_w(s,a)+v(s) \\big]$$\n- 위의 수식에 대한 추가설명\n    - $v$ ($v$는 $S\\rightarrow\\Re$) 는 arbitrary function입니다.\n    - 이 수식은 $\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}=0$이기 때문에 가능해집니다.\n    - 즉, 이 수식은 $\\pi(s,a)$의 gradient에만 dependent하기 때문에 advantage 역할을 하는 함수들을 넣어도 아무런 상관이 없습니다.\n    - $v$의 선택은 Theorem들에 영향을 미치지 못하지만, 실질적으로 gradient estimator의 variance에 영향을 미칩니다.\n    - 이러한 문제는 전체적으로 이전의 연구에 reinforcement baseline의 사용에 있어서 유사합니다.\n    - (comment) 위의 수식에서 $f_w(s,a)+v(s)$와 Application to Deriving Algorithms의 $f_w(s,a)$와는 다른 것입니다. Application to Deriving Algorithms의 $f_w(s,a)$은 softmax에 의해 스스로 advantage function의 역할을 할 수 있습니다. 하지만 보통의 경우에는 그러지 못할 수도 있기 때문에 위의 수식처럼 $f_w(s,a)+v(s)$을 추가하여 zero mean만들어서 variance를 줄일 수 있습니다.\n\n<br><br>\n\n# 5. Convergence of Policy Iteration with Function Approximation\n\n<br>\n## 5.1 Theorem 3: Convergence of Policy Iteration with Function Approximation\n\npolicy iteration with function approximation은 locally optimal policy에 수렴합니다. $\\pi$와 $f_w$를 \n\n1. compatibility condition을 만족하는 policy와 value function에 대한\n2. 그리고 $\\max_{\\theta, s, a, i, j}\\big| \\frac{\\partial^2\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}\\big|<B<\\infty$에 대한\n\n어떠한 미분가능한 function approximator라고 합시다.\n\n- (comment) $\\max_{\\theta, s, a, i, j}\\big| \\frac{\\partial^2\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}\\big|<B<\\infty$에서 $\\pi(s,a)$라는 function은 이계미분값이 존재하고, 임의의 상수인 (Bound) B에 bound되어 있기 때문에 function의 그래프는 smooth하다고 볼 수 있습니다. (아래의 그림 중 빨간색 그래프 참고)\n\n<center> <img src=\"https://www.dropbox.com/s/xx02ejfg5ao19ps/Screen%20Shot%202018-07-10%20at%203.41.06%20PM.png?dl=1\" width=\"200\"> </center>\n\n이어서 ${\\alpha_k}$는 $\\lim_{k\\rightarrow\\infty}\\alpha_k=0$이며 $\\sum_k\\alpha_k=\\infty$를 만족하는 step-size sequence라고 합시다.\n\n그 때, bounded reward를 가진 MDP에 대해\n1. 어떠한 $\\theta_0, \\pi_k=\\pi(\\cdot,\\cdot,\\theta_k)$\n2. 그리고 $\\sum_sd^{\\pi_{k} }(s)\\sum_a\\pi_k(s,a)\\big[ Q^{\\pi_{k} }(s,a)-f_w(s,a)\\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$로 인하여 $w_k=w$, $\\theta_{k+1}=\\theta_k+\\alpha_k\\sum_sd^{\\pi_{k} }(s)\\sum_a\\frac{\\partial\\pi_k(s,a)}{\\partial\\theta}f_{w_{k} }(s,a)$\n\n으로 정의된 sequence $\\rho(\\pi_k)$는 $\\lim_{k\\rightarrow\\infty}\\frac{\\partial\\rho(\\pi_k)}{\\partial\\theta}=0$이기 때문에 수렴합니다.\n- sequence $\\rho(\\pi_k)_{k=0}^\\infty$에 대한 추가 설명\n    - 2번의 식을 통해 자연스럽게 actor-critic으로 연결됩니다.\n    - $\\theta_{k+1}=\\theta_k+\\alpha_k\\sum_sd^{\\pi_{k} }(s)\\sum_a\\frac{\\partial\\pi_k(s,a)}{\\partial\\theta}f_{w_{k} }(s,a)$ 에 따라 $\\theta$가 1, 2, ..., $\\infty$로 갈텐데, 거기에 따른 objective function or performance의 sequence입니다.\n    - (comment) 굳이 sequence라는 표현이 없어도 될 것 같습니다. 어짜피 k가 $\\infty$로 가면 $\\rho(\\pi_k)$가 수렴한다는 의미이기 때문에 불필요해보입니다.\n\n<br>\n## 5.2 Proof of Theorem 3\n- Theorem 2는 $\\theta_k$ update가 gradient의 error를 최소화한다는 것을 증명했습니다.\n- $\\frac{\\partial^{2}\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}$와 MDP의 reward에서의 bound는, $\\frac{\\partial^{2}\\rho}{\\partial\\theta_i\\partial\\theta_j}$ 또한 bound된다는 것을 증명합니다.\n- step-size 필요조건 때문에 이러한 bound된 것들은 Proposition 3.5 from page 96 of Bertsekas and Tsitsiklis (1996)에 적용하기 위해 필요한 조건입니다.\n- Proposition 3.5 from page 96 of Bertsekas and Tsitsiklis (1996)은 local optimum으로 수렴한다는 것을 증명했습니다.\n- Proposition 3.5를 자세하게 설명하지는 않겠습니다. 일반적인 gradient method의 수렴성을 증명한 것이며, 이 논문의 policy gradient method도 gradient method의 일종이므로 특정 조건을 만족할 때 수렴한다는 것입니다. 수렴하는 원리는 소위 말하는 Lipschitz continuity condition을 만족하기 때문인데 다음과 같습니다. $L>0$인 임의의 상수입니다.\n$$\n\\parallel \\nabla f(r) - \\nabla f(\\bar{r}) \\parallel \\leq L \\parallel r - \\bar{r} \\parallel\n$$\n즉, 업데이트가 진행되어 나감에 따라서 gradient의 차이가 점점 줄어들게 되므로 언젠가는 0으로 수렴하게 됩니다.\n\n<br><br>\n\n# 6. Summary \n\n논문에서 설명한 policy gradient 기법을 요약하면 다음과 같습니다.\n\n* Original maximization problem: $\\max\\rho_{\\theta}=E\\left[R\\right]$\n* gradient ascent method를 이용한 parameter update: $\\theta_{t+1} = \\theta_{t} + \\alpha\\left.\\frac{\\partial E[R]}{\\partial\\theta}\\right|_{\\theta_t}$\n    - 그러나 우리는 gradient를 모르고, 추정하기도 어렵습니다. 왜냐하면 expectation이 안에 있기 때문입니다.\n    - 그렇다면 이것을 추정 가능한 형태로 바꿔야합니다. 다시 말해 expectation이 밖에 있는 형태로 바꾸는 것입니다.\n    - Expectation이 밖에 있으면 왜 추정이 유리할까요? 바로 Sample mean을 취하면 되기 때문입니다.\n* Approximate gradient\n    - 방법1: 논문의 Theorem 1을 이용하면 gradient와 expectation의 위치를 바꿀 수 있습니다.\n    $$\\frac{\\partial E[R]}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)$$\n    - 방법2: Log derivative trick을 이용하면 gradient와 expectation의 위치를 바꿀 수 있습니다. (Theorem 2)\n    $$\\theta_{t+1} = \\theta_{t} + \\alpha E\\left[ \\left. R\\frac{\\partial}{\\partial\\theta}\\log p_{\\theta} \\right\\vert \\theta_t\\right]$$\n    - 그리고 특정 trajectory를 따라가면서 return값을 구하고 이것을 여러 번 수행하여 sample mean을 취하면 gradient를 추정하는 것이 가능합니다.\n    - 이러한 기법을 제시한 기존의 연구가 REINFORCE입니다.\n    - 여러 Trajectory를 이용하므로 variance가 높을 수 밖에 없습니다.\n    - Advantage를 활용하거나 하는 방식으로 이후 여러 연구가 진행되었습니다.\n\n<br><br>\n\n# 처음으로\n\n## [PG Travel Guide](https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/)\n\n<br>\n\n# 다음으로\n\n## [DPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/27/2_dpg/)","slug":"1_sutton-pg","published":1,"updated":"2018-09-08T11:00:20.605Z","_id":"cjkj6tyru000hj715be3owc0l","comments":1,"layout":"post","photos":[],"link":"","content":"<center> <img src=\"https://www.dropbox.com/s/83i4atz733ali0w/Screen%20Shot%202018-07-18%20at%2012.10.52%20AM.png?dl=1\" width=\"600\"> </center>\n\n<p>논문 저자 : Richard S. Sutton, David McAllester, Satinder Singh, Yishay Mansour<br>논문 링크 : <a href=\"http://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf\" target=\"_blank\" rel=\"noopener\">NIPS</a><br>Proceeding : Advances in Neural Information Processing Systems (NIPS) 2000<br>정리 : 김동민, 이동민</p>\n<hr>\n<h1 id=\"1-Intro-to-Policy-Gradient\"><a href=\"#1-Intro-to-Policy-Gradient\" class=\"headerlink\" title=\"1. Intro to Policy Gradient\"></a>1. Intro to Policy Gradient</h1><p>이 논문은 policy gradient (PG) 기법의 효시와도 같으며 향후 많은 파생연구를 낳은 중요한 논문입니다. 7페이지의 짧은 논문이지만 읽기에 만만한 논문은 아닙니다. 이 논문을 이해하기 위해 필요한 배경지식을 먼저 설명하고 논문을 차근차근 살펴보도록 하겠습니다.</p>\n<p><br></p>\n<h2 id=\"1-1-Value-Function-Approach\"><a href=\"#1-1-Value-Function-Approach\" class=\"headerlink\" title=\"1.1 Value Function Approach\"></a>1.1 Value Function Approach</h2><p>전통적으로 강화학습 기법은 value function을 기반으로 동작하였습니다. 특정 state에서 vaue function 또는 value function을 근사하는 함수(function approximation)를 최대화하는 action을 찾는 greedy action-selection policy가  대표적입니다.</p>\n<p>논문에서는 이러한 방법은 deterministic한 policy를 찾는 쪽으로 나아가게 되지만 종종 최적의 policy는 stochastic한 성질을 가지기 때문에 이 방법으로는 최적의 policy를 찾을 수 없다고 언급하고 있습니다. (그러나 이 논문이 나오고 나서 한참 후 David Silver를 필두로한 DeepMind의 연구진들은 high-dimensional action space를 가지는 application에서 보다 빠르게 동작하는 <a href=\"../../../06/27/2_dpg/\">deterministic policy gradient</a>을 개발하였습니다.) 아마도 이러한 부분은 $\\epsilon$-greedy action-selection method로 개선될 수 있을 것입니다. 또 하나의 문제는 value function의 작은 변화로 인해서 action이 크게 변할 수도 있다는 것입니다. 이것은 알고리듬의 수렴성에 문제를 야기할 수 있습니다. 이러한 문제점을 해결하기 위하여 policy search라는 새로운 기법이 고안됩니다.</p>\n<p><br></p>\n<h2 id=\"1-2-Policy-Search\"><a href=\"#1-2-Policy-Search\" class=\"headerlink\" title=\"1.2 Policy Search\"></a>1.2 Policy Search</h2><p>policy search는 최적의 policy $\\pi^*$를 reward로부터 직접 찾습니다. policy search 방법은 크게 두 가지로 분류할 수 있습니다. 첫 번째는 gradient-based optimization으로 policy gradient method가 이에 속합니다. 두 번째는 gradient-free optimization으로 진화(evolutionary) 연산을 이용하는 것이 이에 속합니다. 이 논문에서는 제목에서 알 수 있듯이 gradient-based optimization을 다룹니다. gradient는 변화량을 의미합니다. 특정 policy x와 또 다른 policy y가 있을 때 policy가 얼마나 많이 변화했는지 어떻게 모델링할 수 있을까요? policy의 변화를 제어하는 어떤 파라미터가 있다면 이 파라미터를 조정하여 policy를 변화시킬 수 있습니다. policy x에 해당하는 파라미터값과 policy y에 해당하는 파라미터값의 차이가 policy의 변화량이라고 할 수 있습니다. 이와 같은 방식으로 policy의 변화를 모델링하기 위하여 파라미터 $\\theta$를 이용하여 policy를 $\\pi_{\\theta}$로 표현할 수 있습니다. 최적의 policy를 찾기 위하여 expected return $E\\left[R|\\theta\\right]$이 최대화되도록 parameter를 조정합니다. 이를 간단한 수식으로 정리하면 다음과 같습니다.</p>\n<p>$$<br>\\pi^* = \\arg \\max\\limits_\\pi  E\\left[ {\\left. R \\right|\\pi } \\right] \\to {\\text{original problem} }<br>\\\\  {\\Downarrow} \\\\<br>{\\text{policy parameterization by } } \\pi_{\\theta}: \\Theta  \\to \\Pi \\\\ {\\Downarrow} $$</p>\n<p>$$\\pi^* = \\arg \\max\\limits_{\\theta}  E\\left[ {\\left. R \\right|\\theta } \\right] \\to {\\text{policy search problem} }<br>$$</p>\n<p><br></p>\n<h2 id=\"1-3-How-to-Obtain-the-Expected-Return\"><a href=\"#1-3-How-to-Obtain-the-Expected-Return\" class=\"headerlink\" title=\"1.3 How to Obtain the Expected Return\"></a>1.3 How to Obtain the Expected Return</h2><p>expected return을 최대화하는 방향으로 policy를 update한다고 하였습니다. 그렇다면 expected return은 어떻게 구할 수 있을까요? 여기에도 크게 두 가지 방법이 있습니다. 첫 번째는 deterministic approximation으로 Markov decision process의 dynamics를 모델링한 후 수식을 통해 구하는 것입니다. 두 번째 방법은 monte carlo estimation으로 dynamics에 대한 모델을 하지 않고 많은 sample들을 얻은 후 empirical하게 expected return을 계산하는 방법입니다. 어느 방법이 더 좋을지는 풀고자 하는 문제의 특성에 따라 다를 것입니다. dynamics에 대한 모델이 어렵거나 변화가 큰 경우에는 두 번째 방법이 좀 더 현실적이지만 gradient를 구하는 것은 더 어렵습니다. 결국 gradient를 esimate하는방법을 고안해야 합니다. 가장 유명한 gradient estimation 방법이 1992년 R. J. Williams에 의해서 제안된 <a href=\"https://link.springer.com/content/pdf/10.1007/BF00992696.pdf\" target=\"_blank\" rel=\"noopener\">REINFORCE</a> 기법입니다. REINFORCE는 Monte Carlo estimate 또는 likelihood-ratio estimate라고 부르는 방법을 이용합니다. 이 방법에 대해서 좀 더 알아볼까요?</p>\n<h3 id=\"1-3-1-Monte-Carlo-Gradient-Estimation\"><a href=\"#1-3-1-Monte-Carlo-Gradient-Estimation\" class=\"headerlink\" title=\"1.3.1 Monte Carlo Gradient Estimation\"></a>1.3.1 Monte Carlo Gradient Estimation</h3><p>다음과 같은 parameter $\\theta$를 가지는 random variable $X$가 있습니다: $X:\\Omega\\mapsto\\mathcal{X}$. 그리고 이 $x$에 대한 함수 $f$가 있습니다: $f:\\mathcal{X}\\mapsto\\mathbb{R}$. expected return처럼 $E[f(x)]$를 최대화하고자 합니다. 이를 위해서는 $\\nabla_{\\theta}E[f(x)]$를 구해야 합니다. 이 때 log derivate trick을 이용하여 다음과 같이 수식을 변형시킬 수 있습니다.</p>\n<p>$$<br>\\begin{align}<br>\\nabla_{\\theta} E_{p(x;\\theta)}\\left[ {f(x)} \\right]<br>&amp;= \\nabla_\\theta\\int{f\\left( x \\right)p\\left( {x;\\theta} \\right) dx}<br>\\\\<br>&amp;= \\int { {\\nabla_\\theta }p\\left( {x;\\theta } \\right)f\\left( x \\right)dx}<br>\\\\<br>&amp;= \\int {\\frac{ {p\\left( {x;\\theta } \\right)} }{ {p\\left( {x;\\theta } \\right)} }{\\nabla_\\theta }p\\left( {x;\\theta } \\right)f\\left( x \\right)dx}<br>\\\\<br>&amp;= \\int {p\\left( {x;\\theta } \\right){\\nabla_\\theta }\\log p\\left( {x;\\theta } \\right)f\\left( x \\right)dx}<br>\\\\<br>&amp;= E_{p(x;\\theta)}[f(x)\\nabla_\\theta \\log p({x;\\theta})]<br>\\end{align}<br>$$</p>\n<p>이제 expectation은 많은 샘플들을 모아서 평균을 취하는 Monte Carlo 기법으로 근사화할 수 있습니다.</p>\n<p>$${\\nabla_{\\theta} }{E_{p\\left( {x;\\theta } \\right)} }\\left[ {f\\left( x \\right)} \\right] = \\frac{1}{N}\\sum_{n=1}^N f\\left(X_n\\right)\\nabla_\\theta\\log p\\left(X_n;\\theta\\right)$$</p>\n<p>이 방법을 사용하기 위해서 필요한 조건은 $\\log p\\left(X_n;\\theta\\right)$가 미분가능해야 한다는 것 뿐입니다. 그러나 이 방법은 얻은 샘플들에 의존하기 때문에 경우에 따라서 큰 variance를 가질 수 있습니다.</p>\n<p><br><br></p>\n<h1 id=\"2-Policy-Gradient-Methods\"><a href=\"#2-Policy-Gradient-Methods\" class=\"headerlink\" title=\"2. Policy Gradient Methods\"></a>2. Policy Gradient Methods</h1><p>이제 본격적으로 policy gradient 기법에 대해서 알아보겠습니다.</p>\n<p><br></p>\n<h2 id=\"2-1-System-Model\"><a href=\"#2-1-System-Model\" class=\"headerlink\" title=\"2.1 System Model\"></a>2.1 System Model</h2><p>논문에서 사용하는 수학기호(notation)와 가정들을 설명하는 시스템모델은 다음과 같습니다.</p>\n<ul>\n<li>Markov decision process (MDP) at each time $t\\in{0,1,2,…}$</li>\n<li>$S_t\\in\\mathcal{S}$: state</li>\n<li>$A_t\\in\\mathcal{A}$: action</li>\n<li>$r_t\\in \\mathcal{R}$: reward</li>\n<li>$\\theta\\in\\mathbb{R}^l$: vector of policy parameters, for $l\\ll\\left|\\mathcal{S}\\right|$</li>\n<li>$\\mathcal{P} _{s s’}^a = \\Pr[S _{t+1}=s’ \\vert S_t=s,A_t=a]$: state transition probabilities</li>\n<li>$\\mathcal{R} _{s s’}^a = E[R _{t+1}\\vert S_t=s,A_t=a]$: expected reward</li>\n<li>$\\pi(s,a,\\theta)=\\Pr[A_t=a|S_t=s,\\theta]$: policy, shortened as $\\pi(s,a)$ or $\\pi(a|s)$</li>\n<li>$\\theta$: policy parameter들의 vector  </li>\n<li>$\\rho$: 해당 policy들의 성능을 나타내는 척도 (예. average reward per step)  </li>\n<li>$\\alpha$: positive-definite한 step size </li>\n</ul>\n<p><br></p>\n<h2 id=\"2-2-Policy-Gradent-Approach\"><a href=\"#2-2-Policy-Gradent-Approach\" class=\"headerlink\" title=\"2.2 Policy Gradent Approach\"></a>2.2 Policy Gradent Approach</h2><p>policy gradient는 stochastic policy를 자체적인 파리미터를 가진 function approximator를 이용해서 근사화시킵니다.</p>\n<p>policy parameter는 gradient에 비례하여 업데이트됩니다.</p>\n<p>$$\\Delta\\theta \\approx \\alpha\\frac{\\partial\\rho}{\\partial\\theta}$$</p>\n<p>$\\rho$는 $\\theta$에 대하여 미분가능해야겠죠? 위와 같이 업데이트하면 local optimal policy로 수렴합니다. 논문의 가장 중요한 contribution은 특정 조건을 만족하는 function approximator를 이용하여 경험(experience, sample)을 축적하고 이것들을 이용하여 위의 gradient를 unbiased estimate할 수 있음을 증명한 것입니다.</p>\n<p>먼저 $\\rho$, 즉, reward를 표현하는 두 가지 방법에 대해서 알아보겠습니다.</p>\n<p><br></p>\n<h2 id=\"2-3-Average-Reward-Formulation\"><a href=\"#2-3-Average-Reward-Formulation\" class=\"headerlink\" title=\"2.3 Average Reward Formulation\"></a>2.3 Average Reward Formulation</h2><p>Average reward formulation은 시간의 흐름에 따른 reward를 표현한다기보다는 모든 시간의 reward를 평균을 내서 표현하는 방법입니다.</p>\n<ul>\n<li><p>Long‐term expected reward per step<br>$$\\rho(\\pi)=\\lim_{n\\to\\infty}\\frac{1}{n}E[r_1+r_2+\\cdots+r_n|\\pi] = \\sum_s d^\\pi(s)\\sum_a\\pi(s,a)\\mathcal{R}_s^a$$</p>\n<ul>\n<li>$d^\\pi(s): \\lim_{t\\to\\infty}\\Pr[S_t=s|s_0,\\pi]$: stationary distribution of states under $\\pi$</li>\n<li>$d^\\pi(s)$가 존재한다고 가정합니다. 그리고 모든 policy에 대해서 $d^\\pi(s)$는 $s_0$와 independent합니다.</li>\n<li>위의 (limit수식과 summation수식 사이의) 등식은 왜 성립할까요? Time average와 Ensemble average가 같다는 뜻으로 <a href=\"https://en.wikipedia.org/wiki/Ergodicity\" target=\"_blank\" rel=\"noopener\">ergodic</a>한 system에서 성립합니다.</li>\n</ul>\n</li>\n<li><p>Value of a state-action pair given a policy<br>$$Q^{\\pi(s,a)} = \\sum_{t=1}^\\infty E[r_t - \\rho(\\pi)|S_0=s, A_0=a, \\pi]$$</p>\n<ul>\n<li>위의 Q-function의 표현식이 정의인지 유도된 것인지는 이 포스트를 작성하는 저희도 아직 확실히 모르겠습니다. 다만, $r_t$들을 더하면 무한대로 발산할 가능성이 매우 높은데 $\\rho$를 빼줌으로해서 어찌보면 평균 reward로부터의 차이를 더하는 것이라고 할 수 있고 이것은 bound된 값일 가능성이 더 높기 때문에 Q-function을 이렇게 표현하는 것이 더 안전하다고 할 수 있습니다. 이 부분에 대해서 좋은 의견이 있으시면 피드백을 주시면 감사하겠습니다!</li>\n</ul>\n</li>\n<li><p>State-value function<br>$$V^\\pi(s) = \\sum_{a} \\pi(s,a)Q^\\pi(s,a)$$</p>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-4-Start-State-Formulation\"><a href=\"#2-4-Start-State-Formulation\" class=\"headerlink\" title=\"2.4 Start-State Formulation\"></a>2.4 Start-State Formulation</h2><p>start-state formulation은 시간의 흐름에 따라 감소하는 reward들을 표현합니다.</p>\n<ul>\n<li><p>Long-term expected reward per step with a designated start state $S_0$<br>$$\\rho(\\pi) = E\\left[\\left.\\sum_{t=1}^\\infty \\gamma^{t-1}r_t\\right|S_0,\\pi\\right]$$</p>\n<ul>\n<li>$\\gamma\\in\\left[0,1\\right]$: discount rate</li>\n</ul>\n</li>\n<li><p>Value of a state‐action pair given a policy<br>$$Q^\\pi(s,a) = E\\left[\\left.\\sum_{k=1}^\\infty \\gamma^{k-1} r_{t+k}\\right|S_t=s, A_t=a, \\pi\\right]$$</p>\n</li>\n<li><p>Alternative form of $Q^\\pi(s,a)$<br>$$<br>Q^\\pi(s,a) = R_s^a + \\gamma\\sum _{s’} \\mathcal{P} _{s s’}^a V^{\\pi}(s’)<br>$$</p>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-5-Policy-Gradient-Theorem\"><a href=\"#2-5-Policy-Gradient-Theorem\" class=\"headerlink\" title=\"2.5 Policy Gradient Theorem\"></a>2.5 Policy Gradient Theorem</h2><p>논문의 중요한 결과인 Theorem 1은 다음과 같습니다.</p>\n<p><strong>Theorem 1 (Policy Gradient)</strong> <em>For any MDP, in either the average‐reward or start‐state formulations,</em><br>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)$$</p>\n<p>놀라운 부분은 expected return의 gradient를 취할 때 $\\frac{\\partial d^\\pi(s)}{\\partial\\theta}$를 구하지 않아도 된다는 점입니다. 즉, policy의 변화가 state distribution에 영향을 주지 않는다는 것입니다. 이것은 다시 말하면 아래 수식과 같이 우리는 $\\frac{\\partial\\rho}{\\partial\\theta}$에 대한 unbiased estimator를 구할 수 있다는 뜻입니다.</p>\n<p>$$E_s\\left[\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)\\right] = \\frac{\\partial\\rho}{\\partial\\theta}$$</p>\n<p>이로인해서 우리는 gradient를 sampling을 통해서 추정할 수 있습니다. 그렇지만 이것은 sample이 아주 많을 때만 성립합니다!</p>\n<p>또 한 가지 문제는 우리는 $Q^\\pi(s,a)$의 정확한 값을 알 수 없다는 것 입니다. 이것을 estimate하기 위해서 현재의 return값 $R_t$를 이용할 수 있습니다.</p>\n<p><br></p>\n<h2 id=\"2-6-Proof-of-Policy-Gradient-Theorem\"><a href=\"#2-6-Proof-of-Policy-Gradient-Theorem\" class=\"headerlink\" title=\"2.6 Proof of Policy Gradient Theorem\"></a>2.6 Proof of Policy Gradient Theorem</h2><p>증명을 살펴보겠습니다.</p>\n<ul>\n<li>average-reward formulation을 이용할 때의 증명은 아래와 같습니다.</li>\n</ul>\n<p>$$<br>\\begin{align}<br>\\frac{\\partial V^\\pi(s)}{\\partial\\theta} &amp;\\overset{\\underset{\\mathrm{def} }{} }{=}\\frac{\\partial}{\\partial\\theta}\\sum_a \\pi(s,a)Q^\\pi(s,a)<br>\\\\<br>&amp;=\\sum_a \\frac{\\partial}{\\partial\\theta}\\left[\\pi(s,a)Q^\\pi(s,a)\\right]\\\\<br>&amp;=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\frac{\\partial Q^\\pi(s,a)}{\\partial\\theta}\\right]\\\\<br>&amp;=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\frac{\\partial}{\\partial\\theta}\\left[R_s^a -\\rho(\\pi) + \\sum_{s’}\\mathcal{P}_{s s’}^a V^{\\pi}(s’)\\right]\\right]\\\\<br>&amp;=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\left[-\\frac{\\partial\\rho}{\\partial\\theta} + \\sum_{s’}\\mathcal{P}_{s s’}^a \\frac{\\partial V^{\\pi}(s’)}{\\partial\\theta}\\right]\\right]\\\\<br>\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\sum_{s’}\\mathcal{P}_{s s’}^a \\frac{\\partial V^{\\pi}(s’)}{\\partial\\theta}\\right] - \\frac{\\partial V^\\pi(s)}{\\partial\\theta}<br>\\end{align}<br>$$</p>\n<p>양변에 stationary distribution에 대한 평균을 취합니다.<br>$$<br>\\begin{align}<br>\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+ \\sum_s d^\\pi(s)\\sum_a\\pi(s,a)\\sum_{s’}\\mathcal{P}_{s s’}^a \\frac{\\partial V^{\\pi}(s’)}{\\partial\\theta}<br>\\\\<br>&amp;- \\sum_s d^\\pi(s)\\frac{\\partial V^\\pi(s)}{\\partial\\theta}<br>\\end{align}<br>$$</p>\n<p>모든 state $s$에서 모든 state $s’$으로 이동하는 것은 state $s’$에 존재할 확률이므로 다음과 같이 표현할 수 있습니다.</p>\n<p>$$<br>\\begin{align}<br>\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+ \\sum_s d^\\pi(s’) \\frac{\\partial V^{\\pi}(s’)}{\\partial\\theta} - \\sum_s d^\\pi(s)\\frac{\\partial V^\\pi(s)}{\\partial\\theta}<br>\\end{align}<br>$$</p>\n<p>뒤의 두 항은 같은 식이므로 0이 됩니다. 따라서 다음과 같이 표현됩니다.<br>$$<br>\\begin{align}<br>\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)<br>\\end{align}<br>$$</p>\n<p>$\\frac{\\partial\\rho}{\\partial\\theta}$는 reward의 파라미터에 대한 미분값입니다. reward 자체가 이미 모든 state에 대한 평균값이므로 $s$의 함수가 아닙니다.  $s$에 대한 함수가 아니므로 $\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}=\\frac{\\partial\\rho}{\\partial\\theta}\\sum_s d^\\pi(s)=\\frac{\\partial\\rho}{\\partial\\theta}$입니다. 여기서 $d^\\pi(s)$는 state에 대한 확률값이죠. 모든 state에 대해서 이 값을 다 더하면 1이 됩니다.<br>이를 이용해서 위의 식을 다시 표현하면 아래와 같고 이를 통해 증명이 완성됩니다.</p>\n<p>$$<br>\\begin{align}<br>\\therefore\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)<br>\\end{align}<br>$$</p>\n<ul>\n<li>다음으로 start-state formulation에 대한 증명입니다.</li>\n</ul>\n<p>start-state formulation은 이 논문에서도 나오고 서튼책에서도 나오는데, 서튼책의 증명이 더 설명이 자세합니다. 그래서 논문에 있는 증명이 아닌 서튼책에 있는 증명으로 설명드리겠습니다. 다만, notation이 약간 다른데, 큰 문제 없이 이해하실 수 있으므로 별도의 설명은 하지 않겠습니다. (서튼책 <a href=\"https://drive.google.com/file/d/1xeUDVGWGUUv1-ccUMAZHJLej2C7aAFWY/view\" target=\"_blank\" rel=\"noopener\">Link</a>)</p>\n<p>$$\\nabla{v_\\pi}(s)=\\nabla\\big[\\sum_a\\pi(a|s)q_\\pi(s,a)\\big]$$ </p>\n<p>다음으로 $(f\\cdot g)’=f’\\cdot g+f\\cdot g’$ (product rule of calculus)에 의해 다음과 같이 나타낼 수 있습니다.<br>$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\nabla q_\\pi(s,a)\\big]$$  </p>\n<p>이어서 $p(s’,r|s,a) := Pr[S_t=s’, R_t=r|S_{t-1}=s,A_{t-1}=a ]$ (서튼책 3.2 공식) 에 의해 다음과 같이 나타낼 수 있습니다.</p>\n<p>$$=\\sum_a\\big[\\nabla\\pi(a|s)q_{\\pi}(s,a)+\\pi(a|s)\\nabla\\big[\\sum_{s’, r}p(s’,r|s,a)(r+v_{\\pi}(s’))\\big]\\big]$$  </p>\n<p>계속해서 $p(s’|s,a)=\\sum_{r\\in R}p(s’,r|s,a)$ (서튼책 3.4 공식) 에 의해 다음과 같이 나타낼 수 있습니다.<br>$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\sum_{s’}p(s’|s,a)\\nabla v_\\pi(s’) \\big]$$ </p>\n<p>여기서 $\\nabla v_\\pi(s’)$ 부분을 unrolling을 하면 다음과 같이 나타낼 수 있습니다.<br>$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\sum_{s’}p(s’|s,a)\\sum_{a’}\\big[\\nabla\\pi(a’|s’)q_\\pi(s’,a’)\\\\+\\pi(a’|s’)\\sum_{s’’}p(s’’|s’,a’)\\nabla v_\\pi(s’’))\\big] \\big]$$ </p>\n<p>$\\nabla v_\\pi(s’’)$에 대해서 unrolling을 하고 또 나오는 다른 항에 대해서도 계속 반복하다보면 다음과 같이 나타낼 수 있습니다.<br>$$=\\sum_{x\\in S}\\sum_{k=0}^{\\infty}\\Pr(s\\rightarrow x,k,\\pi)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$</p>\n<p>여기에서 $\\Pr(s\\rightarrow x,k,\\pi)$ 는 policy $\\pi$ 에 대해 state $s$에서 state $x$까지 $k$ step만큼 움직일 때의 변환 확률입니다. 따라서 위의 수식을 아래처럼 다시 나타낼 수 있습니다.<br> <!-- \\begin{align}  --><br>$$\\nabla J(\\theta)=\\nabla v_\\pi (s_0)$$<br>$$=\\sum_s(\\sum_{k=0}^{\\infty}\\Pr(s_0\\rightarrow s,k,\\pi))\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$<br>$$=\\sum_s \\eta(s) \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$<br><!-- \\end{align} --></p>\n<ul>\n<li>서튼책에서 $\\eta(s)$는 다음과 같이 표현할 수 있습니다.<ul>\n<li>$\\eta(s)$ = 평균적으로 하나의 episode마다 state에서 머무른 time step의 수 (page 199)</li>\n</ul>\n</li>\n<li>논문에서는 $\\eta(s)$를 다음과 같이 표현합니다.<ul>\n<li>$d^\\pi(s)=\\sum_{t=0}^{\\infty}\\gamma^t Pr(s_t=s|s_0,\\pi)$<br>($\\gamma=1$ is allowed only in episodic tasks = discounted weighting of states)</li>\n</ul>\n</li>\n</ul>\n<p>여기서부터가 논문에는 없는 내용입니다. 위의 최종 수식을 다시 한 번 적으면 아래와 같습니다.<br>$$\\sum_s \\eta(s) \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$</p>\n<p>위의 수식을 아래와 같이 바꿀 수 있습니다.<br>$$\\sum_{s’} \\eta(s’) \\cdot\\sum_s \\frac{\\eta(s)}{\\sum_{s’}\\eta(s’)} \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$</p>\n<p>그러면 이것을 새로운 기호로 다시 나타낼 수 있습니다.<br>$$\\sum_{s’} \\eta(s’) \\cdot\\sum_s \\mu(s)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$ </p>\n<ul>\n<li>여기서 $\\mu(s)$ 는 state distribution이라고 하며, weight를 state distribution으로 바꿔주는 과정이라고 생각할 수 있습니다. 쉽게 말해 어떠한 state에 agent가 머무르는 확률입니다.</li>\n</ul>\n<p>위의 수식은 아래의 수식과 비례합니다. 따라서 최종 형태는 다음과 같습니다.<br>$$\\propto \\sum_s \\mu(s)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$</p>\n<p><br><br></p>\n<h1 id=\"3-Policy-Gradient-with-Approximation\"><a href=\"#3-Policy-Gradient-with-Approximation\" class=\"headerlink\" title=\"3. Policy Gradient with Approximation\"></a>3. Policy Gradient with Approximation</h1><p>이 장에서는 앞서 다뤘던 Theorem 1 중에 $Q^\\pi$에 대해서 중점적으로 다룹니다. 어떠한 $Q^\\pi$가 학습된 function approximator로 근사된다고 합시다. </p>\n<p>$f_w$ ($f_w$는 $S × A \\rightarrow \\Re$)는 parameter $w$를 가지는 어떠한 $Q^\\pi$ 에 대한 근사치입니다. 학습된 function approximator에 대해서 생각하기 때문에, 학습된 $f_w$는 error를 최소화하는 방향으로 parameter $w$를 업데이트합니다. 그러면 다음과 같은 관계식을 생각할 수 있습니다.<br>$$\\Delta w_t \\propto \\frac{\\partial}{\\partial w}\\big[ {\\hat Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]^2 \\propto \\big[{\\hat Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s_t,a_t)}{\\partial w}$$<br>(여기서 ${\\hat Q}^\\pi(s_t,a_t)$ 는 $Q^\\pi(s_t,a_t)$의 unbiased estimator $R_t$입니다.)</p>\n<p>그러면 위와 같은 수식이 local optimum에 수렴을 했을 때, 다음과 같이 나타낼 수 있습니다.<br>$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$</p>\n<ul>\n<li>위의 수식에 대한 추가 설명<ul>\n<li>local optimum으로 수렴을 하기 때문에 당연히 위의 수식은 0이 됩니다.</li>\n<li>또한 stochastic policy이기 때문에 local optimum으로 수렴하려면 모든 state, action에 대해 expectation을 해야합니다. 따라서 $\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)$이 붙게 됩니다.</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-1-Theorem-2-Policy-Gradient-with-Function-Approximation\"><a href=\"#3-1-Theorem-2-Policy-Gradient-with-Function-Approximation\" class=\"headerlink\" title=\"3.1 Theorem 2: Policy Gradient with Function Approximation\"></a>3.1 Theorem 2: Policy Gradient with Function Approximation</h2><p>만약 $f_w$가 아래의 등식을 만족한다고 합시다.<br>$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$</p>\n<p>또한 $f_w$가 policy parameterization과 양립할 수 있다고 합시다. policy parameterization에 대한 여러가지 의미가 있는데 이 논문에서는 다음을 의미합니다.<br>$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}$$</p>\n<ul>\n<li>위의 수식에 대한 추가 설명<ul>\n<li>이 수식은 Monte-Calro Estimation을 이용하기 위한 조건입니다.</li>\n<li>Compatibility Condition이라고 부릅니다.</li>\n<li>우변(= $\\nabla\\log\\pi(s,a)$)으로 유도함으로써 아래의 수식과 같이 sampling을 이용한 추정이 가능해집니다.</li>\n</ul>\n</li>\n</ul>\n<p>따라서 위의 두 수식을 이용하여 아래와 같이 나타낼 수 있습니다.<br>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}f_w(s,a)$$</p>\n<p><br></p>\n<h2 id=\"3-2-Proof-of-Theorem-2\"><a href=\"#3-2-Proof-of-Theorem-2\" class=\"headerlink\" title=\"3.2 Proof of Theorem 2\"></a>3.2 Proof of Theorem 2</h2><p>$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$</p>\n<p>$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}$$</p>\n<p>위의 두 수식을 합치면 다음과 같이 나타낼 수 있습니다.<br>$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}=0$$</p>\n<p>분자와 분모에 있는 $\\pi(s,a)$를 지우면 아래와 같습니다.<br>$$\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]=0$$</p>\n<p>이어서 위의 수식이 0이기 때문에 policy gradient theorem(앞서 다뤘던 Theorem 1)에서 위의 수식을 뺄 수 있습니다.<br>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)-\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ Q^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]$$</p>\n<p>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ Q^\\pi(s_t,a_t)-Q^\\pi(s_t,a_t)+f_w(s_t,a_t) \\big]$$</p>\n<p>$$\\therefore\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}f_w(s_t,a_t)$$</p>\n<p><br><br></p>\n<h1 id=\"4-Application-to-Deriving-Algorithms-and-Advantages\"><a href=\"#4-Application-to-Deriving-Algorithms-and-Advantages\" class=\"headerlink\" title=\"4. Application to Deriving Algorithms and Advantages\"></a>4. Application to Deriving Algorithms and Advantages</h1><p><br></p>\n<h2 id=\"4-1-Application-to-Deriving-Algorithms\"><a href=\"#4-1-Application-to-Deriving-Algorithms\" class=\"headerlink\" title=\"4.1 Application to Deriving Algorithms\"></a>4.1 Application to Deriving Algorithms</h2><p>feature의 linear combination에서 Gibbs distribution (softmax)를 하나의 policy로 생각해볼 수 있습니다.<br>$$\\pi(s,a)=\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_b e^{\\theta^{T}\\phi_{sb} }}$$</p>\n<ul>\n<li>$\\phi_{sa}$: state-action pair $s$, $a$를 나타내는 $l$-dimensional feature vector</li>\n</ul>\n<p>compatible condition을 적용하면 다음과 같습니다. (4.2 Proof of application of compatible condition 참고)<br>$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\frac{1}{\\pi(s,a)}=\\phi_{sa}-\\sum_b \\pi(s,b)\\phi_{sb}$$ </p>\n<p>이어서 $f_w$을 적분한 natural parameterization은 다음과 같습니다.<br>$$f_w(s,a)=w^T\\big[ \\phi_{sa}-\\sum_b \\pi(s,b)\\phi_{sb}\\big]$$</p>\n<ul>\n<li>위의 수식에 대한 추가 설명<ul>\n<li>$f_w$는 policy로서 같은 feature들에 대해 linear합니다.</li>\n<li>$f_w$는 각각의 state에 대해서 평균이 0입니다. ($\\sum_a\\pi(s,a)f_w(s,a)=0$)</li>\n<li>advantage function $A^\\pi(s,a)=Q^\\pi(s,a)-V^\\pi(s)$의 하나의 근사치로서 $f_w$를 생각해도 좋습니다.</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"4-2-Proof-of-application-of-compatible-condition\"><a href=\"#4-2-Proof-of-application-of-compatible-condition\" class=\"headerlink\" title=\"4.2 Proof of application of compatible condition\"></a>4.2 Proof of application of compatible condition</h2><p>증명을 하기 전에 다음을 가정합니다.</p>\n<ol>\n<li>$(\\frac{f(x)}{g(x)})’=f’(x)\\cdot \\frac{1}{g(x)}-f(x)\\cdot \\frac{g’(x)}{g(x)^{2} }=\\frac{f’(x)\\cdot g(x)-f(x)\\cdot g’(x)}{g(x)^{2} }$</li>\n<li>$f(\\theta)=e^{\\theta^{T}\\phi_{sa} }$, $g(\\theta)=\\sum_be^{\\theta^{T}\\phi_{sb} }$</li>\n</ol>\n<p>$$<br>\\begin{align}<br>\\frac{\\partial f_w(s,a)}{\\partial w}&amp;=\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\frac{1}{\\pi(s,a)}<br>\\\\<br>&amp;=\\big(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }}\\big)’\\frac{1}{\\pi(s,a)}<br>\\\\<br>&amp;=\\big[ \\phi_{sa}(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }})-(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }})\\frac{\\sum_b\\phi_{sb}e^{ {\\theta^T}\\phi_{sb} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }}\\big]\\frac{1}{\\pi(s,a)}\\\\<br>&amp;=\\big[ \\phi_{sa}\\pi(s,a)-\\pi(s,a)\\sum_b\\phi_{sb}\\pi(s,b)\\big]\\frac{1}{\\pi(s,a)}<br>\\\\<br>&amp;=\\pi(s,a)\\big[ \\phi_{sa}-\\sum_b\\pi(s,b)\\phi_{sb}\\big]\\frac{1}{\\pi(s,a)}<br>\\\\<br>&amp;\\therefore\\phi_{sa}-\\sum_b\\pi(s,b)\\phi_{sb}<br>\\end{align}<br>$$</p>\n<p><br></p>\n<h2 id=\"4-3-Application-to-Advantages\"><a href=\"#4-3-Application-to-Advantages\" class=\"headerlink\" title=\"4.3 Application to Advantages\"></a>4.3 Application to Advantages</h2><p>Policy Gradient with Function Approximation Theorem (Theorem 2)는 advantage function으로 확장될 수 있습니다.<br>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_sd^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ f_w(s,a)+v(s) \\big]$$</p>\n<ul>\n<li>위의 수식에 대한 추가설명<ul>\n<li>$v$ ($v$는 $S\\rightarrow\\Re$) 는 arbitrary function입니다.</li>\n<li>이 수식은 $\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}=0$이기 때문에 가능해집니다.</li>\n<li>즉, 이 수식은 $\\pi(s,a)$의 gradient에만 dependent하기 때문에 advantage 역할을 하는 함수들을 넣어도 아무런 상관이 없습니다.</li>\n<li>$v$의 선택은 Theorem들에 영향을 미치지 못하지만, 실질적으로 gradient estimator의 variance에 영향을 미칩니다.</li>\n<li>이러한 문제는 전체적으로 이전의 연구에 reinforcement baseline의 사용에 있어서 유사합니다.</li>\n<li>(comment) 위의 수식에서 $f_w(s,a)+v(s)$와 Application to Deriving Algorithms의 $f_w(s,a)$와는 다른 것입니다. Application to Deriving Algorithms의 $f_w(s,a)$은 softmax에 의해 스스로 advantage function의 역할을 할 수 있습니다. 하지만 보통의 경우에는 그러지 못할 수도 있기 때문에 위의 수식처럼 $f_w(s,a)+v(s)$을 추가하여 zero mean만들어서 variance를 줄일 수 있습니다.</li>\n</ul>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"5-Convergence-of-Policy-Iteration-with-Function-Approximation\"><a href=\"#5-Convergence-of-Policy-Iteration-with-Function-Approximation\" class=\"headerlink\" title=\"5. Convergence of Policy Iteration with Function Approximation\"></a>5. Convergence of Policy Iteration with Function Approximation</h1><p><br></p>\n<h2 id=\"5-1-Theorem-3-Convergence-of-Policy-Iteration-with-Function-Approximation\"><a href=\"#5-1-Theorem-3-Convergence-of-Policy-Iteration-with-Function-Approximation\" class=\"headerlink\" title=\"5.1 Theorem 3: Convergence of Policy Iteration with Function Approximation\"></a>5.1 Theorem 3: Convergence of Policy Iteration with Function Approximation</h2><p>policy iteration with function approximation은 locally optimal policy에 수렴합니다. $\\pi$와 $f_w$를 </p>\n<ol>\n<li>compatibility condition을 만족하는 policy와 value function에 대한</li>\n<li>그리고 $\\max_{\\theta, s, a, i, j}\\big| \\frac{\\partial^2\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}\\big|&lt;B&lt;\\infty$에 대한</li>\n</ol>\n<p>어떠한 미분가능한 function approximator라고 합시다.</p>\n<ul>\n<li>(comment) $\\max_{\\theta, s, a, i, j}\\big| \\frac{\\partial^2\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}\\big|&lt;B&lt;\\infty$에서 $\\pi(s,a)$라는 function은 이계미분값이 존재하고, 임의의 상수인 (Bound) B에 bound되어 있기 때문에 function의 그래프는 smooth하다고 볼 수 있습니다. (아래의 그림 중 빨간색 그래프 참고)</li>\n</ul>\n<center> <img src=\"https://www.dropbox.com/s/xx02ejfg5ao19ps/Screen%20Shot%202018-07-10%20at%203.41.06%20PM.png?dl=1\" width=\"200\"> </center>\n\n<p>이어서 ${\\alpha_k}$는 $\\lim_{k\\rightarrow\\infty}\\alpha_k=0$이며 $\\sum_k\\alpha_k=\\infty$를 만족하는 step-size sequence라고 합시다.</p>\n<p>그 때, bounded reward를 가진 MDP에 대해</p>\n<ol>\n<li>어떠한 $\\theta_0, \\pi_k=\\pi(\\cdot,\\cdot,\\theta_k)$</li>\n<li>그리고 $\\sum_sd^{\\pi_{k} }(s)\\sum_a\\pi_k(s,a)\\big[ Q^{\\pi_{k} }(s,a)-f_w(s,a)\\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$로 인하여 $w_k=w$, $\\theta_{k+1}=\\theta_k+\\alpha_k\\sum_sd^{\\pi_{k} }(s)\\sum_a\\frac{\\partial\\pi_k(s,a)}{\\partial\\theta}f_{w_{k} }(s,a)$</li>\n</ol>\n<p>으로 정의된 sequence $\\rho(\\pi_k)$는 $\\lim_{k\\rightarrow\\infty}\\frac{\\partial\\rho(\\pi_k)}{\\partial\\theta}=0$이기 때문에 수렴합니다.</p>\n<ul>\n<li>sequence $\\rho(\\pi_k)_{k=0}^\\infty$에 대한 추가 설명<ul>\n<li>2번의 식을 통해 자연스럽게 actor-critic으로 연결됩니다.</li>\n<li>$\\theta_{k+1}=\\theta_k+\\alpha_k\\sum_sd^{\\pi_{k} }(s)\\sum_a\\frac{\\partial\\pi_k(s,a)}{\\partial\\theta}f_{w_{k} }(s,a)$ 에 따라 $\\theta$가 1, 2, …, $\\infty$로 갈텐데, 거기에 따른 objective function or performance의 sequence입니다.</li>\n<li>(comment) 굳이 sequence라는 표현이 없어도 될 것 같습니다. 어짜피 k가 $\\infty$로 가면 $\\rho(\\pi_k)$가 수렴한다는 의미이기 때문에 불필요해보입니다.</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"5-2-Proof-of-Theorem-3\"><a href=\"#5-2-Proof-of-Theorem-3\" class=\"headerlink\" title=\"5.2 Proof of Theorem 3\"></a>5.2 Proof of Theorem 3</h2><ul>\n<li>Theorem 2는 $\\theta_k$ update가 gradient의 error를 최소화한다는 것을 증명했습니다.</li>\n<li>$\\frac{\\partial^{2}\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}$와 MDP의 reward에서의 bound는, $\\frac{\\partial^{2}\\rho}{\\partial\\theta_i\\partial\\theta_j}$ 또한 bound된다는 것을 증명합니다.</li>\n<li>step-size 필요조건 때문에 이러한 bound된 것들은 Proposition 3.5 from page 96 of Bertsekas and Tsitsiklis (1996)에 적용하기 위해 필요한 조건입니다.</li>\n<li>Proposition 3.5 from page 96 of Bertsekas and Tsitsiklis (1996)은 local optimum으로 수렴한다는 것을 증명했습니다.</li>\n<li>Proposition 3.5를 자세하게 설명하지는 않겠습니다. 일반적인 gradient method의 수렴성을 증명한 것이며, 이 논문의 policy gradient method도 gradient method의 일종이므로 특정 조건을 만족할 때 수렴한다는 것입니다. 수렴하는 원리는 소위 말하는 Lipschitz continuity condition을 만족하기 때문인데 다음과 같습니다. $L&gt;0$인 임의의 상수입니다.<br>$$<br>\\parallel \\nabla f(r) - \\nabla f(\\bar{r}) \\parallel \\leq L \\parallel r - \\bar{r} \\parallel<br>$$<br>즉, 업데이트가 진행되어 나감에 따라서 gradient의 차이가 점점 줄어들게 되므로 언젠가는 0으로 수렴하게 됩니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"6-Summary\"><a href=\"#6-Summary\" class=\"headerlink\" title=\"6. Summary\"></a>6. Summary</h1><p>논문에서 설명한 policy gradient 기법을 요약하면 다음과 같습니다.</p>\n<ul>\n<li>Original maximization problem: $\\max\\rho_{\\theta}=E\\left[R\\right]$</li>\n<li>gradient ascent method를 이용한 parameter update: $\\theta_{t+1} = \\theta_{t} + \\alpha\\left.\\frac{\\partial E[R]}{\\partial\\theta}\\right|_{\\theta_t}$<ul>\n<li>그러나 우리는 gradient를 모르고, 추정하기도 어렵습니다. 왜냐하면 expectation이 안에 있기 때문입니다.</li>\n<li>그렇다면 이것을 추정 가능한 형태로 바꿔야합니다. 다시 말해 expectation이 밖에 있는 형태로 바꾸는 것입니다.</li>\n<li>Expectation이 밖에 있으면 왜 추정이 유리할까요? 바로 Sample mean을 취하면 되기 때문입니다.</li>\n</ul>\n</li>\n<li>Approximate gradient<ul>\n<li>방법1: 논문의 Theorem 1을 이용하면 gradient와 expectation의 위치를 바꿀 수 있습니다.<br>$$\\frac{\\partial E[R]}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)$$</li>\n<li>방법2: Log derivative trick을 이용하면 gradient와 expectation의 위치를 바꿀 수 있습니다. (Theorem 2)<br>$$\\theta_{t+1} = \\theta_{t} + \\alpha E\\left[ \\left. R\\frac{\\partial}{\\partial\\theta}\\log p_{\\theta} \\right\\vert \\theta_t\\right]$$</li>\n<li>그리고 특정 trajectory를 따라가면서 return값을 구하고 이것을 여러 번 수행하여 sample mean을 취하면 gradient를 추정하는 것이 가능합니다.</li>\n<li>이러한 기법을 제시한 기존의 연구가 REINFORCE입니다.</li>\n<li>여러 Trajectory를 이용하므로 variance가 높을 수 밖에 없습니다.</li>\n<li>Advantage를 활용하거나 하는 방식으로 이후 여러 연구가 진행되었습니다.</li>\n</ul>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"처음으로\"><a href=\"#처음으로\" class=\"headerlink\" title=\"처음으로\"></a>처음으로</h1><h2 id=\"PG-Travel-Guide\"><a href=\"#PG-Travel-Guide\" class=\"headerlink\" title=\"PG Travel Guide\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/\">PG Travel Guide</a></h2><p><br></p>\n<h1 id=\"다음으로\"><a href=\"#다음으로\" class=\"headerlink\" title=\"다음으로\"></a>다음으로</h1><h2 id=\"DPG-여행하기\"><a href=\"#DPG-여행하기\" class=\"headerlink\" title=\"DPG 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/27/2_dpg/\">DPG 여행하기</a></h2>","site":{"data":{}},"excerpt":"","more":"<center> <img src=\"https://www.dropbox.com/s/83i4atz733ali0w/Screen%20Shot%202018-07-18%20at%2012.10.52%20AM.png?dl=1\" width=\"600\"> </center>\n\n<p>논문 저자 : Richard S. Sutton, David McAllester, Satinder Singh, Yishay Mansour<br>논문 링크 : <a href=\"http://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf\" target=\"_blank\" rel=\"noopener\">NIPS</a><br>Proceeding : Advances in Neural Information Processing Systems (NIPS) 2000<br>정리 : 김동민, 이동민</p>\n<hr>\n<h1 id=\"1-Intro-to-Policy-Gradient\"><a href=\"#1-Intro-to-Policy-Gradient\" class=\"headerlink\" title=\"1. Intro to Policy Gradient\"></a>1. Intro to Policy Gradient</h1><p>이 논문은 policy gradient (PG) 기법의 효시와도 같으며 향후 많은 파생연구를 낳은 중요한 논문입니다. 7페이지의 짧은 논문이지만 읽기에 만만한 논문은 아닙니다. 이 논문을 이해하기 위해 필요한 배경지식을 먼저 설명하고 논문을 차근차근 살펴보도록 하겠습니다.</p>\n<p><br></p>\n<h2 id=\"1-1-Value-Function-Approach\"><a href=\"#1-1-Value-Function-Approach\" class=\"headerlink\" title=\"1.1 Value Function Approach\"></a>1.1 Value Function Approach</h2><p>전통적으로 강화학습 기법은 value function을 기반으로 동작하였습니다. 특정 state에서 vaue function 또는 value function을 근사하는 함수(function approximation)를 최대화하는 action을 찾는 greedy action-selection policy가  대표적입니다.</p>\n<p>논문에서는 이러한 방법은 deterministic한 policy를 찾는 쪽으로 나아가게 되지만 종종 최적의 policy는 stochastic한 성질을 가지기 때문에 이 방법으로는 최적의 policy를 찾을 수 없다고 언급하고 있습니다. (그러나 이 논문이 나오고 나서 한참 후 David Silver를 필두로한 DeepMind의 연구진들은 high-dimensional action space를 가지는 application에서 보다 빠르게 동작하는 <a href=\"../../../06/27/2_dpg/\">deterministic policy gradient</a>을 개발하였습니다.) 아마도 이러한 부분은 $\\epsilon$-greedy action-selection method로 개선될 수 있을 것입니다. 또 하나의 문제는 value function의 작은 변화로 인해서 action이 크게 변할 수도 있다는 것입니다. 이것은 알고리듬의 수렴성에 문제를 야기할 수 있습니다. 이러한 문제점을 해결하기 위하여 policy search라는 새로운 기법이 고안됩니다.</p>\n<p><br></p>\n<h2 id=\"1-2-Policy-Search\"><a href=\"#1-2-Policy-Search\" class=\"headerlink\" title=\"1.2 Policy Search\"></a>1.2 Policy Search</h2><p>policy search는 최적의 policy $\\pi^*$를 reward로부터 직접 찾습니다. policy search 방법은 크게 두 가지로 분류할 수 있습니다. 첫 번째는 gradient-based optimization으로 policy gradient method가 이에 속합니다. 두 번째는 gradient-free optimization으로 진화(evolutionary) 연산을 이용하는 것이 이에 속합니다. 이 논문에서는 제목에서 알 수 있듯이 gradient-based optimization을 다룹니다. gradient는 변화량을 의미합니다. 특정 policy x와 또 다른 policy y가 있을 때 policy가 얼마나 많이 변화했는지 어떻게 모델링할 수 있을까요? policy의 변화를 제어하는 어떤 파라미터가 있다면 이 파라미터를 조정하여 policy를 변화시킬 수 있습니다. policy x에 해당하는 파라미터값과 policy y에 해당하는 파라미터값의 차이가 policy의 변화량이라고 할 수 있습니다. 이와 같은 방식으로 policy의 변화를 모델링하기 위하여 파라미터 $\\theta$를 이용하여 policy를 $\\pi_{\\theta}$로 표현할 수 있습니다. 최적의 policy를 찾기 위하여 expected return $E\\left[R|\\theta\\right]$이 최대화되도록 parameter를 조정합니다. 이를 간단한 수식으로 정리하면 다음과 같습니다.</p>\n<p>$$<br>\\pi^* = \\arg \\max\\limits_\\pi  E\\left[ {\\left. R \\right|\\pi } \\right] \\to {\\text{original problem} }<br>\\\\  {\\Downarrow} \\\\<br>{\\text{policy parameterization by } } \\pi_{\\theta}: \\Theta  \\to \\Pi \\\\ {\\Downarrow} $$</p>\n<p>$$\\pi^* = \\arg \\max\\limits_{\\theta}  E\\left[ {\\left. R \\right|\\theta } \\right] \\to {\\text{policy search problem} }<br>$$</p>\n<p><br></p>\n<h2 id=\"1-3-How-to-Obtain-the-Expected-Return\"><a href=\"#1-3-How-to-Obtain-the-Expected-Return\" class=\"headerlink\" title=\"1.3 How to Obtain the Expected Return\"></a>1.3 How to Obtain the Expected Return</h2><p>expected return을 최대화하는 방향으로 policy를 update한다고 하였습니다. 그렇다면 expected return은 어떻게 구할 수 있을까요? 여기에도 크게 두 가지 방법이 있습니다. 첫 번째는 deterministic approximation으로 Markov decision process의 dynamics를 모델링한 후 수식을 통해 구하는 것입니다. 두 번째 방법은 monte carlo estimation으로 dynamics에 대한 모델을 하지 않고 많은 sample들을 얻은 후 empirical하게 expected return을 계산하는 방법입니다. 어느 방법이 더 좋을지는 풀고자 하는 문제의 특성에 따라 다를 것입니다. dynamics에 대한 모델이 어렵거나 변화가 큰 경우에는 두 번째 방법이 좀 더 현실적이지만 gradient를 구하는 것은 더 어렵습니다. 결국 gradient를 esimate하는방법을 고안해야 합니다. 가장 유명한 gradient estimation 방법이 1992년 R. J. Williams에 의해서 제안된 <a href=\"https://link.springer.com/content/pdf/10.1007/BF00992696.pdf\" target=\"_blank\" rel=\"noopener\">REINFORCE</a> 기법입니다. REINFORCE는 Monte Carlo estimate 또는 likelihood-ratio estimate라고 부르는 방법을 이용합니다. 이 방법에 대해서 좀 더 알아볼까요?</p>\n<h3 id=\"1-3-1-Monte-Carlo-Gradient-Estimation\"><a href=\"#1-3-1-Monte-Carlo-Gradient-Estimation\" class=\"headerlink\" title=\"1.3.1 Monte Carlo Gradient Estimation\"></a>1.3.1 Monte Carlo Gradient Estimation</h3><p>다음과 같은 parameter $\\theta$를 가지는 random variable $X$가 있습니다: $X:\\Omega\\mapsto\\mathcal{X}$. 그리고 이 $x$에 대한 함수 $f$가 있습니다: $f:\\mathcal{X}\\mapsto\\mathbb{R}$. expected return처럼 $E[f(x)]$를 최대화하고자 합니다. 이를 위해서는 $\\nabla_{\\theta}E[f(x)]$를 구해야 합니다. 이 때 log derivate trick을 이용하여 다음과 같이 수식을 변형시킬 수 있습니다.</p>\n<p>$$<br>\\begin{align}<br>\\nabla_{\\theta} E_{p(x;\\theta)}\\left[ {f(x)} \\right]<br>&amp;= \\nabla_\\theta\\int{f\\left( x \\right)p\\left( {x;\\theta} \\right) dx}<br>\\\\<br>&amp;= \\int { {\\nabla_\\theta }p\\left( {x;\\theta } \\right)f\\left( x \\right)dx}<br>\\\\<br>&amp;= \\int {\\frac{ {p\\left( {x;\\theta } \\right)} }{ {p\\left( {x;\\theta } \\right)} }{\\nabla_\\theta }p\\left( {x;\\theta } \\right)f\\left( x \\right)dx}<br>\\\\<br>&amp;= \\int {p\\left( {x;\\theta } \\right){\\nabla_\\theta }\\log p\\left( {x;\\theta } \\right)f\\left( x \\right)dx}<br>\\\\<br>&amp;= E_{p(x;\\theta)}[f(x)\\nabla_\\theta \\log p({x;\\theta})]<br>\\end{align}<br>$$</p>\n<p>이제 expectation은 많은 샘플들을 모아서 평균을 취하는 Monte Carlo 기법으로 근사화할 수 있습니다.</p>\n<p>$${\\nabla_{\\theta} }{E_{p\\left( {x;\\theta } \\right)} }\\left[ {f\\left( x \\right)} \\right] = \\frac{1}{N}\\sum_{n=1}^N f\\left(X_n\\right)\\nabla_\\theta\\log p\\left(X_n;\\theta\\right)$$</p>\n<p>이 방법을 사용하기 위해서 필요한 조건은 $\\log p\\left(X_n;\\theta\\right)$가 미분가능해야 한다는 것 뿐입니다. 그러나 이 방법은 얻은 샘플들에 의존하기 때문에 경우에 따라서 큰 variance를 가질 수 있습니다.</p>\n<p><br><br></p>\n<h1 id=\"2-Policy-Gradient-Methods\"><a href=\"#2-Policy-Gradient-Methods\" class=\"headerlink\" title=\"2. Policy Gradient Methods\"></a>2. Policy Gradient Methods</h1><p>이제 본격적으로 policy gradient 기법에 대해서 알아보겠습니다.</p>\n<p><br></p>\n<h2 id=\"2-1-System-Model\"><a href=\"#2-1-System-Model\" class=\"headerlink\" title=\"2.1 System Model\"></a>2.1 System Model</h2><p>논문에서 사용하는 수학기호(notation)와 가정들을 설명하는 시스템모델은 다음과 같습니다.</p>\n<ul>\n<li>Markov decision process (MDP) at each time $t\\in{0,1,2,…}$</li>\n<li>$S_t\\in\\mathcal{S}$: state</li>\n<li>$A_t\\in\\mathcal{A}$: action</li>\n<li>$r_t\\in \\mathcal{R}$: reward</li>\n<li>$\\theta\\in\\mathbb{R}^l$: vector of policy parameters, for $l\\ll\\left|\\mathcal{S}\\right|$</li>\n<li>$\\mathcal{P} _{s s’}^a = \\Pr[S _{t+1}=s’ \\vert S_t=s,A_t=a]$: state transition probabilities</li>\n<li>$\\mathcal{R} _{s s’}^a = E[R _{t+1}\\vert S_t=s,A_t=a]$: expected reward</li>\n<li>$\\pi(s,a,\\theta)=\\Pr[A_t=a|S_t=s,\\theta]$: policy, shortened as $\\pi(s,a)$ or $\\pi(a|s)$</li>\n<li>$\\theta$: policy parameter들의 vector  </li>\n<li>$\\rho$: 해당 policy들의 성능을 나타내는 척도 (예. average reward per step)  </li>\n<li>$\\alpha$: positive-definite한 step size </li>\n</ul>\n<p><br></p>\n<h2 id=\"2-2-Policy-Gradent-Approach\"><a href=\"#2-2-Policy-Gradent-Approach\" class=\"headerlink\" title=\"2.2 Policy Gradent Approach\"></a>2.2 Policy Gradent Approach</h2><p>policy gradient는 stochastic policy를 자체적인 파리미터를 가진 function approximator를 이용해서 근사화시킵니다.</p>\n<p>policy parameter는 gradient에 비례하여 업데이트됩니다.</p>\n<p>$$\\Delta\\theta \\approx \\alpha\\frac{\\partial\\rho}{\\partial\\theta}$$</p>\n<p>$\\rho$는 $\\theta$에 대하여 미분가능해야겠죠? 위와 같이 업데이트하면 local optimal policy로 수렴합니다. 논문의 가장 중요한 contribution은 특정 조건을 만족하는 function approximator를 이용하여 경험(experience, sample)을 축적하고 이것들을 이용하여 위의 gradient를 unbiased estimate할 수 있음을 증명한 것입니다.</p>\n<p>먼저 $\\rho$, 즉, reward를 표현하는 두 가지 방법에 대해서 알아보겠습니다.</p>\n<p><br></p>\n<h2 id=\"2-3-Average-Reward-Formulation\"><a href=\"#2-3-Average-Reward-Formulation\" class=\"headerlink\" title=\"2.3 Average Reward Formulation\"></a>2.3 Average Reward Formulation</h2><p>Average reward formulation은 시간의 흐름에 따른 reward를 표현한다기보다는 모든 시간의 reward를 평균을 내서 표현하는 방법입니다.</p>\n<ul>\n<li><p>Long‐term expected reward per step<br>$$\\rho(\\pi)=\\lim_{n\\to\\infty}\\frac{1}{n}E[r_1+r_2+\\cdots+r_n|\\pi] = \\sum_s d^\\pi(s)\\sum_a\\pi(s,a)\\mathcal{R}_s^a$$</p>\n<ul>\n<li>$d^\\pi(s): \\lim_{t\\to\\infty}\\Pr[S_t=s|s_0,\\pi]$: stationary distribution of states under $\\pi$</li>\n<li>$d^\\pi(s)$가 존재한다고 가정합니다. 그리고 모든 policy에 대해서 $d^\\pi(s)$는 $s_0$와 independent합니다.</li>\n<li>위의 (limit수식과 summation수식 사이의) 등식은 왜 성립할까요? Time average와 Ensemble average가 같다는 뜻으로 <a href=\"https://en.wikipedia.org/wiki/Ergodicity\" target=\"_blank\" rel=\"noopener\">ergodic</a>한 system에서 성립합니다.</li>\n</ul>\n</li>\n<li><p>Value of a state-action pair given a policy<br>$$Q^{\\pi(s,a)} = \\sum_{t=1}^\\infty E[r_t - \\rho(\\pi)|S_0=s, A_0=a, \\pi]$$</p>\n<ul>\n<li>위의 Q-function의 표현식이 정의인지 유도된 것인지는 이 포스트를 작성하는 저희도 아직 확실히 모르겠습니다. 다만, $r_t$들을 더하면 무한대로 발산할 가능성이 매우 높은데 $\\rho$를 빼줌으로해서 어찌보면 평균 reward로부터의 차이를 더하는 것이라고 할 수 있고 이것은 bound된 값일 가능성이 더 높기 때문에 Q-function을 이렇게 표현하는 것이 더 안전하다고 할 수 있습니다. 이 부분에 대해서 좋은 의견이 있으시면 피드백을 주시면 감사하겠습니다!</li>\n</ul>\n</li>\n<li><p>State-value function<br>$$V^\\pi(s) = \\sum_{a} \\pi(s,a)Q^\\pi(s,a)$$</p>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-4-Start-State-Formulation\"><a href=\"#2-4-Start-State-Formulation\" class=\"headerlink\" title=\"2.4 Start-State Formulation\"></a>2.4 Start-State Formulation</h2><p>start-state formulation은 시간의 흐름에 따라 감소하는 reward들을 표현합니다.</p>\n<ul>\n<li><p>Long-term expected reward per step with a designated start state $S_0$<br>$$\\rho(\\pi) = E\\left[\\left.\\sum_{t=1}^\\infty \\gamma^{t-1}r_t\\right|S_0,\\pi\\right]$$</p>\n<ul>\n<li>$\\gamma\\in\\left[0,1\\right]$: discount rate</li>\n</ul>\n</li>\n<li><p>Value of a state‐action pair given a policy<br>$$Q^\\pi(s,a) = E\\left[\\left.\\sum_{k=1}^\\infty \\gamma^{k-1} r_{t+k}\\right|S_t=s, A_t=a, \\pi\\right]$$</p>\n</li>\n<li><p>Alternative form of $Q^\\pi(s,a)$<br>$$<br>Q^\\pi(s,a) = R_s^a + \\gamma\\sum _{s’} \\mathcal{P} _{s s’}^a V^{\\pi}(s’)<br>$$</p>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-5-Policy-Gradient-Theorem\"><a href=\"#2-5-Policy-Gradient-Theorem\" class=\"headerlink\" title=\"2.5 Policy Gradient Theorem\"></a>2.5 Policy Gradient Theorem</h2><p>논문의 중요한 결과인 Theorem 1은 다음과 같습니다.</p>\n<p><strong>Theorem 1 (Policy Gradient)</strong> <em>For any MDP, in either the average‐reward or start‐state formulations,</em><br>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)$$</p>\n<p>놀라운 부분은 expected return의 gradient를 취할 때 $\\frac{\\partial d^\\pi(s)}{\\partial\\theta}$를 구하지 않아도 된다는 점입니다. 즉, policy의 변화가 state distribution에 영향을 주지 않는다는 것입니다. 이것은 다시 말하면 아래 수식과 같이 우리는 $\\frac{\\partial\\rho}{\\partial\\theta}$에 대한 unbiased estimator를 구할 수 있다는 뜻입니다.</p>\n<p>$$E_s\\left[\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)\\right] = \\frac{\\partial\\rho}{\\partial\\theta}$$</p>\n<p>이로인해서 우리는 gradient를 sampling을 통해서 추정할 수 있습니다. 그렇지만 이것은 sample이 아주 많을 때만 성립합니다!</p>\n<p>또 한 가지 문제는 우리는 $Q^\\pi(s,a)$의 정확한 값을 알 수 없다는 것 입니다. 이것을 estimate하기 위해서 현재의 return값 $R_t$를 이용할 수 있습니다.</p>\n<p><br></p>\n<h2 id=\"2-6-Proof-of-Policy-Gradient-Theorem\"><a href=\"#2-6-Proof-of-Policy-Gradient-Theorem\" class=\"headerlink\" title=\"2.6 Proof of Policy Gradient Theorem\"></a>2.6 Proof of Policy Gradient Theorem</h2><p>증명을 살펴보겠습니다.</p>\n<ul>\n<li>average-reward formulation을 이용할 때의 증명은 아래와 같습니다.</li>\n</ul>\n<p>$$<br>\\begin{align}<br>\\frac{\\partial V^\\pi(s)}{\\partial\\theta} &amp;\\overset{\\underset{\\mathrm{def} }{} }{=}\\frac{\\partial}{\\partial\\theta}\\sum_a \\pi(s,a)Q^\\pi(s,a)<br>\\\\<br>&amp;=\\sum_a \\frac{\\partial}{\\partial\\theta}\\left[\\pi(s,a)Q^\\pi(s,a)\\right]\\\\<br>&amp;=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\frac{\\partial Q^\\pi(s,a)}{\\partial\\theta}\\right]\\\\<br>&amp;=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\frac{\\partial}{\\partial\\theta}\\left[R_s^a -\\rho(\\pi) + \\sum_{s’}\\mathcal{P}_{s s’}^a V^{\\pi}(s’)\\right]\\right]\\\\<br>&amp;=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\left[-\\frac{\\partial\\rho}{\\partial\\theta} + \\sum_{s’}\\mathcal{P}_{s s’}^a \\frac{\\partial V^{\\pi}(s’)}{\\partial\\theta}\\right]\\right]\\\\<br>\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\sum_{s’}\\mathcal{P}_{s s’}^a \\frac{\\partial V^{\\pi}(s’)}{\\partial\\theta}\\right] - \\frac{\\partial V^\\pi(s)}{\\partial\\theta}<br>\\end{align}<br>$$</p>\n<p>양변에 stationary distribution에 대한 평균을 취합니다.<br>$$<br>\\begin{align}<br>\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+ \\sum_s d^\\pi(s)\\sum_a\\pi(s,a)\\sum_{s’}\\mathcal{P}_{s s’}^a \\frac{\\partial V^{\\pi}(s’)}{\\partial\\theta}<br>\\\\<br>&amp;- \\sum_s d^\\pi(s)\\frac{\\partial V^\\pi(s)}{\\partial\\theta}<br>\\end{align}<br>$$</p>\n<p>모든 state $s$에서 모든 state $s’$으로 이동하는 것은 state $s’$에 존재할 확률이므로 다음과 같이 표현할 수 있습니다.</p>\n<p>$$<br>\\begin{align}<br>\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+ \\sum_s d^\\pi(s’) \\frac{\\partial V^{\\pi}(s’)}{\\partial\\theta} - \\sum_s d^\\pi(s)\\frac{\\partial V^\\pi(s)}{\\partial\\theta}<br>\\end{align}<br>$$</p>\n<p>뒤의 두 항은 같은 식이므로 0이 됩니다. 따라서 다음과 같이 표현됩니다.<br>$$<br>\\begin{align}<br>\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)<br>\\end{align}<br>$$</p>\n<p>$\\frac{\\partial\\rho}{\\partial\\theta}$는 reward의 파라미터에 대한 미분값입니다. reward 자체가 이미 모든 state에 대한 평균값이므로 $s$의 함수가 아닙니다.  $s$에 대한 함수가 아니므로 $\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}=\\frac{\\partial\\rho}{\\partial\\theta}\\sum_s d^\\pi(s)=\\frac{\\partial\\rho}{\\partial\\theta}$입니다. 여기서 $d^\\pi(s)$는 state에 대한 확률값이죠. 모든 state에 대해서 이 값을 다 더하면 1이 됩니다.<br>이를 이용해서 위의 식을 다시 표현하면 아래와 같고 이를 통해 증명이 완성됩니다.</p>\n<p>$$<br>\\begin{align}<br>\\therefore\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)<br>\\end{align}<br>$$</p>\n<ul>\n<li>다음으로 start-state formulation에 대한 증명입니다.</li>\n</ul>\n<p>start-state formulation은 이 논문에서도 나오고 서튼책에서도 나오는데, 서튼책의 증명이 더 설명이 자세합니다. 그래서 논문에 있는 증명이 아닌 서튼책에 있는 증명으로 설명드리겠습니다. 다만, notation이 약간 다른데, 큰 문제 없이 이해하실 수 있으므로 별도의 설명은 하지 않겠습니다. (서튼책 <a href=\"https://drive.google.com/file/d/1xeUDVGWGUUv1-ccUMAZHJLej2C7aAFWY/view\" target=\"_blank\" rel=\"noopener\">Link</a>)</p>\n<p>$$\\nabla{v_\\pi}(s)=\\nabla\\big[\\sum_a\\pi(a|s)q_\\pi(s,a)\\big]$$ </p>\n<p>다음으로 $(f\\cdot g)’=f’\\cdot g+f\\cdot g’$ (product rule of calculus)에 의해 다음과 같이 나타낼 수 있습니다.<br>$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\nabla q_\\pi(s,a)\\big]$$  </p>\n<p>이어서 $p(s’,r|s,a) := Pr[S_t=s’, R_t=r|S_{t-1}=s,A_{t-1}=a ]$ (서튼책 3.2 공식) 에 의해 다음과 같이 나타낼 수 있습니다.</p>\n<p>$$=\\sum_a\\big[\\nabla\\pi(a|s)q_{\\pi}(s,a)+\\pi(a|s)\\nabla\\big[\\sum_{s’, r}p(s’,r|s,a)(r+v_{\\pi}(s’))\\big]\\big]$$  </p>\n<p>계속해서 $p(s’|s,a)=\\sum_{r\\in R}p(s’,r|s,a)$ (서튼책 3.4 공식) 에 의해 다음과 같이 나타낼 수 있습니다.<br>$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\sum_{s’}p(s’|s,a)\\nabla v_\\pi(s’) \\big]$$ </p>\n<p>여기서 $\\nabla v_\\pi(s’)$ 부분을 unrolling을 하면 다음과 같이 나타낼 수 있습니다.<br>$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\sum_{s’}p(s’|s,a)\\sum_{a’}\\big[\\nabla\\pi(a’|s’)q_\\pi(s’,a’)\\\\+\\pi(a’|s’)\\sum_{s’’}p(s’’|s’,a’)\\nabla v_\\pi(s’’))\\big] \\big]$$ </p>\n<p>$\\nabla v_\\pi(s’’)$에 대해서 unrolling을 하고 또 나오는 다른 항에 대해서도 계속 반복하다보면 다음과 같이 나타낼 수 있습니다.<br>$$=\\sum_{x\\in S}\\sum_{k=0}^{\\infty}\\Pr(s\\rightarrow x,k,\\pi)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$</p>\n<p>여기에서 $\\Pr(s\\rightarrow x,k,\\pi)$ 는 policy $\\pi$ 에 대해 state $s$에서 state $x$까지 $k$ step만큼 움직일 때의 변환 확률입니다. 따라서 위의 수식을 아래처럼 다시 나타낼 수 있습니다.<br> <!-- \\begin{align}  --><br>$$\\nabla J(\\theta)=\\nabla v_\\pi (s_0)$$<br>$$=\\sum_s(\\sum_{k=0}^{\\infty}\\Pr(s_0\\rightarrow s,k,\\pi))\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$<br>$$=\\sum_s \\eta(s) \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$<br><!-- \\end{align} --></p>\n<ul>\n<li>서튼책에서 $\\eta(s)$는 다음과 같이 표현할 수 있습니다.<ul>\n<li>$\\eta(s)$ = 평균적으로 하나의 episode마다 state에서 머무른 time step의 수 (page 199)</li>\n</ul>\n</li>\n<li>논문에서는 $\\eta(s)$를 다음과 같이 표현합니다.<ul>\n<li>$d^\\pi(s)=\\sum_{t=0}^{\\infty}\\gamma^t Pr(s_t=s|s_0,\\pi)$<br>($\\gamma=1$ is allowed only in episodic tasks = discounted weighting of states)</li>\n</ul>\n</li>\n</ul>\n<p>여기서부터가 논문에는 없는 내용입니다. 위의 최종 수식을 다시 한 번 적으면 아래와 같습니다.<br>$$\\sum_s \\eta(s) \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$</p>\n<p>위의 수식을 아래와 같이 바꿀 수 있습니다.<br>$$\\sum_{s’} \\eta(s’) \\cdot\\sum_s \\frac{\\eta(s)}{\\sum_{s’}\\eta(s’)} \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$</p>\n<p>그러면 이것을 새로운 기호로 다시 나타낼 수 있습니다.<br>$$\\sum_{s’} \\eta(s’) \\cdot\\sum_s \\mu(s)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$ </p>\n<ul>\n<li>여기서 $\\mu(s)$ 는 state distribution이라고 하며, weight를 state distribution으로 바꿔주는 과정이라고 생각할 수 있습니다. 쉽게 말해 어떠한 state에 agent가 머무르는 확률입니다.</li>\n</ul>\n<p>위의 수식은 아래의 수식과 비례합니다. 따라서 최종 형태는 다음과 같습니다.<br>$$\\propto \\sum_s \\mu(s)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$</p>\n<p><br><br></p>\n<h1 id=\"3-Policy-Gradient-with-Approximation\"><a href=\"#3-Policy-Gradient-with-Approximation\" class=\"headerlink\" title=\"3. Policy Gradient with Approximation\"></a>3. Policy Gradient with Approximation</h1><p>이 장에서는 앞서 다뤘던 Theorem 1 중에 $Q^\\pi$에 대해서 중점적으로 다룹니다. 어떠한 $Q^\\pi$가 학습된 function approximator로 근사된다고 합시다. </p>\n<p>$f_w$ ($f_w$는 $S × A \\rightarrow \\Re$)는 parameter $w$를 가지는 어떠한 $Q^\\pi$ 에 대한 근사치입니다. 학습된 function approximator에 대해서 생각하기 때문에, 학습된 $f_w$는 error를 최소화하는 방향으로 parameter $w$를 업데이트합니다. 그러면 다음과 같은 관계식을 생각할 수 있습니다.<br>$$\\Delta w_t \\propto \\frac{\\partial}{\\partial w}\\big[ {\\hat Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]^2 \\propto \\big[{\\hat Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s_t,a_t)}{\\partial w}$$<br>(여기서 ${\\hat Q}^\\pi(s_t,a_t)$ 는 $Q^\\pi(s_t,a_t)$의 unbiased estimator $R_t$입니다.)</p>\n<p>그러면 위와 같은 수식이 local optimum에 수렴을 했을 때, 다음과 같이 나타낼 수 있습니다.<br>$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$</p>\n<ul>\n<li>위의 수식에 대한 추가 설명<ul>\n<li>local optimum으로 수렴을 하기 때문에 당연히 위의 수식은 0이 됩니다.</li>\n<li>또한 stochastic policy이기 때문에 local optimum으로 수렴하려면 모든 state, action에 대해 expectation을 해야합니다. 따라서 $\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)$이 붙게 됩니다.</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-1-Theorem-2-Policy-Gradient-with-Function-Approximation\"><a href=\"#3-1-Theorem-2-Policy-Gradient-with-Function-Approximation\" class=\"headerlink\" title=\"3.1 Theorem 2: Policy Gradient with Function Approximation\"></a>3.1 Theorem 2: Policy Gradient with Function Approximation</h2><p>만약 $f_w$가 아래의 등식을 만족한다고 합시다.<br>$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$</p>\n<p>또한 $f_w$가 policy parameterization과 양립할 수 있다고 합시다. policy parameterization에 대한 여러가지 의미가 있는데 이 논문에서는 다음을 의미합니다.<br>$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}$$</p>\n<ul>\n<li>위의 수식에 대한 추가 설명<ul>\n<li>이 수식은 Monte-Calro Estimation을 이용하기 위한 조건입니다.</li>\n<li>Compatibility Condition이라고 부릅니다.</li>\n<li>우변(= $\\nabla\\log\\pi(s,a)$)으로 유도함으로써 아래의 수식과 같이 sampling을 이용한 추정이 가능해집니다.</li>\n</ul>\n</li>\n</ul>\n<p>따라서 위의 두 수식을 이용하여 아래와 같이 나타낼 수 있습니다.<br>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}f_w(s,a)$$</p>\n<p><br></p>\n<h2 id=\"3-2-Proof-of-Theorem-2\"><a href=\"#3-2-Proof-of-Theorem-2\" class=\"headerlink\" title=\"3.2 Proof of Theorem 2\"></a>3.2 Proof of Theorem 2</h2><p>$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$</p>\n<p>$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}$$</p>\n<p>위의 두 수식을 합치면 다음과 같이 나타낼 수 있습니다.<br>$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}=0$$</p>\n<p>분자와 분모에 있는 $\\pi(s,a)$를 지우면 아래와 같습니다.<br>$$\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]=0$$</p>\n<p>이어서 위의 수식이 0이기 때문에 policy gradient theorem(앞서 다뤘던 Theorem 1)에서 위의 수식을 뺄 수 있습니다.<br>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)-\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ Q^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]$$</p>\n<p>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ Q^\\pi(s_t,a_t)-Q^\\pi(s_t,a_t)+f_w(s_t,a_t) \\big]$$</p>\n<p>$$\\therefore\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}f_w(s_t,a_t)$$</p>\n<p><br><br></p>\n<h1 id=\"4-Application-to-Deriving-Algorithms-and-Advantages\"><a href=\"#4-Application-to-Deriving-Algorithms-and-Advantages\" class=\"headerlink\" title=\"4. Application to Deriving Algorithms and Advantages\"></a>4. Application to Deriving Algorithms and Advantages</h1><p><br></p>\n<h2 id=\"4-1-Application-to-Deriving-Algorithms\"><a href=\"#4-1-Application-to-Deriving-Algorithms\" class=\"headerlink\" title=\"4.1 Application to Deriving Algorithms\"></a>4.1 Application to Deriving Algorithms</h2><p>feature의 linear combination에서 Gibbs distribution (softmax)를 하나의 policy로 생각해볼 수 있습니다.<br>$$\\pi(s,a)=\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_b e^{\\theta^{T}\\phi_{sb} }}$$</p>\n<ul>\n<li>$\\phi_{sa}$: state-action pair $s$, $a$를 나타내는 $l$-dimensional feature vector</li>\n</ul>\n<p>compatible condition을 적용하면 다음과 같습니다. (4.2 Proof of application of compatible condition 참고)<br>$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\frac{1}{\\pi(s,a)}=\\phi_{sa}-\\sum_b \\pi(s,b)\\phi_{sb}$$ </p>\n<p>이어서 $f_w$을 적분한 natural parameterization은 다음과 같습니다.<br>$$f_w(s,a)=w^T\\big[ \\phi_{sa}-\\sum_b \\pi(s,b)\\phi_{sb}\\big]$$</p>\n<ul>\n<li>위의 수식에 대한 추가 설명<ul>\n<li>$f_w$는 policy로서 같은 feature들에 대해 linear합니다.</li>\n<li>$f_w$는 각각의 state에 대해서 평균이 0입니다. ($\\sum_a\\pi(s,a)f_w(s,a)=0$)</li>\n<li>advantage function $A^\\pi(s,a)=Q^\\pi(s,a)-V^\\pi(s)$의 하나의 근사치로서 $f_w$를 생각해도 좋습니다.</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"4-2-Proof-of-application-of-compatible-condition\"><a href=\"#4-2-Proof-of-application-of-compatible-condition\" class=\"headerlink\" title=\"4.2 Proof of application of compatible condition\"></a>4.2 Proof of application of compatible condition</h2><p>증명을 하기 전에 다음을 가정합니다.</p>\n<ol>\n<li>$(\\frac{f(x)}{g(x)})’=f’(x)\\cdot \\frac{1}{g(x)}-f(x)\\cdot \\frac{g’(x)}{g(x)^{2} }=\\frac{f’(x)\\cdot g(x)-f(x)\\cdot g’(x)}{g(x)^{2} }$</li>\n<li>$f(\\theta)=e^{\\theta^{T}\\phi_{sa} }$, $g(\\theta)=\\sum_be^{\\theta^{T}\\phi_{sb} }$</li>\n</ol>\n<p>$$<br>\\begin{align}<br>\\frac{\\partial f_w(s,a)}{\\partial w}&amp;=\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\frac{1}{\\pi(s,a)}<br>\\\\<br>&amp;=\\big(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }}\\big)’\\frac{1}{\\pi(s,a)}<br>\\\\<br>&amp;=\\big[ \\phi_{sa}(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }})-(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }})\\frac{\\sum_b\\phi_{sb}e^{ {\\theta^T}\\phi_{sb} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }}\\big]\\frac{1}{\\pi(s,a)}\\\\<br>&amp;=\\big[ \\phi_{sa}\\pi(s,a)-\\pi(s,a)\\sum_b\\phi_{sb}\\pi(s,b)\\big]\\frac{1}{\\pi(s,a)}<br>\\\\<br>&amp;=\\pi(s,a)\\big[ \\phi_{sa}-\\sum_b\\pi(s,b)\\phi_{sb}\\big]\\frac{1}{\\pi(s,a)}<br>\\\\<br>&amp;\\therefore\\phi_{sa}-\\sum_b\\pi(s,b)\\phi_{sb}<br>\\end{align}<br>$$</p>\n<p><br></p>\n<h2 id=\"4-3-Application-to-Advantages\"><a href=\"#4-3-Application-to-Advantages\" class=\"headerlink\" title=\"4.3 Application to Advantages\"></a>4.3 Application to Advantages</h2><p>Policy Gradient with Function Approximation Theorem (Theorem 2)는 advantage function으로 확장될 수 있습니다.<br>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_sd^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ f_w(s,a)+v(s) \\big]$$</p>\n<ul>\n<li>위의 수식에 대한 추가설명<ul>\n<li>$v$ ($v$는 $S\\rightarrow\\Re$) 는 arbitrary function입니다.</li>\n<li>이 수식은 $\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}=0$이기 때문에 가능해집니다.</li>\n<li>즉, 이 수식은 $\\pi(s,a)$의 gradient에만 dependent하기 때문에 advantage 역할을 하는 함수들을 넣어도 아무런 상관이 없습니다.</li>\n<li>$v$의 선택은 Theorem들에 영향을 미치지 못하지만, 실질적으로 gradient estimator의 variance에 영향을 미칩니다.</li>\n<li>이러한 문제는 전체적으로 이전의 연구에 reinforcement baseline의 사용에 있어서 유사합니다.</li>\n<li>(comment) 위의 수식에서 $f_w(s,a)+v(s)$와 Application to Deriving Algorithms의 $f_w(s,a)$와는 다른 것입니다. Application to Deriving Algorithms의 $f_w(s,a)$은 softmax에 의해 스스로 advantage function의 역할을 할 수 있습니다. 하지만 보통의 경우에는 그러지 못할 수도 있기 때문에 위의 수식처럼 $f_w(s,a)+v(s)$을 추가하여 zero mean만들어서 variance를 줄일 수 있습니다.</li>\n</ul>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"5-Convergence-of-Policy-Iteration-with-Function-Approximation\"><a href=\"#5-Convergence-of-Policy-Iteration-with-Function-Approximation\" class=\"headerlink\" title=\"5. Convergence of Policy Iteration with Function Approximation\"></a>5. Convergence of Policy Iteration with Function Approximation</h1><p><br></p>\n<h2 id=\"5-1-Theorem-3-Convergence-of-Policy-Iteration-with-Function-Approximation\"><a href=\"#5-1-Theorem-3-Convergence-of-Policy-Iteration-with-Function-Approximation\" class=\"headerlink\" title=\"5.1 Theorem 3: Convergence of Policy Iteration with Function Approximation\"></a>5.1 Theorem 3: Convergence of Policy Iteration with Function Approximation</h2><p>policy iteration with function approximation은 locally optimal policy에 수렴합니다. $\\pi$와 $f_w$를 </p>\n<ol>\n<li>compatibility condition을 만족하는 policy와 value function에 대한</li>\n<li>그리고 $\\max_{\\theta, s, a, i, j}\\big| \\frac{\\partial^2\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}\\big|&lt;B&lt;\\infty$에 대한</li>\n</ol>\n<p>어떠한 미분가능한 function approximator라고 합시다.</p>\n<ul>\n<li>(comment) $\\max_{\\theta, s, a, i, j}\\big| \\frac{\\partial^2\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}\\big|&lt;B&lt;\\infty$에서 $\\pi(s,a)$라는 function은 이계미분값이 존재하고, 임의의 상수인 (Bound) B에 bound되어 있기 때문에 function의 그래프는 smooth하다고 볼 수 있습니다. (아래의 그림 중 빨간색 그래프 참고)</li>\n</ul>\n<center> <img src=\"https://www.dropbox.com/s/xx02ejfg5ao19ps/Screen%20Shot%202018-07-10%20at%203.41.06%20PM.png?dl=1\" width=\"200\"> </center>\n\n<p>이어서 ${\\alpha_k}$는 $\\lim_{k\\rightarrow\\infty}\\alpha_k=0$이며 $\\sum_k\\alpha_k=\\infty$를 만족하는 step-size sequence라고 합시다.</p>\n<p>그 때, bounded reward를 가진 MDP에 대해</p>\n<ol>\n<li>어떠한 $\\theta_0, \\pi_k=\\pi(\\cdot,\\cdot,\\theta_k)$</li>\n<li>그리고 $\\sum_sd^{\\pi_{k} }(s)\\sum_a\\pi_k(s,a)\\big[ Q^{\\pi_{k} }(s,a)-f_w(s,a)\\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$로 인하여 $w_k=w$, $\\theta_{k+1}=\\theta_k+\\alpha_k\\sum_sd^{\\pi_{k} }(s)\\sum_a\\frac{\\partial\\pi_k(s,a)}{\\partial\\theta}f_{w_{k} }(s,a)$</li>\n</ol>\n<p>으로 정의된 sequence $\\rho(\\pi_k)$는 $\\lim_{k\\rightarrow\\infty}\\frac{\\partial\\rho(\\pi_k)}{\\partial\\theta}=0$이기 때문에 수렴합니다.</p>\n<ul>\n<li>sequence $\\rho(\\pi_k)_{k=0}^\\infty$에 대한 추가 설명<ul>\n<li>2번의 식을 통해 자연스럽게 actor-critic으로 연결됩니다.</li>\n<li>$\\theta_{k+1}=\\theta_k+\\alpha_k\\sum_sd^{\\pi_{k} }(s)\\sum_a\\frac{\\partial\\pi_k(s,a)}{\\partial\\theta}f_{w_{k} }(s,a)$ 에 따라 $\\theta$가 1, 2, …, $\\infty$로 갈텐데, 거기에 따른 objective function or performance의 sequence입니다.</li>\n<li>(comment) 굳이 sequence라는 표현이 없어도 될 것 같습니다. 어짜피 k가 $\\infty$로 가면 $\\rho(\\pi_k)$가 수렴한다는 의미이기 때문에 불필요해보입니다.</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"5-2-Proof-of-Theorem-3\"><a href=\"#5-2-Proof-of-Theorem-3\" class=\"headerlink\" title=\"5.2 Proof of Theorem 3\"></a>5.2 Proof of Theorem 3</h2><ul>\n<li>Theorem 2는 $\\theta_k$ update가 gradient의 error를 최소화한다는 것을 증명했습니다.</li>\n<li>$\\frac{\\partial^{2}\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}$와 MDP의 reward에서의 bound는, $\\frac{\\partial^{2}\\rho}{\\partial\\theta_i\\partial\\theta_j}$ 또한 bound된다는 것을 증명합니다.</li>\n<li>step-size 필요조건 때문에 이러한 bound된 것들은 Proposition 3.5 from page 96 of Bertsekas and Tsitsiklis (1996)에 적용하기 위해 필요한 조건입니다.</li>\n<li>Proposition 3.5 from page 96 of Bertsekas and Tsitsiklis (1996)은 local optimum으로 수렴한다는 것을 증명했습니다.</li>\n<li>Proposition 3.5를 자세하게 설명하지는 않겠습니다. 일반적인 gradient method의 수렴성을 증명한 것이며, 이 논문의 policy gradient method도 gradient method의 일종이므로 특정 조건을 만족할 때 수렴한다는 것입니다. 수렴하는 원리는 소위 말하는 Lipschitz continuity condition을 만족하기 때문인데 다음과 같습니다. $L&gt;0$인 임의의 상수입니다.<br>$$<br>\\parallel \\nabla f(r) - \\nabla f(\\bar{r}) \\parallel \\leq L \\parallel r - \\bar{r} \\parallel<br>$$<br>즉, 업데이트가 진행되어 나감에 따라서 gradient의 차이가 점점 줄어들게 되므로 언젠가는 0으로 수렴하게 됩니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"6-Summary\"><a href=\"#6-Summary\" class=\"headerlink\" title=\"6. Summary\"></a>6. Summary</h1><p>논문에서 설명한 policy gradient 기법을 요약하면 다음과 같습니다.</p>\n<ul>\n<li>Original maximization problem: $\\max\\rho_{\\theta}=E\\left[R\\right]$</li>\n<li>gradient ascent method를 이용한 parameter update: $\\theta_{t+1} = \\theta_{t} + \\alpha\\left.\\frac{\\partial E[R]}{\\partial\\theta}\\right|_{\\theta_t}$<ul>\n<li>그러나 우리는 gradient를 모르고, 추정하기도 어렵습니다. 왜냐하면 expectation이 안에 있기 때문입니다.</li>\n<li>그렇다면 이것을 추정 가능한 형태로 바꿔야합니다. 다시 말해 expectation이 밖에 있는 형태로 바꾸는 것입니다.</li>\n<li>Expectation이 밖에 있으면 왜 추정이 유리할까요? 바로 Sample mean을 취하면 되기 때문입니다.</li>\n</ul>\n</li>\n<li>Approximate gradient<ul>\n<li>방법1: 논문의 Theorem 1을 이용하면 gradient와 expectation의 위치를 바꿀 수 있습니다.<br>$$\\frac{\\partial E[R]}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)$$</li>\n<li>방법2: Log derivative trick을 이용하면 gradient와 expectation의 위치를 바꿀 수 있습니다. (Theorem 2)<br>$$\\theta_{t+1} = \\theta_{t} + \\alpha E\\left[ \\left. R\\frac{\\partial}{\\partial\\theta}\\log p_{\\theta} \\right\\vert \\theta_t\\right]$$</li>\n<li>그리고 특정 trajectory를 따라가면서 return값을 구하고 이것을 여러 번 수행하여 sample mean을 취하면 gradient를 추정하는 것이 가능합니다.</li>\n<li>이러한 기법을 제시한 기존의 연구가 REINFORCE입니다.</li>\n<li>여러 Trajectory를 이용하므로 variance가 높을 수 밖에 없습니다.</li>\n<li>Advantage를 활용하거나 하는 방식으로 이후 여러 연구가 진행되었습니다.</li>\n</ul>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"처음으로\"><a href=\"#처음으로\" class=\"headerlink\" title=\"처음으로\"></a>처음으로</h1><h2 id=\"PG-Travel-Guide\"><a href=\"#PG-Travel-Guide\" class=\"headerlink\" title=\"PG Travel Guide\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/\">PG Travel Guide</a></h2><p><br></p>\n<h1 id=\"다음으로\"><a href=\"#다음으로\" class=\"headerlink\" title=\"다음으로\"></a>다음으로</h1><h2 id=\"DPG-여행하기\"><a href=\"#DPG-여행하기\" class=\"headerlink\" title=\"DPG 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/27/2_dpg/\">DPG 여행하기</a></h2>"},{"title":"High-Dimensional Continuous Control using Generalized Advantage Estimation","date":"2018-06-23T10:18:45.000Z","author":"양혁렬, 이동민","subtitle":"피지여행 6번째 논문","_content":"\n<center> <img src=\"https://www.dropbox.com/s/p8gfpyo6xf9wm5w/Screen%20Shot%202018-07-18%20at%201.25.53%20AM.png?dl=1\" width=\"700\"> </center>\n\n논문 저자 : John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan and Pieter Abbeel\n논문 링크 : https://arxiv.org/pdf/1506.02438.pdf\nProceeding : International Conference of Learning Representations (ICLR) 2016\n정리 : 양혁렬, 이동민\n\n---\n\n# 1. 들어가며...\n\n현존하는 Policy Gradient Method들의 목적은 누적되는 reward들을 optimization하는 것입니다. 하지만 학습할 때에 많은 양의 sample이 필요로 하고, 들어오는 data가 nonstationarity임에도 불구하고 stable and steady improvement가 어렵습니다.\n\n그래서 이 논문에서는 다음과 같은 방법을 제시합니다.\n\n- TD($\\lambda$)와 유사한 advantage function의 exponentially-weighted estimator를 사용하여 policy gradient estimate의 variance를 줄이는 것 \n- policy와 value function에 대한 Trust Region Optimization 사용하는 것\n\n3D locomotion tasks에 대한 empirical results는 다음과 같습니다.\n\n- bipedal and quadrupedal simulated robots의 달리는 자세를 학습\n- bipedal 사람이 땅에 누워있다가 일어서는 것을 학습\n\n※ TD($\\lambda$)를 혹시 모르실 경우도 있을 것 같아 간략하게 정리하였습니다. http://dongminlee.tistory.com/10 를 먼저 보시고 아래의 내용을 봐주시기 바랍니다!\n\n<br><br>\n\n# 2. Introduction\n\n기본적으로 \"parameterized stochastic policy\"를 가정합니다. 이 때 expected total returns의 gradient에 대한 unbiased estimate를 얻을 수 있는데 이것을 REINFORCE라고 부릅니다. 하지만 하나의 action의 결과가 과거와 미래의 action의 결과로 혼동되기 때문에 gradient estimator의 high variance는 시간에 따라 scaling됩니다.\n\n또 다른 방법은 Actor-Critic이 있습니다. 이 방법은 empirical returns보다 하나의 value function을 사용합니다. 또한 bias하고 lower variance를 가진 estimator입니다. 구체적으로 말하자면, high variance하다면 더 sampling을 하면 되는 반면에 bias는 매우 치명적입니다. 다시 말해 bias는 algorithm이 converge하는 데에 실패하거나 또는 local optimum이 아닌 poor solution에 converge하도록 만듭니다.\n\n따라서 이 논문에서는 $\\gamma\\in [0,1]$ and $\\lambda\\in [0,1]$에 의해 parameterized estimation scheme인 Generalized Advantage Estimator(GAE)를 다룹니다.\n\n이 논문을 대략적으로 요약하자면 다음과 같습니다.\n\n- 효과적인 variance reduction scheme인 GAE를 다룹니다. 또한 실험할 때 batch trust region algorithm에 더하여 다양한 algorithm들에 적용됩니다.\n- value function에 대해 trust region optimization method를 사용합니다. 이렇게 함으로서 더 robust하고 efficient한 방법이 됩니다.\n- A와 B를 합쳐서, 실험적으로 control task에 neural network policies를 learning하는 데에 있어서 효과적인 algorithm을 얻습니다. 이러한 결과는 high-demensional continuous control에 RL을 사용함으로서 state of the art로 확장되었습니다.\n\n<br><br>\n\n# 3. Preliminaries\n\n먼저 policy optimization의 \"undiscounted formulation\"을 가정합니다. (undiscounted formulation에 주목합시다.)\n\n- initial state $s_0$는 distribution $\\rho_0$으로부터 sampling된 것입니다.\n- 하나의 trajectory ($s_0, a_0, s_1, a_1, ...$)는 terminal state에 도달될 때 까지 policy $a_t$ ~ $\\pi(a_t | s_t)$에 따라서 action을 sampling하고, dynamics $s_{t+1}$ ~ $P(s_{t+1} | s_t, a_t)$에 따라서 state를 sampling함으로써 생성됩니다.\n- reward $r_t = r(s_t, a_t, s_{t+1})$은 매 time step마다 받아집니다.\n- 목표는 모든 policies에 대해 finite하다고 가정됨으로서 expected total reward $\\sum_{t=0}^{\\infty} r_t$를 maximize하는 것입니다.\n\n여기서 중요한 점은 $\\gamma$를 discount의 parameter로 사용하는 것이 아니라 \"bias-variance tradeoff를 조절하는 parameter로 사용한다\" 는 것입니다.\n\npolicy gradient method는 gradient $g := \\nabla_\\theta \\mathbb{E} [\\sum_{t=0}^\\infty r_t]$를 반복적으로 estimate함으로써 expected total reward를 maximize하는 것인데, policy gradient에는 여러 다른 표현들이 있습니다.\n$$g = \\mathbb{E} [\\sum_{t=0}^\\infty \\Phi_t \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$\n\n여기서 $\\Phi_t$는 아래 중의 하나일 수 있습니다.\n\n1. $\\sum_{t=0}^\\infty r_t$: trajectory의 total reward\n2. $\\sum_{t'=t}^\\infty r_t'$: action $a_t$ 후의 reward\n3. $\\sum_{t'=t}^\\infty r_t' - b(s_t)$: 2의 baselined version\n4. $Q^\\pi (s_t, a_t)$: state-action value function\n5. $A^\\pi (s_t, a_t)$: advantage function\n6. $r_t + V^\\pi (s_{t+1}) - V^\\pi (s_t)$: TD residual\n\n위의 번호 중 4, 5, 6의 수식들은 다음의 정의를 사용합니다.\n\n- $V^\\pi (s_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_t:\\infty} [\\sum_{l=0}^\\infty r_{t+1}]$\n- $Q^\\pi (s_t, a_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty} [\\sum_{l=0}^\\infty r_{t+l}]$\n- $A^\\pi (s_t, a_t) := Q^\\pi (s_t, a_t) - V^\\pi (s_t)$, (Advantage function)\n\n추가적으로 colon notation $a : b$는 포괄적인 범위 $(a, a+1, ... , b)$입니다. (잘 기억해둡시다. 뒤에 계속해서 colon notation이 나옵니다.)\n\n여기서부터 parameter $\\gamma$에 대해 좀 더 자세히 알아봅시다. parameter $\\gamma$는 bias하면서 동시에 reward를 downweighting함으로써 variance를 줄입니다. 다시 말해 MDP의 discounted formulation에서 사용된 discounted factor와 일치하지만, 이 논문에서는 \"$\\gamma$를 undiscounted MDP에서 variance reduction parameter\"로 다룹니다. -> 결과는 같지만, 의미와 의도가 다릅니다.\n\ndiscounted value function들은 다음과 같습니다.\n\n- $V^{\\pi, \\gamma} (s_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_t:\\infty} [\\sum_{l=0}^\\infty \\gamma^l r_{t+l}]$ \n- $Q^\\pi (s_t, a_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty} [\\sum_{l=0}^\\infty \\gamma^l r_{t+l}]$\n- $A^{\\pi, \\gamma} (s_t, a_t) := Q^{\\pi, \\gamma} (s_t, a_t) - V^{\\pi, \\gamma} (s_t)$\n\n따라서 policy gradient에서의 discounted approximation은 다음과 같이 나타낼 수 있습니다.\n$$g^\\gamma := \\mathbb{E}_{s_{0:\\infty} a_{0:\\infty}} [\\sum_{t=0}^\\infty A^{\\pi, \\gamma} (s_t, a_t) \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$\n뒤이어 나오는 section에서는 위의 수식에서 $A^{\\pi, \\gamma}$에 대해 biased (but not too biased) estimator를 얻는 방법에 대해서 나옵니다.\n\n다음으로 advantage function에서 새롭게 접하는 \"$\\gamma$-just estimator\"의 notation에 대해 알아봅시다.\n\n- 먼저 $\\gamma$-just estimator는 $g^\\gamma$ ${}^1$를 estimate하기 위해 위의 수식에서 $A^{\\pi, \\gamma}$를 $\\gamma$-just estimator로 사용할 때, bias하지 않은 estimator라고 합시다. 그리고 이 $\\gamma$-just advantage estimator를 <img src=\"https://www.dropbox.com/s/7g2glqx2kbfynaa/Screen%20Shot%202018-08-22%20at%209.03.46%20PM.png?dl=1\" width=\"100\"> 라고 하고, 전체의 trajectory에 대한 하나의 function이라고 합시다.\n- ${}^1$에서 이 논문의 저자가 하고 싶은 말이 있는 것 같습니다. 개인적으로 $\\gamma$-just estimator를 이해하는 데에 있어서 중요한 주석이라 정확히 해석하고자 합니다.\n    - $A^\\pi$를 $A^{\\pi, \\gamma}$로 사용함으로써 이미 bias하다라고 말했지만, \"이 논문에서 하고자 하는 것은 $g^\\gamma$에 대해 unbiased estimate를 얻고 싶은 것입니다.\" 하지만 undiscounted MDP의 policy gradient에 대해서는 당연히 $\\gamma$를 사용하기 때문에 biased estimate를 얻습니다. 개인적으로 이것은 일단 무시하고 $\\gamma$를 사용할 때 어떻게 unbiased estimate를 얻을 지에 대해 좀 더 포커스를 맞추고 있는 것 같습니다.\n    - 그러니까 저 $A^{\\pi, \\gamma}$를 $\\gamma$-just estimator로 바꿔줌으로써 unbiased estimate를 하고 싶다는 것이 뒤이어 나오는 정의와 명제의 핵심이라고 할 수 있습니다.\n\nDefinition 1.\n먼저 가정을 합니다. (가정을 바탕으로 이루어지는 정의라는 것을 주목합시다.) 만약\n<center ><img src=\"https://www.dropbox.com/s/aia9f1l99wyxgah/Screen%20Shot%202018-08-22%20at%209.07.05%20PM.png?dl=1\" width=\"700\"> </center>\n두 수식이 같다면, estimator $\\hat{A}_t$는 $\\gamma$-just입니다.\n\n그리고 만약 모든 t에 대해서 $\\hat{A}_t$이 $\\gamma$-just이라면, 다음과 같이 표현할 수 있습니다.\n\n<center ><img src=\"https://www.dropbox.com/s/r4m6ktw8geeu7pe/Screen%20Shot%202018-08-22%20at%209.29.18%20PM.png?dl=1\" width=\"420\"> </center>\n\n위의 수식이 바로 unbiased estimate입니다.\n\n$\\gamma$-just인 $\\hat{A}_t$에 대한 한가지 조건은 $\\hat{A}_t$이 두 가지 function $Q_t$ and $b_t$로 나뉠 수 있다는 것입니다.\n\n- $Q_t$는 $\\gamma$-discounted Q-function의 unbiased estimator입니다.\n- $b_t$는 action $a_t$전에 sampling된 states and actions의 arbitrary function이다. \n\nProposition 1.\n모든 $(s_t, a_t)$에 대해,\n$$\\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty | s_t, a_t} [Q_t (s_{t:\\infty}, a_{t:\\infty})] = Q^{\\pi, \\gamma} (s_t, a_t)$$\n로 인하여 $\\hat{A}_t$이 \n\n<center> <img src=\"https://www.dropbox.com/s/59ydsav6djwyevl/Screen%20Shot%202018-08-22%20at%209.27.13%20PM.png?dl=1\" width=\"380\"> </center>\n\n형태라고 가정합시다. (가정을 바탕으로 이루어지는 명제라는 점을 주목합시다.)\n\n그 때, $\\hat{A}_t$은 $\\gamma$-just입니다.\n\n이 명제에 대한 증명은 Appendix B에 있습니. 그리고 $\\hat{A}_t$에 대한 $\\gamma$-just advantage estimator들은 다음과 같습니다.\n\n- $\\sum_{l=0}^\\infty \\gamma^l r_{t+1}$\n- $A^{\\pi, \\gamma} (s_t, a_t)$\n- $Q^{\\pi, \\gamma} (s_t, a_t)$\n- $r_t + \\gamma V^{\\pi, \\gamma} (s_{t+1}) - V^{\\pi, \\gamma} (s_t)$\n\n증명을 보기 전에 먼저 Advantage function에 대해서 먼저 살펴봅시다. 아래의 내용은 Sutton이 쓴 논문인 Policy Gradient Methods for Reinforcement Learning with Function Approximation(2000)에서 나온 내용을 리뷰한 것입니다. \n\n<center> <img src=\"https://www.dropbox.com/s/x737yq97ut6gp1a/Screen%20Shot%202018-07-15%20at%201.16.09%20PM.png?dl=1\" width=\"600\"> </center>\n\n여기서는 \"이 수식은 $\\sum_a \\frac{\\partial \\pi (s,a)}{\\partial \\theta} = 0$이기 때문에 가능해진다.\", \"즉, 이 수식은 $\\pi(s,a)$의 gradient에만 dependent하기 때문에 advantage 역할을 하는 함수들을 넣어도 아무런 상관이 없다\"라는 두 문장을 기억해둡시다.\n\n이제 증명을 보면 됩니다.\n\n<center> <img src=\"https://www.dropbox.com/s/e7sj2fwcm1f7hof/figure2.jpg?dl=1\" width=\"600\"> </center>\n<center> <img src=\"https://www.dropbox.com/s/kyk4jes1202az4m/figure3.jpg?dl=1\" width=\"600\"> </center>\n\n위의 증명 수식에서 Q와 b의 각각 세번째 수식과 마지막 수식을 자세히 봅시다.\n\n1. $Q$에 대한 증명\n    - 빨간색 부분\n        - $\\mathbb{E}_{s_{t+1:\\infty}, a_{t+1:\\infty}} [Q_t (s_{0:\\infty}, a_{0:\\infty})]$를 쉽게 말하자면 \"모든(과거, 현재, 미래) state와 action에 대해서 expectation을 $t+1$부터 $\\infty$까지 하겠다.\"라는 말입니다. 이렇게 함으로써 원래는 $Q^\\pi (s_t, a_t)$가 나와야 맞는 건데, 앞에서 살펴봤다시피 4번째 수식 밑줄 친 자리에는 advantage 역할을 하는 함수들을 넣어도 결과값에 아무런 영향을 끼치지 않기 때문에 의도적으로 $A^\\pi (s_t, a_t)$로 바꾼 것입니다. 또한 이 증명을 바탕으로 $\\hat{A}$이 $\\gamma$-just라는 것을 표현하고 싶기 때문이라고 봐도 됩니다.\n\n    - 파란색 부분\n        - 또한 $a_{0:t}$에서 $a_{0:t-1}$로 변한 것은 $A^\\pi (s_t, a_t)$에 baseline이 들어가기 때문에 바꿔준 것입니다.\n\n2. $b$에 대한 증명\n    - 빨간색 부분\n        - $\\mathbb{E}_{s_{t+1:\\infty}, a_{t+1:\\infty}} [\\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$가 0으로 바뀌는 것은 위에서 설명했듯이 $\\nabla_\\theta$자체가 $\\log \\pi_\\theta$만 gradient하기 때문에 이것을 expectation을 취하면 0이 됩니다.\n\n<br><br>\n\n# 4. Advantage Function Estimation\n\n이번 section에는 discounted advantage function $A^{\\pi, \\gamma} (s_t, a_t)$의 accurate estimate $\\hat{A}_t$에 대해서 살펴봅시다. 이에 따른 수식은 다음과 같습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/sn16g5iyzzflju0/Screen%20Shot%202018-08-22%20at%209.22.49%20PM.png?dl=1\" width=\"300\"> </center>\n\n여기서 n은 a batch of episodes에 대하여 index한 것입니다.\n\n$V$를 approximate value function이라고 합시다. 그리고 $\\delta_t^V = r_t + \\gamma V(s_{t+1}) - V(s_t)$이라고 합시다.\n\n만약 (이전과 마찬가지로 가정부터 합니다.) correct value function $V = V^{\\pi, \\gamma}$가 있다고 한다면, 이것은 $\\gamma$-just advantage estimator입니다. 실제로, $A^{\\pi, \\gamma}$의 unbiased estimator는 다음과 같습니다.\n$$\\mathbb{E}_{s_{t+1}} [\\delta_t^{V^{\\pi, \\gamma}}] = \\mathbb{E}_{s_{t+1}} [r_t + \\gamma V^{\\pi, \\gamma} (s_{t+1}) - V^{\\pi, \\gamma} (s_t)]$$\n$$= \\mathbb{E}_{s_{t+1}} [Q^{\\pi, \\gamma} (s_t, a_t) - V^{\\pi, \\gamma} (s_t)] = A^{\\pi, \\gamma} (s_t, a_t)$$\n그러나, \"이 estimator는 유일하게 $V = V^{\\pi, \\gamma}$에 대한 $\\gamma$-just입니다.\" 다른 경우라면 이것은 biased policy gradient estimate일 것입니다. (우리가 하고 싶은 것은 $V$에 대해서만 unbiased estimator가 아니라 advantage function에 대해서 일반화된 unbiased estimator를 얻고 싶은 것입니다. 그래서 아래에서도 나오겠지만, $\\gamma$와 함께 $\\lambda$를 이용한 estimator가 나옵니다.ㅏ)\n\n그래서 $\\delta$에 대해 $k$의 sum으로 생각해봅시다. 이것을 $\\hat{A}_t^{(k)}$라고 하자. 그러면 아래와 같이 표현할 수 있습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/ra7hxksveg2hz45/figure4.jpg?dl=1\" width=\"600\"> </center>\n\n$\\hat{A}_t^{(k)}$은 returns의 $k$-step estimate와 연관지을 수 있고, $\\delta_t^V = \\hat{A}_t^{(1)}$의 case와 유사하게도 $\\hat{A}_t^{(k)}$를 advantage function의 estimator로 생각할 수 있습니다.\n\n여기서 $k \\rightarrow \\infty$로 생각해보면 bias가 일반적으로 점점 더 작아집니다. 왜냐하면 $\\gamma^k V(s_{t+k})$가 점점 많이 discounted되서 $-V(s_t)$가 bias에 영향을 미치지 못하기 때문입니다. $k \\rightarrow \\infty$를 취하면 다음과 같은 수식이 나옵니다.\n\n<center> <img src=\"https://www.dropbox.com/s/13fn9wcup8pfh9u/Screen%20Shot%202018-08-22%20at%209.19.52%20PM.png?dl=1\" width=\"320\"> </center>\n\n우변의 수식과 같이 empirical returns에서 value function baseline을 뺀 것으로 나타낼 수 있습니다.\n\nGeneralized Advantage Estimator GAE($\\gamma, \\lambda$)는 $k$-step estimators의 exponentially-weighted average로 나타낼 수 있습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/yg1ybmfkep3towu/figure5.jpg?dl=1\" width=\"600\"> </center>\n\n(TD($\\lambda$)를 떠올려주세요..!)\n\n위의 마지막 수식에서도 알 수 있듯이, advantage estimator는 Bellman residual terms의 discounted sum과 관련있는 간단한 수식입니다. 다음 section에서 modified reward function을 가진 MDP에 returns로서의 관점에서 위의 마지막 수식을 더 자세히 살펴봅시다. 위의 수식은 TD($\\lambda$)와 많이 유사합니다. 그러나 TD($\\lambda$)는 value function를 estimator하고, 여기서는 advantage function을 estimator합니다.\n\n위의 수식에서 $\\lambda = 0$ and $\\lambda = 1$에 대해서는 특별한 case가 존재한다. 수식으로 표현하면 다음과 같습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/ufglxzanhcnbi36/figure6.jpg?dl=1\" width=\"600\"> </center>\n\n- $GAE(\\gamma, 1)$은 $V$의 정확도와 관계없이 $\\gamma$-just입니다. 그러나 returns의 sum때문에 high variance합니다.\n- $GAE(\\gamma, 0)$은 $V = V^{\\pi, \\gamma}$에 대해 $\\gamma$-just입니다. 그리고 bias하지만 일반적으로 훨씬 lower variance를 가집니다. \n- $0 < \\lambda < 1$에 대해 GAE는 parameter $\\lambda$를 control하여 bias와 variance사이에 compromise를 만듭니다.\n\n두 가지 별개의 parameter $\\gamma$ and $\\lambda$를 가진 advantage estimator는 bias-variance tradeoff에 도움을 줍니다. 그러나 이 두 가지 parameter는 각각 다른 목적을 가지고 작동합니다.\n\n- $\\gamma$는 가장 중요하게 value function $V^{\\pi, \\gamma}$ 의 scale을 결정합니다. 또한 $\\gamma$는 $\\lambda$에 의존하지 않습니다. $\\gamma < 1$로 정하는 것은 policy gradient estimate에서 bias합니다.\n- 반면에, $\\lambda < 1$는 유일하게 value function이 부정확할 때 bias합니다. 그리고 경험상, $\\lambda$의 best value는 $\\gamma$의 best value보다 훨씬 더 낮습니다. 왜냐하면 $\\lambda$가 정확한 value function에 대해 $\\gamma$보다 훨씬 덜 bias하기 때문입니다.\n\nGAE를 사용할 때, $g^\\gamma$의 biased estimator를 구성할 수 있습니다. 수식은 다음과 같습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/ok57dsk52o7pl9r/Screen%20Shot%202018-08-22%20at%209.17.54%20PM.png?dl=1\" width=\"670\"> </center>\n\n여기서 $\\lambda = 1$일 때 동일해집니다.\n\n<br><br>\n\n# 5. Interpretation as Reward Shaping\n\n이번 section에서는 앞서 다뤘던 수식 $\\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^V$를 modified reward function의 MDP의 관점으로 생각해봅시다. 조금 더 구체적으로 말하자면, MDP에서 reward shaping transformation을 실행한 후에 적용된 extra discounted factor로서 $\\lambda$를 어떻게 볼 것인지에 대해서 다룹니다.\n\n(개인적인 comment) 한 가지 먼저 언급하자면, 본래의 목적은 reward shaping이 아니라 variance reduction입니다. 이번 section은 그저 이전에 이러한 개념이 있었고, gae를 다른 관점에서 생각해보자라는 뜻에서 나온 section인 것 같습니다. '아~ 이러한 개념이 있구나~!' 정도로만 알면 될 것 같습니다. 'gae가 reward shaping의 효과까지 있어서 이러한 section을 넣은 것일까?' 라는 생각도 해봤지만 아직 잘 모르겠습니다. 실험부분에도 딱히 하지 않은 걸로 봐서는.. 아닌 것 같기도 하고.. 아직까지는 그저 'reward shaping의 관점에서 봤을 때에도 gae로 만들어줄 수 있다.' 정도만 생각하려고 합니다.\n\nReward Shaping이란 개념 자체는 일반적인 알고리즘 분야(최단경로 문제 등)에도 있습니다. 하지만 이 개념이 머신러닝에 적용된 건 Andrew Y. Ng이 쓴 Policy Invariance under Reward Transformations Theory and Application to Reward Shaping(1999) 논문입니다. 논문에서 Reward Shaping을 설명하는 부분은 간략하게 다음과 같습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/0w9cuxjkhz2mbcy/figure7.jpg?dl=1\" width=\"400\"> </center>\n<center> <img src=\"https://www.dropbox.com/s/xcp9eqvcbv3268g/figure8.jpg?dl=1\" width=\"400\"> </center>\n\n위의 글만 보고 이해가 잘 안됩니다. 그래서 다른 자료들을 찾던 중에 다음의 그림을 찾게 되었습니다. (아래의 그림은 Youtube에 있는 [Udacity 영상](https://www.youtube.com/watch?v=xManAGjbx2k&t=95s)에서 있는 그림입니다.)\n\n<center> <img src=\"https://www.dropbox.com/s/87kacngez9cl2e6/figure9.jpg?dl=1\" width=\"400\"> </center>\n\n이 그림을 통해서 Reward Shaping을 이해해봅시다.\n\nReward Shaping(보상 형성)을 통해서 뭘 하고 싶은지 목적부터 봅시다. reward space가 sparse 한 경우에 reward가 너무 드문드문 나옵니다. 따라서 이것을 꾸준히 reward를 받을 수 있도록 바꿉니다. potential-based shaping function인 $\\Phi$를 만들어서 더하고 빼줍니다. 저 $\\Phi$ 자리에는 대표적으로 state value function이 많이 들어간다고 합니다. (아직 목적이 이해가 안될 수 있습니다. Reward Shaping에 대해서 다 보고 나서 다시 한 번 읽어봅시다.)\n\n위의 그림은 돌고래가 점프하여 불구멍을 통과해야하는 환경입니다. 하지만 저 불구멍을 통과하기 위해서는 (1) 점프도 해야하고, (2) 불도 피해야하고, (3) 알맞게 착지까지 완료해야합니다. 이렇게 해야 reward를 +1 얻습니다. \n\n생각해봅시다. 어느 세월에 점프도 해야하고.. 불도 피해야하고.. 알맞게 착지까지해서 reward를 +1 얻겠습니까? 따라서 그 전에도 잘하고 있는 지, 못하고 있는 지를 판단하기 위해, 다시 말해 reward를 꾸준히 받도록 다음과 같이 transformed reward function $\\tilde{r}$을 정의합니다.\n\n$$\\tilde{r} (s, a, s') = r(s, a, s') + \\gamma \\Phi (s') - \\Phi (s)$$\n\n- 여기서 $\\Phi : S \\rightarrow \\mathbb{R}$를 state space에서의 arbitrary scalar-valued function을 나타냅니다. 그리고 $\\Phi$자리에는 대표적으로 state value function이 들어간다고 생각합시다.\n- 형태가 TD residual term의 결과와 비슷하지만 의미와 의도가 다릅니다. reward shaping은 sparse reward 때문이고, 이전 section에서 봤던 gae는 variance reduction때문에 나온 것입니다.\n\n<center> <img src=\"https://www.dropbox.com/s/tkfpqniaazwfbld/figure10.jpg?dl=1\" width=\"600\"> </center>\n\n이 transformation은 discounted advantage function $A^{\\pi, \\gamma}$으로도 둘 수 있습니다. state $s_t$를 시작하여 하나의 trajectory의 rewards의 discounted sum을 표현하면 다음과 같습니다.\n$$\\sum_{l=0}^\\infty \\gamma^l \\tilde{r} (s_{t+l}, a_t, s_{t+l+1}) = \\sum_{l=0}^\\infty \\gamma^l r(s_{t+l}, a_{t+l}, s_{t+l+1}) - \\Phi(s_t)$$\n\n$\\tilde{Q}^{\\pi, \\gamma}, \\tilde{V}^{\\pi, \\gamma}, \\tilde{A}^{\\pi, \\gamma}$를  transformed MDP의 value function과 advantage function이라고 하면, 다음과 같은 수식이 나옵니다.\n$$\\tilde{Q}^{\\pi, \\gamma} (s, a) = Q^{\\pi, \\gamma} (s, a) - \\Phi (s)$$\n$$\\tilde{V}^{\\pi, \\gamma} (s, a) = V^{\\pi, \\gamma} (s, a) - \\Phi (s)$$\n$$\\tilde{A}^{\\pi, \\gamma} (s, a) = (Q^{\\pi, \\gamma} (s, a) - \\Phi (s)) - (V^\\pi (s) - \\Phi (s)) = A^{\\pi, \\gamma} (s, a)$$\n\n이제 reward shaping의 idea를 가지고 어떻게 policy gradient estimate를 얻을 수 있는 지에 대해서 알아봅시다.\n\n- $0 \\le \\lambda \\le 1$의 범위에 있는 steeper discount $\\gamma \\lambda$를 사용합니다.\n- shaped reward $\\tilde{r}$는 Bellman residual term $\\delta^V$와 동일합니다.\n- 그리고 $\\Phi = V$와 같다고 보면 다음과 같은 수식이 나옵니다.\n\n$$\\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\tilde{r} (s_{t+l}, a_t, s_{t+l+1}) = \\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^V = \\hat{A}_t^{GAE(\\gamma, \\lambda)}$$\n이렇게 shaped rewards의 $\\gamma \\lambda$-discounted sum을 고려함으로써 GAE를 얻을 수 있습니다. 더 정확하게 $\\lambda = 1$은 $g^\\gamma$의 unbiased estimate이고, 반면에 $\\lambda < 1$은 biased estimate입니다.\n\n좀 더 나아가서, shaping transformation과 parameters $\\gamma$ and $\\lambda$의 결과를 보기 위해, response function $\\chi$를 이용하면 다음과 같은 수식이 나옵니다.\n$$\\chi (l; s_t, a_t) = \\mathbb{E} [r_{t+l} | s_t, a_t] - \\mathbb{E} [r_{t+l} | s_t]$$\n추가적으로 $A^{\\pi, \\gamma} (s, a) = \\sum_{l=0}^\\infty \\gamma^l \\chi (l; s, a)$입니다.\n그래서 discounted policy gradient estimator는 다음과 같이 쓸 수 있다.\n$$\\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) A^{\\pi, \\gamma} (s_t, a_t) = \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) \\sum_{l=0}^\\infty \\gamma^l \\chi (l; s, a)$$\n\n<br><br>\n\n# 6. Value Fuction Estimation\n\n이번 section에서는 Trust Region Optimization Scheme에 따라 Value function을 Estimation합니다.\n\n<br>\n## 6.1 Simplest approach\n\n$$minimize_{\\phi} \\sum_{n=1}^{N} \\vert\\vert V_{\\phi}(s_n) - \\hat{V_n} \\vert\\vert^{2}$$\n\n- 위는 가장 간단하게 non-linear approximation으로 푸는 방법입니다.\n- $\\hat{V_t} = \\sum_{l=0}^{\\infty}\\gamma^l r_{t+l}$은 reward 에 대한 discounted sum을 의미합니다.\n\n<br>\n## 6.2 Trust region method to optimize the value function\n\n- Value function을 최적화 하기 위해 trust region method를 사용합니다.\n- Trust region은 최근 데이터에 대해 overfitting되는 것을 막아줍니다.\n\nTrust region문제를 풀기 위해서는 다음 스텝을 따릅니다.\n\n- $\\sigma^2 = \\frac{1}{N} \\sum_{n=1}^{N} \\vert\\vert V_{\\phi old}(s_n) - \\hat{V_n} \\vert\\vert^{2}$을 계산합니다.\n- 그 후에 다음과 같은 constrained opimization문제를 풉니다.\n\n<center> <img src=\"https://www.dropbox.com/s/d74cg3votxi2dbg/Screen%20Shot%202018-08-22%20at%209.13.28%20PM.png?dl=1\" width=\"300\"> </center>\n\n<center> <img src=\"https://www.dropbox.com/s/jrkjax71t3j3vk4/Screen%20Shot%202018-08-22%20at%209.13.35%20PM.png?dl=1\" width=\"370\"> </center>\n\n- 위의 수식은 사실 old Value function과 새로운 Value function KL distance가 $\\epsilon$ 다 작아야한다는 수식과 같습니다. Value function이 평균은 $V_{\\phi}(s)$이고 분산이 $\\sigma^2$인 conditional Gaussian distribution으로 parameterize되었을 뿐입니다.\n\n<img width =\"400px\" src=\"https://www.dropbox.com/s/uw0v05feu8chqmc/Screenshot%202018-07-08%2010.04.38.png?dl=1\"> \n\n이 trust region문제의 답을 conjudate gradient algorithm을 이용하여 근사값을 구할 수 있습니다. 특히, 다음과 같은  quadratic program을 풀게됩니다.\n\n<center> <img src=\"https://www.dropbox.com/s/pawjkm8b9ycwsl5/Screen%20Shot%202018-08-22%20at%209.10.36%20PM.png?dl=1\" width=\"250\"> </center>\n\n<center> <img src=\"https://www.dropbox.com/s/va7wms8uqxnw5rq/Screen%20Shot%202018-08-22%20at%209.10.40%20PM.png?dl=1\" width=\"370\"> </center>\n\n- 여기서 $g$는 objective 의 gradient입니다.\n- $j_n = \\nabla_{\\phi} V_{\\phi}(s_n)$일때, $H = \\frac{1}{N} \\sum_{n} j_n j^T_n$이며, $H$는 objective의 hessian에 대해서 gaussian newton method로 근사한 값입니다. 따라서, value function을 conditional probability로 해석한다면 Fisher information matrix가 됩니다.\n- 구현할때의 방법은 TRPO에서 사용한 방법과 모두 같습니다.\n\n<br><br>\n\n# 7.Experiments\n\n실험은 다음 두 가지 물음에 대해서 디자인 되었습니다.\n\n- GAE에 따라서 episodic total reward를 최적화 할때, $\\lambda$와 $\\gamma$가 변함에 따라서 어떤 경험적인 효과를 볼 수 있는지?\n- GAE와 trust region alogorithm을 policy와 value function 모두에 함께 사용했을 때 어려운 문제에 적용되는 큰 뉴럴넷을 최적화 할 수 있을지?\n\n<br>\n## 7.1 Policy Optimization Algorithm\n\nPolicy update는 TRPO를 사용합니다. TRPO에 대한 설명은 여기서는 생략하겠습니다. TRPO 포스트를 보고 돌아와주세요!\n\n- 이전 TRPO에서 이미 TRPO와 다른 많은 알고리즘들을 비교하였기 때문에, 여기서는 똑같은 짓을 반복하지 않고 $\\lambda$, $\\gamma$가 변함에 따라 어떤 영향이 있는 지에 대한 실험에 집중하겠다고 합니다. (귀찮았던거죠..)\n\n- TRPO를 적용한 GAE의 최종 알고리즘은 다음과 같습니다.\n\n<center> <img width = \"600px\" src=\"https://www.dropbox.com/s/b1klz11f2frrvg4/Screenshot%202018-07-08%2010.33.54.png?dl=1\"> </center>\n<center> <img width = \"300px\" src=\"https://www.dropbox.com/s/35fxte05pqtiel7/Screenshot%202018-07-08%2010.35.22.png?dl=1\"> </center>\n<center> <img width = \"300px\" src=\"https://www.dropbox.com/s/u35pjow3w50bvz1/Screenshot%202018-07-08%2010.36.03.png?dl=1\"> </center>\n\n- 여기서 주의할 점은 Policy update($\\theta_i \\rightarrow \\theta_{i+1}$)에서 $V_{\\phi_i}$를 사용했다는 점입니다.\n- 만약 Value function을 먼저 update하게 된다면 추가적인 bias가 발생합니다.\n- 극단적으로 생각해보아서, 우리가 Value function을 완벽하게 overfit을 해낸다면 Bellman residual($r_t + \\gamma V(s_{t+1}) - V(S_t)$)은 0이 됩니다. 그럼 Policy gradient의 estimation도 거의 0이 될 것입니다.\n\n<br>\n## 7.2 Expermint details\n\n### 7.2.1 Environment \n실험에서 사용된 환경은 다음 네 가지 입니다.\n\n1. classic cart-pole (x 3D)\n2. bipedal locomotion\n3. quadrupedal locomotion\n4. dynamically standing up for the biped\n\n### 7.2.2 Architecture\n\n- 3D robot task에 대해서는 같은 모델을 사용하였습니다.\n    - layers  = [100, 50, 25] 각각 tanh 사용. (Policy와 Value 네트워크 모두)\n    - Final output layer은 linear\n- Cartpole에 대해서는 1개의 layer 안에 20개의 hidden unit만 있는 linear policy를 사용했다고 합니다.\n\n### 7.2.3 Task\n\n- Cartpole\n    - 한 배치당 20 개의 trajectory를 모았고, maximum length는 1000입니다.\n- 3D biped locomotion\n    - 33 dim state , 10 dim action\n    - 50000 time step per batch\n- 3D quadruped locomotion\n    - 29 dim state, 8 dim action\n    - 200000 time step per batch\n- 3D biped locomotion Standing\n    - 33 dim state , 10 dim action\n    - 200000 time step per batch\n\n\n### 7.2.3 results\ncost의 관점에서 결과를 나타내었다고 합니다. Cost는 negative reward와 이것이 최소화 되었는가로 정의되었습니다.\n\n#### 7.2.3.1 Cartpole\n\n<center> <img width = \"500px\" src=\"https://www.dropbox.com/s/x9pbms1wvg38lda/Screenshot%202018-07-08%2011.08.22.png?dl=1\"> </center>\n\n- 왼쪽 그림은 $\\gamma$를 0.99로 고정시켜놓은 상태에서 $\\lambda$를 변화시킴에 따라서 cost를 측정한 것입니다. \n- 오른쪽은 $\\gamma$와 $\\lambda$를 둘 다 변화 시키면서 성능을 그림으로 나타낸 표입니다. 흰색에 가까울 수록 좋은 성능입니다. \n\n#### 7.2.3.2 3D BIPEDAL LOCOMOTINO\n\n<center> <img width = \"500px\" src=\"https://www.dropbox.com/s/i9wj4p6ijojsy82/Screenshot%202018-07-08%2011.25.08.png?dl=1\"> </center>\n\n\n- 다른 random seed로 부터 9번 씩 시도한 결과를 mean을 취해서 사용합니다.\n- Best performance는   $\\gamma \\in [0.99, 0.995]$ 그리고  $\\lambda \\in [0.96, 0.99]$일때. \n- 1000 iteration 후에 빠르고 부드럽고 안정적인 걸음거이가 나옵니다.\n- 실제로 걸린 시간은 0.01(타입스텝당 시간) * 50000(배치당 타임스텝) * 1000(배치) * 3600(초->시간) * 24 = 5.8일 정도가 걸렸습니다.\n\n#### 7.2.3.3 다른 ROBOT TASKS\n\n<center> <img width = \"500px\" src=\"https://www.dropbox.com/s/fuuat65we52quht/Screenshot%202018-07-08%2011.33.11.png?dl=1\"> </center>\n\n- 다른 로봇 TASK에 대해서는 아주 제한적인 실험만 진행합니다.(시간이 부족했던듯 하네요..)\n- Quadruped에 대해서는 $\\gamma = 0.995$로 fix, $\\lambda \\in {0, 0.96}$\n- Standingup에 대해서는 $\\gamma = 0.99$로 fix, $\\lambda \\in {0, 0.96}$\n\n<br><br>\n\n# 8. Discussion\n\n<br>\n## 8.1 Main discussion\n\n지금까지 복잡하고 어려운 control problem에서 Reinforcement Learning(RL)은 high sample complexity 때문에 제한이 되어왔습니다. 따라서 이 논문에서 그 제한을 풀고자 advantage function의 good estimate를 얻는 \"variance reduction\"에 대해 연구하였습니다.\n\n\"Generalized Advantage Estimator(GAE)\"라는 것을 제안했고, 이것은 bias-variance tradeoff를 조절하는 두 개의 parameter $\\gamma,\\lambda$를 가집니다.\n또한 어떻게 Trust Region Policy Optimization과 value function을 optimize하는 Trust Region Algorithm의 idea를 합치는 지를 보였습니다.\n\n이렇게 함으로써 보다 더 복잡하고 어려운 control task들을 해결할 수 있었습니다.\n\nGAE의 실험적인 입증으로는 robotic locomotion을 simulation하는 domain입니다. 실험에서도 보여준 것처럼 [0.9, 0.99]의 범위에서 $\\lambda$의 적절한 중간의 값을 통해 best performance를 얻습니다. 좀 더 나아가 연구되어야할 점은 adaptive or automatic하도록 estimator parameter $\\gamma,\\lambda$를 조절하는 방법입니다.\n\n<br>\n## 8.2 Future work\n\nValue function estimation error와 Policy gradient estimation error사이의 관계를 알아낸다면, 우리는 Value function fitting에 더 잘 맞는 error metric(policy gradient estimation 의 정확성과 더 잘 맞는 value function)을 사용할 수 있습니다. 여기서 Policy와 Value function의 파라미터를 공유하는 모델을 만드는 것은 아주 흥미롭고 이점이 많습니다. 하지만 수렴을 보장하도록 적절한 numerical optimization을 제시해야 할 것입니다.\n\n추가적으로 DDPG는 별로라고 합니다. 그리고 TD(0)는 bias가 너무 크고, poor performance로 이끈다고 합니다. 특히나 이 논문에서는 low-dimention의 쉬운 문제들만 해결했습니다.\n\n<center> <img width = \"500px\" src=\"https://www.dropbox.com/s/nhc7t9psul5lr3x/Screenshot%202018-07-08%2011.45.15.png?dl=1\"> </center>\n\n<br>\n## 8.3 FAQ\n\n- Compatible features와는 무슨 관계?\n     - Compatible features는 value function을 이용하는 policy gradient 알고리즘들과 함께 자주 언급됩니다.\n     - Actor Critic의 저자는 policy의 제한된 representation power때문에, policy gradient는 단지 advantage function space의 subspace에만 의존하게 됩니다.\n     - 이 subspace는 compatible features에 의해 span됩니다.\n     - 이 이론은 현재 문제 구조를 어떻게 이용해야 advantage function에 대해 더 나은 estimation을 할 수 있는 지에 대한 지침을 주지 않습니다. GAE 논문의 idea와 orthogonal합니다.\n- 왜 Q function을 사용하지 않는가?\n     - 먼저 state-value function이 더 낮은 차원의 input을 가진다. 그래서 Q function보다 더 학습하기가 쉽습니다.\n     - 두 번째로 이 논문에서 제안하는 방법으로는 high bias estimator에서 low bias estimator로 $\\lambda$를 통해서 부드럽게 interpolate를 할 수 있습니다.\n     - 반면에 Q를 사용하면 단지 high-bias estimator 밖에 사용할 수 없습니다.\n     - 특히나 return에 대한 one-step estimation은 엄두를 못낼 정도로 bias가 큽니다.\n\n<br><br>\n\n# 처음으로\n\n## [PG Travel Guide](https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/)\n\n<br>\n\n# 이전으로\n\n## [TRPO 여행하기](https://reinforcement-learning-kr.github.io/2018/06/24/5_trpo/)\n\n## [TRPO Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/trpo_gae.py)\n\n<br>\n\n# 다음으로\n\n## [PPO 여행하기](https://reinforcement-learning-kr.github.io/2018/06/22/7_ppo/)\n","source":"_posts/6_gae.md","raw":"---\ntitle: High-Dimensional Continuous Control using Generalized Advantage Estimation\ndate: 2018-06-23 19:18:45\ntags: [\"프로젝트\", \"피지여행\"]\ncategories: 프로젝트\nauthor: 양혁렬, 이동민\nsubtitle: 피지여행 6번째 논문\n---\n\n<center> <img src=\"https://www.dropbox.com/s/p8gfpyo6xf9wm5w/Screen%20Shot%202018-07-18%20at%201.25.53%20AM.png?dl=1\" width=\"700\"> </center>\n\n논문 저자 : John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan and Pieter Abbeel\n논문 링크 : https://arxiv.org/pdf/1506.02438.pdf\nProceeding : International Conference of Learning Representations (ICLR) 2016\n정리 : 양혁렬, 이동민\n\n---\n\n# 1. 들어가며...\n\n현존하는 Policy Gradient Method들의 목적은 누적되는 reward들을 optimization하는 것입니다. 하지만 학습할 때에 많은 양의 sample이 필요로 하고, 들어오는 data가 nonstationarity임에도 불구하고 stable and steady improvement가 어렵습니다.\n\n그래서 이 논문에서는 다음과 같은 방법을 제시합니다.\n\n- TD($\\lambda$)와 유사한 advantage function의 exponentially-weighted estimator를 사용하여 policy gradient estimate의 variance를 줄이는 것 \n- policy와 value function에 대한 Trust Region Optimization 사용하는 것\n\n3D locomotion tasks에 대한 empirical results는 다음과 같습니다.\n\n- bipedal and quadrupedal simulated robots의 달리는 자세를 학습\n- bipedal 사람이 땅에 누워있다가 일어서는 것을 학습\n\n※ TD($\\lambda$)를 혹시 모르실 경우도 있을 것 같아 간략하게 정리하였습니다. http://dongminlee.tistory.com/10 를 먼저 보시고 아래의 내용을 봐주시기 바랍니다!\n\n<br><br>\n\n# 2. Introduction\n\n기본적으로 \"parameterized stochastic policy\"를 가정합니다. 이 때 expected total returns의 gradient에 대한 unbiased estimate를 얻을 수 있는데 이것을 REINFORCE라고 부릅니다. 하지만 하나의 action의 결과가 과거와 미래의 action의 결과로 혼동되기 때문에 gradient estimator의 high variance는 시간에 따라 scaling됩니다.\n\n또 다른 방법은 Actor-Critic이 있습니다. 이 방법은 empirical returns보다 하나의 value function을 사용합니다. 또한 bias하고 lower variance를 가진 estimator입니다. 구체적으로 말하자면, high variance하다면 더 sampling을 하면 되는 반면에 bias는 매우 치명적입니다. 다시 말해 bias는 algorithm이 converge하는 데에 실패하거나 또는 local optimum이 아닌 poor solution에 converge하도록 만듭니다.\n\n따라서 이 논문에서는 $\\gamma\\in [0,1]$ and $\\lambda\\in [0,1]$에 의해 parameterized estimation scheme인 Generalized Advantage Estimator(GAE)를 다룹니다.\n\n이 논문을 대략적으로 요약하자면 다음과 같습니다.\n\n- 효과적인 variance reduction scheme인 GAE를 다룹니다. 또한 실험할 때 batch trust region algorithm에 더하여 다양한 algorithm들에 적용됩니다.\n- value function에 대해 trust region optimization method를 사용합니다. 이렇게 함으로서 더 robust하고 efficient한 방법이 됩니다.\n- A와 B를 합쳐서, 실험적으로 control task에 neural network policies를 learning하는 데에 있어서 효과적인 algorithm을 얻습니다. 이러한 결과는 high-demensional continuous control에 RL을 사용함으로서 state of the art로 확장되었습니다.\n\n<br><br>\n\n# 3. Preliminaries\n\n먼저 policy optimization의 \"undiscounted formulation\"을 가정합니다. (undiscounted formulation에 주목합시다.)\n\n- initial state $s_0$는 distribution $\\rho_0$으로부터 sampling된 것입니다.\n- 하나의 trajectory ($s_0, a_0, s_1, a_1, ...$)는 terminal state에 도달될 때 까지 policy $a_t$ ~ $\\pi(a_t | s_t)$에 따라서 action을 sampling하고, dynamics $s_{t+1}$ ~ $P(s_{t+1} | s_t, a_t)$에 따라서 state를 sampling함으로써 생성됩니다.\n- reward $r_t = r(s_t, a_t, s_{t+1})$은 매 time step마다 받아집니다.\n- 목표는 모든 policies에 대해 finite하다고 가정됨으로서 expected total reward $\\sum_{t=0}^{\\infty} r_t$를 maximize하는 것입니다.\n\n여기서 중요한 점은 $\\gamma$를 discount의 parameter로 사용하는 것이 아니라 \"bias-variance tradeoff를 조절하는 parameter로 사용한다\" 는 것입니다.\n\npolicy gradient method는 gradient $g := \\nabla_\\theta \\mathbb{E} [\\sum_{t=0}^\\infty r_t]$를 반복적으로 estimate함으로써 expected total reward를 maximize하는 것인데, policy gradient에는 여러 다른 표현들이 있습니다.\n$$g = \\mathbb{E} [\\sum_{t=0}^\\infty \\Phi_t \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$\n\n여기서 $\\Phi_t$는 아래 중의 하나일 수 있습니다.\n\n1. $\\sum_{t=0}^\\infty r_t$: trajectory의 total reward\n2. $\\sum_{t'=t}^\\infty r_t'$: action $a_t$ 후의 reward\n3. $\\sum_{t'=t}^\\infty r_t' - b(s_t)$: 2의 baselined version\n4. $Q^\\pi (s_t, a_t)$: state-action value function\n5. $A^\\pi (s_t, a_t)$: advantage function\n6. $r_t + V^\\pi (s_{t+1}) - V^\\pi (s_t)$: TD residual\n\n위의 번호 중 4, 5, 6의 수식들은 다음의 정의를 사용합니다.\n\n- $V^\\pi (s_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_t:\\infty} [\\sum_{l=0}^\\infty r_{t+1}]$\n- $Q^\\pi (s_t, a_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty} [\\sum_{l=0}^\\infty r_{t+l}]$\n- $A^\\pi (s_t, a_t) := Q^\\pi (s_t, a_t) - V^\\pi (s_t)$, (Advantage function)\n\n추가적으로 colon notation $a : b$는 포괄적인 범위 $(a, a+1, ... , b)$입니다. (잘 기억해둡시다. 뒤에 계속해서 colon notation이 나옵니다.)\n\n여기서부터 parameter $\\gamma$에 대해 좀 더 자세히 알아봅시다. parameter $\\gamma$는 bias하면서 동시에 reward를 downweighting함으로써 variance를 줄입니다. 다시 말해 MDP의 discounted formulation에서 사용된 discounted factor와 일치하지만, 이 논문에서는 \"$\\gamma$를 undiscounted MDP에서 variance reduction parameter\"로 다룹니다. -> 결과는 같지만, 의미와 의도가 다릅니다.\n\ndiscounted value function들은 다음과 같습니다.\n\n- $V^{\\pi, \\gamma} (s_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_t:\\infty} [\\sum_{l=0}^\\infty \\gamma^l r_{t+l}]$ \n- $Q^\\pi (s_t, a_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty} [\\sum_{l=0}^\\infty \\gamma^l r_{t+l}]$\n- $A^{\\pi, \\gamma} (s_t, a_t) := Q^{\\pi, \\gamma} (s_t, a_t) - V^{\\pi, \\gamma} (s_t)$\n\n따라서 policy gradient에서의 discounted approximation은 다음과 같이 나타낼 수 있습니다.\n$$g^\\gamma := \\mathbb{E}_{s_{0:\\infty} a_{0:\\infty}} [\\sum_{t=0}^\\infty A^{\\pi, \\gamma} (s_t, a_t) \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$\n뒤이어 나오는 section에서는 위의 수식에서 $A^{\\pi, \\gamma}$에 대해 biased (but not too biased) estimator를 얻는 방법에 대해서 나옵니다.\n\n다음으로 advantage function에서 새롭게 접하는 \"$\\gamma$-just estimator\"의 notation에 대해 알아봅시다.\n\n- 먼저 $\\gamma$-just estimator는 $g^\\gamma$ ${}^1$를 estimate하기 위해 위의 수식에서 $A^{\\pi, \\gamma}$를 $\\gamma$-just estimator로 사용할 때, bias하지 않은 estimator라고 합시다. 그리고 이 $\\gamma$-just advantage estimator를 <img src=\"https://www.dropbox.com/s/7g2glqx2kbfynaa/Screen%20Shot%202018-08-22%20at%209.03.46%20PM.png?dl=1\" width=\"100\"> 라고 하고, 전체의 trajectory에 대한 하나의 function이라고 합시다.\n- ${}^1$에서 이 논문의 저자가 하고 싶은 말이 있는 것 같습니다. 개인적으로 $\\gamma$-just estimator를 이해하는 데에 있어서 중요한 주석이라 정확히 해석하고자 합니다.\n    - $A^\\pi$를 $A^{\\pi, \\gamma}$로 사용함으로써 이미 bias하다라고 말했지만, \"이 논문에서 하고자 하는 것은 $g^\\gamma$에 대해 unbiased estimate를 얻고 싶은 것입니다.\" 하지만 undiscounted MDP의 policy gradient에 대해서는 당연히 $\\gamma$를 사용하기 때문에 biased estimate를 얻습니다. 개인적으로 이것은 일단 무시하고 $\\gamma$를 사용할 때 어떻게 unbiased estimate를 얻을 지에 대해 좀 더 포커스를 맞추고 있는 것 같습니다.\n    - 그러니까 저 $A^{\\pi, \\gamma}$를 $\\gamma$-just estimator로 바꿔줌으로써 unbiased estimate를 하고 싶다는 것이 뒤이어 나오는 정의와 명제의 핵심이라고 할 수 있습니다.\n\nDefinition 1.\n먼저 가정을 합니다. (가정을 바탕으로 이루어지는 정의라는 것을 주목합시다.) 만약\n<center ><img src=\"https://www.dropbox.com/s/aia9f1l99wyxgah/Screen%20Shot%202018-08-22%20at%209.07.05%20PM.png?dl=1\" width=\"700\"> </center>\n두 수식이 같다면, estimator $\\hat{A}_t$는 $\\gamma$-just입니다.\n\n그리고 만약 모든 t에 대해서 $\\hat{A}_t$이 $\\gamma$-just이라면, 다음과 같이 표현할 수 있습니다.\n\n<center ><img src=\"https://www.dropbox.com/s/r4m6ktw8geeu7pe/Screen%20Shot%202018-08-22%20at%209.29.18%20PM.png?dl=1\" width=\"420\"> </center>\n\n위의 수식이 바로 unbiased estimate입니다.\n\n$\\gamma$-just인 $\\hat{A}_t$에 대한 한가지 조건은 $\\hat{A}_t$이 두 가지 function $Q_t$ and $b_t$로 나뉠 수 있다는 것입니다.\n\n- $Q_t$는 $\\gamma$-discounted Q-function의 unbiased estimator입니다.\n- $b_t$는 action $a_t$전에 sampling된 states and actions의 arbitrary function이다. \n\nProposition 1.\n모든 $(s_t, a_t)$에 대해,\n$$\\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty | s_t, a_t} [Q_t (s_{t:\\infty}, a_{t:\\infty})] = Q^{\\pi, \\gamma} (s_t, a_t)$$\n로 인하여 $\\hat{A}_t$이 \n\n<center> <img src=\"https://www.dropbox.com/s/59ydsav6djwyevl/Screen%20Shot%202018-08-22%20at%209.27.13%20PM.png?dl=1\" width=\"380\"> </center>\n\n형태라고 가정합시다. (가정을 바탕으로 이루어지는 명제라는 점을 주목합시다.)\n\n그 때, $\\hat{A}_t$은 $\\gamma$-just입니다.\n\n이 명제에 대한 증명은 Appendix B에 있습니. 그리고 $\\hat{A}_t$에 대한 $\\gamma$-just advantage estimator들은 다음과 같습니다.\n\n- $\\sum_{l=0}^\\infty \\gamma^l r_{t+1}$\n- $A^{\\pi, \\gamma} (s_t, a_t)$\n- $Q^{\\pi, \\gamma} (s_t, a_t)$\n- $r_t + \\gamma V^{\\pi, \\gamma} (s_{t+1}) - V^{\\pi, \\gamma} (s_t)$\n\n증명을 보기 전에 먼저 Advantage function에 대해서 먼저 살펴봅시다. 아래의 내용은 Sutton이 쓴 논문인 Policy Gradient Methods for Reinforcement Learning with Function Approximation(2000)에서 나온 내용을 리뷰한 것입니다. \n\n<center> <img src=\"https://www.dropbox.com/s/x737yq97ut6gp1a/Screen%20Shot%202018-07-15%20at%201.16.09%20PM.png?dl=1\" width=\"600\"> </center>\n\n여기서는 \"이 수식은 $\\sum_a \\frac{\\partial \\pi (s,a)}{\\partial \\theta} = 0$이기 때문에 가능해진다.\", \"즉, 이 수식은 $\\pi(s,a)$의 gradient에만 dependent하기 때문에 advantage 역할을 하는 함수들을 넣어도 아무런 상관이 없다\"라는 두 문장을 기억해둡시다.\n\n이제 증명을 보면 됩니다.\n\n<center> <img src=\"https://www.dropbox.com/s/e7sj2fwcm1f7hof/figure2.jpg?dl=1\" width=\"600\"> </center>\n<center> <img src=\"https://www.dropbox.com/s/kyk4jes1202az4m/figure3.jpg?dl=1\" width=\"600\"> </center>\n\n위의 증명 수식에서 Q와 b의 각각 세번째 수식과 마지막 수식을 자세히 봅시다.\n\n1. $Q$에 대한 증명\n    - 빨간색 부분\n        - $\\mathbb{E}_{s_{t+1:\\infty}, a_{t+1:\\infty}} [Q_t (s_{0:\\infty}, a_{0:\\infty})]$를 쉽게 말하자면 \"모든(과거, 현재, 미래) state와 action에 대해서 expectation을 $t+1$부터 $\\infty$까지 하겠다.\"라는 말입니다. 이렇게 함으로써 원래는 $Q^\\pi (s_t, a_t)$가 나와야 맞는 건데, 앞에서 살펴봤다시피 4번째 수식 밑줄 친 자리에는 advantage 역할을 하는 함수들을 넣어도 결과값에 아무런 영향을 끼치지 않기 때문에 의도적으로 $A^\\pi (s_t, a_t)$로 바꾼 것입니다. 또한 이 증명을 바탕으로 $\\hat{A}$이 $\\gamma$-just라는 것을 표현하고 싶기 때문이라고 봐도 됩니다.\n\n    - 파란색 부분\n        - 또한 $a_{0:t}$에서 $a_{0:t-1}$로 변한 것은 $A^\\pi (s_t, a_t)$에 baseline이 들어가기 때문에 바꿔준 것입니다.\n\n2. $b$에 대한 증명\n    - 빨간색 부분\n        - $\\mathbb{E}_{s_{t+1:\\infty}, a_{t+1:\\infty}} [\\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$가 0으로 바뀌는 것은 위에서 설명했듯이 $\\nabla_\\theta$자체가 $\\log \\pi_\\theta$만 gradient하기 때문에 이것을 expectation을 취하면 0이 됩니다.\n\n<br><br>\n\n# 4. Advantage Function Estimation\n\n이번 section에는 discounted advantage function $A^{\\pi, \\gamma} (s_t, a_t)$의 accurate estimate $\\hat{A}_t$에 대해서 살펴봅시다. 이에 따른 수식은 다음과 같습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/sn16g5iyzzflju0/Screen%20Shot%202018-08-22%20at%209.22.49%20PM.png?dl=1\" width=\"300\"> </center>\n\n여기서 n은 a batch of episodes에 대하여 index한 것입니다.\n\n$V$를 approximate value function이라고 합시다. 그리고 $\\delta_t^V = r_t + \\gamma V(s_{t+1}) - V(s_t)$이라고 합시다.\n\n만약 (이전과 마찬가지로 가정부터 합니다.) correct value function $V = V^{\\pi, \\gamma}$가 있다고 한다면, 이것은 $\\gamma$-just advantage estimator입니다. 실제로, $A^{\\pi, \\gamma}$의 unbiased estimator는 다음과 같습니다.\n$$\\mathbb{E}_{s_{t+1}} [\\delta_t^{V^{\\pi, \\gamma}}] = \\mathbb{E}_{s_{t+1}} [r_t + \\gamma V^{\\pi, \\gamma} (s_{t+1}) - V^{\\pi, \\gamma} (s_t)]$$\n$$= \\mathbb{E}_{s_{t+1}} [Q^{\\pi, \\gamma} (s_t, a_t) - V^{\\pi, \\gamma} (s_t)] = A^{\\pi, \\gamma} (s_t, a_t)$$\n그러나, \"이 estimator는 유일하게 $V = V^{\\pi, \\gamma}$에 대한 $\\gamma$-just입니다.\" 다른 경우라면 이것은 biased policy gradient estimate일 것입니다. (우리가 하고 싶은 것은 $V$에 대해서만 unbiased estimator가 아니라 advantage function에 대해서 일반화된 unbiased estimator를 얻고 싶은 것입니다. 그래서 아래에서도 나오겠지만, $\\gamma$와 함께 $\\lambda$를 이용한 estimator가 나옵니다.ㅏ)\n\n그래서 $\\delta$에 대해 $k$의 sum으로 생각해봅시다. 이것을 $\\hat{A}_t^{(k)}$라고 하자. 그러면 아래와 같이 표현할 수 있습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/ra7hxksveg2hz45/figure4.jpg?dl=1\" width=\"600\"> </center>\n\n$\\hat{A}_t^{(k)}$은 returns의 $k$-step estimate와 연관지을 수 있고, $\\delta_t^V = \\hat{A}_t^{(1)}$의 case와 유사하게도 $\\hat{A}_t^{(k)}$를 advantage function의 estimator로 생각할 수 있습니다.\n\n여기서 $k \\rightarrow \\infty$로 생각해보면 bias가 일반적으로 점점 더 작아집니다. 왜냐하면 $\\gamma^k V(s_{t+k})$가 점점 많이 discounted되서 $-V(s_t)$가 bias에 영향을 미치지 못하기 때문입니다. $k \\rightarrow \\infty$를 취하면 다음과 같은 수식이 나옵니다.\n\n<center> <img src=\"https://www.dropbox.com/s/13fn9wcup8pfh9u/Screen%20Shot%202018-08-22%20at%209.19.52%20PM.png?dl=1\" width=\"320\"> </center>\n\n우변의 수식과 같이 empirical returns에서 value function baseline을 뺀 것으로 나타낼 수 있습니다.\n\nGeneralized Advantage Estimator GAE($\\gamma, \\lambda$)는 $k$-step estimators의 exponentially-weighted average로 나타낼 수 있습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/yg1ybmfkep3towu/figure5.jpg?dl=1\" width=\"600\"> </center>\n\n(TD($\\lambda$)를 떠올려주세요..!)\n\n위의 마지막 수식에서도 알 수 있듯이, advantage estimator는 Bellman residual terms의 discounted sum과 관련있는 간단한 수식입니다. 다음 section에서 modified reward function을 가진 MDP에 returns로서의 관점에서 위의 마지막 수식을 더 자세히 살펴봅시다. 위의 수식은 TD($\\lambda$)와 많이 유사합니다. 그러나 TD($\\lambda$)는 value function를 estimator하고, 여기서는 advantage function을 estimator합니다.\n\n위의 수식에서 $\\lambda = 0$ and $\\lambda = 1$에 대해서는 특별한 case가 존재한다. 수식으로 표현하면 다음과 같습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/ufglxzanhcnbi36/figure6.jpg?dl=1\" width=\"600\"> </center>\n\n- $GAE(\\gamma, 1)$은 $V$의 정확도와 관계없이 $\\gamma$-just입니다. 그러나 returns의 sum때문에 high variance합니다.\n- $GAE(\\gamma, 0)$은 $V = V^{\\pi, \\gamma}$에 대해 $\\gamma$-just입니다. 그리고 bias하지만 일반적으로 훨씬 lower variance를 가집니다. \n- $0 < \\lambda < 1$에 대해 GAE는 parameter $\\lambda$를 control하여 bias와 variance사이에 compromise를 만듭니다.\n\n두 가지 별개의 parameter $\\gamma$ and $\\lambda$를 가진 advantage estimator는 bias-variance tradeoff에 도움을 줍니다. 그러나 이 두 가지 parameter는 각각 다른 목적을 가지고 작동합니다.\n\n- $\\gamma$는 가장 중요하게 value function $V^{\\pi, \\gamma}$ 의 scale을 결정합니다. 또한 $\\gamma$는 $\\lambda$에 의존하지 않습니다. $\\gamma < 1$로 정하는 것은 policy gradient estimate에서 bias합니다.\n- 반면에, $\\lambda < 1$는 유일하게 value function이 부정확할 때 bias합니다. 그리고 경험상, $\\lambda$의 best value는 $\\gamma$의 best value보다 훨씬 더 낮습니다. 왜냐하면 $\\lambda$가 정확한 value function에 대해 $\\gamma$보다 훨씬 덜 bias하기 때문입니다.\n\nGAE를 사용할 때, $g^\\gamma$의 biased estimator를 구성할 수 있습니다. 수식은 다음과 같습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/ok57dsk52o7pl9r/Screen%20Shot%202018-08-22%20at%209.17.54%20PM.png?dl=1\" width=\"670\"> </center>\n\n여기서 $\\lambda = 1$일 때 동일해집니다.\n\n<br><br>\n\n# 5. Interpretation as Reward Shaping\n\n이번 section에서는 앞서 다뤘던 수식 $\\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^V$를 modified reward function의 MDP의 관점으로 생각해봅시다. 조금 더 구체적으로 말하자면, MDP에서 reward shaping transformation을 실행한 후에 적용된 extra discounted factor로서 $\\lambda$를 어떻게 볼 것인지에 대해서 다룹니다.\n\n(개인적인 comment) 한 가지 먼저 언급하자면, 본래의 목적은 reward shaping이 아니라 variance reduction입니다. 이번 section은 그저 이전에 이러한 개념이 있었고, gae를 다른 관점에서 생각해보자라는 뜻에서 나온 section인 것 같습니다. '아~ 이러한 개념이 있구나~!' 정도로만 알면 될 것 같습니다. 'gae가 reward shaping의 효과까지 있어서 이러한 section을 넣은 것일까?' 라는 생각도 해봤지만 아직 잘 모르겠습니다. 실험부분에도 딱히 하지 않은 걸로 봐서는.. 아닌 것 같기도 하고.. 아직까지는 그저 'reward shaping의 관점에서 봤을 때에도 gae로 만들어줄 수 있다.' 정도만 생각하려고 합니다.\n\nReward Shaping이란 개념 자체는 일반적인 알고리즘 분야(최단경로 문제 등)에도 있습니다. 하지만 이 개념이 머신러닝에 적용된 건 Andrew Y. Ng이 쓴 Policy Invariance under Reward Transformations Theory and Application to Reward Shaping(1999) 논문입니다. 논문에서 Reward Shaping을 설명하는 부분은 간략하게 다음과 같습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/0w9cuxjkhz2mbcy/figure7.jpg?dl=1\" width=\"400\"> </center>\n<center> <img src=\"https://www.dropbox.com/s/xcp9eqvcbv3268g/figure8.jpg?dl=1\" width=\"400\"> </center>\n\n위의 글만 보고 이해가 잘 안됩니다. 그래서 다른 자료들을 찾던 중에 다음의 그림을 찾게 되었습니다. (아래의 그림은 Youtube에 있는 [Udacity 영상](https://www.youtube.com/watch?v=xManAGjbx2k&t=95s)에서 있는 그림입니다.)\n\n<center> <img src=\"https://www.dropbox.com/s/87kacngez9cl2e6/figure9.jpg?dl=1\" width=\"400\"> </center>\n\n이 그림을 통해서 Reward Shaping을 이해해봅시다.\n\nReward Shaping(보상 형성)을 통해서 뭘 하고 싶은지 목적부터 봅시다. reward space가 sparse 한 경우에 reward가 너무 드문드문 나옵니다. 따라서 이것을 꾸준히 reward를 받을 수 있도록 바꿉니다. potential-based shaping function인 $\\Phi$를 만들어서 더하고 빼줍니다. 저 $\\Phi$ 자리에는 대표적으로 state value function이 많이 들어간다고 합니다. (아직 목적이 이해가 안될 수 있습니다. Reward Shaping에 대해서 다 보고 나서 다시 한 번 읽어봅시다.)\n\n위의 그림은 돌고래가 점프하여 불구멍을 통과해야하는 환경입니다. 하지만 저 불구멍을 통과하기 위해서는 (1) 점프도 해야하고, (2) 불도 피해야하고, (3) 알맞게 착지까지 완료해야합니다. 이렇게 해야 reward를 +1 얻습니다. \n\n생각해봅시다. 어느 세월에 점프도 해야하고.. 불도 피해야하고.. 알맞게 착지까지해서 reward를 +1 얻겠습니까? 따라서 그 전에도 잘하고 있는 지, 못하고 있는 지를 판단하기 위해, 다시 말해 reward를 꾸준히 받도록 다음과 같이 transformed reward function $\\tilde{r}$을 정의합니다.\n\n$$\\tilde{r} (s, a, s') = r(s, a, s') + \\gamma \\Phi (s') - \\Phi (s)$$\n\n- 여기서 $\\Phi : S \\rightarrow \\mathbb{R}$를 state space에서의 arbitrary scalar-valued function을 나타냅니다. 그리고 $\\Phi$자리에는 대표적으로 state value function이 들어간다고 생각합시다.\n- 형태가 TD residual term의 결과와 비슷하지만 의미와 의도가 다릅니다. reward shaping은 sparse reward 때문이고, 이전 section에서 봤던 gae는 variance reduction때문에 나온 것입니다.\n\n<center> <img src=\"https://www.dropbox.com/s/tkfpqniaazwfbld/figure10.jpg?dl=1\" width=\"600\"> </center>\n\n이 transformation은 discounted advantage function $A^{\\pi, \\gamma}$으로도 둘 수 있습니다. state $s_t$를 시작하여 하나의 trajectory의 rewards의 discounted sum을 표현하면 다음과 같습니다.\n$$\\sum_{l=0}^\\infty \\gamma^l \\tilde{r} (s_{t+l}, a_t, s_{t+l+1}) = \\sum_{l=0}^\\infty \\gamma^l r(s_{t+l}, a_{t+l}, s_{t+l+1}) - \\Phi(s_t)$$\n\n$\\tilde{Q}^{\\pi, \\gamma}, \\tilde{V}^{\\pi, \\gamma}, \\tilde{A}^{\\pi, \\gamma}$를  transformed MDP의 value function과 advantage function이라고 하면, 다음과 같은 수식이 나옵니다.\n$$\\tilde{Q}^{\\pi, \\gamma} (s, a) = Q^{\\pi, \\gamma} (s, a) - \\Phi (s)$$\n$$\\tilde{V}^{\\pi, \\gamma} (s, a) = V^{\\pi, \\gamma} (s, a) - \\Phi (s)$$\n$$\\tilde{A}^{\\pi, \\gamma} (s, a) = (Q^{\\pi, \\gamma} (s, a) - \\Phi (s)) - (V^\\pi (s) - \\Phi (s)) = A^{\\pi, \\gamma} (s, a)$$\n\n이제 reward shaping의 idea를 가지고 어떻게 policy gradient estimate를 얻을 수 있는 지에 대해서 알아봅시다.\n\n- $0 \\le \\lambda \\le 1$의 범위에 있는 steeper discount $\\gamma \\lambda$를 사용합니다.\n- shaped reward $\\tilde{r}$는 Bellman residual term $\\delta^V$와 동일합니다.\n- 그리고 $\\Phi = V$와 같다고 보면 다음과 같은 수식이 나옵니다.\n\n$$\\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\tilde{r} (s_{t+l}, a_t, s_{t+l+1}) = \\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^V = \\hat{A}_t^{GAE(\\gamma, \\lambda)}$$\n이렇게 shaped rewards의 $\\gamma \\lambda$-discounted sum을 고려함으로써 GAE를 얻을 수 있습니다. 더 정확하게 $\\lambda = 1$은 $g^\\gamma$의 unbiased estimate이고, 반면에 $\\lambda < 1$은 biased estimate입니다.\n\n좀 더 나아가서, shaping transformation과 parameters $\\gamma$ and $\\lambda$의 결과를 보기 위해, response function $\\chi$를 이용하면 다음과 같은 수식이 나옵니다.\n$$\\chi (l; s_t, a_t) = \\mathbb{E} [r_{t+l} | s_t, a_t] - \\mathbb{E} [r_{t+l} | s_t]$$\n추가적으로 $A^{\\pi, \\gamma} (s, a) = \\sum_{l=0}^\\infty \\gamma^l \\chi (l; s, a)$입니다.\n그래서 discounted policy gradient estimator는 다음과 같이 쓸 수 있다.\n$$\\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) A^{\\pi, \\gamma} (s_t, a_t) = \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) \\sum_{l=0}^\\infty \\gamma^l \\chi (l; s, a)$$\n\n<br><br>\n\n# 6. Value Fuction Estimation\n\n이번 section에서는 Trust Region Optimization Scheme에 따라 Value function을 Estimation합니다.\n\n<br>\n## 6.1 Simplest approach\n\n$$minimize_{\\phi} \\sum_{n=1}^{N} \\vert\\vert V_{\\phi}(s_n) - \\hat{V_n} \\vert\\vert^{2}$$\n\n- 위는 가장 간단하게 non-linear approximation으로 푸는 방법입니다.\n- $\\hat{V_t} = \\sum_{l=0}^{\\infty}\\gamma^l r_{t+l}$은 reward 에 대한 discounted sum을 의미합니다.\n\n<br>\n## 6.2 Trust region method to optimize the value function\n\n- Value function을 최적화 하기 위해 trust region method를 사용합니다.\n- Trust region은 최근 데이터에 대해 overfitting되는 것을 막아줍니다.\n\nTrust region문제를 풀기 위해서는 다음 스텝을 따릅니다.\n\n- $\\sigma^2 = \\frac{1}{N} \\sum_{n=1}^{N} \\vert\\vert V_{\\phi old}(s_n) - \\hat{V_n} \\vert\\vert^{2}$을 계산합니다.\n- 그 후에 다음과 같은 constrained opimization문제를 풉니다.\n\n<center> <img src=\"https://www.dropbox.com/s/d74cg3votxi2dbg/Screen%20Shot%202018-08-22%20at%209.13.28%20PM.png?dl=1\" width=\"300\"> </center>\n\n<center> <img src=\"https://www.dropbox.com/s/jrkjax71t3j3vk4/Screen%20Shot%202018-08-22%20at%209.13.35%20PM.png?dl=1\" width=\"370\"> </center>\n\n- 위의 수식은 사실 old Value function과 새로운 Value function KL distance가 $\\epsilon$ 다 작아야한다는 수식과 같습니다. Value function이 평균은 $V_{\\phi}(s)$이고 분산이 $\\sigma^2$인 conditional Gaussian distribution으로 parameterize되었을 뿐입니다.\n\n<img width =\"400px\" src=\"https://www.dropbox.com/s/uw0v05feu8chqmc/Screenshot%202018-07-08%2010.04.38.png?dl=1\"> \n\n이 trust region문제의 답을 conjudate gradient algorithm을 이용하여 근사값을 구할 수 있습니다. 특히, 다음과 같은  quadratic program을 풀게됩니다.\n\n<center> <img src=\"https://www.dropbox.com/s/pawjkm8b9ycwsl5/Screen%20Shot%202018-08-22%20at%209.10.36%20PM.png?dl=1\" width=\"250\"> </center>\n\n<center> <img src=\"https://www.dropbox.com/s/va7wms8uqxnw5rq/Screen%20Shot%202018-08-22%20at%209.10.40%20PM.png?dl=1\" width=\"370\"> </center>\n\n- 여기서 $g$는 objective 의 gradient입니다.\n- $j_n = \\nabla_{\\phi} V_{\\phi}(s_n)$일때, $H = \\frac{1}{N} \\sum_{n} j_n j^T_n$이며, $H$는 objective의 hessian에 대해서 gaussian newton method로 근사한 값입니다. 따라서, value function을 conditional probability로 해석한다면 Fisher information matrix가 됩니다.\n- 구현할때의 방법은 TRPO에서 사용한 방법과 모두 같습니다.\n\n<br><br>\n\n# 7.Experiments\n\n실험은 다음 두 가지 물음에 대해서 디자인 되었습니다.\n\n- GAE에 따라서 episodic total reward를 최적화 할때, $\\lambda$와 $\\gamma$가 변함에 따라서 어떤 경험적인 효과를 볼 수 있는지?\n- GAE와 trust region alogorithm을 policy와 value function 모두에 함께 사용했을 때 어려운 문제에 적용되는 큰 뉴럴넷을 최적화 할 수 있을지?\n\n<br>\n## 7.1 Policy Optimization Algorithm\n\nPolicy update는 TRPO를 사용합니다. TRPO에 대한 설명은 여기서는 생략하겠습니다. TRPO 포스트를 보고 돌아와주세요!\n\n- 이전 TRPO에서 이미 TRPO와 다른 많은 알고리즘들을 비교하였기 때문에, 여기서는 똑같은 짓을 반복하지 않고 $\\lambda$, $\\gamma$가 변함에 따라 어떤 영향이 있는 지에 대한 실험에 집중하겠다고 합니다. (귀찮았던거죠..)\n\n- TRPO를 적용한 GAE의 최종 알고리즘은 다음과 같습니다.\n\n<center> <img width = \"600px\" src=\"https://www.dropbox.com/s/b1klz11f2frrvg4/Screenshot%202018-07-08%2010.33.54.png?dl=1\"> </center>\n<center> <img width = \"300px\" src=\"https://www.dropbox.com/s/35fxte05pqtiel7/Screenshot%202018-07-08%2010.35.22.png?dl=1\"> </center>\n<center> <img width = \"300px\" src=\"https://www.dropbox.com/s/u35pjow3w50bvz1/Screenshot%202018-07-08%2010.36.03.png?dl=1\"> </center>\n\n- 여기서 주의할 점은 Policy update($\\theta_i \\rightarrow \\theta_{i+1}$)에서 $V_{\\phi_i}$를 사용했다는 점입니다.\n- 만약 Value function을 먼저 update하게 된다면 추가적인 bias가 발생합니다.\n- 극단적으로 생각해보아서, 우리가 Value function을 완벽하게 overfit을 해낸다면 Bellman residual($r_t + \\gamma V(s_{t+1}) - V(S_t)$)은 0이 됩니다. 그럼 Policy gradient의 estimation도 거의 0이 될 것입니다.\n\n<br>\n## 7.2 Expermint details\n\n### 7.2.1 Environment \n실험에서 사용된 환경은 다음 네 가지 입니다.\n\n1. classic cart-pole (x 3D)\n2. bipedal locomotion\n3. quadrupedal locomotion\n4. dynamically standing up for the biped\n\n### 7.2.2 Architecture\n\n- 3D robot task에 대해서는 같은 모델을 사용하였습니다.\n    - layers  = [100, 50, 25] 각각 tanh 사용. (Policy와 Value 네트워크 모두)\n    - Final output layer은 linear\n- Cartpole에 대해서는 1개의 layer 안에 20개의 hidden unit만 있는 linear policy를 사용했다고 합니다.\n\n### 7.2.3 Task\n\n- Cartpole\n    - 한 배치당 20 개의 trajectory를 모았고, maximum length는 1000입니다.\n- 3D biped locomotion\n    - 33 dim state , 10 dim action\n    - 50000 time step per batch\n- 3D quadruped locomotion\n    - 29 dim state, 8 dim action\n    - 200000 time step per batch\n- 3D biped locomotion Standing\n    - 33 dim state , 10 dim action\n    - 200000 time step per batch\n\n\n### 7.2.3 results\ncost의 관점에서 결과를 나타내었다고 합니다. Cost는 negative reward와 이것이 최소화 되었는가로 정의되었습니다.\n\n#### 7.2.3.1 Cartpole\n\n<center> <img width = \"500px\" src=\"https://www.dropbox.com/s/x9pbms1wvg38lda/Screenshot%202018-07-08%2011.08.22.png?dl=1\"> </center>\n\n- 왼쪽 그림은 $\\gamma$를 0.99로 고정시켜놓은 상태에서 $\\lambda$를 변화시킴에 따라서 cost를 측정한 것입니다. \n- 오른쪽은 $\\gamma$와 $\\lambda$를 둘 다 변화 시키면서 성능을 그림으로 나타낸 표입니다. 흰색에 가까울 수록 좋은 성능입니다. \n\n#### 7.2.3.2 3D BIPEDAL LOCOMOTINO\n\n<center> <img width = \"500px\" src=\"https://www.dropbox.com/s/i9wj4p6ijojsy82/Screenshot%202018-07-08%2011.25.08.png?dl=1\"> </center>\n\n\n- 다른 random seed로 부터 9번 씩 시도한 결과를 mean을 취해서 사용합니다.\n- Best performance는   $\\gamma \\in [0.99, 0.995]$ 그리고  $\\lambda \\in [0.96, 0.99]$일때. \n- 1000 iteration 후에 빠르고 부드럽고 안정적인 걸음거이가 나옵니다.\n- 실제로 걸린 시간은 0.01(타입스텝당 시간) * 50000(배치당 타임스텝) * 1000(배치) * 3600(초->시간) * 24 = 5.8일 정도가 걸렸습니다.\n\n#### 7.2.3.3 다른 ROBOT TASKS\n\n<center> <img width = \"500px\" src=\"https://www.dropbox.com/s/fuuat65we52quht/Screenshot%202018-07-08%2011.33.11.png?dl=1\"> </center>\n\n- 다른 로봇 TASK에 대해서는 아주 제한적인 실험만 진행합니다.(시간이 부족했던듯 하네요..)\n- Quadruped에 대해서는 $\\gamma = 0.995$로 fix, $\\lambda \\in {0, 0.96}$\n- Standingup에 대해서는 $\\gamma = 0.99$로 fix, $\\lambda \\in {0, 0.96}$\n\n<br><br>\n\n# 8. Discussion\n\n<br>\n## 8.1 Main discussion\n\n지금까지 복잡하고 어려운 control problem에서 Reinforcement Learning(RL)은 high sample complexity 때문에 제한이 되어왔습니다. 따라서 이 논문에서 그 제한을 풀고자 advantage function의 good estimate를 얻는 \"variance reduction\"에 대해 연구하였습니다.\n\n\"Generalized Advantage Estimator(GAE)\"라는 것을 제안했고, 이것은 bias-variance tradeoff를 조절하는 두 개의 parameter $\\gamma,\\lambda$를 가집니다.\n또한 어떻게 Trust Region Policy Optimization과 value function을 optimize하는 Trust Region Algorithm의 idea를 합치는 지를 보였습니다.\n\n이렇게 함으로써 보다 더 복잡하고 어려운 control task들을 해결할 수 있었습니다.\n\nGAE의 실험적인 입증으로는 robotic locomotion을 simulation하는 domain입니다. 실험에서도 보여준 것처럼 [0.9, 0.99]의 범위에서 $\\lambda$의 적절한 중간의 값을 통해 best performance를 얻습니다. 좀 더 나아가 연구되어야할 점은 adaptive or automatic하도록 estimator parameter $\\gamma,\\lambda$를 조절하는 방법입니다.\n\n<br>\n## 8.2 Future work\n\nValue function estimation error와 Policy gradient estimation error사이의 관계를 알아낸다면, 우리는 Value function fitting에 더 잘 맞는 error metric(policy gradient estimation 의 정확성과 더 잘 맞는 value function)을 사용할 수 있습니다. 여기서 Policy와 Value function의 파라미터를 공유하는 모델을 만드는 것은 아주 흥미롭고 이점이 많습니다. 하지만 수렴을 보장하도록 적절한 numerical optimization을 제시해야 할 것입니다.\n\n추가적으로 DDPG는 별로라고 합니다. 그리고 TD(0)는 bias가 너무 크고, poor performance로 이끈다고 합니다. 특히나 이 논문에서는 low-dimention의 쉬운 문제들만 해결했습니다.\n\n<center> <img width = \"500px\" src=\"https://www.dropbox.com/s/nhc7t9psul5lr3x/Screenshot%202018-07-08%2011.45.15.png?dl=1\"> </center>\n\n<br>\n## 8.3 FAQ\n\n- Compatible features와는 무슨 관계?\n     - Compatible features는 value function을 이용하는 policy gradient 알고리즘들과 함께 자주 언급됩니다.\n     - Actor Critic의 저자는 policy의 제한된 representation power때문에, policy gradient는 단지 advantage function space의 subspace에만 의존하게 됩니다.\n     - 이 subspace는 compatible features에 의해 span됩니다.\n     - 이 이론은 현재 문제 구조를 어떻게 이용해야 advantage function에 대해 더 나은 estimation을 할 수 있는 지에 대한 지침을 주지 않습니다. GAE 논문의 idea와 orthogonal합니다.\n- 왜 Q function을 사용하지 않는가?\n     - 먼저 state-value function이 더 낮은 차원의 input을 가진다. 그래서 Q function보다 더 학습하기가 쉽습니다.\n     - 두 번째로 이 논문에서 제안하는 방법으로는 high bias estimator에서 low bias estimator로 $\\lambda$를 통해서 부드럽게 interpolate를 할 수 있습니다.\n     - 반면에 Q를 사용하면 단지 high-bias estimator 밖에 사용할 수 없습니다.\n     - 특히나 return에 대한 one-step estimation은 엄두를 못낼 정도로 bias가 큽니다.\n\n<br><br>\n\n# 처음으로\n\n## [PG Travel Guide](https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/)\n\n<br>\n\n# 이전으로\n\n## [TRPO 여행하기](https://reinforcement-learning-kr.github.io/2018/06/24/5_trpo/)\n\n## [TRPO Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/trpo_gae.py)\n\n<br>\n\n# 다음으로\n\n## [PPO 여행하기](https://reinforcement-learning-kr.github.io/2018/06/22/7_ppo/)\n","slug":"6_gae","published":1,"updated":"2018-09-08T11:00:20.611Z","_id":"cjkj6tysm000oj715gn62q8pn","comments":1,"layout":"post","photos":[],"link":"","content":"<center> <img src=\"https://www.dropbox.com/s/p8gfpyo6xf9wm5w/Screen%20Shot%202018-07-18%20at%201.25.53%20AM.png?dl=1\" width=\"700\"> </center>\n\n<p>논문 저자 : John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan and Pieter Abbeel<br>논문 링크 : <a href=\"https://arxiv.org/pdf/1506.02438.pdf\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/pdf/1506.02438.pdf</a><br>Proceeding : International Conference of Learning Representations (ICLR) 2016<br>정리 : 양혁렬, 이동민</p>\n<hr>\n<h1 id=\"1-들어가며…\"><a href=\"#1-들어가며…\" class=\"headerlink\" title=\"1. 들어가며…\"></a>1. 들어가며…</h1><p>현존하는 Policy Gradient Method들의 목적은 누적되는 reward들을 optimization하는 것입니다. 하지만 학습할 때에 많은 양의 sample이 필요로 하고, 들어오는 data가 nonstationarity임에도 불구하고 stable and steady improvement가 어렵습니다.</p>\n<p>그래서 이 논문에서는 다음과 같은 방법을 제시합니다.</p>\n<ul>\n<li>TD($\\lambda$)와 유사한 advantage function의 exponentially-weighted estimator를 사용하여 policy gradient estimate의 variance를 줄이는 것 </li>\n<li>policy와 value function에 대한 Trust Region Optimization 사용하는 것</li>\n</ul>\n<p>3D locomotion tasks에 대한 empirical results는 다음과 같습니다.</p>\n<ul>\n<li>bipedal and quadrupedal simulated robots의 달리는 자세를 학습</li>\n<li>bipedal 사람이 땅에 누워있다가 일어서는 것을 학습</li>\n</ul>\n<p>※ TD($\\lambda$)를 혹시 모르실 경우도 있을 것 같아 간략하게 정리하였습니다. <a href=\"http://dongminlee.tistory.com/10\" target=\"_blank\" rel=\"noopener\">http://dongminlee.tistory.com/10</a> 를 먼저 보시고 아래의 내용을 봐주시기 바랍니다!</p>\n<p><br><br></p>\n<h1 id=\"2-Introduction\"><a href=\"#2-Introduction\" class=\"headerlink\" title=\"2. Introduction\"></a>2. Introduction</h1><p>기본적으로 “parameterized stochastic policy”를 가정합니다. 이 때 expected total returns의 gradient에 대한 unbiased estimate를 얻을 수 있는데 이것을 REINFORCE라고 부릅니다. 하지만 하나의 action의 결과가 과거와 미래의 action의 결과로 혼동되기 때문에 gradient estimator의 high variance는 시간에 따라 scaling됩니다.</p>\n<p>또 다른 방법은 Actor-Critic이 있습니다. 이 방법은 empirical returns보다 하나의 value function을 사용합니다. 또한 bias하고 lower variance를 가진 estimator입니다. 구체적으로 말하자면, high variance하다면 더 sampling을 하면 되는 반면에 bias는 매우 치명적입니다. 다시 말해 bias는 algorithm이 converge하는 데에 실패하거나 또는 local optimum이 아닌 poor solution에 converge하도록 만듭니다.</p>\n<p>따라서 이 논문에서는 $\\gamma\\in [0,1]$ and $\\lambda\\in [0,1]$에 의해 parameterized estimation scheme인 Generalized Advantage Estimator(GAE)를 다룹니다.</p>\n<p>이 논문을 대략적으로 요약하자면 다음과 같습니다.</p>\n<ul>\n<li>효과적인 variance reduction scheme인 GAE를 다룹니다. 또한 실험할 때 batch trust region algorithm에 더하여 다양한 algorithm들에 적용됩니다.</li>\n<li>value function에 대해 trust region optimization method를 사용합니다. 이렇게 함으로서 더 robust하고 efficient한 방법이 됩니다.</li>\n<li>A와 B를 합쳐서, 실험적으로 control task에 neural network policies를 learning하는 데에 있어서 효과적인 algorithm을 얻습니다. 이러한 결과는 high-demensional continuous control에 RL을 사용함으로서 state of the art로 확장되었습니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"3-Preliminaries\"><a href=\"#3-Preliminaries\" class=\"headerlink\" title=\"3. Preliminaries\"></a>3. Preliminaries</h1><p>먼저 policy optimization의 “undiscounted formulation”을 가정합니다. (undiscounted formulation에 주목합시다.)</p>\n<ul>\n<li>initial state $s_0$는 distribution $\\rho_0$으로부터 sampling된 것입니다.</li>\n<li>하나의 trajectory ($s_0, a_0, s_1, a_1, …$)는 terminal state에 도달될 때 까지 policy $a_t$ ~ $\\pi(a_t | s_t)$에 따라서 action을 sampling하고, dynamics $s_{t+1}$ ~ $P(s_{t+1} | s_t, a_t)$에 따라서 state를 sampling함으로써 생성됩니다.</li>\n<li>reward $r_t = r(s_t, a_t, s_{t+1})$은 매 time step마다 받아집니다.</li>\n<li>목표는 모든 policies에 대해 finite하다고 가정됨으로서 expected total reward $\\sum_{t=0}^{\\infty} r_t$를 maximize하는 것입니다.</li>\n</ul>\n<p>여기서 중요한 점은 $\\gamma$를 discount의 parameter로 사용하는 것이 아니라 “bias-variance tradeoff를 조절하는 parameter로 사용한다” 는 것입니다.</p>\n<p>policy gradient method는 gradient $g := \\nabla_\\theta \\mathbb{E} [\\sum_{t=0}^\\infty r_t]$를 반복적으로 estimate함으로써 expected total reward를 maximize하는 것인데, policy gradient에는 여러 다른 표현들이 있습니다.<br>$$g = \\mathbb{E} [\\sum_{t=0}^\\infty \\Phi_t \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$</p>\n<p>여기서 $\\Phi_t$는 아래 중의 하나일 수 있습니다.</p>\n<ol>\n<li>$\\sum_{t=0}^\\infty r_t$: trajectory의 total reward</li>\n<li>$\\sum_{t’=t}^\\infty r_t’$: action $a_t$ 후의 reward</li>\n<li>$\\sum_{t’=t}^\\infty r_t’ - b(s_t)$: 2의 baselined version</li>\n<li>$Q^\\pi (s_t, a_t)$: state-action value function</li>\n<li>$A^\\pi (s_t, a_t)$: advantage function</li>\n<li>$r_t + V^\\pi (s_{t+1}) - V^\\pi (s_t)$: TD residual</li>\n</ol>\n<p>위의 번호 중 4, 5, 6의 수식들은 다음의 정의를 사용합니다.</p>\n<ul>\n<li>$V^\\pi (s_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_t:\\infty} [\\sum_{l=0}^\\infty r_{t+1}]$</li>\n<li>$Q^\\pi (s_t, a_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty} [\\sum_{l=0}^\\infty r_{t+l}]$</li>\n<li>$A^\\pi (s_t, a_t) := Q^\\pi (s_t, a_t) - V^\\pi (s_t)$, (Advantage function)</li>\n</ul>\n<p>추가적으로 colon notation $a : b$는 포괄적인 범위 $(a, a+1, … , b)$입니다. (잘 기억해둡시다. 뒤에 계속해서 colon notation이 나옵니다.)</p>\n<p>여기서부터 parameter $\\gamma$에 대해 좀 더 자세히 알아봅시다. parameter $\\gamma$는 bias하면서 동시에 reward를 downweighting함으로써 variance를 줄입니다. 다시 말해 MDP의 discounted formulation에서 사용된 discounted factor와 일치하지만, 이 논문에서는 “$\\gamma$를 undiscounted MDP에서 variance reduction parameter”로 다룹니다. -&gt; 결과는 같지만, 의미와 의도가 다릅니다.</p>\n<p>discounted value function들은 다음과 같습니다.</p>\n<ul>\n<li>$V^{\\pi, \\gamma} (s_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_t:\\infty} [\\sum_{l=0}^\\infty \\gamma^l r_{t+l}]$ </li>\n<li>$Q^\\pi (s_t, a_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty} [\\sum_{l=0}^\\infty \\gamma^l r_{t+l}]$</li>\n<li>$A^{\\pi, \\gamma} (s_t, a_t) := Q^{\\pi, \\gamma} (s_t, a_t) - V^{\\pi, \\gamma} (s_t)$</li>\n</ul>\n<p>따라서 policy gradient에서의 discounted approximation은 다음과 같이 나타낼 수 있습니다.<br>$$g^\\gamma := \\mathbb{E}_{s_{0:\\infty} a_{0:\\infty}} [\\sum_{t=0}^\\infty A^{\\pi, \\gamma} (s_t, a_t) \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$<br>뒤이어 나오는 section에서는 위의 수식에서 $A^{\\pi, \\gamma}$에 대해 biased (but not too biased) estimator를 얻는 방법에 대해서 나옵니다.</p>\n<p>다음으로 advantage function에서 새롭게 접하는 “$\\gamma$-just estimator”의 notation에 대해 알아봅시다.</p>\n<ul>\n<li>먼저 $\\gamma$-just estimator는 $g^\\gamma$ ${}^1$를 estimate하기 위해 위의 수식에서 $A^{\\pi, \\gamma}$를 $\\gamma$-just estimator로 사용할 때, bias하지 않은 estimator라고 합시다. 그리고 이 $\\gamma$-just advantage estimator를 <img src=\"https://www.dropbox.com/s/7g2glqx2kbfynaa/Screen%20Shot%202018-08-22%20at%209.03.46%20PM.png?dl=1\" width=\"100\"> 라고 하고, 전체의 trajectory에 대한 하나의 function이라고 합시다.</li>\n<li>${}^1$에서 이 논문의 저자가 하고 싶은 말이 있는 것 같습니다. 개인적으로 $\\gamma$-just estimator를 이해하는 데에 있어서 중요한 주석이라 정확히 해석하고자 합니다.<ul>\n<li>$A^\\pi$를 $A^{\\pi, \\gamma}$로 사용함으로써 이미 bias하다라고 말했지만, “이 논문에서 하고자 하는 것은 $g^\\gamma$에 대해 unbiased estimate를 얻고 싶은 것입니다.” 하지만 undiscounted MDP의 policy gradient에 대해서는 당연히 $\\gamma$를 사용하기 때문에 biased estimate를 얻습니다. 개인적으로 이것은 일단 무시하고 $\\gamma$를 사용할 때 어떻게 unbiased estimate를 얻을 지에 대해 좀 더 포커스를 맞추고 있는 것 같습니다.</li>\n<li>그러니까 저 $A^{\\pi, \\gamma}$를 $\\gamma$-just estimator로 바꿔줌으로써 unbiased estimate를 하고 싶다는 것이 뒤이어 나오는 정의와 명제의 핵심이라고 할 수 있습니다.</li>\n</ul>\n</li>\n</ul>\n<p>Definition 1.<br>먼저 가정을 합니다. (가정을 바탕으로 이루어지는 정의라는 것을 주목합시다.) 만약</p>\n<center><img src=\"https://www.dropbox.com/s/aia9f1l99wyxgah/Screen%20Shot%202018-08-22%20at%209.07.05%20PM.png?dl=1\" width=\"700\"> </center><br>두 수식이 같다면, estimator $\\hat{A}_t$는 $\\gamma$-just입니다.<br><br>그리고 만약 모든 t에 대해서 $\\hat{A}_t$이 $\\gamma$-just이라면, 다음과 같이 표현할 수 있습니다.<br><br><center><img src=\"https://www.dropbox.com/s/r4m6ktw8geeu7pe/Screen%20Shot%202018-08-22%20at%209.29.18%20PM.png?dl=1\" width=\"420\"> </center>\n\n<p>위의 수식이 바로 unbiased estimate입니다.</p>\n<p>$\\gamma$-just인 $\\hat{A}_t$에 대한 한가지 조건은 $\\hat{A}_t$이 두 가지 function $Q_t$ and $b_t$로 나뉠 수 있다는 것입니다.</p>\n<ul>\n<li>$Q_t$는 $\\gamma$-discounted Q-function의 unbiased estimator입니다.</li>\n<li>$b_t$는 action $a_t$전에 sampling된 states and actions의 arbitrary function이다. </li>\n</ul>\n<p>Proposition 1.<br>모든 $(s_t, a_t)$에 대해,<br>$$\\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty | s_t, a_t} [Q_t (s_{t:\\infty}, a_{t:\\infty})] = Q^{\\pi, \\gamma} (s_t, a_t)$$<br>로 인하여 $\\hat{A}_t$이 </p>\n<center> <img src=\"https://www.dropbox.com/s/59ydsav6djwyevl/Screen%20Shot%202018-08-22%20at%209.27.13%20PM.png?dl=1\" width=\"380\"> </center>\n\n<p>형태라고 가정합시다. (가정을 바탕으로 이루어지는 명제라는 점을 주목합시다.)</p>\n<p>그 때, $\\hat{A}_t$은 $\\gamma$-just입니다.</p>\n<p>이 명제에 대한 증명은 Appendix B에 있습니. 그리고 $\\hat{A}_t$에 대한 $\\gamma$-just advantage estimator들은 다음과 같습니다.</p>\n<ul>\n<li>$\\sum_{l=0}^\\infty \\gamma^l r_{t+1}$</li>\n<li>$A^{\\pi, \\gamma} (s_t, a_t)$</li>\n<li>$Q^{\\pi, \\gamma} (s_t, a_t)$</li>\n<li>$r_t + \\gamma V^{\\pi, \\gamma} (s_{t+1}) - V^{\\pi, \\gamma} (s_t)$</li>\n</ul>\n<p>증명을 보기 전에 먼저 Advantage function에 대해서 먼저 살펴봅시다. 아래의 내용은 Sutton이 쓴 논문인 Policy Gradient Methods for Reinforcement Learning with Function Approximation(2000)에서 나온 내용을 리뷰한 것입니다. </p>\n<center> <img src=\"https://www.dropbox.com/s/x737yq97ut6gp1a/Screen%20Shot%202018-07-15%20at%201.16.09%20PM.png?dl=1\" width=\"600\"> </center>\n\n<p>여기서는 “이 수식은 $\\sum_a \\frac{\\partial \\pi (s,a)}{\\partial \\theta} = 0$이기 때문에 가능해진다.”, “즉, 이 수식은 $\\pi(s,a)$의 gradient에만 dependent하기 때문에 advantage 역할을 하는 함수들을 넣어도 아무런 상관이 없다”라는 두 문장을 기억해둡시다.</p>\n<p>이제 증명을 보면 됩니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/e7sj2fwcm1f7hof/figure2.jpg?dl=1\" width=\"600\"> </center><br><center> <img src=\"https://www.dropbox.com/s/kyk4jes1202az4m/figure3.jpg?dl=1\" width=\"600\"> </center>\n\n<p>위의 증명 수식에서 Q와 b의 각각 세번째 수식과 마지막 수식을 자세히 봅시다.</p>\n<ol>\n<li><p>$Q$에 대한 증명</p>\n<ul>\n<li><p>빨간색 부분</p>\n<ul>\n<li>$\\mathbb{E}_{s_{t+1:\\infty}, a_{t+1:\\infty}} [Q_t (s_{0:\\infty}, a_{0:\\infty})]$를 쉽게 말하자면 “모든(과거, 현재, 미래) state와 action에 대해서 expectation을 $t+1$부터 $\\infty$까지 하겠다.”라는 말입니다. 이렇게 함으로써 원래는 $Q^\\pi (s_t, a_t)$가 나와야 맞는 건데, 앞에서 살펴봤다시피 4번째 수식 밑줄 친 자리에는 advantage 역할을 하는 함수들을 넣어도 결과값에 아무런 영향을 끼치지 않기 때문에 의도적으로 $A^\\pi (s_t, a_t)$로 바꾼 것입니다. 또한 이 증명을 바탕으로 $\\hat{A}$이 $\\gamma$-just라는 것을 표현하고 싶기 때문이라고 봐도 됩니다.</li>\n</ul>\n</li>\n<li><p>파란색 부분</p>\n<ul>\n<li>또한 $a_{0:t}$에서 $a_{0:t-1}$로 변한 것은 $A^\\pi (s_t, a_t)$에 baseline이 들어가기 때문에 바꿔준 것입니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>$b$에 대한 증명</p>\n<ul>\n<li>빨간색 부분<ul>\n<li>$\\mathbb{E}_{s_{t+1:\\infty}, a_{t+1:\\infty}} [\\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$가 0으로 바뀌는 것은 위에서 설명했듯이 $\\nabla_\\theta$자체가 $\\log \\pi_\\theta$만 gradient하기 때문에 이것을 expectation을 취하면 0이 됩니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<p><br><br></p>\n<h1 id=\"4-Advantage-Function-Estimation\"><a href=\"#4-Advantage-Function-Estimation\" class=\"headerlink\" title=\"4. Advantage Function Estimation\"></a>4. Advantage Function Estimation</h1><p>이번 section에는 discounted advantage function $A^{\\pi, \\gamma} (s_t, a_t)$의 accurate estimate $\\hat{A}_t$에 대해서 살펴봅시다. 이에 따른 수식은 다음과 같습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/sn16g5iyzzflju0/Screen%20Shot%202018-08-22%20at%209.22.49%20PM.png?dl=1\" width=\"300\"> </center>\n\n<p>여기서 n은 a batch of episodes에 대하여 index한 것입니다.</p>\n<p>$V$를 approximate value function이라고 합시다. 그리고 $\\delta_t^V = r_t + \\gamma V(s_{t+1}) - V(s_t)$이라고 합시다.</p>\n<p>만약 (이전과 마찬가지로 가정부터 합니다.) correct value function $V = V^{\\pi, \\gamma}$가 있다고 한다면, 이것은 $\\gamma$-just advantage estimator입니다. 실제로, $A^{\\pi, \\gamma}$의 unbiased estimator는 다음과 같습니다.<br>$$\\mathbb{E}_{s_{t+1}} [\\delta_t^{V^{\\pi, \\gamma}}] = \\mathbb{E}_{s_{t+1}} [r_t + \\gamma V^{\\pi, \\gamma} (s_{t+1}) - V^{\\pi, \\gamma} (s_t)]$$<br>$$= \\mathbb{E}_{s_{t+1}} [Q^{\\pi, \\gamma} (s_t, a_t) - V^{\\pi, \\gamma} (s_t)] = A^{\\pi, \\gamma} (s_t, a_t)$$<br>그러나, “이 estimator는 유일하게 $V = V^{\\pi, \\gamma}$에 대한 $\\gamma$-just입니다.” 다른 경우라면 이것은 biased policy gradient estimate일 것입니다. (우리가 하고 싶은 것은 $V$에 대해서만 unbiased estimator가 아니라 advantage function에 대해서 일반화된 unbiased estimator를 얻고 싶은 것입니다. 그래서 아래에서도 나오겠지만, $\\gamma$와 함께 $\\lambda$를 이용한 estimator가 나옵니다.ㅏ)</p>\n<p>그래서 $\\delta$에 대해 $k$의 sum으로 생각해봅시다. 이것을 $\\hat{A}_t^{(k)}$라고 하자. 그러면 아래와 같이 표현할 수 있습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/ra7hxksveg2hz45/figure4.jpg?dl=1\" width=\"600\"> </center>\n\n<p>$\\hat{A}_t^{(k)}$은 returns의 $k$-step estimate와 연관지을 수 있고, $\\delta_t^V = \\hat{A}_t^{(1)}$의 case와 유사하게도 $\\hat{A}_t^{(k)}$를 advantage function의 estimator로 생각할 수 있습니다.</p>\n<p>여기서 $k \\rightarrow \\infty$로 생각해보면 bias가 일반적으로 점점 더 작아집니다. 왜냐하면 $\\gamma^k V(s_{t+k})$가 점점 많이 discounted되서 $-V(s_t)$가 bias에 영향을 미치지 못하기 때문입니다. $k \\rightarrow \\infty$를 취하면 다음과 같은 수식이 나옵니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/13fn9wcup8pfh9u/Screen%20Shot%202018-08-22%20at%209.19.52%20PM.png?dl=1\" width=\"320\"> </center>\n\n<p>우변의 수식과 같이 empirical returns에서 value function baseline을 뺀 것으로 나타낼 수 있습니다.</p>\n<p>Generalized Advantage Estimator GAE($\\gamma, \\lambda$)는 $k$-step estimators의 exponentially-weighted average로 나타낼 수 있습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/yg1ybmfkep3towu/figure5.jpg?dl=1\" width=\"600\"> </center>\n\n<p>(TD($\\lambda$)를 떠올려주세요..!)</p>\n<p>위의 마지막 수식에서도 알 수 있듯이, advantage estimator는 Bellman residual terms의 discounted sum과 관련있는 간단한 수식입니다. 다음 section에서 modified reward function을 가진 MDP에 returns로서의 관점에서 위의 마지막 수식을 더 자세히 살펴봅시다. 위의 수식은 TD($\\lambda$)와 많이 유사합니다. 그러나 TD($\\lambda$)는 value function를 estimator하고, 여기서는 advantage function을 estimator합니다.</p>\n<p>위의 수식에서 $\\lambda = 0$ and $\\lambda = 1$에 대해서는 특별한 case가 존재한다. 수식으로 표현하면 다음과 같습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/ufglxzanhcnbi36/figure6.jpg?dl=1\" width=\"600\"> </center>\n\n<ul>\n<li>$GAE(\\gamma, 1)$은 $V$의 정확도와 관계없이 $\\gamma$-just입니다. 그러나 returns의 sum때문에 high variance합니다.</li>\n<li>$GAE(\\gamma, 0)$은 $V = V^{\\pi, \\gamma}$에 대해 $\\gamma$-just입니다. 그리고 bias하지만 일반적으로 훨씬 lower variance를 가집니다. </li>\n<li>$0 &lt; \\lambda &lt; 1$에 대해 GAE는 parameter $\\lambda$를 control하여 bias와 variance사이에 compromise를 만듭니다.</li>\n</ul>\n<p>두 가지 별개의 parameter $\\gamma$ and $\\lambda$를 가진 advantage estimator는 bias-variance tradeoff에 도움을 줍니다. 그러나 이 두 가지 parameter는 각각 다른 목적을 가지고 작동합니다.</p>\n<ul>\n<li>$\\gamma$는 가장 중요하게 value function $V^{\\pi, \\gamma}$ 의 scale을 결정합니다. 또한 $\\gamma$는 $\\lambda$에 의존하지 않습니다. $\\gamma &lt; 1$로 정하는 것은 policy gradient estimate에서 bias합니다.</li>\n<li>반면에, $\\lambda &lt; 1$는 유일하게 value function이 부정확할 때 bias합니다. 그리고 경험상, $\\lambda$의 best value는 $\\gamma$의 best value보다 훨씬 더 낮습니다. 왜냐하면 $\\lambda$가 정확한 value function에 대해 $\\gamma$보다 훨씬 덜 bias하기 때문입니다.</li>\n</ul>\n<p>GAE를 사용할 때, $g^\\gamma$의 biased estimator를 구성할 수 있습니다. 수식은 다음과 같습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/ok57dsk52o7pl9r/Screen%20Shot%202018-08-22%20at%209.17.54%20PM.png?dl=1\" width=\"670\"> </center>\n\n<p>여기서 $\\lambda = 1$일 때 동일해집니다.</p>\n<p><br><br></p>\n<h1 id=\"5-Interpretation-as-Reward-Shaping\"><a href=\"#5-Interpretation-as-Reward-Shaping\" class=\"headerlink\" title=\"5. Interpretation as Reward Shaping\"></a>5. Interpretation as Reward Shaping</h1><p>이번 section에서는 앞서 다뤘던 수식 $\\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^V$를 modified reward function의 MDP의 관점으로 생각해봅시다. 조금 더 구체적으로 말하자면, MDP에서 reward shaping transformation을 실행한 후에 적용된 extra discounted factor로서 $\\lambda$를 어떻게 볼 것인지에 대해서 다룹니다.</p>\n<p>(개인적인 comment) 한 가지 먼저 언급하자면, 본래의 목적은 reward shaping이 아니라 variance reduction입니다. 이번 section은 그저 이전에 이러한 개념이 있었고, gae를 다른 관점에서 생각해보자라는 뜻에서 나온 section인 것 같습니다. ‘아~ 이러한 개념이 있구나~!’ 정도로만 알면 될 것 같습니다. ‘gae가 reward shaping의 효과까지 있어서 이러한 section을 넣은 것일까?’ 라는 생각도 해봤지만 아직 잘 모르겠습니다. 실험부분에도 딱히 하지 않은 걸로 봐서는.. 아닌 것 같기도 하고.. 아직까지는 그저 ‘reward shaping의 관점에서 봤을 때에도 gae로 만들어줄 수 있다.’ 정도만 생각하려고 합니다.</p>\n<p>Reward Shaping이란 개념 자체는 일반적인 알고리즘 분야(최단경로 문제 등)에도 있습니다. 하지만 이 개념이 머신러닝에 적용된 건 Andrew Y. Ng이 쓴 Policy Invariance under Reward Transformations Theory and Application to Reward Shaping(1999) 논문입니다. 논문에서 Reward Shaping을 설명하는 부분은 간략하게 다음과 같습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/0w9cuxjkhz2mbcy/figure7.jpg?dl=1\" width=\"400\"> </center><br><center> <img src=\"https://www.dropbox.com/s/xcp9eqvcbv3268g/figure8.jpg?dl=1\" width=\"400\"> </center>\n\n<p>위의 글만 보고 이해가 잘 안됩니다. 그래서 다른 자료들을 찾던 중에 다음의 그림을 찾게 되었습니다. (아래의 그림은 Youtube에 있는 <a href=\"https://www.youtube.com/watch?v=xManAGjbx2k&amp;t=95s\" target=\"_blank\" rel=\"noopener\">Udacity 영상</a>에서 있는 그림입니다.)</p>\n<center> <img src=\"https://www.dropbox.com/s/87kacngez9cl2e6/figure9.jpg?dl=1\" width=\"400\"> </center>\n\n<p>이 그림을 통해서 Reward Shaping을 이해해봅시다.</p>\n<p>Reward Shaping(보상 형성)을 통해서 뭘 하고 싶은지 목적부터 봅시다. reward space가 sparse 한 경우에 reward가 너무 드문드문 나옵니다. 따라서 이것을 꾸준히 reward를 받을 수 있도록 바꿉니다. potential-based shaping function인 $\\Phi$를 만들어서 더하고 빼줍니다. 저 $\\Phi$ 자리에는 대표적으로 state value function이 많이 들어간다고 합니다. (아직 목적이 이해가 안될 수 있습니다. Reward Shaping에 대해서 다 보고 나서 다시 한 번 읽어봅시다.)</p>\n<p>위의 그림은 돌고래가 점프하여 불구멍을 통과해야하는 환경입니다. 하지만 저 불구멍을 통과하기 위해서는 (1) 점프도 해야하고, (2) 불도 피해야하고, (3) 알맞게 착지까지 완료해야합니다. 이렇게 해야 reward를 +1 얻습니다. </p>\n<p>생각해봅시다. 어느 세월에 점프도 해야하고.. 불도 피해야하고.. 알맞게 착지까지해서 reward를 +1 얻겠습니까? 따라서 그 전에도 잘하고 있는 지, 못하고 있는 지를 판단하기 위해, 다시 말해 reward를 꾸준히 받도록 다음과 같이 transformed reward function $\\tilde{r}$을 정의합니다.</p>\n<p>$$\\tilde{r} (s, a, s’) = r(s, a, s’) + \\gamma \\Phi (s’) - \\Phi (s)$$</p>\n<ul>\n<li>여기서 $\\Phi : S \\rightarrow \\mathbb{R}$를 state space에서의 arbitrary scalar-valued function을 나타냅니다. 그리고 $\\Phi$자리에는 대표적으로 state value function이 들어간다고 생각합시다.</li>\n<li>형태가 TD residual term의 결과와 비슷하지만 의미와 의도가 다릅니다. reward shaping은 sparse reward 때문이고, 이전 section에서 봤던 gae는 variance reduction때문에 나온 것입니다.</li>\n</ul>\n<center> <img src=\"https://www.dropbox.com/s/tkfpqniaazwfbld/figure10.jpg?dl=1\" width=\"600\"> </center>\n\n<p>이 transformation은 discounted advantage function $A^{\\pi, \\gamma}$으로도 둘 수 있습니다. state $s_t$를 시작하여 하나의 trajectory의 rewards의 discounted sum을 표현하면 다음과 같습니다.<br>$$\\sum_{l=0}^\\infty \\gamma^l \\tilde{r} (s_{t+l}, a_t, s_{t+l+1}) = \\sum_{l=0}^\\infty \\gamma^l r(s_{t+l}, a_{t+l}, s_{t+l+1}) - \\Phi(s_t)$$</p>\n<p>$\\tilde{Q}^{\\pi, \\gamma}, \\tilde{V}^{\\pi, \\gamma}, \\tilde{A}^{\\pi, \\gamma}$를  transformed MDP의 value function과 advantage function이라고 하면, 다음과 같은 수식이 나옵니다.<br>$$\\tilde{Q}^{\\pi, \\gamma} (s, a) = Q^{\\pi, \\gamma} (s, a) - \\Phi (s)$$<br>$$\\tilde{V}^{\\pi, \\gamma} (s, a) = V^{\\pi, \\gamma} (s, a) - \\Phi (s)$$<br>$$\\tilde{A}^{\\pi, \\gamma} (s, a) = (Q^{\\pi, \\gamma} (s, a) - \\Phi (s)) - (V^\\pi (s) - \\Phi (s)) = A^{\\pi, \\gamma} (s, a)$$</p>\n<p>이제 reward shaping의 idea를 가지고 어떻게 policy gradient estimate를 얻을 수 있는 지에 대해서 알아봅시다.</p>\n<ul>\n<li>$0 \\le \\lambda \\le 1$의 범위에 있는 steeper discount $\\gamma \\lambda$를 사용합니다.</li>\n<li>shaped reward $\\tilde{r}$는 Bellman residual term $\\delta^V$와 동일합니다.</li>\n<li>그리고 $\\Phi = V$와 같다고 보면 다음과 같은 수식이 나옵니다.</li>\n</ul>\n<p>$$\\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\tilde{r} (s_{t+l}, a_t, s_{t+l+1}) = \\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^V = \\hat{A}_t^{GAE(\\gamma, \\lambda)}$$<br>이렇게 shaped rewards의 $\\gamma \\lambda$-discounted sum을 고려함으로써 GAE를 얻을 수 있습니다. 더 정확하게 $\\lambda = 1$은 $g^\\gamma$의 unbiased estimate이고, 반면에 $\\lambda &lt; 1$은 biased estimate입니다.</p>\n<p>좀 더 나아가서, shaping transformation과 parameters $\\gamma$ and $\\lambda$의 결과를 보기 위해, response function $\\chi$를 이용하면 다음과 같은 수식이 나옵니다.<br>$$\\chi (l; s_t, a_t) = \\mathbb{E} [r_{t+l} | s_t, a_t] - \\mathbb{E} [r_{t+l} | s_t]$$<br>추가적으로 $A^{\\pi, \\gamma} (s, a) = \\sum_{l=0}^\\infty \\gamma^l \\chi (l; s, a)$입니다.<br>그래서 discounted policy gradient estimator는 다음과 같이 쓸 수 있다.<br>$$\\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) A^{\\pi, \\gamma} (s_t, a_t) = \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) \\sum_{l=0}^\\infty \\gamma^l \\chi (l; s, a)$$</p>\n<p><br><br></p>\n<h1 id=\"6-Value-Fuction-Estimation\"><a href=\"#6-Value-Fuction-Estimation\" class=\"headerlink\" title=\"6. Value Fuction Estimation\"></a>6. Value Fuction Estimation</h1><p>이번 section에서는 Trust Region Optimization Scheme에 따라 Value function을 Estimation합니다.</p>\n<p><br></p>\n<h2 id=\"6-1-Simplest-approach\"><a href=\"#6-1-Simplest-approach\" class=\"headerlink\" title=\"6.1 Simplest approach\"></a>6.1 Simplest approach</h2><p>$$minimize_{\\phi} \\sum_{n=1}^{N} \\vert\\vert V_{\\phi}(s_n) - \\hat{V_n} \\vert\\vert^{2}$$</p>\n<ul>\n<li>위는 가장 간단하게 non-linear approximation으로 푸는 방법입니다.</li>\n<li>$\\hat{V_t} = \\sum_{l=0}^{\\infty}\\gamma^l r_{t+l}$은 reward 에 대한 discounted sum을 의미합니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"6-2-Trust-region-method-to-optimize-the-value-function\"><a href=\"#6-2-Trust-region-method-to-optimize-the-value-function\" class=\"headerlink\" title=\"6.2 Trust region method to optimize the value function\"></a>6.2 Trust region method to optimize the value function</h2><ul>\n<li>Value function을 최적화 하기 위해 trust region method를 사용합니다.</li>\n<li>Trust region은 최근 데이터에 대해 overfitting되는 것을 막아줍니다.</li>\n</ul>\n<p>Trust region문제를 풀기 위해서는 다음 스텝을 따릅니다.</p>\n<ul>\n<li>$\\sigma^2 = \\frac{1}{N} \\sum_{n=1}^{N} \\vert\\vert V_{\\phi old}(s_n) - \\hat{V_n} \\vert\\vert^{2}$을 계산합니다.</li>\n<li>그 후에 다음과 같은 constrained opimization문제를 풉니다.</li>\n</ul>\n<center> <img src=\"https://www.dropbox.com/s/d74cg3votxi2dbg/Screen%20Shot%202018-08-22%20at%209.13.28%20PM.png?dl=1\" width=\"300\"> </center>\n\n<center> <img src=\"https://www.dropbox.com/s/jrkjax71t3j3vk4/Screen%20Shot%202018-08-22%20at%209.13.35%20PM.png?dl=1\" width=\"370\"> </center>\n\n<ul>\n<li>위의 수식은 사실 old Value function과 새로운 Value function KL distance가 $\\epsilon$ 다 작아야한다는 수식과 같습니다. Value function이 평균은 $V_{\\phi}(s)$이고 분산이 $\\sigma^2$인 conditional Gaussian distribution으로 parameterize되었을 뿐입니다.</li>\n</ul>\n<p><img width=\"400px\" src=\"https://www.dropbox.com/s/uw0v05feu8chqmc/Screenshot%202018-07-08%2010.04.38.png?dl=1\"> </p>\n<p>이 trust region문제의 답을 conjudate gradient algorithm을 이용하여 근사값을 구할 수 있습니다. 특히, 다음과 같은  quadratic program을 풀게됩니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/pawjkm8b9ycwsl5/Screen%20Shot%202018-08-22%20at%209.10.36%20PM.png?dl=1\" width=\"250\"> </center>\n\n<center> <img src=\"https://www.dropbox.com/s/va7wms8uqxnw5rq/Screen%20Shot%202018-08-22%20at%209.10.40%20PM.png?dl=1\" width=\"370\"> </center>\n\n<ul>\n<li>여기서 $g$는 objective 의 gradient입니다.</li>\n<li>$j_n = \\nabla_{\\phi} V_{\\phi}(s_n)$일때, $H = \\frac{1}{N} \\sum_{n} j_n j^T_n$이며, $H$는 objective의 hessian에 대해서 gaussian newton method로 근사한 값입니다. 따라서, value function을 conditional probability로 해석한다면 Fisher information matrix가 됩니다.</li>\n<li>구현할때의 방법은 TRPO에서 사용한 방법과 모두 같습니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"7-Experiments\"><a href=\"#7-Experiments\" class=\"headerlink\" title=\"7.Experiments\"></a>7.Experiments</h1><p>실험은 다음 두 가지 물음에 대해서 디자인 되었습니다.</p>\n<ul>\n<li>GAE에 따라서 episodic total reward를 최적화 할때, $\\lambda$와 $\\gamma$가 변함에 따라서 어떤 경험적인 효과를 볼 수 있는지?</li>\n<li>GAE와 trust region alogorithm을 policy와 value function 모두에 함께 사용했을 때 어려운 문제에 적용되는 큰 뉴럴넷을 최적화 할 수 있을지?</li>\n</ul>\n<p><br></p>\n<h2 id=\"7-1-Policy-Optimization-Algorithm\"><a href=\"#7-1-Policy-Optimization-Algorithm\" class=\"headerlink\" title=\"7.1 Policy Optimization Algorithm\"></a>7.1 Policy Optimization Algorithm</h2><p>Policy update는 TRPO를 사용합니다. TRPO에 대한 설명은 여기서는 생략하겠습니다. TRPO 포스트를 보고 돌아와주세요!</p>\n<ul>\n<li><p>이전 TRPO에서 이미 TRPO와 다른 많은 알고리즘들을 비교하였기 때문에, 여기서는 똑같은 짓을 반복하지 않고 $\\lambda$, $\\gamma$가 변함에 따라 어떤 영향이 있는 지에 대한 실험에 집중하겠다고 합니다. (귀찮았던거죠..)</p>\n</li>\n<li><p>TRPO를 적용한 GAE의 최종 알고리즘은 다음과 같습니다.</p>\n</li>\n</ul>\n<center> <img width=\"600px\" src=\"https://www.dropbox.com/s/b1klz11f2frrvg4/Screenshot%202018-07-08%2010.33.54.png?dl=1\"> </center><br><center> <img width=\"300px\" src=\"https://www.dropbox.com/s/35fxte05pqtiel7/Screenshot%202018-07-08%2010.35.22.png?dl=1\"> </center><br><center> <img width=\"300px\" src=\"https://www.dropbox.com/s/u35pjow3w50bvz1/Screenshot%202018-07-08%2010.36.03.png?dl=1\"> </center>\n\n<ul>\n<li>여기서 주의할 점은 Policy update($\\theta_i \\rightarrow \\theta_{i+1}$)에서 $V_{\\phi_i}$를 사용했다는 점입니다.</li>\n<li>만약 Value function을 먼저 update하게 된다면 추가적인 bias가 발생합니다.</li>\n<li>극단적으로 생각해보아서, 우리가 Value function을 완벽하게 overfit을 해낸다면 Bellman residual($r_t + \\gamma V(s_{t+1}) - V(S_t)$)은 0이 됩니다. 그럼 Policy gradient의 estimation도 거의 0이 될 것입니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"7-2-Expermint-details\"><a href=\"#7-2-Expermint-details\" class=\"headerlink\" title=\"7.2 Expermint details\"></a>7.2 Expermint details</h2><h3 id=\"7-2-1-Environment\"><a href=\"#7-2-1-Environment\" class=\"headerlink\" title=\"7.2.1 Environment\"></a>7.2.1 Environment</h3><p>실험에서 사용된 환경은 다음 네 가지 입니다.</p>\n<ol>\n<li>classic cart-pole (x 3D)</li>\n<li>bipedal locomotion</li>\n<li>quadrupedal locomotion</li>\n<li>dynamically standing up for the biped</li>\n</ol>\n<h3 id=\"7-2-2-Architecture\"><a href=\"#7-2-2-Architecture\" class=\"headerlink\" title=\"7.2.2 Architecture\"></a>7.2.2 Architecture</h3><ul>\n<li>3D robot task에 대해서는 같은 모델을 사용하였습니다.<ul>\n<li>layers  = [100, 50, 25] 각각 tanh 사용. (Policy와 Value 네트워크 모두)</li>\n<li>Final output layer은 linear</li>\n</ul>\n</li>\n<li>Cartpole에 대해서는 1개의 layer 안에 20개의 hidden unit만 있는 linear policy를 사용했다고 합니다.</li>\n</ul>\n<h3 id=\"7-2-3-Task\"><a href=\"#7-2-3-Task\" class=\"headerlink\" title=\"7.2.3 Task\"></a>7.2.3 Task</h3><ul>\n<li>Cartpole<ul>\n<li>한 배치당 20 개의 trajectory를 모았고, maximum length는 1000입니다.</li>\n</ul>\n</li>\n<li>3D biped locomotion<ul>\n<li>33 dim state , 10 dim action</li>\n<li>50000 time step per batch</li>\n</ul>\n</li>\n<li>3D quadruped locomotion<ul>\n<li>29 dim state, 8 dim action</li>\n<li>200000 time step per batch</li>\n</ul>\n</li>\n<li>3D biped locomotion Standing<ul>\n<li>33 dim state , 10 dim action</li>\n<li>200000 time step per batch</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"7-2-3-results\"><a href=\"#7-2-3-results\" class=\"headerlink\" title=\"7.2.3 results\"></a>7.2.3 results</h3><p>cost의 관점에서 결과를 나타내었다고 합니다. Cost는 negative reward와 이것이 최소화 되었는가로 정의되었습니다.</p>\n<h4 id=\"7-2-3-1-Cartpole\"><a href=\"#7-2-3-1-Cartpole\" class=\"headerlink\" title=\"7.2.3.1 Cartpole\"></a>7.2.3.1 Cartpole</h4><center> <img width=\"500px\" src=\"https://www.dropbox.com/s/x9pbms1wvg38lda/Screenshot%202018-07-08%2011.08.22.png?dl=1\"> </center>\n\n<ul>\n<li>왼쪽 그림은 $\\gamma$를 0.99로 고정시켜놓은 상태에서 $\\lambda$를 변화시킴에 따라서 cost를 측정한 것입니다. </li>\n<li>오른쪽은 $\\gamma$와 $\\lambda$를 둘 다 변화 시키면서 성능을 그림으로 나타낸 표입니다. 흰색에 가까울 수록 좋은 성능입니다. </li>\n</ul>\n<h4 id=\"7-2-3-2-3D-BIPEDAL-LOCOMOTINO\"><a href=\"#7-2-3-2-3D-BIPEDAL-LOCOMOTINO\" class=\"headerlink\" title=\"7.2.3.2 3D BIPEDAL LOCOMOTINO\"></a>7.2.3.2 3D BIPEDAL LOCOMOTINO</h4><center> <img width=\"500px\" src=\"https://www.dropbox.com/s/i9wj4p6ijojsy82/Screenshot%202018-07-08%2011.25.08.png?dl=1\"> </center>\n\n\n<ul>\n<li>다른 random seed로 부터 9번 씩 시도한 결과를 mean을 취해서 사용합니다.</li>\n<li>Best performance는   $\\gamma \\in [0.99, 0.995]$ 그리고  $\\lambda \\in [0.96, 0.99]$일때. </li>\n<li>1000 iteration 후에 빠르고 부드럽고 안정적인 걸음거이가 나옵니다.</li>\n<li>실제로 걸린 시간은 0.01(타입스텝당 시간) <em> 50000(배치당 타임스텝) </em> 1000(배치) <em> 3600(초-&gt;시간) </em> 24 = 5.8일 정도가 걸렸습니다.</li>\n</ul>\n<h4 id=\"7-2-3-3-다른-ROBOT-TASKS\"><a href=\"#7-2-3-3-다른-ROBOT-TASKS\" class=\"headerlink\" title=\"7.2.3.3 다른 ROBOT TASKS\"></a>7.2.3.3 다른 ROBOT TASKS</h4><center> <img width=\"500px\" src=\"https://www.dropbox.com/s/fuuat65we52quht/Screenshot%202018-07-08%2011.33.11.png?dl=1\"> </center>\n\n<ul>\n<li>다른 로봇 TASK에 대해서는 아주 제한적인 실험만 진행합니다.(시간이 부족했던듯 하네요..)</li>\n<li>Quadruped에 대해서는 $\\gamma = 0.995$로 fix, $\\lambda \\in {0, 0.96}$</li>\n<li>Standingup에 대해서는 $\\gamma = 0.99$로 fix, $\\lambda \\in {0, 0.96}$</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"8-Discussion\"><a href=\"#8-Discussion\" class=\"headerlink\" title=\"8. Discussion\"></a>8. Discussion</h1><p><br></p>\n<h2 id=\"8-1-Main-discussion\"><a href=\"#8-1-Main-discussion\" class=\"headerlink\" title=\"8.1 Main discussion\"></a>8.1 Main discussion</h2><p>지금까지 복잡하고 어려운 control problem에서 Reinforcement Learning(RL)은 high sample complexity 때문에 제한이 되어왔습니다. 따라서 이 논문에서 그 제한을 풀고자 advantage function의 good estimate를 얻는 “variance reduction”에 대해 연구하였습니다.</p>\n<p>“Generalized Advantage Estimator(GAE)”라는 것을 제안했고, 이것은 bias-variance tradeoff를 조절하는 두 개의 parameter $\\gamma,\\lambda$를 가집니다.<br>또한 어떻게 Trust Region Policy Optimization과 value function을 optimize하는 Trust Region Algorithm의 idea를 합치는 지를 보였습니다.</p>\n<p>이렇게 함으로써 보다 더 복잡하고 어려운 control task들을 해결할 수 있었습니다.</p>\n<p>GAE의 실험적인 입증으로는 robotic locomotion을 simulation하는 domain입니다. 실험에서도 보여준 것처럼 [0.9, 0.99]의 범위에서 $\\lambda$의 적절한 중간의 값을 통해 best performance를 얻습니다. 좀 더 나아가 연구되어야할 점은 adaptive or automatic하도록 estimator parameter $\\gamma,\\lambda$를 조절하는 방법입니다.</p>\n<p><br></p>\n<h2 id=\"8-2-Future-work\"><a href=\"#8-2-Future-work\" class=\"headerlink\" title=\"8.2 Future work\"></a>8.2 Future work</h2><p>Value function estimation error와 Policy gradient estimation error사이의 관계를 알아낸다면, 우리는 Value function fitting에 더 잘 맞는 error metric(policy gradient estimation 의 정확성과 더 잘 맞는 value function)을 사용할 수 있습니다. 여기서 Policy와 Value function의 파라미터를 공유하는 모델을 만드는 것은 아주 흥미롭고 이점이 많습니다. 하지만 수렴을 보장하도록 적절한 numerical optimization을 제시해야 할 것입니다.</p>\n<p>추가적으로 DDPG는 별로라고 합니다. 그리고 TD(0)는 bias가 너무 크고, poor performance로 이끈다고 합니다. 특히나 이 논문에서는 low-dimention의 쉬운 문제들만 해결했습니다.</p>\n<center> <img width=\"500px\" src=\"https://www.dropbox.com/s/nhc7t9psul5lr3x/Screenshot%202018-07-08%2011.45.15.png?dl=1\"> </center>\n\n<p><br></p>\n<h2 id=\"8-3-FAQ\"><a href=\"#8-3-FAQ\" class=\"headerlink\" title=\"8.3 FAQ\"></a>8.3 FAQ</h2><ul>\n<li>Compatible features와는 무슨 관계?<ul>\n<li>Compatible features는 value function을 이용하는 policy gradient 알고리즘들과 함께 자주 언급됩니다.</li>\n<li>Actor Critic의 저자는 policy의 제한된 representation power때문에, policy gradient는 단지 advantage function space의 subspace에만 의존하게 됩니다.</li>\n<li>이 subspace는 compatible features에 의해 span됩니다.</li>\n<li>이 이론은 현재 문제 구조를 어떻게 이용해야 advantage function에 대해 더 나은 estimation을 할 수 있는 지에 대한 지침을 주지 않습니다. GAE 논문의 idea와 orthogonal합니다.</li>\n</ul>\n</li>\n<li>왜 Q function을 사용하지 않는가?<ul>\n<li>먼저 state-value function이 더 낮은 차원의 input을 가진다. 그래서 Q function보다 더 학습하기가 쉽습니다.</li>\n<li>두 번째로 이 논문에서 제안하는 방법으로는 high bias estimator에서 low bias estimator로 $\\lambda$를 통해서 부드럽게 interpolate를 할 수 있습니다.</li>\n<li>반면에 Q를 사용하면 단지 high-bias estimator 밖에 사용할 수 없습니다.</li>\n<li>특히나 return에 대한 one-step estimation은 엄두를 못낼 정도로 bias가 큽니다.</li>\n</ul>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"처음으로\"><a href=\"#처음으로\" class=\"headerlink\" title=\"처음으로\"></a>처음으로</h1><h2 id=\"PG-Travel-Guide\"><a href=\"#PG-Travel-Guide\" class=\"headerlink\" title=\"PG Travel Guide\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/\">PG Travel Guide</a></h2><p><br></p>\n<h1 id=\"이전으로\"><a href=\"#이전으로\" class=\"headerlink\" title=\"이전으로\"></a>이전으로</h1><h2 id=\"TRPO-여행하기\"><a href=\"#TRPO-여행하기\" class=\"headerlink\" title=\"TRPO 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/24/5_trpo/\">TRPO 여행하기</a></h2><h2 id=\"TRPO-Code\"><a href=\"#TRPO-Code\" class=\"headerlink\" title=\"TRPO Code\"></a><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/trpo_gae.py\" target=\"_blank\" rel=\"noopener\">TRPO Code</a></h2><p><br></p>\n<h1 id=\"다음으로\"><a href=\"#다음으로\" class=\"headerlink\" title=\"다음으로\"></a>다음으로</h1><h2 id=\"PPO-여행하기\"><a href=\"#PPO-여행하기\" class=\"headerlink\" title=\"PPO 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/22/7_ppo/\">PPO 여행하기</a></h2>","site":{"data":{}},"excerpt":"","more":"<center> <img src=\"https://www.dropbox.com/s/p8gfpyo6xf9wm5w/Screen%20Shot%202018-07-18%20at%201.25.53%20AM.png?dl=1\" width=\"700\"> </center>\n\n<p>논문 저자 : John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan and Pieter Abbeel<br>논문 링크 : <a href=\"https://arxiv.org/pdf/1506.02438.pdf\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/pdf/1506.02438.pdf</a><br>Proceeding : International Conference of Learning Representations (ICLR) 2016<br>정리 : 양혁렬, 이동민</p>\n<hr>\n<h1 id=\"1-들어가며…\"><a href=\"#1-들어가며…\" class=\"headerlink\" title=\"1. 들어가며…\"></a>1. 들어가며…</h1><p>현존하는 Policy Gradient Method들의 목적은 누적되는 reward들을 optimization하는 것입니다. 하지만 학습할 때에 많은 양의 sample이 필요로 하고, 들어오는 data가 nonstationarity임에도 불구하고 stable and steady improvement가 어렵습니다.</p>\n<p>그래서 이 논문에서는 다음과 같은 방법을 제시합니다.</p>\n<ul>\n<li>TD($\\lambda$)와 유사한 advantage function의 exponentially-weighted estimator를 사용하여 policy gradient estimate의 variance를 줄이는 것 </li>\n<li>policy와 value function에 대한 Trust Region Optimization 사용하는 것</li>\n</ul>\n<p>3D locomotion tasks에 대한 empirical results는 다음과 같습니다.</p>\n<ul>\n<li>bipedal and quadrupedal simulated robots의 달리는 자세를 학습</li>\n<li>bipedal 사람이 땅에 누워있다가 일어서는 것을 학습</li>\n</ul>\n<p>※ TD($\\lambda$)를 혹시 모르실 경우도 있을 것 같아 간략하게 정리하였습니다. <a href=\"http://dongminlee.tistory.com/10\" target=\"_blank\" rel=\"noopener\">http://dongminlee.tistory.com/10</a> 를 먼저 보시고 아래의 내용을 봐주시기 바랍니다!</p>\n<p><br><br></p>\n<h1 id=\"2-Introduction\"><a href=\"#2-Introduction\" class=\"headerlink\" title=\"2. Introduction\"></a>2. Introduction</h1><p>기본적으로 “parameterized stochastic policy”를 가정합니다. 이 때 expected total returns의 gradient에 대한 unbiased estimate를 얻을 수 있는데 이것을 REINFORCE라고 부릅니다. 하지만 하나의 action의 결과가 과거와 미래의 action의 결과로 혼동되기 때문에 gradient estimator의 high variance는 시간에 따라 scaling됩니다.</p>\n<p>또 다른 방법은 Actor-Critic이 있습니다. 이 방법은 empirical returns보다 하나의 value function을 사용합니다. 또한 bias하고 lower variance를 가진 estimator입니다. 구체적으로 말하자면, high variance하다면 더 sampling을 하면 되는 반면에 bias는 매우 치명적입니다. 다시 말해 bias는 algorithm이 converge하는 데에 실패하거나 또는 local optimum이 아닌 poor solution에 converge하도록 만듭니다.</p>\n<p>따라서 이 논문에서는 $\\gamma\\in [0,1]$ and $\\lambda\\in [0,1]$에 의해 parameterized estimation scheme인 Generalized Advantage Estimator(GAE)를 다룹니다.</p>\n<p>이 논문을 대략적으로 요약하자면 다음과 같습니다.</p>\n<ul>\n<li>효과적인 variance reduction scheme인 GAE를 다룹니다. 또한 실험할 때 batch trust region algorithm에 더하여 다양한 algorithm들에 적용됩니다.</li>\n<li>value function에 대해 trust region optimization method를 사용합니다. 이렇게 함으로서 더 robust하고 efficient한 방법이 됩니다.</li>\n<li>A와 B를 합쳐서, 실험적으로 control task에 neural network policies를 learning하는 데에 있어서 효과적인 algorithm을 얻습니다. 이러한 결과는 high-demensional continuous control에 RL을 사용함으로서 state of the art로 확장되었습니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"3-Preliminaries\"><a href=\"#3-Preliminaries\" class=\"headerlink\" title=\"3. Preliminaries\"></a>3. Preliminaries</h1><p>먼저 policy optimization의 “undiscounted formulation”을 가정합니다. (undiscounted formulation에 주목합시다.)</p>\n<ul>\n<li>initial state $s_0$는 distribution $\\rho_0$으로부터 sampling된 것입니다.</li>\n<li>하나의 trajectory ($s_0, a_0, s_1, a_1, …$)는 terminal state에 도달될 때 까지 policy $a_t$ ~ $\\pi(a_t | s_t)$에 따라서 action을 sampling하고, dynamics $s_{t+1}$ ~ $P(s_{t+1} | s_t, a_t)$에 따라서 state를 sampling함으로써 생성됩니다.</li>\n<li>reward $r_t = r(s_t, a_t, s_{t+1})$은 매 time step마다 받아집니다.</li>\n<li>목표는 모든 policies에 대해 finite하다고 가정됨으로서 expected total reward $\\sum_{t=0}^{\\infty} r_t$를 maximize하는 것입니다.</li>\n</ul>\n<p>여기서 중요한 점은 $\\gamma$를 discount의 parameter로 사용하는 것이 아니라 “bias-variance tradeoff를 조절하는 parameter로 사용한다” 는 것입니다.</p>\n<p>policy gradient method는 gradient $g := \\nabla_\\theta \\mathbb{E} [\\sum_{t=0}^\\infty r_t]$를 반복적으로 estimate함으로써 expected total reward를 maximize하는 것인데, policy gradient에는 여러 다른 표현들이 있습니다.<br>$$g = \\mathbb{E} [\\sum_{t=0}^\\infty \\Phi_t \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$</p>\n<p>여기서 $\\Phi_t$는 아래 중의 하나일 수 있습니다.</p>\n<ol>\n<li>$\\sum_{t=0}^\\infty r_t$: trajectory의 total reward</li>\n<li>$\\sum_{t’=t}^\\infty r_t’$: action $a_t$ 후의 reward</li>\n<li>$\\sum_{t’=t}^\\infty r_t’ - b(s_t)$: 2의 baselined version</li>\n<li>$Q^\\pi (s_t, a_t)$: state-action value function</li>\n<li>$A^\\pi (s_t, a_t)$: advantage function</li>\n<li>$r_t + V^\\pi (s_{t+1}) - V^\\pi (s_t)$: TD residual</li>\n</ol>\n<p>위의 번호 중 4, 5, 6의 수식들은 다음의 정의를 사용합니다.</p>\n<ul>\n<li>$V^\\pi (s_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_t:\\infty} [\\sum_{l=0}^\\infty r_{t+1}]$</li>\n<li>$Q^\\pi (s_t, a_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty} [\\sum_{l=0}^\\infty r_{t+l}]$</li>\n<li>$A^\\pi (s_t, a_t) := Q^\\pi (s_t, a_t) - V^\\pi (s_t)$, (Advantage function)</li>\n</ul>\n<p>추가적으로 colon notation $a : b$는 포괄적인 범위 $(a, a+1, … , b)$입니다. (잘 기억해둡시다. 뒤에 계속해서 colon notation이 나옵니다.)</p>\n<p>여기서부터 parameter $\\gamma$에 대해 좀 더 자세히 알아봅시다. parameter $\\gamma$는 bias하면서 동시에 reward를 downweighting함으로써 variance를 줄입니다. 다시 말해 MDP의 discounted formulation에서 사용된 discounted factor와 일치하지만, 이 논문에서는 “$\\gamma$를 undiscounted MDP에서 variance reduction parameter”로 다룹니다. -&gt; 결과는 같지만, 의미와 의도가 다릅니다.</p>\n<p>discounted value function들은 다음과 같습니다.</p>\n<ul>\n<li>$V^{\\pi, \\gamma} (s_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_t:\\infty} [\\sum_{l=0}^\\infty \\gamma^l r_{t+l}]$ </li>\n<li>$Q^\\pi (s_t, a_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty} [\\sum_{l=0}^\\infty \\gamma^l r_{t+l}]$</li>\n<li>$A^{\\pi, \\gamma} (s_t, a_t) := Q^{\\pi, \\gamma} (s_t, a_t) - V^{\\pi, \\gamma} (s_t)$</li>\n</ul>\n<p>따라서 policy gradient에서의 discounted approximation은 다음과 같이 나타낼 수 있습니다.<br>$$g^\\gamma := \\mathbb{E}_{s_{0:\\infty} a_{0:\\infty}} [\\sum_{t=0}^\\infty A^{\\pi, \\gamma} (s_t, a_t) \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$<br>뒤이어 나오는 section에서는 위의 수식에서 $A^{\\pi, \\gamma}$에 대해 biased (but not too biased) estimator를 얻는 방법에 대해서 나옵니다.</p>\n<p>다음으로 advantage function에서 새롭게 접하는 “$\\gamma$-just estimator”의 notation에 대해 알아봅시다.</p>\n<ul>\n<li>먼저 $\\gamma$-just estimator는 $g^\\gamma$ ${}^1$를 estimate하기 위해 위의 수식에서 $A^{\\pi, \\gamma}$를 $\\gamma$-just estimator로 사용할 때, bias하지 않은 estimator라고 합시다. 그리고 이 $\\gamma$-just advantage estimator를 <img src=\"https://www.dropbox.com/s/7g2glqx2kbfynaa/Screen%20Shot%202018-08-22%20at%209.03.46%20PM.png?dl=1\" width=\"100\"> 라고 하고, 전체의 trajectory에 대한 하나의 function이라고 합시다.</li>\n<li>${}^1$에서 이 논문의 저자가 하고 싶은 말이 있는 것 같습니다. 개인적으로 $\\gamma$-just estimator를 이해하는 데에 있어서 중요한 주석이라 정확히 해석하고자 합니다.<ul>\n<li>$A^\\pi$를 $A^{\\pi, \\gamma}$로 사용함으로써 이미 bias하다라고 말했지만, “이 논문에서 하고자 하는 것은 $g^\\gamma$에 대해 unbiased estimate를 얻고 싶은 것입니다.” 하지만 undiscounted MDP의 policy gradient에 대해서는 당연히 $\\gamma$를 사용하기 때문에 biased estimate를 얻습니다. 개인적으로 이것은 일단 무시하고 $\\gamma$를 사용할 때 어떻게 unbiased estimate를 얻을 지에 대해 좀 더 포커스를 맞추고 있는 것 같습니다.</li>\n<li>그러니까 저 $A^{\\pi, \\gamma}$를 $\\gamma$-just estimator로 바꿔줌으로써 unbiased estimate를 하고 싶다는 것이 뒤이어 나오는 정의와 명제의 핵심이라고 할 수 있습니다.</li>\n</ul>\n</li>\n</ul>\n<p>Definition 1.<br>먼저 가정을 합니다. (가정을 바탕으로 이루어지는 정의라는 것을 주목합시다.) 만약</p>\n<center><img src=\"https://www.dropbox.com/s/aia9f1l99wyxgah/Screen%20Shot%202018-08-22%20at%209.07.05%20PM.png?dl=1\" width=\"700\"> </center><br>두 수식이 같다면, estimator $\\hat{A}_t$는 $\\gamma$-just입니다.<br><br>그리고 만약 모든 t에 대해서 $\\hat{A}_t$이 $\\gamma$-just이라면, 다음과 같이 표현할 수 있습니다.<br><br><center><img src=\"https://www.dropbox.com/s/r4m6ktw8geeu7pe/Screen%20Shot%202018-08-22%20at%209.29.18%20PM.png?dl=1\" width=\"420\"> </center>\n\n<p>위의 수식이 바로 unbiased estimate입니다.</p>\n<p>$\\gamma$-just인 $\\hat{A}_t$에 대한 한가지 조건은 $\\hat{A}_t$이 두 가지 function $Q_t$ and $b_t$로 나뉠 수 있다는 것입니다.</p>\n<ul>\n<li>$Q_t$는 $\\gamma$-discounted Q-function의 unbiased estimator입니다.</li>\n<li>$b_t$는 action $a_t$전에 sampling된 states and actions의 arbitrary function이다. </li>\n</ul>\n<p>Proposition 1.<br>모든 $(s_t, a_t)$에 대해,<br>$$\\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty | s_t, a_t} [Q_t (s_{t:\\infty}, a_{t:\\infty})] = Q^{\\pi, \\gamma} (s_t, a_t)$$<br>로 인하여 $\\hat{A}_t$이 </p>\n<center> <img src=\"https://www.dropbox.com/s/59ydsav6djwyevl/Screen%20Shot%202018-08-22%20at%209.27.13%20PM.png?dl=1\" width=\"380\"> </center>\n\n<p>형태라고 가정합시다. (가정을 바탕으로 이루어지는 명제라는 점을 주목합시다.)</p>\n<p>그 때, $\\hat{A}_t$은 $\\gamma$-just입니다.</p>\n<p>이 명제에 대한 증명은 Appendix B에 있습니. 그리고 $\\hat{A}_t$에 대한 $\\gamma$-just advantage estimator들은 다음과 같습니다.</p>\n<ul>\n<li>$\\sum_{l=0}^\\infty \\gamma^l r_{t+1}$</li>\n<li>$A^{\\pi, \\gamma} (s_t, a_t)$</li>\n<li>$Q^{\\pi, \\gamma} (s_t, a_t)$</li>\n<li>$r_t + \\gamma V^{\\pi, \\gamma} (s_{t+1}) - V^{\\pi, \\gamma} (s_t)$</li>\n</ul>\n<p>증명을 보기 전에 먼저 Advantage function에 대해서 먼저 살펴봅시다. 아래의 내용은 Sutton이 쓴 논문인 Policy Gradient Methods for Reinforcement Learning with Function Approximation(2000)에서 나온 내용을 리뷰한 것입니다. </p>\n<center> <img src=\"https://www.dropbox.com/s/x737yq97ut6gp1a/Screen%20Shot%202018-07-15%20at%201.16.09%20PM.png?dl=1\" width=\"600\"> </center>\n\n<p>여기서는 “이 수식은 $\\sum_a \\frac{\\partial \\pi (s,a)}{\\partial \\theta} = 0$이기 때문에 가능해진다.”, “즉, 이 수식은 $\\pi(s,a)$의 gradient에만 dependent하기 때문에 advantage 역할을 하는 함수들을 넣어도 아무런 상관이 없다”라는 두 문장을 기억해둡시다.</p>\n<p>이제 증명을 보면 됩니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/e7sj2fwcm1f7hof/figure2.jpg?dl=1\" width=\"600\"> </center><br><center> <img src=\"https://www.dropbox.com/s/kyk4jes1202az4m/figure3.jpg?dl=1\" width=\"600\"> </center>\n\n<p>위의 증명 수식에서 Q와 b의 각각 세번째 수식과 마지막 수식을 자세히 봅시다.</p>\n<ol>\n<li><p>$Q$에 대한 증명</p>\n<ul>\n<li><p>빨간색 부분</p>\n<ul>\n<li>$\\mathbb{E}_{s_{t+1:\\infty}, a_{t+1:\\infty}} [Q_t (s_{0:\\infty}, a_{0:\\infty})]$를 쉽게 말하자면 “모든(과거, 현재, 미래) state와 action에 대해서 expectation을 $t+1$부터 $\\infty$까지 하겠다.”라는 말입니다. 이렇게 함으로써 원래는 $Q^\\pi (s_t, a_t)$가 나와야 맞는 건데, 앞에서 살펴봤다시피 4번째 수식 밑줄 친 자리에는 advantage 역할을 하는 함수들을 넣어도 결과값에 아무런 영향을 끼치지 않기 때문에 의도적으로 $A^\\pi (s_t, a_t)$로 바꾼 것입니다. 또한 이 증명을 바탕으로 $\\hat{A}$이 $\\gamma$-just라는 것을 표현하고 싶기 때문이라고 봐도 됩니다.</li>\n</ul>\n</li>\n<li><p>파란색 부분</p>\n<ul>\n<li>또한 $a_{0:t}$에서 $a_{0:t-1}$로 변한 것은 $A^\\pi (s_t, a_t)$에 baseline이 들어가기 때문에 바꿔준 것입니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>$b$에 대한 증명</p>\n<ul>\n<li>빨간색 부분<ul>\n<li>$\\mathbb{E}_{s_{t+1:\\infty}, a_{t+1:\\infty}} [\\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$가 0으로 바뀌는 것은 위에서 설명했듯이 $\\nabla_\\theta$자체가 $\\log \\pi_\\theta$만 gradient하기 때문에 이것을 expectation을 취하면 0이 됩니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<p><br><br></p>\n<h1 id=\"4-Advantage-Function-Estimation\"><a href=\"#4-Advantage-Function-Estimation\" class=\"headerlink\" title=\"4. Advantage Function Estimation\"></a>4. Advantage Function Estimation</h1><p>이번 section에는 discounted advantage function $A^{\\pi, \\gamma} (s_t, a_t)$의 accurate estimate $\\hat{A}_t$에 대해서 살펴봅시다. 이에 따른 수식은 다음과 같습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/sn16g5iyzzflju0/Screen%20Shot%202018-08-22%20at%209.22.49%20PM.png?dl=1\" width=\"300\"> </center>\n\n<p>여기서 n은 a batch of episodes에 대하여 index한 것입니다.</p>\n<p>$V$를 approximate value function이라고 합시다. 그리고 $\\delta_t^V = r_t + \\gamma V(s_{t+1}) - V(s_t)$이라고 합시다.</p>\n<p>만약 (이전과 마찬가지로 가정부터 합니다.) correct value function $V = V^{\\pi, \\gamma}$가 있다고 한다면, 이것은 $\\gamma$-just advantage estimator입니다. 실제로, $A^{\\pi, \\gamma}$의 unbiased estimator는 다음과 같습니다.<br>$$\\mathbb{E}_{s_{t+1}} [\\delta_t^{V^{\\pi, \\gamma}}] = \\mathbb{E}_{s_{t+1}} [r_t + \\gamma V^{\\pi, \\gamma} (s_{t+1}) - V^{\\pi, \\gamma} (s_t)]$$<br>$$= \\mathbb{E}_{s_{t+1}} [Q^{\\pi, \\gamma} (s_t, a_t) - V^{\\pi, \\gamma} (s_t)] = A^{\\pi, \\gamma} (s_t, a_t)$$<br>그러나, “이 estimator는 유일하게 $V = V^{\\pi, \\gamma}$에 대한 $\\gamma$-just입니다.” 다른 경우라면 이것은 biased policy gradient estimate일 것입니다. (우리가 하고 싶은 것은 $V$에 대해서만 unbiased estimator가 아니라 advantage function에 대해서 일반화된 unbiased estimator를 얻고 싶은 것입니다. 그래서 아래에서도 나오겠지만, $\\gamma$와 함께 $\\lambda$를 이용한 estimator가 나옵니다.ㅏ)</p>\n<p>그래서 $\\delta$에 대해 $k$의 sum으로 생각해봅시다. 이것을 $\\hat{A}_t^{(k)}$라고 하자. 그러면 아래와 같이 표현할 수 있습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/ra7hxksveg2hz45/figure4.jpg?dl=1\" width=\"600\"> </center>\n\n<p>$\\hat{A}_t^{(k)}$은 returns의 $k$-step estimate와 연관지을 수 있고, $\\delta_t^V = \\hat{A}_t^{(1)}$의 case와 유사하게도 $\\hat{A}_t^{(k)}$를 advantage function의 estimator로 생각할 수 있습니다.</p>\n<p>여기서 $k \\rightarrow \\infty$로 생각해보면 bias가 일반적으로 점점 더 작아집니다. 왜냐하면 $\\gamma^k V(s_{t+k})$가 점점 많이 discounted되서 $-V(s_t)$가 bias에 영향을 미치지 못하기 때문입니다. $k \\rightarrow \\infty$를 취하면 다음과 같은 수식이 나옵니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/13fn9wcup8pfh9u/Screen%20Shot%202018-08-22%20at%209.19.52%20PM.png?dl=1\" width=\"320\"> </center>\n\n<p>우변의 수식과 같이 empirical returns에서 value function baseline을 뺀 것으로 나타낼 수 있습니다.</p>\n<p>Generalized Advantage Estimator GAE($\\gamma, \\lambda$)는 $k$-step estimators의 exponentially-weighted average로 나타낼 수 있습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/yg1ybmfkep3towu/figure5.jpg?dl=1\" width=\"600\"> </center>\n\n<p>(TD($\\lambda$)를 떠올려주세요..!)</p>\n<p>위의 마지막 수식에서도 알 수 있듯이, advantage estimator는 Bellman residual terms의 discounted sum과 관련있는 간단한 수식입니다. 다음 section에서 modified reward function을 가진 MDP에 returns로서의 관점에서 위의 마지막 수식을 더 자세히 살펴봅시다. 위의 수식은 TD($\\lambda$)와 많이 유사합니다. 그러나 TD($\\lambda$)는 value function를 estimator하고, 여기서는 advantage function을 estimator합니다.</p>\n<p>위의 수식에서 $\\lambda = 0$ and $\\lambda = 1$에 대해서는 특별한 case가 존재한다. 수식으로 표현하면 다음과 같습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/ufglxzanhcnbi36/figure6.jpg?dl=1\" width=\"600\"> </center>\n\n<ul>\n<li>$GAE(\\gamma, 1)$은 $V$의 정확도와 관계없이 $\\gamma$-just입니다. 그러나 returns의 sum때문에 high variance합니다.</li>\n<li>$GAE(\\gamma, 0)$은 $V = V^{\\pi, \\gamma}$에 대해 $\\gamma$-just입니다. 그리고 bias하지만 일반적으로 훨씬 lower variance를 가집니다. </li>\n<li>$0 &lt; \\lambda &lt; 1$에 대해 GAE는 parameter $\\lambda$를 control하여 bias와 variance사이에 compromise를 만듭니다.</li>\n</ul>\n<p>두 가지 별개의 parameter $\\gamma$ and $\\lambda$를 가진 advantage estimator는 bias-variance tradeoff에 도움을 줍니다. 그러나 이 두 가지 parameter는 각각 다른 목적을 가지고 작동합니다.</p>\n<ul>\n<li>$\\gamma$는 가장 중요하게 value function $V^{\\pi, \\gamma}$ 의 scale을 결정합니다. 또한 $\\gamma$는 $\\lambda$에 의존하지 않습니다. $\\gamma &lt; 1$로 정하는 것은 policy gradient estimate에서 bias합니다.</li>\n<li>반면에, $\\lambda &lt; 1$는 유일하게 value function이 부정확할 때 bias합니다. 그리고 경험상, $\\lambda$의 best value는 $\\gamma$의 best value보다 훨씬 더 낮습니다. 왜냐하면 $\\lambda$가 정확한 value function에 대해 $\\gamma$보다 훨씬 덜 bias하기 때문입니다.</li>\n</ul>\n<p>GAE를 사용할 때, $g^\\gamma$의 biased estimator를 구성할 수 있습니다. 수식은 다음과 같습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/ok57dsk52o7pl9r/Screen%20Shot%202018-08-22%20at%209.17.54%20PM.png?dl=1\" width=\"670\"> </center>\n\n<p>여기서 $\\lambda = 1$일 때 동일해집니다.</p>\n<p><br><br></p>\n<h1 id=\"5-Interpretation-as-Reward-Shaping\"><a href=\"#5-Interpretation-as-Reward-Shaping\" class=\"headerlink\" title=\"5. Interpretation as Reward Shaping\"></a>5. Interpretation as Reward Shaping</h1><p>이번 section에서는 앞서 다뤘던 수식 $\\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^V$를 modified reward function의 MDP의 관점으로 생각해봅시다. 조금 더 구체적으로 말하자면, MDP에서 reward shaping transformation을 실행한 후에 적용된 extra discounted factor로서 $\\lambda$를 어떻게 볼 것인지에 대해서 다룹니다.</p>\n<p>(개인적인 comment) 한 가지 먼저 언급하자면, 본래의 목적은 reward shaping이 아니라 variance reduction입니다. 이번 section은 그저 이전에 이러한 개념이 있었고, gae를 다른 관점에서 생각해보자라는 뜻에서 나온 section인 것 같습니다. ‘아~ 이러한 개념이 있구나~!’ 정도로만 알면 될 것 같습니다. ‘gae가 reward shaping의 효과까지 있어서 이러한 section을 넣은 것일까?’ 라는 생각도 해봤지만 아직 잘 모르겠습니다. 실험부분에도 딱히 하지 않은 걸로 봐서는.. 아닌 것 같기도 하고.. 아직까지는 그저 ‘reward shaping의 관점에서 봤을 때에도 gae로 만들어줄 수 있다.’ 정도만 생각하려고 합니다.</p>\n<p>Reward Shaping이란 개념 자체는 일반적인 알고리즘 분야(최단경로 문제 등)에도 있습니다. 하지만 이 개념이 머신러닝에 적용된 건 Andrew Y. Ng이 쓴 Policy Invariance under Reward Transformations Theory and Application to Reward Shaping(1999) 논문입니다. 논문에서 Reward Shaping을 설명하는 부분은 간략하게 다음과 같습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/0w9cuxjkhz2mbcy/figure7.jpg?dl=1\" width=\"400\"> </center><br><center> <img src=\"https://www.dropbox.com/s/xcp9eqvcbv3268g/figure8.jpg?dl=1\" width=\"400\"> </center>\n\n<p>위의 글만 보고 이해가 잘 안됩니다. 그래서 다른 자료들을 찾던 중에 다음의 그림을 찾게 되었습니다. (아래의 그림은 Youtube에 있는 <a href=\"https://www.youtube.com/watch?v=xManAGjbx2k&amp;t=95s\" target=\"_blank\" rel=\"noopener\">Udacity 영상</a>에서 있는 그림입니다.)</p>\n<center> <img src=\"https://www.dropbox.com/s/87kacngez9cl2e6/figure9.jpg?dl=1\" width=\"400\"> </center>\n\n<p>이 그림을 통해서 Reward Shaping을 이해해봅시다.</p>\n<p>Reward Shaping(보상 형성)을 통해서 뭘 하고 싶은지 목적부터 봅시다. reward space가 sparse 한 경우에 reward가 너무 드문드문 나옵니다. 따라서 이것을 꾸준히 reward를 받을 수 있도록 바꿉니다. potential-based shaping function인 $\\Phi$를 만들어서 더하고 빼줍니다. 저 $\\Phi$ 자리에는 대표적으로 state value function이 많이 들어간다고 합니다. (아직 목적이 이해가 안될 수 있습니다. Reward Shaping에 대해서 다 보고 나서 다시 한 번 읽어봅시다.)</p>\n<p>위의 그림은 돌고래가 점프하여 불구멍을 통과해야하는 환경입니다. 하지만 저 불구멍을 통과하기 위해서는 (1) 점프도 해야하고, (2) 불도 피해야하고, (3) 알맞게 착지까지 완료해야합니다. 이렇게 해야 reward를 +1 얻습니다. </p>\n<p>생각해봅시다. 어느 세월에 점프도 해야하고.. 불도 피해야하고.. 알맞게 착지까지해서 reward를 +1 얻겠습니까? 따라서 그 전에도 잘하고 있는 지, 못하고 있는 지를 판단하기 위해, 다시 말해 reward를 꾸준히 받도록 다음과 같이 transformed reward function $\\tilde{r}$을 정의합니다.</p>\n<p>$$\\tilde{r} (s, a, s’) = r(s, a, s’) + \\gamma \\Phi (s’) - \\Phi (s)$$</p>\n<ul>\n<li>여기서 $\\Phi : S \\rightarrow \\mathbb{R}$를 state space에서의 arbitrary scalar-valued function을 나타냅니다. 그리고 $\\Phi$자리에는 대표적으로 state value function이 들어간다고 생각합시다.</li>\n<li>형태가 TD residual term의 결과와 비슷하지만 의미와 의도가 다릅니다. reward shaping은 sparse reward 때문이고, 이전 section에서 봤던 gae는 variance reduction때문에 나온 것입니다.</li>\n</ul>\n<center> <img src=\"https://www.dropbox.com/s/tkfpqniaazwfbld/figure10.jpg?dl=1\" width=\"600\"> </center>\n\n<p>이 transformation은 discounted advantage function $A^{\\pi, \\gamma}$으로도 둘 수 있습니다. state $s_t$를 시작하여 하나의 trajectory의 rewards의 discounted sum을 표현하면 다음과 같습니다.<br>$$\\sum_{l=0}^\\infty \\gamma^l \\tilde{r} (s_{t+l}, a_t, s_{t+l+1}) = \\sum_{l=0}^\\infty \\gamma^l r(s_{t+l}, a_{t+l}, s_{t+l+1}) - \\Phi(s_t)$$</p>\n<p>$\\tilde{Q}^{\\pi, \\gamma}, \\tilde{V}^{\\pi, \\gamma}, \\tilde{A}^{\\pi, \\gamma}$를  transformed MDP의 value function과 advantage function이라고 하면, 다음과 같은 수식이 나옵니다.<br>$$\\tilde{Q}^{\\pi, \\gamma} (s, a) = Q^{\\pi, \\gamma} (s, a) - \\Phi (s)$$<br>$$\\tilde{V}^{\\pi, \\gamma} (s, a) = V^{\\pi, \\gamma} (s, a) - \\Phi (s)$$<br>$$\\tilde{A}^{\\pi, \\gamma} (s, a) = (Q^{\\pi, \\gamma} (s, a) - \\Phi (s)) - (V^\\pi (s) - \\Phi (s)) = A^{\\pi, \\gamma} (s, a)$$</p>\n<p>이제 reward shaping의 idea를 가지고 어떻게 policy gradient estimate를 얻을 수 있는 지에 대해서 알아봅시다.</p>\n<ul>\n<li>$0 \\le \\lambda \\le 1$의 범위에 있는 steeper discount $\\gamma \\lambda$를 사용합니다.</li>\n<li>shaped reward $\\tilde{r}$는 Bellman residual term $\\delta^V$와 동일합니다.</li>\n<li>그리고 $\\Phi = V$와 같다고 보면 다음과 같은 수식이 나옵니다.</li>\n</ul>\n<p>$$\\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\tilde{r} (s_{t+l}, a_t, s_{t+l+1}) = \\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^V = \\hat{A}_t^{GAE(\\gamma, \\lambda)}$$<br>이렇게 shaped rewards의 $\\gamma \\lambda$-discounted sum을 고려함으로써 GAE를 얻을 수 있습니다. 더 정확하게 $\\lambda = 1$은 $g^\\gamma$의 unbiased estimate이고, 반면에 $\\lambda &lt; 1$은 biased estimate입니다.</p>\n<p>좀 더 나아가서, shaping transformation과 parameters $\\gamma$ and $\\lambda$의 결과를 보기 위해, response function $\\chi$를 이용하면 다음과 같은 수식이 나옵니다.<br>$$\\chi (l; s_t, a_t) = \\mathbb{E} [r_{t+l} | s_t, a_t] - \\mathbb{E} [r_{t+l} | s_t]$$<br>추가적으로 $A^{\\pi, \\gamma} (s, a) = \\sum_{l=0}^\\infty \\gamma^l \\chi (l; s, a)$입니다.<br>그래서 discounted policy gradient estimator는 다음과 같이 쓸 수 있다.<br>$$\\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) A^{\\pi, \\gamma} (s_t, a_t) = \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) \\sum_{l=0}^\\infty \\gamma^l \\chi (l; s, a)$$</p>\n<p><br><br></p>\n<h1 id=\"6-Value-Fuction-Estimation\"><a href=\"#6-Value-Fuction-Estimation\" class=\"headerlink\" title=\"6. Value Fuction Estimation\"></a>6. Value Fuction Estimation</h1><p>이번 section에서는 Trust Region Optimization Scheme에 따라 Value function을 Estimation합니다.</p>\n<p><br></p>\n<h2 id=\"6-1-Simplest-approach\"><a href=\"#6-1-Simplest-approach\" class=\"headerlink\" title=\"6.1 Simplest approach\"></a>6.1 Simplest approach</h2><p>$$minimize_{\\phi} \\sum_{n=1}^{N} \\vert\\vert V_{\\phi}(s_n) - \\hat{V_n} \\vert\\vert^{2}$$</p>\n<ul>\n<li>위는 가장 간단하게 non-linear approximation으로 푸는 방법입니다.</li>\n<li>$\\hat{V_t} = \\sum_{l=0}^{\\infty}\\gamma^l r_{t+l}$은 reward 에 대한 discounted sum을 의미합니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"6-2-Trust-region-method-to-optimize-the-value-function\"><a href=\"#6-2-Trust-region-method-to-optimize-the-value-function\" class=\"headerlink\" title=\"6.2 Trust region method to optimize the value function\"></a>6.2 Trust region method to optimize the value function</h2><ul>\n<li>Value function을 최적화 하기 위해 trust region method를 사용합니다.</li>\n<li>Trust region은 최근 데이터에 대해 overfitting되는 것을 막아줍니다.</li>\n</ul>\n<p>Trust region문제를 풀기 위해서는 다음 스텝을 따릅니다.</p>\n<ul>\n<li>$\\sigma^2 = \\frac{1}{N} \\sum_{n=1}^{N} \\vert\\vert V_{\\phi old}(s_n) - \\hat{V_n} \\vert\\vert^{2}$을 계산합니다.</li>\n<li>그 후에 다음과 같은 constrained opimization문제를 풉니다.</li>\n</ul>\n<center> <img src=\"https://www.dropbox.com/s/d74cg3votxi2dbg/Screen%20Shot%202018-08-22%20at%209.13.28%20PM.png?dl=1\" width=\"300\"> </center>\n\n<center> <img src=\"https://www.dropbox.com/s/jrkjax71t3j3vk4/Screen%20Shot%202018-08-22%20at%209.13.35%20PM.png?dl=1\" width=\"370\"> </center>\n\n<ul>\n<li>위의 수식은 사실 old Value function과 새로운 Value function KL distance가 $\\epsilon$ 다 작아야한다는 수식과 같습니다. Value function이 평균은 $V_{\\phi}(s)$이고 분산이 $\\sigma^2$인 conditional Gaussian distribution으로 parameterize되었을 뿐입니다.</li>\n</ul>\n<p><img width=\"400px\" src=\"https://www.dropbox.com/s/uw0v05feu8chqmc/Screenshot%202018-07-08%2010.04.38.png?dl=1\"> </p>\n<p>이 trust region문제의 답을 conjudate gradient algorithm을 이용하여 근사값을 구할 수 있습니다. 특히, 다음과 같은  quadratic program을 풀게됩니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/pawjkm8b9ycwsl5/Screen%20Shot%202018-08-22%20at%209.10.36%20PM.png?dl=1\" width=\"250\"> </center>\n\n<center> <img src=\"https://www.dropbox.com/s/va7wms8uqxnw5rq/Screen%20Shot%202018-08-22%20at%209.10.40%20PM.png?dl=1\" width=\"370\"> </center>\n\n<ul>\n<li>여기서 $g$는 objective 의 gradient입니다.</li>\n<li>$j_n = \\nabla_{\\phi} V_{\\phi}(s_n)$일때, $H = \\frac{1}{N} \\sum_{n} j_n j^T_n$이며, $H$는 objective의 hessian에 대해서 gaussian newton method로 근사한 값입니다. 따라서, value function을 conditional probability로 해석한다면 Fisher information matrix가 됩니다.</li>\n<li>구현할때의 방법은 TRPO에서 사용한 방법과 모두 같습니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"7-Experiments\"><a href=\"#7-Experiments\" class=\"headerlink\" title=\"7.Experiments\"></a>7.Experiments</h1><p>실험은 다음 두 가지 물음에 대해서 디자인 되었습니다.</p>\n<ul>\n<li>GAE에 따라서 episodic total reward를 최적화 할때, $\\lambda$와 $\\gamma$가 변함에 따라서 어떤 경험적인 효과를 볼 수 있는지?</li>\n<li>GAE와 trust region alogorithm을 policy와 value function 모두에 함께 사용했을 때 어려운 문제에 적용되는 큰 뉴럴넷을 최적화 할 수 있을지?</li>\n</ul>\n<p><br></p>\n<h2 id=\"7-1-Policy-Optimization-Algorithm\"><a href=\"#7-1-Policy-Optimization-Algorithm\" class=\"headerlink\" title=\"7.1 Policy Optimization Algorithm\"></a>7.1 Policy Optimization Algorithm</h2><p>Policy update는 TRPO를 사용합니다. TRPO에 대한 설명은 여기서는 생략하겠습니다. TRPO 포스트를 보고 돌아와주세요!</p>\n<ul>\n<li><p>이전 TRPO에서 이미 TRPO와 다른 많은 알고리즘들을 비교하였기 때문에, 여기서는 똑같은 짓을 반복하지 않고 $\\lambda$, $\\gamma$가 변함에 따라 어떤 영향이 있는 지에 대한 실험에 집중하겠다고 합니다. (귀찮았던거죠..)</p>\n</li>\n<li><p>TRPO를 적용한 GAE의 최종 알고리즘은 다음과 같습니다.</p>\n</li>\n</ul>\n<center> <img width=\"600px\" src=\"https://www.dropbox.com/s/b1klz11f2frrvg4/Screenshot%202018-07-08%2010.33.54.png?dl=1\"> </center><br><center> <img width=\"300px\" src=\"https://www.dropbox.com/s/35fxte05pqtiel7/Screenshot%202018-07-08%2010.35.22.png?dl=1\"> </center><br><center> <img width=\"300px\" src=\"https://www.dropbox.com/s/u35pjow3w50bvz1/Screenshot%202018-07-08%2010.36.03.png?dl=1\"> </center>\n\n<ul>\n<li>여기서 주의할 점은 Policy update($\\theta_i \\rightarrow \\theta_{i+1}$)에서 $V_{\\phi_i}$를 사용했다는 점입니다.</li>\n<li>만약 Value function을 먼저 update하게 된다면 추가적인 bias가 발생합니다.</li>\n<li>극단적으로 생각해보아서, 우리가 Value function을 완벽하게 overfit을 해낸다면 Bellman residual($r_t + \\gamma V(s_{t+1}) - V(S_t)$)은 0이 됩니다. 그럼 Policy gradient의 estimation도 거의 0이 될 것입니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"7-2-Expermint-details\"><a href=\"#7-2-Expermint-details\" class=\"headerlink\" title=\"7.2 Expermint details\"></a>7.2 Expermint details</h2><h3 id=\"7-2-1-Environment\"><a href=\"#7-2-1-Environment\" class=\"headerlink\" title=\"7.2.1 Environment\"></a>7.2.1 Environment</h3><p>실험에서 사용된 환경은 다음 네 가지 입니다.</p>\n<ol>\n<li>classic cart-pole (x 3D)</li>\n<li>bipedal locomotion</li>\n<li>quadrupedal locomotion</li>\n<li>dynamically standing up for the biped</li>\n</ol>\n<h3 id=\"7-2-2-Architecture\"><a href=\"#7-2-2-Architecture\" class=\"headerlink\" title=\"7.2.2 Architecture\"></a>7.2.2 Architecture</h3><ul>\n<li>3D robot task에 대해서는 같은 모델을 사용하였습니다.<ul>\n<li>layers  = [100, 50, 25] 각각 tanh 사용. (Policy와 Value 네트워크 모두)</li>\n<li>Final output layer은 linear</li>\n</ul>\n</li>\n<li>Cartpole에 대해서는 1개의 layer 안에 20개의 hidden unit만 있는 linear policy를 사용했다고 합니다.</li>\n</ul>\n<h3 id=\"7-2-3-Task\"><a href=\"#7-2-3-Task\" class=\"headerlink\" title=\"7.2.3 Task\"></a>7.2.3 Task</h3><ul>\n<li>Cartpole<ul>\n<li>한 배치당 20 개의 trajectory를 모았고, maximum length는 1000입니다.</li>\n</ul>\n</li>\n<li>3D biped locomotion<ul>\n<li>33 dim state , 10 dim action</li>\n<li>50000 time step per batch</li>\n</ul>\n</li>\n<li>3D quadruped locomotion<ul>\n<li>29 dim state, 8 dim action</li>\n<li>200000 time step per batch</li>\n</ul>\n</li>\n<li>3D biped locomotion Standing<ul>\n<li>33 dim state , 10 dim action</li>\n<li>200000 time step per batch</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"7-2-3-results\"><a href=\"#7-2-3-results\" class=\"headerlink\" title=\"7.2.3 results\"></a>7.2.3 results</h3><p>cost의 관점에서 결과를 나타내었다고 합니다. Cost는 negative reward와 이것이 최소화 되었는가로 정의되었습니다.</p>\n<h4 id=\"7-2-3-1-Cartpole\"><a href=\"#7-2-3-1-Cartpole\" class=\"headerlink\" title=\"7.2.3.1 Cartpole\"></a>7.2.3.1 Cartpole</h4><center> <img width=\"500px\" src=\"https://www.dropbox.com/s/x9pbms1wvg38lda/Screenshot%202018-07-08%2011.08.22.png?dl=1\"> </center>\n\n<ul>\n<li>왼쪽 그림은 $\\gamma$를 0.99로 고정시켜놓은 상태에서 $\\lambda$를 변화시킴에 따라서 cost를 측정한 것입니다. </li>\n<li>오른쪽은 $\\gamma$와 $\\lambda$를 둘 다 변화 시키면서 성능을 그림으로 나타낸 표입니다. 흰색에 가까울 수록 좋은 성능입니다. </li>\n</ul>\n<h4 id=\"7-2-3-2-3D-BIPEDAL-LOCOMOTINO\"><a href=\"#7-2-3-2-3D-BIPEDAL-LOCOMOTINO\" class=\"headerlink\" title=\"7.2.3.2 3D BIPEDAL LOCOMOTINO\"></a>7.2.3.2 3D BIPEDAL LOCOMOTINO</h4><center> <img width=\"500px\" src=\"https://www.dropbox.com/s/i9wj4p6ijojsy82/Screenshot%202018-07-08%2011.25.08.png?dl=1\"> </center>\n\n\n<ul>\n<li>다른 random seed로 부터 9번 씩 시도한 결과를 mean을 취해서 사용합니다.</li>\n<li>Best performance는   $\\gamma \\in [0.99, 0.995]$ 그리고  $\\lambda \\in [0.96, 0.99]$일때. </li>\n<li>1000 iteration 후에 빠르고 부드럽고 안정적인 걸음거이가 나옵니다.</li>\n<li>실제로 걸린 시간은 0.01(타입스텝당 시간) <em> 50000(배치당 타임스텝) </em> 1000(배치) <em> 3600(초-&gt;시간) </em> 24 = 5.8일 정도가 걸렸습니다.</li>\n</ul>\n<h4 id=\"7-2-3-3-다른-ROBOT-TASKS\"><a href=\"#7-2-3-3-다른-ROBOT-TASKS\" class=\"headerlink\" title=\"7.2.3.3 다른 ROBOT TASKS\"></a>7.2.3.3 다른 ROBOT TASKS</h4><center> <img width=\"500px\" src=\"https://www.dropbox.com/s/fuuat65we52quht/Screenshot%202018-07-08%2011.33.11.png?dl=1\"> </center>\n\n<ul>\n<li>다른 로봇 TASK에 대해서는 아주 제한적인 실험만 진행합니다.(시간이 부족했던듯 하네요..)</li>\n<li>Quadruped에 대해서는 $\\gamma = 0.995$로 fix, $\\lambda \\in {0, 0.96}$</li>\n<li>Standingup에 대해서는 $\\gamma = 0.99$로 fix, $\\lambda \\in {0, 0.96}$</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"8-Discussion\"><a href=\"#8-Discussion\" class=\"headerlink\" title=\"8. Discussion\"></a>8. Discussion</h1><p><br></p>\n<h2 id=\"8-1-Main-discussion\"><a href=\"#8-1-Main-discussion\" class=\"headerlink\" title=\"8.1 Main discussion\"></a>8.1 Main discussion</h2><p>지금까지 복잡하고 어려운 control problem에서 Reinforcement Learning(RL)은 high sample complexity 때문에 제한이 되어왔습니다. 따라서 이 논문에서 그 제한을 풀고자 advantage function의 good estimate를 얻는 “variance reduction”에 대해 연구하였습니다.</p>\n<p>“Generalized Advantage Estimator(GAE)”라는 것을 제안했고, 이것은 bias-variance tradeoff를 조절하는 두 개의 parameter $\\gamma,\\lambda$를 가집니다.<br>또한 어떻게 Trust Region Policy Optimization과 value function을 optimize하는 Trust Region Algorithm의 idea를 합치는 지를 보였습니다.</p>\n<p>이렇게 함으로써 보다 더 복잡하고 어려운 control task들을 해결할 수 있었습니다.</p>\n<p>GAE의 실험적인 입증으로는 robotic locomotion을 simulation하는 domain입니다. 실험에서도 보여준 것처럼 [0.9, 0.99]의 범위에서 $\\lambda$의 적절한 중간의 값을 통해 best performance를 얻습니다. 좀 더 나아가 연구되어야할 점은 adaptive or automatic하도록 estimator parameter $\\gamma,\\lambda$를 조절하는 방법입니다.</p>\n<p><br></p>\n<h2 id=\"8-2-Future-work\"><a href=\"#8-2-Future-work\" class=\"headerlink\" title=\"8.2 Future work\"></a>8.2 Future work</h2><p>Value function estimation error와 Policy gradient estimation error사이의 관계를 알아낸다면, 우리는 Value function fitting에 더 잘 맞는 error metric(policy gradient estimation 의 정확성과 더 잘 맞는 value function)을 사용할 수 있습니다. 여기서 Policy와 Value function의 파라미터를 공유하는 모델을 만드는 것은 아주 흥미롭고 이점이 많습니다. 하지만 수렴을 보장하도록 적절한 numerical optimization을 제시해야 할 것입니다.</p>\n<p>추가적으로 DDPG는 별로라고 합니다. 그리고 TD(0)는 bias가 너무 크고, poor performance로 이끈다고 합니다. 특히나 이 논문에서는 low-dimention의 쉬운 문제들만 해결했습니다.</p>\n<center> <img width=\"500px\" src=\"https://www.dropbox.com/s/nhc7t9psul5lr3x/Screenshot%202018-07-08%2011.45.15.png?dl=1\"> </center>\n\n<p><br></p>\n<h2 id=\"8-3-FAQ\"><a href=\"#8-3-FAQ\" class=\"headerlink\" title=\"8.3 FAQ\"></a>8.3 FAQ</h2><ul>\n<li>Compatible features와는 무슨 관계?<ul>\n<li>Compatible features는 value function을 이용하는 policy gradient 알고리즘들과 함께 자주 언급됩니다.</li>\n<li>Actor Critic의 저자는 policy의 제한된 representation power때문에, policy gradient는 단지 advantage function space의 subspace에만 의존하게 됩니다.</li>\n<li>이 subspace는 compatible features에 의해 span됩니다.</li>\n<li>이 이론은 현재 문제 구조를 어떻게 이용해야 advantage function에 대해 더 나은 estimation을 할 수 있는 지에 대한 지침을 주지 않습니다. GAE 논문의 idea와 orthogonal합니다.</li>\n</ul>\n</li>\n<li>왜 Q function을 사용하지 않는가?<ul>\n<li>먼저 state-value function이 더 낮은 차원의 input을 가진다. 그래서 Q function보다 더 학습하기가 쉽습니다.</li>\n<li>두 번째로 이 논문에서 제안하는 방법으로는 high bias estimator에서 low bias estimator로 $\\lambda$를 통해서 부드럽게 interpolate를 할 수 있습니다.</li>\n<li>반면에 Q를 사용하면 단지 high-bias estimator 밖에 사용할 수 없습니다.</li>\n<li>특히나 return에 대한 one-step estimation은 엄두를 못낼 정도로 bias가 큽니다.</li>\n</ul>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"처음으로\"><a href=\"#처음으로\" class=\"headerlink\" title=\"처음으로\"></a>처음으로</h1><h2 id=\"PG-Travel-Guide\"><a href=\"#PG-Travel-Guide\" class=\"headerlink\" title=\"PG Travel Guide\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/\">PG Travel Guide</a></h2><p><br></p>\n<h1 id=\"이전으로\"><a href=\"#이전으로\" class=\"headerlink\" title=\"이전으로\"></a>이전으로</h1><h2 id=\"TRPO-여행하기\"><a href=\"#TRPO-여행하기\" class=\"headerlink\" title=\"TRPO 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/24/5_trpo/\">TRPO 여행하기</a></h2><h2 id=\"TRPO-Code\"><a href=\"#TRPO-Code\" class=\"headerlink\" title=\"TRPO Code\"></a><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/trpo_gae.py\" target=\"_blank\" rel=\"noopener\">TRPO Code</a></h2><p><br></p>\n<h1 id=\"다음으로\"><a href=\"#다음으로\" class=\"headerlink\" title=\"다음으로\"></a>다음으로</h1><h2 id=\"PPO-여행하기\"><a href=\"#PPO-여행하기\" class=\"headerlink\" title=\"PPO 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/22/7_ppo/\">PPO 여행하기</a></h2>"},{"title":"Trust Region Policy Optimization","date":"2018-06-24T07:53:12.000Z","author":"공민서, 김동민","subtitle":"피지여행 5번째 논문","_content":"\n<center> <img src=\"https://www.dropbox.com/s/o7cjcn0e17mpizr/Screen%20Shot%202018-07-18%20at%201.14.22%20AM.png?dl=1\" width=\"700\"> </center>\n\n논문 저자 : John Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, Pieter Abbeel\n논문 링크 : https://arxiv.org/pdf/1502.05477.pdf\nProceeding : International Conference on Machine Learning (ICML) 2015\n정리 : 공민서, 김동민\n\n---\n\n# 1. 들어가며...\n\nTrust region policy optimization (TRPO)는 상당히 우수한 성능을 보여주는 policy gradient 기법으로 알려져 있습니다. 높은 차원의 action space를 가진 robot locomotion부터 action은 적지만 화면을 그대로 처리하여 플레이하기 때문에 control parameter가 매우 많은 Atari game까지 각 application에 세부적으로 hyperparameter들을 특화시키지 않아도 두루두루 좋은 성능을 나타내기 때문에 일반화성능이 매우 좋은 기법입니다. 이 TRPO에 대해서 알아보겠습니다.\n\n※TRPO를 매우 재밌게 설명한 Crazymuse AI의 [Youtube video](https://www.youtube.com/watch?v=CKaN5PgkSBc&t=90s)에서 일부 그림을 차용했습니다. 이 비디오를 시청하시는 것을 강력하게 추천합니다!\n\n<br>\n## 1.1 TRPO 흐름 잡기\n\nTRPO 논문은 많은 수식이 등장하여 이 수식들을 따라가다보면 큰 그림을 놓칠 수도 있습니다. 세부적인 내용을 살펴보기 전에 기존 연구에서 출발해서 TRPO로 어떻게 발전해나가는지 간략하게 살펴보겠습니다.\n\n### 1.1.1 Original Problem\n\n$$\\max\\_\\pi \\eta(\\pi)$$\n\n모든 강화학습이 그렇듯이 expected discounted reward를 최대화하는 policy를 찾는 문제로부터 출발합니다.\n\n### 1.1.2 Conservative policy iteration\n\n$$\\max L\\_\\pi(\\tilde\\pi) = \\eta(\\pi) + \\sum\\_s \\rho_\\pi(s)\\sum\\_a\\tilde\\pi(a\\vert s)A\\_\\pi(s,a)$$\n\n$\\eta(\\pi)$를 바로 최대화하는 것은 많은 경우 어렵습니다. $\\eta(\\pi)$의 성능향상을 보장하면서 policy를 update하는 conservative policy iteration 기법이 [Kakade와 Langford](http://www.cs.cmu.edu/~./jcl/papers/aoarl/Final.pdf)에 의하여 제안되었습니다. 이 기법을 이용하면 policy update가 성능을 향상시키는지는 못하더라도 최소한 성능을 악화시키지는 않는다는 것이 이론적으로 보장됩니다.\n\n### 1.1.3 Theorem 1 of TRPO\n\n$$\\max L\\_{\\pi\\_\\mathrm{old} }\\left(\\pi\\_\\mathrm{new}\\right) - \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2,\\quad\\left(\\alpha=D\\_\\mathrm{TV}^\\max\\left(\\pi\\_\\mathrm{old},\\pi\\_\\mathrm{new}\\right)\\right)$$\n\n기존의 conservative policy iteration은 과거 policy와 새로운 policy를 섞어서 사용해서 실용적이지 않다는 단점이 있었는데 이것을 보완하여 온전히 새로운 policy만으로 update할 수 있는 기법을 제안합니다. \n\n### 1.1.4 KL divergence version of Theorem 1\n\n$$\\max L\\_{\\pi}\\left(\\tilde\\pi\\right) - C\\cdot D_\\mathrm{KL}^\\max\\left(\\pi,\\tilde\\pi\\right),\\quad\\left(C = \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\right)$$\n\ndistance metric을 KL divergence로 바꿀 수 있습니다.\n\n### 1.1.5 Using parameterized policy\n\n$$\\max\\_\\theta L\\_{\\theta\\_\\mathrm{old} }(\\theta) - C\\cdot D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right)$$\n\n최적화문제를 더욱 편리하게 풀 수 있도록 낮은 dimension을 가지는 parameter들로 parameterized된 policy를 사용할 수 있습니다.\n\n### 1.1.6 Trust region constraint\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ }&D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right) \\leq \\delta\n\\end{align}$$\n\n1.1.5까지는 아직 conservative policy iteration을 약간 변형시킨 것입니다. policy를 update할 때 지나치게 많이 변하는 것을 방지하기 위하여 [trust region](https://en.wikipedia.org/wiki/Trust_region)을 constraint로 설정할 수 있습니다. 이 아이디어로 인해서 TRPO라는 명칭을 가지게 됩니다.\n\n### 1.1.7 Heuristic approximation\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ }&\\overline{D}\\_\\mathrm{KL}^{\\rho\\_\\mathrm{old} }\\left(\\theta\\_\\mathrm{old}, \\theta\\right) \\leq \\delta\n\\end{align}$$\n\n사실 1.1.6의 constraint는 모든 state에 대해서 성립해야 하기 때문에 문제를 매우 어렵게 만듭니다. 이것을 좀 더 다루기 쉽게 state distribution에 대한 평균을 취한 것으로 변형합니다.\n\n### 1.1.8 Monte Carlo simulation\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi\\_\\theta(a\\vert s)}{q(a\\vert s)}Q\\_{\\theta\\_\\mathrm{old} }(s,a)\\right] \\\\\\\\\n\\mathrm{s.t.\\ }&E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} }}\\left[D\\_\\mathrm{KL}\\left(\\pi\\_{\\theta\\_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi\\_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta\n\\end{align}$$\n\nSampling을 통한 계산이 가능하도록 식을 다시 표현할 수 있습니다. \n\n### 1.1.9 Efficiently solving TRPO\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &\\nabla\\_\\theta \\left. L\\_{\\theta\\_\\mathrm{old} }(\\theta)\\right\\vert \\_{\\theta=\\theta\\_\\mathrm{old} } \\left(\\theta - \\theta\\_\\mathrm{old}\\right) \\\\\\\\\n\\mathrm{s.t.\\ }&\\frac{1}{2}\\left(\\theta\\_\\mathrm{old} - \\theta \\right)^T A\\left(\\theta\\_\\mathrm{old}\\right)\\left(\\theta\\_\\mathrm{old} - \\theta \\right) \\leq \\delta \\\\\\\\\n&A\\_{ij} = \\frac{\\partial}{\\partial \\theta\\_i}\\frac{\\partial}{\\partial \\theta\\_j}\\overline{D}\\_\\mathrm{KL}^{\\rho\\_\\mathrm{old} }\\left(\\theta\\_\\mathrm{old}, \\theta\\right)\n\\end{align}$$\n\n문제를 효율적으로 풀기 위하여 approximation을 적용할 수 있습니다. objective function은 first order approximation, constraint는 quadratic approximation을 취하면 효율적으로 문제를 풀 수 있는 형태로 바뀌는데 이것을 natural gradient로 풀 수도 있고 conjugate gradient로 풀 수도 있습니다.\n\nTRPO는 이렇게 다양한 방법으로 문제를 변형한 것입니다! 이제 좀 더 자세히 살펴보겠습니다.\n\n<br><br>\n\n# 2. Preliminaries\n\n다음과 같은 파라미터를 가지는 infinite-horizon discounted Markov decision process (MDP)를 고려합니다. \n\n* $\\mathcal{S}$: finite set of states\n* $\\\\mathcal{A}$: finite set of actions\n* $P: \\mathcal{S}\\times\\mathcal{A}\\rightarrow\\mathbb{R}$: transition probability distribution\n* $r: \\mathcal{S}\\rightarrow\\mathbb{R}$: reward function\n* $\\rho\\_0:\\mathcal{S}\\rightarrow\\mathbb{R}$: distribution of the initial state $s\\_0$\n* $\\gamma\\in(0,1)$: discount factor\n* $\\eta(\\pi)=E\\_{s\\_0,a_0,\\ldots}\\left[\\sum\\_{t=0}^\\infty\\gamma^t r\\left(s\\_t\\right)\\right]$, where $s\\_0\\sim\\rho\\_0\\left(s\\_0\\right), a\\_t\\sim\\pi\\left(\\left.a\\_t \\right\\vert s\\_t\\right), s\\_{t+1}\\sim P\\left(\\left.s\\_{t+1}\\right\\vert s\\_t,a\\_t\\right)$\n* $Q\\_\\pi\\left(s\\_t,a\\_t\\right)=E\\_{ { \\color{red}{s\\_{t+1}} },a\\_{t+1},\\ldots}\\left[\\sum\\_{l=0}^\\infty \\gamma^l r\\left(s\\_{t+l}\\right)\\right]$: action value function\n* $V\\_\\pi\\left(s\\_t\\right)=E\\_{ { \\color{red}{a\\_t} }, s\\_{t+1},a\\_{t+1},\\ldots}\\left[\\sum\\_{l=0}^\\infty \\gamma^l r\\left(s\\_{t+l}\\right)\\right]$: value function (action value function과 expectation의 첨자가 다른 부분을 유의하세요.)\n* $A\\_\\pi(s,a) = Q\\_\\pi(s,a) - V\\_\\pi(s)$: advantage function\n\n<br>\n## 2.1 Useful identity [Kakade & Langford 2002]\n\n우리의 목표는 $\\eta\\(\\pi\\)$가 최대화되도록 만드는 것입니다. 하지만 $\\pi$의 변화에 따라 $\\eta$가 어떻게 변하는지 알아내는 것도 쉽지 않습니다. $\\pi$는 기존의 policy이고 $\\tilde\\pi$는 새로운 policy를 나타낸다고 할 때 $\\eta$와 policy update 사이에 다음과 같은 관계가 있다는 것이 밝혀졌습니다.\n\n**Lemma 1.** $\\eta\\left(\\tilde\\pi\\right) = \\eta(\\pi) + E\\_{s\\_{0},a\\_{0},\\ldots\\sim\\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t A\\_\\pi\\left(s\\_t,a\\_t\\right)\\right]$\n\n*Proof.* $A\\_\\pi(s,a)=E\\_{s'\\sim P\\left(s'\\vert s,a\\right)}\\left[r(s)+\\gamma V\\_\\pi\\left(s'\\right)-V\\_\\pi(s)\\right]$로 다시 표현할 수 있습니다. 표기의 편의를 위하여 $\\tau:=\\left(s\\_0,a\\_0,s\\_1,a\\_1,\\ldots\\right)$를 정의하겠습니다. 다음과 같은 수식 전개가 가능합니다.\n$$\n\\begin{align}\n&E\\_{\\tau\\vert \\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t A\\_\\pi\\left(s\\_t,a\\_t\\right)\\right] \\\\\\\\\n &= E\\_{\\tau\\vert \\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t \\left(r(s\\_t)+\\gamma V\\_\\pi\\left(s\\_{t+1}\\right)-V\\_\\pi(s\\_t)\\right)\\right] \\\\\\\\\n&= E\\_{\\tau\\vert \\tilde\\pi}\\left[\\left(\\sum\\_{t=0}^\\infty\\gamma^t r(s\\_t)\\right)+\\gamma V\\_\\pi\\left(s\\_{1}\\right)-V\\_\\pi(s\\_0)+\\gamma^2 V\\_\\pi\\left(s\\_{2}\\right)-\\gamma V\\_\\pi(s\\_1)+\\gamma^3 V\\_\\pi\\left(s\\_{3}\\right)-\\gamma^2 V\\_\\pi(s\\_2)+\\cdots\\right] \\\\\\\\\n&= E\\_{\\tau\\vert \\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t r(s\\_t)+\\color{red}{\\gamma V\\_\\pi\\left(s\\_{1}\\right)}-V\\_\\pi(s\\_0)+\\color{red}{\\gamma^2 V\\_\\pi\\left(s\\_{2}\\right)}-\\color{red}{\\gamma V\\_\\pi(s\\_1)}+\\cdots\\right] \\\\\\\\\n&= E\\_{\\tau\\vert \\tilde\\pi}\\left[-V\\_\\pi(s\\_0)+\\sum\\_{t=0}^\\infty\\gamma^t r(s\\_t)\\right] \\\\\\\\\n&\\overset{\\underset{\\mathrm{(a)} }{} }{=} -E\\_{\\color{red}{s\\_0}}\\left[V\\_\\pi(s\\_0)\\right]+E\\_{\\tau\\vert \\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t r(s\\_t)\\right] \\\\\\\\\n&=-\\eta(\\pi) + \\eta\\left(\\tilde\\pi\\right)\\\\\\\\\n&\\quad\\therefore \\eta\\left(\\tilde\\pi\\right) = \\eta(\\pi) + E\\_{s\\_{0},a\\_{0},\\ldots\\sim\\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t A\\_\\pi\\left(s\\_t,a\\_t\\right)\\right]\n\\end{align}\n$$\n위의 전개의 (a) 부분은 $V\\_\\pi\\left(s\\_0\\right)=E\\_{a\\_0,a\\_1,s\\_1,\\ldots}\\left[\\sum\\_{t=0}^\\infty\\gamma^l r\\left(s\\_l\\right)\\right]$이므로 $\\tau$ 중 많은 부분들이 이미 expectation이 취해진 값이므로 무시됩니다. 오직 $s\\_0$만 expectation이 취해지지 않았기 때문에 남아있습니다.\n\n**Lemma 1**은 새로운 policy와 기존의 policy 사이의 관계를 규정합니다. 다음과 같은 식을 정의하고 이것을 이용해서 **Lemma 1**을 변형시켜 봅시다.\n\n* $\\rho\\_\\pi(s) = P\\left(s\\_0=s\\right) + \\gamma P\\left(s\\_1=s\\right)\\ + \\gamma^2 P\\left(s\\_2=s\\right) + \\cdots $: (unnormalized) discounted visitation frequencies\n\n\n\n$$\n\\begin{align}\n\\eta\\left(\\tilde\\pi\\right) &= \\eta(\\pi) + \\sum\\_{t=0}^\\infty\\sum\\_{s}P\\left(\\left.s\\_t=s\\right\\vert \\tilde\\pi\\right)\\sum\\_a\\tilde\\pi(a\\vert s)\\gamma^t A\\_\\pi(s,a) \\\\\\\\\n&= \\eta(\\pi) + \\sum\\_{s}{\\color{red}{\\sum\\_{t=0}^\\infty \\gamma^t P\\left(\\left.s\\_t=s\\right\\vert \\tilde\\pi\\right)} } \\sum\\_a\\tilde\\pi(a\\vert s) A\\_\\pi(s,a) \\\\\\\\\n&= \\eta(\\pi) + \\sum\\_{s}{\\color{red}{\\rho\\_\\tilde\\pi(s)} }\\sum\\_a\\tilde\\pi(a\\vert s) A\\_\\pi(s,a) \\\\\\\\\n\\end{align}\n$$\n\n이 수식의 의미가 무엇일까요? 만약 $\\sum\\_a\\tilde\\pi(a\\vert s) A\\_\\pi(s,a) \\geq 0$이라면 $\\eta(\\tilde\\pi)$는 항상 $\\eta(\\pi)$보다 큽니다. 즉, policy를 업데이트함으로써 항상 개선됩니다. 즉, 이 수식을 통해서 항상 더 좋은 성능을 내는 policy update가 가져야 할 특징을 알 수 있습니다. 다음과 같은 deterministic policy가 있다고 합시다.\n\n$$\n\\tilde\\pi(s) = \\arg\\max_a A\\_\\pi(s,a)\n$$\n\n이 policy는 적어도 하나의 state-action pair에서 0보다 큰 값을 가지는 advantage가 있고 그 때의 확률이 0이 아니라면 항상 성능을 개선시킵니다. 어려운 점은 policy를 바꾸면 $\\rho$도 바뀐다는 점입니다. \n\n다음 그림을 봅시다. Starting Point에서 여러가지 경로를 거쳐서 Destination으로 갈 수 있습니다. 다른 policy를 이용하는 것은 다른 경로를 이용한 것입니다.\n\n[![policy_change](../../../../img/policy_change.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=16s)\n\n이 때 다른 policy를 이용함에 따라 state visitation frequency도 변하게 됩니다.\n\n[![state_visitation_change](../../../../img/state_visitation_change.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=36s)\n\n이로 인해서 policy를 최적화하는 것은 어려워집니다. 그래서 이러한 변화를 무시하는 다음과 같은 approximation을 취합니다.\n\n$$\n\\begin{align}\nL_\\pi\\left(\\tilde\\pi\\right) &= \\eta(\\pi) + \\sum\\_{s}{\\color{red}{\\rho\\_\\pi(s)} } \\sum\\_a\\tilde\\pi(a\\vert s) A\\_\\pi(s,a)\n\\end{align}\n$$\n\npolicy가 바뀌었음에도 이전의 state distribution을 계속 이용하는 것입니다. 이것은 policy의 변화가 크지 않다면 어느 정도 허용될 수 있을 것입니다. 그렇지만 얼마나 많은 변화가 허용될까요? 이것을 정하기 위해서 이용하는 것이 trust region입니다.\n\n<br>\n## 2.2 Conservative Policy Iteration\n\npolicy의 변화를 다루기 용이하게 하기 위해서 policy를 다음과 같이 파라미터를 이용해서 표현합시다.\n\n* $\\pi\\_\\theta(a\\vert s)$: parameterized policy\n\n$\\pi\\_\\theta$는 $\\theta$에 대하여 미분가능한 함수입니다. $L\\_\\pi\\left(\\tilde\\pi\\right)$을 $\\theta\\_0$에서 $\\eta(\\pi)$에 대한 first order approximation이라고 하면 다음 식이 성립합니다.\n\n$$\n\\begin{align}\nL\\_{\\pi\\_{\\theta\\_0} }\\left(\\pi\\_{\\theta_0}\\right) &= \\eta\\left(\\pi\\_{\\theta\\_0}\\right) \\\\\\\\\n\\nabla\\_\\theta \\left.L\\_{\\pi\\_{\\theta\\_0} }\\left(\\pi\\_{\\theta\\_0}\\right)\\right\\vert \\_{\\theta=\\theta\\_0} &= \\nabla\\_\\theta\\left.\\eta(\\pi\\_{\\theta\\_0})\\right\\vert \\_{\\theta=\\theta\\_0}\n\\end{align}\n$$\n\n이것의 의미는 $\\pi\\_{\\theta\\_0}$가 매우 작게 변한다면 $L\\_{\\pi\\_{\\theta\\_0} }$를 개선시키는 것이 $\\eta$를 개선시키는 것이라는 것입니다. 그러나 지금까지의 설명만으로는 $\\pi\\_{\\theta\\_0}$를 얼마나 작게 변화시켜야 할지에 대해서는 알 수 없습니다.\n\nKakade & Langford가 2002년 발표한 논문에서도 이것에 대해서 고민했습니다. 그 논문에서 *conservative policy iteration*이라는 기법을 제안합니다. 그 논문의 contribution은 다음과 같습니다.\n\n* $\\eta$ 개선의 lower bound를 제공\n* 기존의 policy를 $\\pi\\_\\mathrm{old}$라고 하고 $\\pi'$를 $\\pi'=\\arg\\max\\_\\{\\pi'}L\\_{\\pi\\_\\mathrm{old} }\\left(\\pi'\\right)$과 같이 정의할 때, 새로운 mixture policy $\\pi\\_\\mathrm{new}$를 다음과 같이 제안\n$$\n\\pi\\_\\mathrm{new}(a\\vert s) = (1-\\alpha)\\pi\\_\\mathrm{old}(a\\vert s) + \\alpha \\pi'(a\\vert s)\n$$\n\n그림으로 표현하면 다음과 같습니다.\n\n[![mixure_policy](../../../../img/mixure_policy.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=2m46s)\n\n* 다음과 같은 lower bound를 정의\n$$\n\\eta\\left(\\pi\\_\\mathrm{new}\\right) \\geq L\\_{\\pi\\_{\\theta\\_\\mathrm{old} }}\\left(\\pi\\_\\mathrm{new}\\right) - \\frac{2\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2,\\\\\\\\\n\\mathrm{where\\ }\\epsilon = \\max_s\\left\\vert E\\_{a\\sim\\pi'(a\\vert s)}\\left[A\\_\\pi(s,a)\\right]\\right\\vert \n$$\n\n하지만 mixture된 policy라는 것은 실용적이지 않습니다. \n\n<br><br>\n\n# 3. Monotonic Improvement Guarantee for General Stochastic Policies\n\n이전 장에서 설명한 lower bound는 오직 mixture policy에 대해서만 성립하고 더 많이 사용되는 stochastic policy에는 적용되지 않습니다. 따라서 stochastic policy를 이용할 수 있도록 개선할 필요가 있습니다. 아래와 같이 기존 수식에서 두 가지를 바꿈으로써 이것이 가능합니다.\n\n* $\\alpha\\rightarrow$ distance measure between $\\pi$ and $\\tilde\\pi$\n* constant $\\epsilon\\rightarrow\\max\\_{s,a}\\left\\vert A\\_\\pi(s,a)\\right\\vert $\n\n여기서 distance measure로 total variation divergence를 이용합니다. discrete porbability distribution $p$와 $q$에 대하여 다음과 같이 정의됩니다.\n$$\nD\\_\\mathrm{TV}(p\\parallel q) = \\frac{1}{2}\\sum\\_i\\left\\vert p\\_i - q\\_i\\right\\vert \n$$\n\n이것을 이용하여 $D\\_\\mathrm{TV}^\\max$를 다음과 같이 정의합니다.\n$$\nD\\_\\mathrm{TV}^\\max(\\pi\\parallel \\tilde\\pi) = \\max_s D\\_\\mathrm{TV}\\left(\\pi(\\cdot\\vert s)\\parallel\\tilde\\pi(\\cdot\\vert s)\\right)\n$$\n\n그림으로 표현하면 다음과 같습니다.\n\n[![tvd](../../../../img/tvd.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=3m15s)\n\n이것을 이용하여 다음과 같은 관계식을 얻을 수 있습니다.\n\n**Theorem 1.** Let $\\alpha=D\\_\\mathrm{TV}^\\max(\\pi\\parallel \\tilde\\pi)$. Then the following bound holds:\n$$\n\\begin{align}\n\\eta\\left(\\pi\\_\\mathrm{new}\\right) &\\geq L\\_{\\pi\\_\\mathrm{old} }\\left(\\pi\\_\\mathrm{new}\\right) - \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2, \\\\\\\\\n\\mathrm{where\\ } \\epsilon&= \\max\\_{s,a}\\left\\vert A\\_\\pi(s,a)\\right\\vert \n\\end{align}\n$$\n<!--*Proof.* TBD.-->\n\n또다른 distance metric으로 아래 그림과 같은 KL divergence가 있습니다.\n(그런데 왜 하필 KL divergence로 바꿀까요? 논문의 뒤쪽에서 계산효율을 위해서 conjugate gradient method를 이용하는데 이를 위해서 바꾼게 아닐까 싶습니다. Wasserstein distance 같은 다른 방향으로 발전시킬 수도 있을 것 같습니다. Schulmann이 아마 해봤겠죠?^^a)\n\n[![kld](../../../../img/kld.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=3m34s)\n\ntotal variation divergence와 KL divergence 사이에는 다음과 같은 관계가 있습니다.\n$$\nD\\_\\mathrm{TV}(p\\parallel q)^2 \\leq D\\_\\mathrm{KL}(p\\parallel q)\n$$\n\n다음 수식을 정의합니다.\n$$\nD\\_\\mathrm{KL}^\\max(\\pi\\parallel \\tilde\\pi) = \\max_s D\\_\\mathrm{KL}\\left(\\pi(\\cdot\\vert s)\\parallel\\tilde\\pi(\\cdot\\vert s)\\right)\n$$\n\n**Theorem 1**을 이용하여 다음과 같은 수식이 성립함을 알 수 있습니다.\n$$\n\\begin{align}\n\\eta\\left(\\tilde\\pi\\right) &\\geq L\\_{\\pi}\\left(\\tilde\\pi\\right) - C\\cdot D\\_\\mathrm{KL}^\\max(\\pi, \\tilde\\pi), \\\\\\\\\n\\mathrm{where\\ } C&= \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\n\\end{align}\n$$\n\n이러한 policy imporvement bound를 기반으로 다음과 같은 approximate policy iteration 알고리듬을 고안해낼 수 있습니다.\n\n**Algorithm 1** Policy iteration algorithm guaranteeing non-decreasing expected return $\\eta$\n> Initialize $\\pi\\_0$   \n> **for** $i=0,1,2,\\ldots$ until convergence **do**   \n> $\\quad$Compute all advantage values $A\\_{\\pi\\_i}(s,a)$    \n> $\\quad$Solve the contstrained optimization problem    \n> $\\quad\\pi\\_{i+1}=\\arg\\max\\_\\pi\\left[L\\_{\\pi\\_i} - CD\\_\\mathrm{KL}^\\max\\left(\\pi\\_i,\\pi\\right)\\right]$     \n> $\\quad\\quad$where $C = 4\\epsilon\\gamma / (1-\\gamma)^2$    \n> $\\quad\\quad$and $L\\_{\\pi\\_i}\\left(\\pi\\right) = \\eta(\\pi\\_i) + \\sum\\_{s}\\rho\\_{\\pi\\_i}(s)\\sum\\_a\\pi(a\\vert s) A\\_{\\pi\\_i}(s,a)$    \n> **end for**\n\n**Algorithm 1**은 advantage를 정확하게 계산할 수 있다고 가정하고 있습니다. 이 알고리듬은  monotonical한 성능 증가($\\eta(\\pi\\_0)\\leq\\eta(\\pi\\_1)\\leq\\cdots$)를 한다는 것을 다음과 같이 보일 수 있습니다. $M\\_i(\\pi)=L\\_{\\pi\\_i}(\\pi) - CD\\_\\mathrm{KL}^\\max\\left(\\pi\\_i,\\pi\\right)$라고 합시다.\n\n$$\n\\begin{align}\n\\eta \\left(\\pi\\_{i+1}\\right) &\\geq M\\_i\\left(\\pi\\_{i+1}\\right)\\\\\\\\\n\\eta \\left(\\pi\\_{i}\\right) &= M\\_i\\left(\\pi\\_{i}\\right) \\\\\\\\\n\\eta \\left(\\pi\\_{i+1}\\right) - \\eta \\left(\\pi\\_{i}\\right) &\\geq M\\_i\\left(\\pi\\_{i+1}\\right) - M\\_i\\left(\\pi\\_{i}\\right)\n\\end{align}\n$$\n\n위 수식과 같이 매 iteration 마다 $M\\_i$를 최대화함으로써, $\\eta$가 감소하지 않는다는 것을 보장할 수 있습니다. 이와 같은 타입의 알고리듬을 [minorization-maximization (MM) algorithm](https://www.jstor.org/stable/pdf/27643496.pdf?casa_token=0qHamcl60WoAAAAA:-fkZ9JcA_nrY3-zbCUpqvPOgcAMgw7Gr96MajCZg2byHf8m5GU1KTSxyJJcBy1lPZbBTZVCjHHUXilh4k-iuwF91Wka4B5qdltC1IR2qMWk8q1FoV6__)이라고 합니다. EM 알고리듬도 이 타입의 알고리듬 중 하나입니다.\n\n**Algorithm 1**은 아래 그림과 같이 동작합니다.\n\n[![surrogate](../../../../img/surrogate.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=3m52s)\n\n\n$M\\_i$는 $\\pi\\_i$일 때 equality가 되는 $\\eta$에 대한  surrogate function입니다. TRPO는 이 surrogate function을 최대화하고 KL divergence를 penalty가 아닌 constraint로 두는 알고리듬입니다.\n\n<br><br>\n\n# 4. Optimization of Parameterized Policies\n\n표기의 편의를 위해 다음과 같이 notation들을 더 간략하게 정의합니다.\n\n* $\\eta(\\theta):=\\eta\\left(\\pi\\_\\theta\\right)$\n* $L\\_\\theta\\left(\\tilde\\theta\\right):=L\\_{\\pi\\_\\theta}\\left(\\pi\\_{\\tilde\\theta}\\right)$\n* $D\\_\\mathrm{KL}\\left(\\theta\\parallel\\tilde\\theta\\right):=D\\_\\mathrm{KL}\\left(\\pi\\_\\theta\\parallel\\pi_{\\tilde\\theta}\\right)$\n* $\\theta\\_\\mathrm{old}$: previous policy parameter\n\n<br>\n## 4.1 Trust Region Policy Optimization\n\n이전 장의 중요 결과를 위의 notation으로 다시 표기하면 아래와 같습니다.\n\n$$\n\\eta\\left(\\theta\\right) \\geq L\\_{\\theta\\_\\mathrm{old} }\\left(\\theta\\right) - C\\cdot D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right)\n$$\n\n$\\eta$의 성능 향상을 보장하기 위해서 그것의 lower bound를 최대화할 수 있습니다.\n\n$$\n\\max\\_\\theta \\left[L\\_{\\theta\\_\\mathrm{old} }\\left(\\theta\\right) - C D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right)\\right]\n$$\n\n이 최적화 문제는 step size를 매우 작게 해야 올바른 동작을 합니다. 위에서 살펴봤듯이 first order approximation이기 때문입니다. 좀 더 큰 step size를 가질 수 있도록 이 최적화 문제를 trust region constraint를 도입하여 다음과 같이 바꿉니다.\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ }&D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right) \\leq \\delta\n\\end{align}$$\n\n이 최적화문제의 constraint는 모든 state space에 대해서 성립해야 합니다. 또한 maximum값을 매번 찾아야 합니다. state가 많은 경우 constraint의 수가 매우 많아져서 문제를 풀기 어렵게 만듭니다. constraint의 수를 줄이기 위하여 다음과 같은 avergae KL divergence를 이용하는 heuristic approximation을 취합니다. 이것이 최선의 방법은 아닐 수 있지만 실용적인 방법입니다.\n\n$$\n\\overline D\\_\\mathrm{KL}^{\\rho}\\left(\\theta\\_1, \\theta\\_2\\right):=E\\_{s\\sim\\rho}\\left[D\\_\\mathrm{KL}\\left(\\pi\\_{\\theta_1}(\\cdot\\vert s)\\parallel\\pi\\_{\\theta\\_2}(\\cdot\\vert s)\\right)\\right]\n$$\n\n이것을 기반으로 다음과 같은 최적화 문제를 풀 수 있습니다.\n\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ }&\\overline{D}\\_\\mathrm{KL}^{\\rho\\_\\mathrm{old} }\\left(\\theta\\_\\mathrm{old}, \\theta\\right) \\leq \\delta\n\\end{align}$$\n\n\n아래 그림처럼 step size에 대해서 고려할 수 있습니다.\n\n[![heuristic_approx](../../../../img/heuristic_approx.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=4m35s)\n\n<br><br>\n\n# 5. Sample-Based Estimation of the Objective and Constraint\n\n실용적인 알고리듬을 만들려고 하는 노력은 아직 끝나지 않았습니다. 이제 앞의 알고리듬을 sample-based estimation 즉, Monte Carlo estimation을 할 수 있도록 바꿔보겠습니다. sampling을 편하게 할 수 있도록 아래와 같이 바꿔줍니다.\n\n* $\\sum\\_s \\rho\\_{\\theta\\_\\mathrm{old} }(s)[\\ldots]\\rightarrow\\frac{1}{1-\\gamma}E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} }}[\\ldots]$\n* $A\\_{\\theta\\_\\mathrm{old} }\\rightarrow Q\\_{\\theta\\_\\mathrm{old} }$\n* $\\sum\\_a\\pi\\_{\\theta\\_\\mathrm{old} }(a\\vert s)A\\_{\\theta\\_\\mathrm{old} } \\rightarrow E\\_{a\\sim q}\\left[\\frac{\\pi\\_\\theta(a\\vert s)}{q(a\\vert s)}A\\_{\\theta\\_\\mathrm{old} }\\right]$\n\n이러한 변화를 그림처럼 도식화할 수 있습니다.\n\n[![sample-based](../../../../img/sample-based.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=5m31s)\n\n한 가지 짚고 넘어가야 할 점은 action sampling을 할 때 importance sampling을 사용한다는 것입니다.\n\n[![importance_sampling](../../../../img/importance_sampling.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=5m53s)\n\n바뀐 최적화문제는 아래와 같습니다.\n\n- **Equation (14).**\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi\\_\\theta(a\\vert s)}{q(a\\vert s)}Q\\_{\\theta\\_\\mathrm{old} }(s,a)\\right] \\\\\\\\\n\\mathrm{s.t.\\ }&E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} }}\\left[D\\_\\mathrm{KL}\\left(\\pi\\_{\\theta\\_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi\\_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta\n\\end{align}\n$$\n\n이 때 sampling하는 두 가지 방법이 있습니다.\n\n<br>\n## 5.1 Single Path\n\n*single path*는 개별 trajectory들을 이용하는 방법입니다.\n\n[![single](../../../../img/single.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=6m15s)\n\n<br>\n## 5.2 Vine\n\n*vine*은 한 state에서 rollout을 이용하여 여러 action을 수행하는 방법입니다.\n\n[![vine1](../../../../img/vine1.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=6m32s)\n\n[![vine2](../../../../img/vine2.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=6m49s)\n\nestimation의 variance를 낮출 수 있지만 계산량이 많고 한 state에서 여러 action을 수행할 수 있어야 하기 때문에 현실적인 문제에 적용하기에는 어려움이 있습니다.\n\n\n<br><br>\n\n# 6. Practical Algorithm\n\n앞서 single-path, vine 샘플링을 사용하는 두가지 방식의 policy optimization 알고리즘을 살펴봤습니다. 실용적인 알고리듬은 아래의 과정을 반복해서 수행합니다.\n\n1) Q-values의 몬테카를로 추정을 통해 state-action 쌍 집합을 single path 또는 vine 과정을 통해 수집함.\n\n2) 샘플 평균으로, (14)식의 목적함수와 제약식을 추정함\n + **Equation (14).**\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi\\_\\theta(a\\vert s)}{q(a\\vert s)}Q\\_{\\theta\\_\\mathrm{old} }(s,a)\\right] \\\\\\\\\n\\mathrm{s.t.\\ }&E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} }}\\left[D\\_\\mathrm{KL}\\left(\\pi\\_{\\theta\\_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi\\_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta\n\\end{align}\n$$\n\n3) policy parameter vector인 $\\theta$를 업데이트 하면서 제약조건이 있는 최적화 문제를 근사적으로 풂. 본 논문에서는 gradient를 직접 계산하는 것보다는 약간 더 계산량이 있는 line search와 conjugate gradient algorithm을 사용했습니다.\n\n3)에 대해서,  gradient의 covariance matrix를 사용하지 않고 KL divergence의 Hessian을 해석적으로 계산하여 Fisher Information Matrix를 구성했습니다.\n\n다시말해서\n\n$\\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial}{\\partial \\theta_i} \\log \\pi_{\\theta}(a_n\\vert s_n) \\frac{\\partial}{\\partial \\theta_j} \\log \\pi_{\\theta}(a_n\\vert s_n)$ 대신 $\\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_j} D_{KL}(\\pi_{\\theta_{old} }(\\cdot \\vert  s_n) \\vert \\vert  \\pi_{\\theta}(\\cdot \\vert  s_n))$를 계산한 것입니다.\n\n이 analytic estimator는 Hessian이나 trajectories의 모든 gradient를 저장하지 않아도 되기 때문에 대규모 환경을 고려할 경우 계산 상 이점이 있습니다. 좀 더 자세하게 설명한 Appendix C 부분을 살펴보겠습니다.\n\n\n<br>\n## Appendix C. Efficiently Solving the Trust-Region Constrained Optimization Problem\n\n우리가 풀고자 하는 식은 다음과 같습니다.\n\n$$\n\\begin{align}\n\\max\\quad &L(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ } &\\overline{D}\\_{KL}(\\theta\\_\\mathrm{old}, \\theta) \\leq \\delta\n\\end{align}\n$$\n\n이 최적화 문제를 풀기 위하여 우리는 gradient 기법을 이용할 것입니다. gradient 기법은 결국 update할 방향과 크기를 결정하는 문제로 생각할 수 있습니다. 이에 따라 아래와 같은 두가지 과정으로 진행합니다.\n\n1) 탐색 방향을 계산, 목적함수의 first-order approximation(선형근사)와 제약식의 quadratic approximation(2차 근사).\n\n2) 이동거리 계산을 위해 해당 방향으로 line search 수행.\n\n탐색 방향은 $Ax = g$ 수식을 근사적으로 풀어서 구합니다. 여기서 $A$ 는 Fisher Information Matrix이고, 이것은 KL divergence 제약식의 2차 근사 $\\overline{D}\\_\\mathrm{KL} (\\theta\\_\\mathrm{old}, \\theta) \\approx \\frac{1}{2}(\\theta - \\theta\\_\\mathrm{old})^T A(\\theta - \\theta\\_\\mathrm{old})$를 푸는 것입니다. 여기서 $A\\_{ij} = \\frac{\\partial}{\\partial \\theta\\_i} \\frac{\\partial}{\\partial \\theta\\_j} \\overline{D}\\_\\mathrm{KL}(\\theta\\_\\mathrm{old}, \\theta)$입니다.\n\n논문에서 이 부분에 대한 설명이 약간 부족해서 좀 더 내용을 추가하자면, $\\frac{1}{2}x^T A x - x^T g + b = 0$과 같은 quadratic equation의 해는 1차 derivative를 0으로 만드는 $x$를 찾음으로써 알 수 있습니다. 위에서 우리는 목적함수를 1차 근사시키고 제약조건은 2차 근사를 시킴으로써 quadratic equation 형태로 변형했기 때문에 같은 방식으로 풀 수 있습니다. 따라서 $g$는 1차근사시킨 목적함수의 계수가 될 것입니다.\n\n대규모 문제에서 $A$ 혹은 $A^{-1}$ 을 계산하는 것은 엄청난 계산량이 필요합니다. 하지만 conjugate gradient algorithm을 잘 이용하면 전체 $A$행렬(FIM)을 계산하지 않아도 $Ax=g$ 를 근사적으로 해결할 수 있게 해줍니다.\n\n탐색 방향 $s \\approx A^{-1}g$과 더불어 최대스텝길이(step size) $\\beta$도 계산할 필요가 있습니다.\n\n즉, $\\delta = \\overline{D}\\_\\mathrm{KL} \\approx \\frac{1}{2}(\\beta s)^T A(\\beta s) = \\frac{1}{2}\\beta^2s^TAs$ 이고, \n\n$\\beta = \\sqrt{2\\delta / s^T As}$가 됩니다. 이때 $\\delta$는 KL divergence의 boundary term 입니다.\n\n$s^T As$ 항은 Hessian-vector product로 계산할 수 있고, conjugate gradient 과정에서 계산됩니다.\n\n마지막으로, surrogate objective와 KL divergence 제약식을 위해 다음과 같은 함수를 정의하고 line search를 사용합니다.\n\n$$\nL\\_{\\theta\\_\\mathrm{old} }(\\theta) - \\chi [\\overline{D}\\_{KL}(\\theta\\_\\mathrm{old}, \\theta) \\leq \\delta]\n$$\n\n$\\chi[\\cdot]$ 함수는 []안의 조건이 맞으면 0이고 틀리면 무한으로 발산하는 함수입니다.\n\n위에서 계산한 $\\beta$ 의 최대 값부터 시작해서 목적함수가 개선될때까지 exponential하게 줄여 나갑니다. line search가 없었다면 이 알고리즘은 매우 큰 스텝으로 계산될 것이고 심각한 성능 저하가 발생할 것입니다.\n\n(여기서 살짝 예고를 하자면, 나중에 이 KL 기반 기법을 개선한 더 단순한 방법이 제안됩니다. PPO라고...)\n\n<br><br>\n\n# 7. Connections with Prior Work\n\nNatural Policy Gradient는 $L$의 선형 근사와 $\\overline D\\_\\mathrm{KL}$ 제약식을 2차근사 하는 아래의 식의 special case입니다.\n\n- Equation (17).\n\n$L\\_{\\pi}(\\tilde\\pi) = \\eta(\\pi) + \\underset{s}{\\sum}\\rho\\_{\\pi}(s) \\underset{a}{\\sum}\\tilde\\pi(a\\vert s)A\\_\\pi(s,a)$\n\n$\\underset{\\theta}{\\max}\\quad \\[\\nabla\\_{\\theta}L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\ \\rvert\\_{\\theta=\\theta\\_\\mathrm{old} } \\cdot (\\theta - \\theta\\_\\mathrm{old})]$\n\n$\\mathrm{s.t.}\\quad\\frac{1}{2}(\\theta\\_\\mathrm{old} - \\theta)^{T} A(\\theta\\_\\mathrm{old})(\\theta\\_\\mathrm{old}-\\theta) \\leq \\delta$\n\n$\\mathrm{where\\ }A(\\theta_{old})_{ij} = $\n\n$\\Large \\frac{\\partial}{\\partial \\theta\\_{i} } \\frac{\\partial}{\\partial \\theta\\_{j} }\nE\\_{s \\sim \\rho\\_{\\pi} }[D\\_\\mathrm{KL}(\\pi(\\cdot\\rvert s, \\theta\\_\\mathrm{old})  \\rvert\\rvert \\pi(\\cdot \\rvert s, \\theta))] \\rvert\\_{\\theta=\\theta\\_\\mathrm{old} }$\n\n업데이트 식은 다음과 같습니다.\n\n$\\theta\\_\\mathrm{new} = \\theta\\_\\mathrm{old} +  \\color{Red}{\\frac{1}{\\lambda} } A(\\theta\\_\\mathrm{old})^{-1}\\nabla\\_{\\theta}L(\\theta) \\rvert\\_{\\theta=\\theta\\_\\mathrm{old} }$\n\n여기서 step size인 $\\frac{1}{\\lambda}$ 일반적으로 알고리즘의 파라미터로 취급되지만 TRPO는 각 업데이트 마다 제약식으로 사용한다는 점이 다릅니다. 사소한 차이처럼 보이지만, 큰 차원을 다루는 실험에서 성능을 크게 향상시켰습니다.\n\n또한 $L^2$ 제약식(혹은 페널티) 를 사용하면 표준 Policy Gradient 업데이트 식을 얻었습니다.\n\n- Equation (18).\n\n$\\underset{\\theta}{\\max}\\quad[\\nabla\\_{\\theta} L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\rvert\\_{\\theta=\\theta\\_\\mathrm{old} } \\cdot (\\theta - \\theta\\_\\mathrm{old})]$\n\n$\\mathrm{s.t.}\\quad \\frac{1}{2} \\vert\\vert \\theta - \\theta\\_\\mathrm{old} \\vert\\vert^2 \\leq \\delta$\n\n\n\n$L\\_{\\pi}(\\tilde\\pi) = \\eta(\\pi) + \\underset{s}{\\sum}\\rho\\_{\\pi}(s) \\underset{a}{\\sum}\\tilde\\pi(a\\vert s)A\\_\\pi(s,a) $를 이용해서 제약조건없이 $\\underset{\\pi}{\\max}\\quad L\\_{\\pi\\_\\mathrm{old} }(\\pi)$를 풀면\nPolicy Iteration update를 하는 것과 같습니다.\n\n<br><br>\n\n# 8. Experiments\n\n- 다음과 같은 3 가지 궁금증을 해결하기 위하여 실험을 설계하였습니다.\n\n    1. Single-path 와 vine 의 성능면에서 특징을 알고싶음.\n\n    2. Fixed penalty coefficient(NPG) 보다 fixed KL divergence를 사용하는 것(TRPO)으로    \n    변경한 것이 얼마나 뚜렷한 차이를 보일지, 성능 면에서 어떤 영향을 미치는지 알고싶음.\n\n    3. TRPO가 큰 스케일의 문제를 해결할 수 있는지, 기존에 연구/적용되어왔던 방법들과\n    성능, 계산시간, 샘플 복잡도 면에서 비교하고 싶음.\n\n\n- 1, 2의 궁금증에 대해서 실험 환경에 single path, vine을 비롯한 여러 사전방법들을 함께 실험하였습니다.\n\n- 3에 대해선, robotic locomotion과 Atari game에 TRPO를 실험하였습니다.\n\n\n<br>\n## 8.1 Simulated Robotic Locomotion\n\n- MuJoCo 시뮬레이터를 활용해 로봇 locomotion 실험 진행하였습니다.\n- Robot State: positions and velocities, joint torques\n\n![](https://i.imgur.com/bs5ATS3.png)\n\n- 수영, 점프, 걷기 행동을 학습시키고자 합니다.\n\n    - 수영: 10 dimensional state space , reward: $r(x,u)=v_x - 10^{-5}\\vert \\vert u\\vert \\vert ^2$, quadratic penalty.\n\n    - 점프: 12 dimensional state space, episode의 끝은 높이와 각도 threshol로 점프를 판단, non-terminal state에 대해 +1 보상 추가.\n\n    - 걷기: 18 dimensional state space, 뜀뛰는 듯한 보법(점프에이전트)보다 부드러운 보법을 유도하기위해 페널티를 추가함.\n\n**Detailed Experiment Setup and used parameters, used network model**\n![](https://i.imgur.com/zgnsbw6.png)\n\n![](https://i.imgur.com/FqdWC53.png)\n\nequation (12) : $\\max\\quad L\\_{\\theta\\_\\mathrm{old}(\\theta)}, \\ \\ \\mathrm{s.t.\\ }\\overline{D}\\_\\mathrm{KL}^{\\rho\\_{\\theta\\_\\mathrm{old} }}(\\theta\\_\\mathrm{old}, \\theta) \\leq \\delta$\n\n이 실험에서 $\\delta = 0.01$입니다.\n\n- 비교에 사용된 모델들\n\n    - TRPO Single-path\n    - TRPO vine\n    - CEM(cross-entropy method)\n    - CMA(covariance matrix adaptation)\n    - NPG (Lagrange Multiplier penalty coefficien를 사용하는 것이 차이점)\n    - Empirical FIM\n    - maximum KL(cartpole)\n\n![](https://i.imgur.com/wSqolvS.png)\n\n- 제안한 single path, vine 모두 task를 잘 학습하는 것을 확인할 수 있습니다.\n\n- 페널티를 고정하는 것보다, KL divergenc를 제약하는 것이 step size를 선택하는 부분에서 Robust합니다.\n\n- CEM/CMA는 derivative free 알고리즘으로, 샘플 복잡도가 네트워크 파라미터수 만큼 확장되어 high dimension 문제에서는 속도와 성능이 약한 모습을 보입니다.\n\n- maxKL은 제안방법보다 느린 학습성능을 보입니다.\n\nTRPO video link: https://www.youtube.com/watch?v=jeid0wIrSn4\n\n- 사전지식이 적은 상태에서 여러 액션을 학습하는 실험으로 TRPO의 성능을 보입니다.\n\n- 지금까지의 균형이나 걸음 같은 개념을 명시적으로 인코딩한 robotic locomotion에서의 사전 연구들과 차이를 보였습니다.\n\n<br>\n## 8.2 Playing Games from Images\n\n- 부분관찰가능하며 복잡한 observation인 환경에서 TRPO를 평가하기위해 raw image를 입력으로 하는 아타리게임 환경에서 실험하였습니다.\n\n* Challenging point :\n    - High dimensional space\n    - delayed reward\n    - non-stationary image (Enduro는 배경 이미지가 바뀌고, 반전처리가 됨)\n    - complex action sequence (Q*bert는 21개의 다른 발판에서 점프 해야함)\n\n- ALE(Arcade Learning Environment) 에서 테스트\n- DQN과 전처리 방식은 동일\n    210 x 160 pixel -> gray-scaling & down-sampling -> 110 x 84  -> crop -> 84 x 84\n\n- 2 convolution layer ( 16 channels) , stride 2, FC layer with 20 units\n\n![](https://i.imgur.com/NJBC69d.png) \n\n![](https://i.imgur.com/wTe1OEW.png)\n\n![](https://i.imgur.com/j22VtNQ.png)\n\n- 30시간 동안 500 iteration 학습을 하였습니다.\n\n- DQN과 MCST를 사용한 모델(UCC-I), 사람이 플레이한 것(Human) 과의 비교하였습니다.\n\n- 이전 방법들에 비해 압도적인 점수를 기록하진 못했으나, 다양한 게임에서 적절한 게임 기록하였습니다.\n\n- 이전 방법들과 다른 점은, 특정 task에 초점을 맞춰 설계하지않았는데 robotics나 game playing Task에도 적절한 성능을 내는 점에서 **TRPO의 일반화 성능**을 확인하였습니다.\n\n<br><br>\n\n# 9. Discussion\n\n- Trust Region Policy Optimization 을 제안하였습니다.\n\n- KL Divergence 페널티로 $\\eta(\\pi)$를 최적화하는 알고리즘이 monotonically improve 함을 증명하였습니다.\n\n- Locomotion 도메인에서 여러 행동(수영, 걷기, 점프)을 제어하는 controller를 학습하였습니다.\n\n- Robotics와 게임 실험을 결합해, 시각정보와 센서데이터를 사용하는 로봇제어 정책을 학습시킬 수 있는 가능성을 보았습니다.\n\n- 샘플의 복잡도를 상당히 줄일 수 있어 실제상황에 적용가능성을 보았습니다.\n\n<br><br>\n\n# 처음으로\n\n## [PG Travel Guide](https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/)\n\n<br>\n\n# 이전으로\n\n## [NPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/25/4_npg/)\n\n## [NPG Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/tnpg.py)\n\n<br>\n\n# 다음으로\n\n## [TRPO Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/trpo_gae.py)\n\n## [GAE 여행하기](https://reinforcement-learning-kr.github.io/2018/06/23/6_gae/)\n","source":"_posts/5_trpo.md","raw":"---\ntitle: Trust Region Policy Optimization\ndate: 2018-06-24 16:53:12\ntags: [\"프로젝트\", \"피지여행\"]\ncategories: 프로젝트\nauthor: 공민서, 김동민\nsubtitle: 피지여행 5번째 논문\n---\n\n<center> <img src=\"https://www.dropbox.com/s/o7cjcn0e17mpizr/Screen%20Shot%202018-07-18%20at%201.14.22%20AM.png?dl=1\" width=\"700\"> </center>\n\n논문 저자 : John Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, Pieter Abbeel\n논문 링크 : https://arxiv.org/pdf/1502.05477.pdf\nProceeding : International Conference on Machine Learning (ICML) 2015\n정리 : 공민서, 김동민\n\n---\n\n# 1. 들어가며...\n\nTrust region policy optimization (TRPO)는 상당히 우수한 성능을 보여주는 policy gradient 기법으로 알려져 있습니다. 높은 차원의 action space를 가진 robot locomotion부터 action은 적지만 화면을 그대로 처리하여 플레이하기 때문에 control parameter가 매우 많은 Atari game까지 각 application에 세부적으로 hyperparameter들을 특화시키지 않아도 두루두루 좋은 성능을 나타내기 때문에 일반화성능이 매우 좋은 기법입니다. 이 TRPO에 대해서 알아보겠습니다.\n\n※TRPO를 매우 재밌게 설명한 Crazymuse AI의 [Youtube video](https://www.youtube.com/watch?v=CKaN5PgkSBc&t=90s)에서 일부 그림을 차용했습니다. 이 비디오를 시청하시는 것을 강력하게 추천합니다!\n\n<br>\n## 1.1 TRPO 흐름 잡기\n\nTRPO 논문은 많은 수식이 등장하여 이 수식들을 따라가다보면 큰 그림을 놓칠 수도 있습니다. 세부적인 내용을 살펴보기 전에 기존 연구에서 출발해서 TRPO로 어떻게 발전해나가는지 간략하게 살펴보겠습니다.\n\n### 1.1.1 Original Problem\n\n$$\\max\\_\\pi \\eta(\\pi)$$\n\n모든 강화학습이 그렇듯이 expected discounted reward를 최대화하는 policy를 찾는 문제로부터 출발합니다.\n\n### 1.1.2 Conservative policy iteration\n\n$$\\max L\\_\\pi(\\tilde\\pi) = \\eta(\\pi) + \\sum\\_s \\rho_\\pi(s)\\sum\\_a\\tilde\\pi(a\\vert s)A\\_\\pi(s,a)$$\n\n$\\eta(\\pi)$를 바로 최대화하는 것은 많은 경우 어렵습니다. $\\eta(\\pi)$의 성능향상을 보장하면서 policy를 update하는 conservative policy iteration 기법이 [Kakade와 Langford](http://www.cs.cmu.edu/~./jcl/papers/aoarl/Final.pdf)에 의하여 제안되었습니다. 이 기법을 이용하면 policy update가 성능을 향상시키는지는 못하더라도 최소한 성능을 악화시키지는 않는다는 것이 이론적으로 보장됩니다.\n\n### 1.1.3 Theorem 1 of TRPO\n\n$$\\max L\\_{\\pi\\_\\mathrm{old} }\\left(\\pi\\_\\mathrm{new}\\right) - \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2,\\quad\\left(\\alpha=D\\_\\mathrm{TV}^\\max\\left(\\pi\\_\\mathrm{old},\\pi\\_\\mathrm{new}\\right)\\right)$$\n\n기존의 conservative policy iteration은 과거 policy와 새로운 policy를 섞어서 사용해서 실용적이지 않다는 단점이 있었는데 이것을 보완하여 온전히 새로운 policy만으로 update할 수 있는 기법을 제안합니다. \n\n### 1.1.4 KL divergence version of Theorem 1\n\n$$\\max L\\_{\\pi}\\left(\\tilde\\pi\\right) - C\\cdot D_\\mathrm{KL}^\\max\\left(\\pi,\\tilde\\pi\\right),\\quad\\left(C = \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\right)$$\n\ndistance metric을 KL divergence로 바꿀 수 있습니다.\n\n### 1.1.5 Using parameterized policy\n\n$$\\max\\_\\theta L\\_{\\theta\\_\\mathrm{old} }(\\theta) - C\\cdot D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right)$$\n\n최적화문제를 더욱 편리하게 풀 수 있도록 낮은 dimension을 가지는 parameter들로 parameterized된 policy를 사용할 수 있습니다.\n\n### 1.1.6 Trust region constraint\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ }&D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right) \\leq \\delta\n\\end{align}$$\n\n1.1.5까지는 아직 conservative policy iteration을 약간 변형시킨 것입니다. policy를 update할 때 지나치게 많이 변하는 것을 방지하기 위하여 [trust region](https://en.wikipedia.org/wiki/Trust_region)을 constraint로 설정할 수 있습니다. 이 아이디어로 인해서 TRPO라는 명칭을 가지게 됩니다.\n\n### 1.1.7 Heuristic approximation\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ }&\\overline{D}\\_\\mathrm{KL}^{\\rho\\_\\mathrm{old} }\\left(\\theta\\_\\mathrm{old}, \\theta\\right) \\leq \\delta\n\\end{align}$$\n\n사실 1.1.6의 constraint는 모든 state에 대해서 성립해야 하기 때문에 문제를 매우 어렵게 만듭니다. 이것을 좀 더 다루기 쉽게 state distribution에 대한 평균을 취한 것으로 변형합니다.\n\n### 1.1.8 Monte Carlo simulation\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi\\_\\theta(a\\vert s)}{q(a\\vert s)}Q\\_{\\theta\\_\\mathrm{old} }(s,a)\\right] \\\\\\\\\n\\mathrm{s.t.\\ }&E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} }}\\left[D\\_\\mathrm{KL}\\left(\\pi\\_{\\theta\\_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi\\_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta\n\\end{align}$$\n\nSampling을 통한 계산이 가능하도록 식을 다시 표현할 수 있습니다. \n\n### 1.1.9 Efficiently solving TRPO\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &\\nabla\\_\\theta \\left. L\\_{\\theta\\_\\mathrm{old} }(\\theta)\\right\\vert \\_{\\theta=\\theta\\_\\mathrm{old} } \\left(\\theta - \\theta\\_\\mathrm{old}\\right) \\\\\\\\\n\\mathrm{s.t.\\ }&\\frac{1}{2}\\left(\\theta\\_\\mathrm{old} - \\theta \\right)^T A\\left(\\theta\\_\\mathrm{old}\\right)\\left(\\theta\\_\\mathrm{old} - \\theta \\right) \\leq \\delta \\\\\\\\\n&A\\_{ij} = \\frac{\\partial}{\\partial \\theta\\_i}\\frac{\\partial}{\\partial \\theta\\_j}\\overline{D}\\_\\mathrm{KL}^{\\rho\\_\\mathrm{old} }\\left(\\theta\\_\\mathrm{old}, \\theta\\right)\n\\end{align}$$\n\n문제를 효율적으로 풀기 위하여 approximation을 적용할 수 있습니다. objective function은 first order approximation, constraint는 quadratic approximation을 취하면 효율적으로 문제를 풀 수 있는 형태로 바뀌는데 이것을 natural gradient로 풀 수도 있고 conjugate gradient로 풀 수도 있습니다.\n\nTRPO는 이렇게 다양한 방법으로 문제를 변형한 것입니다! 이제 좀 더 자세히 살펴보겠습니다.\n\n<br><br>\n\n# 2. Preliminaries\n\n다음과 같은 파라미터를 가지는 infinite-horizon discounted Markov decision process (MDP)를 고려합니다. \n\n* $\\mathcal{S}$: finite set of states\n* $\\\\mathcal{A}$: finite set of actions\n* $P: \\mathcal{S}\\times\\mathcal{A}\\rightarrow\\mathbb{R}$: transition probability distribution\n* $r: \\mathcal{S}\\rightarrow\\mathbb{R}$: reward function\n* $\\rho\\_0:\\mathcal{S}\\rightarrow\\mathbb{R}$: distribution of the initial state $s\\_0$\n* $\\gamma\\in(0,1)$: discount factor\n* $\\eta(\\pi)=E\\_{s\\_0,a_0,\\ldots}\\left[\\sum\\_{t=0}^\\infty\\gamma^t r\\left(s\\_t\\right)\\right]$, where $s\\_0\\sim\\rho\\_0\\left(s\\_0\\right), a\\_t\\sim\\pi\\left(\\left.a\\_t \\right\\vert s\\_t\\right), s\\_{t+1}\\sim P\\left(\\left.s\\_{t+1}\\right\\vert s\\_t,a\\_t\\right)$\n* $Q\\_\\pi\\left(s\\_t,a\\_t\\right)=E\\_{ { \\color{red}{s\\_{t+1}} },a\\_{t+1},\\ldots}\\left[\\sum\\_{l=0}^\\infty \\gamma^l r\\left(s\\_{t+l}\\right)\\right]$: action value function\n* $V\\_\\pi\\left(s\\_t\\right)=E\\_{ { \\color{red}{a\\_t} }, s\\_{t+1},a\\_{t+1},\\ldots}\\left[\\sum\\_{l=0}^\\infty \\gamma^l r\\left(s\\_{t+l}\\right)\\right]$: value function (action value function과 expectation의 첨자가 다른 부분을 유의하세요.)\n* $A\\_\\pi(s,a) = Q\\_\\pi(s,a) - V\\_\\pi(s)$: advantage function\n\n<br>\n## 2.1 Useful identity [Kakade & Langford 2002]\n\n우리의 목표는 $\\eta\\(\\pi\\)$가 최대화되도록 만드는 것입니다. 하지만 $\\pi$의 변화에 따라 $\\eta$가 어떻게 변하는지 알아내는 것도 쉽지 않습니다. $\\pi$는 기존의 policy이고 $\\tilde\\pi$는 새로운 policy를 나타낸다고 할 때 $\\eta$와 policy update 사이에 다음과 같은 관계가 있다는 것이 밝혀졌습니다.\n\n**Lemma 1.** $\\eta\\left(\\tilde\\pi\\right) = \\eta(\\pi) + E\\_{s\\_{0},a\\_{0},\\ldots\\sim\\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t A\\_\\pi\\left(s\\_t,a\\_t\\right)\\right]$\n\n*Proof.* $A\\_\\pi(s,a)=E\\_{s'\\sim P\\left(s'\\vert s,a\\right)}\\left[r(s)+\\gamma V\\_\\pi\\left(s'\\right)-V\\_\\pi(s)\\right]$로 다시 표현할 수 있습니다. 표기의 편의를 위하여 $\\tau:=\\left(s\\_0,a\\_0,s\\_1,a\\_1,\\ldots\\right)$를 정의하겠습니다. 다음과 같은 수식 전개가 가능합니다.\n$$\n\\begin{align}\n&E\\_{\\tau\\vert \\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t A\\_\\pi\\left(s\\_t,a\\_t\\right)\\right] \\\\\\\\\n &= E\\_{\\tau\\vert \\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t \\left(r(s\\_t)+\\gamma V\\_\\pi\\left(s\\_{t+1}\\right)-V\\_\\pi(s\\_t)\\right)\\right] \\\\\\\\\n&= E\\_{\\tau\\vert \\tilde\\pi}\\left[\\left(\\sum\\_{t=0}^\\infty\\gamma^t r(s\\_t)\\right)+\\gamma V\\_\\pi\\left(s\\_{1}\\right)-V\\_\\pi(s\\_0)+\\gamma^2 V\\_\\pi\\left(s\\_{2}\\right)-\\gamma V\\_\\pi(s\\_1)+\\gamma^3 V\\_\\pi\\left(s\\_{3}\\right)-\\gamma^2 V\\_\\pi(s\\_2)+\\cdots\\right] \\\\\\\\\n&= E\\_{\\tau\\vert \\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t r(s\\_t)+\\color{red}{\\gamma V\\_\\pi\\left(s\\_{1}\\right)}-V\\_\\pi(s\\_0)+\\color{red}{\\gamma^2 V\\_\\pi\\left(s\\_{2}\\right)}-\\color{red}{\\gamma V\\_\\pi(s\\_1)}+\\cdots\\right] \\\\\\\\\n&= E\\_{\\tau\\vert \\tilde\\pi}\\left[-V\\_\\pi(s\\_0)+\\sum\\_{t=0}^\\infty\\gamma^t r(s\\_t)\\right] \\\\\\\\\n&\\overset{\\underset{\\mathrm{(a)} }{} }{=} -E\\_{\\color{red}{s\\_0}}\\left[V\\_\\pi(s\\_0)\\right]+E\\_{\\tau\\vert \\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t r(s\\_t)\\right] \\\\\\\\\n&=-\\eta(\\pi) + \\eta\\left(\\tilde\\pi\\right)\\\\\\\\\n&\\quad\\therefore \\eta\\left(\\tilde\\pi\\right) = \\eta(\\pi) + E\\_{s\\_{0},a\\_{0},\\ldots\\sim\\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t A\\_\\pi\\left(s\\_t,a\\_t\\right)\\right]\n\\end{align}\n$$\n위의 전개의 (a) 부분은 $V\\_\\pi\\left(s\\_0\\right)=E\\_{a\\_0,a\\_1,s\\_1,\\ldots}\\left[\\sum\\_{t=0}^\\infty\\gamma^l r\\left(s\\_l\\right)\\right]$이므로 $\\tau$ 중 많은 부분들이 이미 expectation이 취해진 값이므로 무시됩니다. 오직 $s\\_0$만 expectation이 취해지지 않았기 때문에 남아있습니다.\n\n**Lemma 1**은 새로운 policy와 기존의 policy 사이의 관계를 규정합니다. 다음과 같은 식을 정의하고 이것을 이용해서 **Lemma 1**을 변형시켜 봅시다.\n\n* $\\rho\\_\\pi(s) = P\\left(s\\_0=s\\right) + \\gamma P\\left(s\\_1=s\\right)\\ + \\gamma^2 P\\left(s\\_2=s\\right) + \\cdots $: (unnormalized) discounted visitation frequencies\n\n\n\n$$\n\\begin{align}\n\\eta\\left(\\tilde\\pi\\right) &= \\eta(\\pi) + \\sum\\_{t=0}^\\infty\\sum\\_{s}P\\left(\\left.s\\_t=s\\right\\vert \\tilde\\pi\\right)\\sum\\_a\\tilde\\pi(a\\vert s)\\gamma^t A\\_\\pi(s,a) \\\\\\\\\n&= \\eta(\\pi) + \\sum\\_{s}{\\color{red}{\\sum\\_{t=0}^\\infty \\gamma^t P\\left(\\left.s\\_t=s\\right\\vert \\tilde\\pi\\right)} } \\sum\\_a\\tilde\\pi(a\\vert s) A\\_\\pi(s,a) \\\\\\\\\n&= \\eta(\\pi) + \\sum\\_{s}{\\color{red}{\\rho\\_\\tilde\\pi(s)} }\\sum\\_a\\tilde\\pi(a\\vert s) A\\_\\pi(s,a) \\\\\\\\\n\\end{align}\n$$\n\n이 수식의 의미가 무엇일까요? 만약 $\\sum\\_a\\tilde\\pi(a\\vert s) A\\_\\pi(s,a) \\geq 0$이라면 $\\eta(\\tilde\\pi)$는 항상 $\\eta(\\pi)$보다 큽니다. 즉, policy를 업데이트함으로써 항상 개선됩니다. 즉, 이 수식을 통해서 항상 더 좋은 성능을 내는 policy update가 가져야 할 특징을 알 수 있습니다. 다음과 같은 deterministic policy가 있다고 합시다.\n\n$$\n\\tilde\\pi(s) = \\arg\\max_a A\\_\\pi(s,a)\n$$\n\n이 policy는 적어도 하나의 state-action pair에서 0보다 큰 값을 가지는 advantage가 있고 그 때의 확률이 0이 아니라면 항상 성능을 개선시킵니다. 어려운 점은 policy를 바꾸면 $\\rho$도 바뀐다는 점입니다. \n\n다음 그림을 봅시다. Starting Point에서 여러가지 경로를 거쳐서 Destination으로 갈 수 있습니다. 다른 policy를 이용하는 것은 다른 경로를 이용한 것입니다.\n\n[![policy_change](../../../../img/policy_change.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=16s)\n\n이 때 다른 policy를 이용함에 따라 state visitation frequency도 변하게 됩니다.\n\n[![state_visitation_change](../../../../img/state_visitation_change.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=36s)\n\n이로 인해서 policy를 최적화하는 것은 어려워집니다. 그래서 이러한 변화를 무시하는 다음과 같은 approximation을 취합니다.\n\n$$\n\\begin{align}\nL_\\pi\\left(\\tilde\\pi\\right) &= \\eta(\\pi) + \\sum\\_{s}{\\color{red}{\\rho\\_\\pi(s)} } \\sum\\_a\\tilde\\pi(a\\vert s) A\\_\\pi(s,a)\n\\end{align}\n$$\n\npolicy가 바뀌었음에도 이전의 state distribution을 계속 이용하는 것입니다. 이것은 policy의 변화가 크지 않다면 어느 정도 허용될 수 있을 것입니다. 그렇지만 얼마나 많은 변화가 허용될까요? 이것을 정하기 위해서 이용하는 것이 trust region입니다.\n\n<br>\n## 2.2 Conservative Policy Iteration\n\npolicy의 변화를 다루기 용이하게 하기 위해서 policy를 다음과 같이 파라미터를 이용해서 표현합시다.\n\n* $\\pi\\_\\theta(a\\vert s)$: parameterized policy\n\n$\\pi\\_\\theta$는 $\\theta$에 대하여 미분가능한 함수입니다. $L\\_\\pi\\left(\\tilde\\pi\\right)$을 $\\theta\\_0$에서 $\\eta(\\pi)$에 대한 first order approximation이라고 하면 다음 식이 성립합니다.\n\n$$\n\\begin{align}\nL\\_{\\pi\\_{\\theta\\_0} }\\left(\\pi\\_{\\theta_0}\\right) &= \\eta\\left(\\pi\\_{\\theta\\_0}\\right) \\\\\\\\\n\\nabla\\_\\theta \\left.L\\_{\\pi\\_{\\theta\\_0} }\\left(\\pi\\_{\\theta\\_0}\\right)\\right\\vert \\_{\\theta=\\theta\\_0} &= \\nabla\\_\\theta\\left.\\eta(\\pi\\_{\\theta\\_0})\\right\\vert \\_{\\theta=\\theta\\_0}\n\\end{align}\n$$\n\n이것의 의미는 $\\pi\\_{\\theta\\_0}$가 매우 작게 변한다면 $L\\_{\\pi\\_{\\theta\\_0} }$를 개선시키는 것이 $\\eta$를 개선시키는 것이라는 것입니다. 그러나 지금까지의 설명만으로는 $\\pi\\_{\\theta\\_0}$를 얼마나 작게 변화시켜야 할지에 대해서는 알 수 없습니다.\n\nKakade & Langford가 2002년 발표한 논문에서도 이것에 대해서 고민했습니다. 그 논문에서 *conservative policy iteration*이라는 기법을 제안합니다. 그 논문의 contribution은 다음과 같습니다.\n\n* $\\eta$ 개선의 lower bound를 제공\n* 기존의 policy를 $\\pi\\_\\mathrm{old}$라고 하고 $\\pi'$를 $\\pi'=\\arg\\max\\_\\{\\pi'}L\\_{\\pi\\_\\mathrm{old} }\\left(\\pi'\\right)$과 같이 정의할 때, 새로운 mixture policy $\\pi\\_\\mathrm{new}$를 다음과 같이 제안\n$$\n\\pi\\_\\mathrm{new}(a\\vert s) = (1-\\alpha)\\pi\\_\\mathrm{old}(a\\vert s) + \\alpha \\pi'(a\\vert s)\n$$\n\n그림으로 표현하면 다음과 같습니다.\n\n[![mixure_policy](../../../../img/mixure_policy.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=2m46s)\n\n* 다음과 같은 lower bound를 정의\n$$\n\\eta\\left(\\pi\\_\\mathrm{new}\\right) \\geq L\\_{\\pi\\_{\\theta\\_\\mathrm{old} }}\\left(\\pi\\_\\mathrm{new}\\right) - \\frac{2\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2,\\\\\\\\\n\\mathrm{where\\ }\\epsilon = \\max_s\\left\\vert E\\_{a\\sim\\pi'(a\\vert s)}\\left[A\\_\\pi(s,a)\\right]\\right\\vert \n$$\n\n하지만 mixture된 policy라는 것은 실용적이지 않습니다. \n\n<br><br>\n\n# 3. Monotonic Improvement Guarantee for General Stochastic Policies\n\n이전 장에서 설명한 lower bound는 오직 mixture policy에 대해서만 성립하고 더 많이 사용되는 stochastic policy에는 적용되지 않습니다. 따라서 stochastic policy를 이용할 수 있도록 개선할 필요가 있습니다. 아래와 같이 기존 수식에서 두 가지를 바꿈으로써 이것이 가능합니다.\n\n* $\\alpha\\rightarrow$ distance measure between $\\pi$ and $\\tilde\\pi$\n* constant $\\epsilon\\rightarrow\\max\\_{s,a}\\left\\vert A\\_\\pi(s,a)\\right\\vert $\n\n여기서 distance measure로 total variation divergence를 이용합니다. discrete porbability distribution $p$와 $q$에 대하여 다음과 같이 정의됩니다.\n$$\nD\\_\\mathrm{TV}(p\\parallel q) = \\frac{1}{2}\\sum\\_i\\left\\vert p\\_i - q\\_i\\right\\vert \n$$\n\n이것을 이용하여 $D\\_\\mathrm{TV}^\\max$를 다음과 같이 정의합니다.\n$$\nD\\_\\mathrm{TV}^\\max(\\pi\\parallel \\tilde\\pi) = \\max_s D\\_\\mathrm{TV}\\left(\\pi(\\cdot\\vert s)\\parallel\\tilde\\pi(\\cdot\\vert s)\\right)\n$$\n\n그림으로 표현하면 다음과 같습니다.\n\n[![tvd](../../../../img/tvd.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=3m15s)\n\n이것을 이용하여 다음과 같은 관계식을 얻을 수 있습니다.\n\n**Theorem 1.** Let $\\alpha=D\\_\\mathrm{TV}^\\max(\\pi\\parallel \\tilde\\pi)$. Then the following bound holds:\n$$\n\\begin{align}\n\\eta\\left(\\pi\\_\\mathrm{new}\\right) &\\geq L\\_{\\pi\\_\\mathrm{old} }\\left(\\pi\\_\\mathrm{new}\\right) - \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2, \\\\\\\\\n\\mathrm{where\\ } \\epsilon&= \\max\\_{s,a}\\left\\vert A\\_\\pi(s,a)\\right\\vert \n\\end{align}\n$$\n<!--*Proof.* TBD.-->\n\n또다른 distance metric으로 아래 그림과 같은 KL divergence가 있습니다.\n(그런데 왜 하필 KL divergence로 바꿀까요? 논문의 뒤쪽에서 계산효율을 위해서 conjugate gradient method를 이용하는데 이를 위해서 바꾼게 아닐까 싶습니다. Wasserstein distance 같은 다른 방향으로 발전시킬 수도 있을 것 같습니다. Schulmann이 아마 해봤겠죠?^^a)\n\n[![kld](../../../../img/kld.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=3m34s)\n\ntotal variation divergence와 KL divergence 사이에는 다음과 같은 관계가 있습니다.\n$$\nD\\_\\mathrm{TV}(p\\parallel q)^2 \\leq D\\_\\mathrm{KL}(p\\parallel q)\n$$\n\n다음 수식을 정의합니다.\n$$\nD\\_\\mathrm{KL}^\\max(\\pi\\parallel \\tilde\\pi) = \\max_s D\\_\\mathrm{KL}\\left(\\pi(\\cdot\\vert s)\\parallel\\tilde\\pi(\\cdot\\vert s)\\right)\n$$\n\n**Theorem 1**을 이용하여 다음과 같은 수식이 성립함을 알 수 있습니다.\n$$\n\\begin{align}\n\\eta\\left(\\tilde\\pi\\right) &\\geq L\\_{\\pi}\\left(\\tilde\\pi\\right) - C\\cdot D\\_\\mathrm{KL}^\\max(\\pi, \\tilde\\pi), \\\\\\\\\n\\mathrm{where\\ } C&= \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\n\\end{align}\n$$\n\n이러한 policy imporvement bound를 기반으로 다음과 같은 approximate policy iteration 알고리듬을 고안해낼 수 있습니다.\n\n**Algorithm 1** Policy iteration algorithm guaranteeing non-decreasing expected return $\\eta$\n> Initialize $\\pi\\_0$   \n> **for** $i=0,1,2,\\ldots$ until convergence **do**   \n> $\\quad$Compute all advantage values $A\\_{\\pi\\_i}(s,a)$    \n> $\\quad$Solve the contstrained optimization problem    \n> $\\quad\\pi\\_{i+1}=\\arg\\max\\_\\pi\\left[L\\_{\\pi\\_i} - CD\\_\\mathrm{KL}^\\max\\left(\\pi\\_i,\\pi\\right)\\right]$     \n> $\\quad\\quad$where $C = 4\\epsilon\\gamma / (1-\\gamma)^2$    \n> $\\quad\\quad$and $L\\_{\\pi\\_i}\\left(\\pi\\right) = \\eta(\\pi\\_i) + \\sum\\_{s}\\rho\\_{\\pi\\_i}(s)\\sum\\_a\\pi(a\\vert s) A\\_{\\pi\\_i}(s,a)$    \n> **end for**\n\n**Algorithm 1**은 advantage를 정확하게 계산할 수 있다고 가정하고 있습니다. 이 알고리듬은  monotonical한 성능 증가($\\eta(\\pi\\_0)\\leq\\eta(\\pi\\_1)\\leq\\cdots$)를 한다는 것을 다음과 같이 보일 수 있습니다. $M\\_i(\\pi)=L\\_{\\pi\\_i}(\\pi) - CD\\_\\mathrm{KL}^\\max\\left(\\pi\\_i,\\pi\\right)$라고 합시다.\n\n$$\n\\begin{align}\n\\eta \\left(\\pi\\_{i+1}\\right) &\\geq M\\_i\\left(\\pi\\_{i+1}\\right)\\\\\\\\\n\\eta \\left(\\pi\\_{i}\\right) &= M\\_i\\left(\\pi\\_{i}\\right) \\\\\\\\\n\\eta \\left(\\pi\\_{i+1}\\right) - \\eta \\left(\\pi\\_{i}\\right) &\\geq M\\_i\\left(\\pi\\_{i+1}\\right) - M\\_i\\left(\\pi\\_{i}\\right)\n\\end{align}\n$$\n\n위 수식과 같이 매 iteration 마다 $M\\_i$를 최대화함으로써, $\\eta$가 감소하지 않는다는 것을 보장할 수 있습니다. 이와 같은 타입의 알고리듬을 [minorization-maximization (MM) algorithm](https://www.jstor.org/stable/pdf/27643496.pdf?casa_token=0qHamcl60WoAAAAA:-fkZ9JcA_nrY3-zbCUpqvPOgcAMgw7Gr96MajCZg2byHf8m5GU1KTSxyJJcBy1lPZbBTZVCjHHUXilh4k-iuwF91Wka4B5qdltC1IR2qMWk8q1FoV6__)이라고 합니다. EM 알고리듬도 이 타입의 알고리듬 중 하나입니다.\n\n**Algorithm 1**은 아래 그림과 같이 동작합니다.\n\n[![surrogate](../../../../img/surrogate.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=3m52s)\n\n\n$M\\_i$는 $\\pi\\_i$일 때 equality가 되는 $\\eta$에 대한  surrogate function입니다. TRPO는 이 surrogate function을 최대화하고 KL divergence를 penalty가 아닌 constraint로 두는 알고리듬입니다.\n\n<br><br>\n\n# 4. Optimization of Parameterized Policies\n\n표기의 편의를 위해 다음과 같이 notation들을 더 간략하게 정의합니다.\n\n* $\\eta(\\theta):=\\eta\\left(\\pi\\_\\theta\\right)$\n* $L\\_\\theta\\left(\\tilde\\theta\\right):=L\\_{\\pi\\_\\theta}\\left(\\pi\\_{\\tilde\\theta}\\right)$\n* $D\\_\\mathrm{KL}\\left(\\theta\\parallel\\tilde\\theta\\right):=D\\_\\mathrm{KL}\\left(\\pi\\_\\theta\\parallel\\pi_{\\tilde\\theta}\\right)$\n* $\\theta\\_\\mathrm{old}$: previous policy parameter\n\n<br>\n## 4.1 Trust Region Policy Optimization\n\n이전 장의 중요 결과를 위의 notation으로 다시 표기하면 아래와 같습니다.\n\n$$\n\\eta\\left(\\theta\\right) \\geq L\\_{\\theta\\_\\mathrm{old} }\\left(\\theta\\right) - C\\cdot D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right)\n$$\n\n$\\eta$의 성능 향상을 보장하기 위해서 그것의 lower bound를 최대화할 수 있습니다.\n\n$$\n\\max\\_\\theta \\left[L\\_{\\theta\\_\\mathrm{old} }\\left(\\theta\\right) - C D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right)\\right]\n$$\n\n이 최적화 문제는 step size를 매우 작게 해야 올바른 동작을 합니다. 위에서 살펴봤듯이 first order approximation이기 때문입니다. 좀 더 큰 step size를 가질 수 있도록 이 최적화 문제를 trust region constraint를 도입하여 다음과 같이 바꿉니다.\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ }&D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right) \\leq \\delta\n\\end{align}$$\n\n이 최적화문제의 constraint는 모든 state space에 대해서 성립해야 합니다. 또한 maximum값을 매번 찾아야 합니다. state가 많은 경우 constraint의 수가 매우 많아져서 문제를 풀기 어렵게 만듭니다. constraint의 수를 줄이기 위하여 다음과 같은 avergae KL divergence를 이용하는 heuristic approximation을 취합니다. 이것이 최선의 방법은 아닐 수 있지만 실용적인 방법입니다.\n\n$$\n\\overline D\\_\\mathrm{KL}^{\\rho}\\left(\\theta\\_1, \\theta\\_2\\right):=E\\_{s\\sim\\rho}\\left[D\\_\\mathrm{KL}\\left(\\pi\\_{\\theta_1}(\\cdot\\vert s)\\parallel\\pi\\_{\\theta\\_2}(\\cdot\\vert s)\\right)\\right]\n$$\n\n이것을 기반으로 다음과 같은 최적화 문제를 풀 수 있습니다.\n\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ }&\\overline{D}\\_\\mathrm{KL}^{\\rho\\_\\mathrm{old} }\\left(\\theta\\_\\mathrm{old}, \\theta\\right) \\leq \\delta\n\\end{align}$$\n\n\n아래 그림처럼 step size에 대해서 고려할 수 있습니다.\n\n[![heuristic_approx](../../../../img/heuristic_approx.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=4m35s)\n\n<br><br>\n\n# 5. Sample-Based Estimation of the Objective and Constraint\n\n실용적인 알고리듬을 만들려고 하는 노력은 아직 끝나지 않았습니다. 이제 앞의 알고리듬을 sample-based estimation 즉, Monte Carlo estimation을 할 수 있도록 바꿔보겠습니다. sampling을 편하게 할 수 있도록 아래와 같이 바꿔줍니다.\n\n* $\\sum\\_s \\rho\\_{\\theta\\_\\mathrm{old} }(s)[\\ldots]\\rightarrow\\frac{1}{1-\\gamma}E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} }}[\\ldots]$\n* $A\\_{\\theta\\_\\mathrm{old} }\\rightarrow Q\\_{\\theta\\_\\mathrm{old} }$\n* $\\sum\\_a\\pi\\_{\\theta\\_\\mathrm{old} }(a\\vert s)A\\_{\\theta\\_\\mathrm{old} } \\rightarrow E\\_{a\\sim q}\\left[\\frac{\\pi\\_\\theta(a\\vert s)}{q(a\\vert s)}A\\_{\\theta\\_\\mathrm{old} }\\right]$\n\n이러한 변화를 그림처럼 도식화할 수 있습니다.\n\n[![sample-based](../../../../img/sample-based.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=5m31s)\n\n한 가지 짚고 넘어가야 할 점은 action sampling을 할 때 importance sampling을 사용한다는 것입니다.\n\n[![importance_sampling](../../../../img/importance_sampling.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=5m53s)\n\n바뀐 최적화문제는 아래와 같습니다.\n\n- **Equation (14).**\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi\\_\\theta(a\\vert s)}{q(a\\vert s)}Q\\_{\\theta\\_\\mathrm{old} }(s,a)\\right] \\\\\\\\\n\\mathrm{s.t.\\ }&E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} }}\\left[D\\_\\mathrm{KL}\\left(\\pi\\_{\\theta\\_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi\\_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta\n\\end{align}\n$$\n\n이 때 sampling하는 두 가지 방법이 있습니다.\n\n<br>\n## 5.1 Single Path\n\n*single path*는 개별 trajectory들을 이용하는 방법입니다.\n\n[![single](../../../../img/single.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=6m15s)\n\n<br>\n## 5.2 Vine\n\n*vine*은 한 state에서 rollout을 이용하여 여러 action을 수행하는 방법입니다.\n\n[![vine1](../../../../img/vine1.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=6m32s)\n\n[![vine2](../../../../img/vine2.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=6m49s)\n\nestimation의 variance를 낮출 수 있지만 계산량이 많고 한 state에서 여러 action을 수행할 수 있어야 하기 때문에 현실적인 문제에 적용하기에는 어려움이 있습니다.\n\n\n<br><br>\n\n# 6. Practical Algorithm\n\n앞서 single-path, vine 샘플링을 사용하는 두가지 방식의 policy optimization 알고리즘을 살펴봤습니다. 실용적인 알고리듬은 아래의 과정을 반복해서 수행합니다.\n\n1) Q-values의 몬테카를로 추정을 통해 state-action 쌍 집합을 single path 또는 vine 과정을 통해 수집함.\n\n2) 샘플 평균으로, (14)식의 목적함수와 제약식을 추정함\n + **Equation (14).**\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi\\_\\theta(a\\vert s)}{q(a\\vert s)}Q\\_{\\theta\\_\\mathrm{old} }(s,a)\\right] \\\\\\\\\n\\mathrm{s.t.\\ }&E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} }}\\left[D\\_\\mathrm{KL}\\left(\\pi\\_{\\theta\\_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi\\_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta\n\\end{align}\n$$\n\n3) policy parameter vector인 $\\theta$를 업데이트 하면서 제약조건이 있는 최적화 문제를 근사적으로 풂. 본 논문에서는 gradient를 직접 계산하는 것보다는 약간 더 계산량이 있는 line search와 conjugate gradient algorithm을 사용했습니다.\n\n3)에 대해서,  gradient의 covariance matrix를 사용하지 않고 KL divergence의 Hessian을 해석적으로 계산하여 Fisher Information Matrix를 구성했습니다.\n\n다시말해서\n\n$\\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial}{\\partial \\theta_i} \\log \\pi_{\\theta}(a_n\\vert s_n) \\frac{\\partial}{\\partial \\theta_j} \\log \\pi_{\\theta}(a_n\\vert s_n)$ 대신 $\\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_j} D_{KL}(\\pi_{\\theta_{old} }(\\cdot \\vert  s_n) \\vert \\vert  \\pi_{\\theta}(\\cdot \\vert  s_n))$를 계산한 것입니다.\n\n이 analytic estimator는 Hessian이나 trajectories의 모든 gradient를 저장하지 않아도 되기 때문에 대규모 환경을 고려할 경우 계산 상 이점이 있습니다. 좀 더 자세하게 설명한 Appendix C 부분을 살펴보겠습니다.\n\n\n<br>\n## Appendix C. Efficiently Solving the Trust-Region Constrained Optimization Problem\n\n우리가 풀고자 하는 식은 다음과 같습니다.\n\n$$\n\\begin{align}\n\\max\\quad &L(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ } &\\overline{D}\\_{KL}(\\theta\\_\\mathrm{old}, \\theta) \\leq \\delta\n\\end{align}\n$$\n\n이 최적화 문제를 풀기 위하여 우리는 gradient 기법을 이용할 것입니다. gradient 기법은 결국 update할 방향과 크기를 결정하는 문제로 생각할 수 있습니다. 이에 따라 아래와 같은 두가지 과정으로 진행합니다.\n\n1) 탐색 방향을 계산, 목적함수의 first-order approximation(선형근사)와 제약식의 quadratic approximation(2차 근사).\n\n2) 이동거리 계산을 위해 해당 방향으로 line search 수행.\n\n탐색 방향은 $Ax = g$ 수식을 근사적으로 풀어서 구합니다. 여기서 $A$ 는 Fisher Information Matrix이고, 이것은 KL divergence 제약식의 2차 근사 $\\overline{D}\\_\\mathrm{KL} (\\theta\\_\\mathrm{old}, \\theta) \\approx \\frac{1}{2}(\\theta - \\theta\\_\\mathrm{old})^T A(\\theta - \\theta\\_\\mathrm{old})$를 푸는 것입니다. 여기서 $A\\_{ij} = \\frac{\\partial}{\\partial \\theta\\_i} \\frac{\\partial}{\\partial \\theta\\_j} \\overline{D}\\_\\mathrm{KL}(\\theta\\_\\mathrm{old}, \\theta)$입니다.\n\n논문에서 이 부분에 대한 설명이 약간 부족해서 좀 더 내용을 추가하자면, $\\frac{1}{2}x^T A x - x^T g + b = 0$과 같은 quadratic equation의 해는 1차 derivative를 0으로 만드는 $x$를 찾음으로써 알 수 있습니다. 위에서 우리는 목적함수를 1차 근사시키고 제약조건은 2차 근사를 시킴으로써 quadratic equation 형태로 변형했기 때문에 같은 방식으로 풀 수 있습니다. 따라서 $g$는 1차근사시킨 목적함수의 계수가 될 것입니다.\n\n대규모 문제에서 $A$ 혹은 $A^{-1}$ 을 계산하는 것은 엄청난 계산량이 필요합니다. 하지만 conjugate gradient algorithm을 잘 이용하면 전체 $A$행렬(FIM)을 계산하지 않아도 $Ax=g$ 를 근사적으로 해결할 수 있게 해줍니다.\n\n탐색 방향 $s \\approx A^{-1}g$과 더불어 최대스텝길이(step size) $\\beta$도 계산할 필요가 있습니다.\n\n즉, $\\delta = \\overline{D}\\_\\mathrm{KL} \\approx \\frac{1}{2}(\\beta s)^T A(\\beta s) = \\frac{1}{2}\\beta^2s^TAs$ 이고, \n\n$\\beta = \\sqrt{2\\delta / s^T As}$가 됩니다. 이때 $\\delta$는 KL divergence의 boundary term 입니다.\n\n$s^T As$ 항은 Hessian-vector product로 계산할 수 있고, conjugate gradient 과정에서 계산됩니다.\n\n마지막으로, surrogate objective와 KL divergence 제약식을 위해 다음과 같은 함수를 정의하고 line search를 사용합니다.\n\n$$\nL\\_{\\theta\\_\\mathrm{old} }(\\theta) - \\chi [\\overline{D}\\_{KL}(\\theta\\_\\mathrm{old}, \\theta) \\leq \\delta]\n$$\n\n$\\chi[\\cdot]$ 함수는 []안의 조건이 맞으면 0이고 틀리면 무한으로 발산하는 함수입니다.\n\n위에서 계산한 $\\beta$ 의 최대 값부터 시작해서 목적함수가 개선될때까지 exponential하게 줄여 나갑니다. line search가 없었다면 이 알고리즘은 매우 큰 스텝으로 계산될 것이고 심각한 성능 저하가 발생할 것입니다.\n\n(여기서 살짝 예고를 하자면, 나중에 이 KL 기반 기법을 개선한 더 단순한 방법이 제안됩니다. PPO라고...)\n\n<br><br>\n\n# 7. Connections with Prior Work\n\nNatural Policy Gradient는 $L$의 선형 근사와 $\\overline D\\_\\mathrm{KL}$ 제약식을 2차근사 하는 아래의 식의 special case입니다.\n\n- Equation (17).\n\n$L\\_{\\pi}(\\tilde\\pi) = \\eta(\\pi) + \\underset{s}{\\sum}\\rho\\_{\\pi}(s) \\underset{a}{\\sum}\\tilde\\pi(a\\vert s)A\\_\\pi(s,a)$\n\n$\\underset{\\theta}{\\max}\\quad \\[\\nabla\\_{\\theta}L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\ \\rvert\\_{\\theta=\\theta\\_\\mathrm{old} } \\cdot (\\theta - \\theta\\_\\mathrm{old})]$\n\n$\\mathrm{s.t.}\\quad\\frac{1}{2}(\\theta\\_\\mathrm{old} - \\theta)^{T} A(\\theta\\_\\mathrm{old})(\\theta\\_\\mathrm{old}-\\theta) \\leq \\delta$\n\n$\\mathrm{where\\ }A(\\theta_{old})_{ij} = $\n\n$\\Large \\frac{\\partial}{\\partial \\theta\\_{i} } \\frac{\\partial}{\\partial \\theta\\_{j} }\nE\\_{s \\sim \\rho\\_{\\pi} }[D\\_\\mathrm{KL}(\\pi(\\cdot\\rvert s, \\theta\\_\\mathrm{old})  \\rvert\\rvert \\pi(\\cdot \\rvert s, \\theta))] \\rvert\\_{\\theta=\\theta\\_\\mathrm{old} }$\n\n업데이트 식은 다음과 같습니다.\n\n$\\theta\\_\\mathrm{new} = \\theta\\_\\mathrm{old} +  \\color{Red}{\\frac{1}{\\lambda} } A(\\theta\\_\\mathrm{old})^{-1}\\nabla\\_{\\theta}L(\\theta) \\rvert\\_{\\theta=\\theta\\_\\mathrm{old} }$\n\n여기서 step size인 $\\frac{1}{\\lambda}$ 일반적으로 알고리즘의 파라미터로 취급되지만 TRPO는 각 업데이트 마다 제약식으로 사용한다는 점이 다릅니다. 사소한 차이처럼 보이지만, 큰 차원을 다루는 실험에서 성능을 크게 향상시켰습니다.\n\n또한 $L^2$ 제약식(혹은 페널티) 를 사용하면 표준 Policy Gradient 업데이트 식을 얻었습니다.\n\n- Equation (18).\n\n$\\underset{\\theta}{\\max}\\quad[\\nabla\\_{\\theta} L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\rvert\\_{\\theta=\\theta\\_\\mathrm{old} } \\cdot (\\theta - \\theta\\_\\mathrm{old})]$\n\n$\\mathrm{s.t.}\\quad \\frac{1}{2} \\vert\\vert \\theta - \\theta\\_\\mathrm{old} \\vert\\vert^2 \\leq \\delta$\n\n\n\n$L\\_{\\pi}(\\tilde\\pi) = \\eta(\\pi) + \\underset{s}{\\sum}\\rho\\_{\\pi}(s) \\underset{a}{\\sum}\\tilde\\pi(a\\vert s)A\\_\\pi(s,a) $를 이용해서 제약조건없이 $\\underset{\\pi}{\\max}\\quad L\\_{\\pi\\_\\mathrm{old} }(\\pi)$를 풀면\nPolicy Iteration update를 하는 것과 같습니다.\n\n<br><br>\n\n# 8. Experiments\n\n- 다음과 같은 3 가지 궁금증을 해결하기 위하여 실험을 설계하였습니다.\n\n    1. Single-path 와 vine 의 성능면에서 특징을 알고싶음.\n\n    2. Fixed penalty coefficient(NPG) 보다 fixed KL divergence를 사용하는 것(TRPO)으로    \n    변경한 것이 얼마나 뚜렷한 차이를 보일지, 성능 면에서 어떤 영향을 미치는지 알고싶음.\n\n    3. TRPO가 큰 스케일의 문제를 해결할 수 있는지, 기존에 연구/적용되어왔던 방법들과\n    성능, 계산시간, 샘플 복잡도 면에서 비교하고 싶음.\n\n\n- 1, 2의 궁금증에 대해서 실험 환경에 single path, vine을 비롯한 여러 사전방법들을 함께 실험하였습니다.\n\n- 3에 대해선, robotic locomotion과 Atari game에 TRPO를 실험하였습니다.\n\n\n<br>\n## 8.1 Simulated Robotic Locomotion\n\n- MuJoCo 시뮬레이터를 활용해 로봇 locomotion 실험 진행하였습니다.\n- Robot State: positions and velocities, joint torques\n\n![](https://i.imgur.com/bs5ATS3.png)\n\n- 수영, 점프, 걷기 행동을 학습시키고자 합니다.\n\n    - 수영: 10 dimensional state space , reward: $r(x,u)=v_x - 10^{-5}\\vert \\vert u\\vert \\vert ^2$, quadratic penalty.\n\n    - 점프: 12 dimensional state space, episode의 끝은 높이와 각도 threshol로 점프를 판단, non-terminal state에 대해 +1 보상 추가.\n\n    - 걷기: 18 dimensional state space, 뜀뛰는 듯한 보법(점프에이전트)보다 부드러운 보법을 유도하기위해 페널티를 추가함.\n\n**Detailed Experiment Setup and used parameters, used network model**\n![](https://i.imgur.com/zgnsbw6.png)\n\n![](https://i.imgur.com/FqdWC53.png)\n\nequation (12) : $\\max\\quad L\\_{\\theta\\_\\mathrm{old}(\\theta)}, \\ \\ \\mathrm{s.t.\\ }\\overline{D}\\_\\mathrm{KL}^{\\rho\\_{\\theta\\_\\mathrm{old} }}(\\theta\\_\\mathrm{old}, \\theta) \\leq \\delta$\n\n이 실험에서 $\\delta = 0.01$입니다.\n\n- 비교에 사용된 모델들\n\n    - TRPO Single-path\n    - TRPO vine\n    - CEM(cross-entropy method)\n    - CMA(covariance matrix adaptation)\n    - NPG (Lagrange Multiplier penalty coefficien를 사용하는 것이 차이점)\n    - Empirical FIM\n    - maximum KL(cartpole)\n\n![](https://i.imgur.com/wSqolvS.png)\n\n- 제안한 single path, vine 모두 task를 잘 학습하는 것을 확인할 수 있습니다.\n\n- 페널티를 고정하는 것보다, KL divergenc를 제약하는 것이 step size를 선택하는 부분에서 Robust합니다.\n\n- CEM/CMA는 derivative free 알고리즘으로, 샘플 복잡도가 네트워크 파라미터수 만큼 확장되어 high dimension 문제에서는 속도와 성능이 약한 모습을 보입니다.\n\n- maxKL은 제안방법보다 느린 학습성능을 보입니다.\n\nTRPO video link: https://www.youtube.com/watch?v=jeid0wIrSn4\n\n- 사전지식이 적은 상태에서 여러 액션을 학습하는 실험으로 TRPO의 성능을 보입니다.\n\n- 지금까지의 균형이나 걸음 같은 개념을 명시적으로 인코딩한 robotic locomotion에서의 사전 연구들과 차이를 보였습니다.\n\n<br>\n## 8.2 Playing Games from Images\n\n- 부분관찰가능하며 복잡한 observation인 환경에서 TRPO를 평가하기위해 raw image를 입력으로 하는 아타리게임 환경에서 실험하였습니다.\n\n* Challenging point :\n    - High dimensional space\n    - delayed reward\n    - non-stationary image (Enduro는 배경 이미지가 바뀌고, 반전처리가 됨)\n    - complex action sequence (Q*bert는 21개의 다른 발판에서 점프 해야함)\n\n- ALE(Arcade Learning Environment) 에서 테스트\n- DQN과 전처리 방식은 동일\n    210 x 160 pixel -> gray-scaling & down-sampling -> 110 x 84  -> crop -> 84 x 84\n\n- 2 convolution layer ( 16 channels) , stride 2, FC layer with 20 units\n\n![](https://i.imgur.com/NJBC69d.png) \n\n![](https://i.imgur.com/wTe1OEW.png)\n\n![](https://i.imgur.com/j22VtNQ.png)\n\n- 30시간 동안 500 iteration 학습을 하였습니다.\n\n- DQN과 MCST를 사용한 모델(UCC-I), 사람이 플레이한 것(Human) 과의 비교하였습니다.\n\n- 이전 방법들에 비해 압도적인 점수를 기록하진 못했으나, 다양한 게임에서 적절한 게임 기록하였습니다.\n\n- 이전 방법들과 다른 점은, 특정 task에 초점을 맞춰 설계하지않았는데 robotics나 game playing Task에도 적절한 성능을 내는 점에서 **TRPO의 일반화 성능**을 확인하였습니다.\n\n<br><br>\n\n# 9. Discussion\n\n- Trust Region Policy Optimization 을 제안하였습니다.\n\n- KL Divergence 페널티로 $\\eta(\\pi)$를 최적화하는 알고리즘이 monotonically improve 함을 증명하였습니다.\n\n- Locomotion 도메인에서 여러 행동(수영, 걷기, 점프)을 제어하는 controller를 학습하였습니다.\n\n- Robotics와 게임 실험을 결합해, 시각정보와 센서데이터를 사용하는 로봇제어 정책을 학습시킬 수 있는 가능성을 보았습니다.\n\n- 샘플의 복잡도를 상당히 줄일 수 있어 실제상황에 적용가능성을 보았습니다.\n\n<br><br>\n\n# 처음으로\n\n## [PG Travel Guide](https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/)\n\n<br>\n\n# 이전으로\n\n## [NPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/25/4_npg/)\n\n## [NPG Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/tnpg.py)\n\n<br>\n\n# 다음으로\n\n## [TRPO Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/trpo_gae.py)\n\n## [GAE 여행하기](https://reinforcement-learning-kr.github.io/2018/06/23/6_gae/)\n","slug":"5_trpo","published":1,"updated":"2018-09-08T11:00:20.610Z","_id":"cjkj6tysn000pj715uqgp4pw0","comments":1,"layout":"post","photos":[],"link":"","content":"<center> <img src=\"https://www.dropbox.com/s/o7cjcn0e17mpizr/Screen%20Shot%202018-07-18%20at%201.14.22%20AM.png?dl=1\" width=\"700\"> </center>\n\n<p>논문 저자 : John Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, Pieter Abbeel<br>논문 링크 : <a href=\"https://arxiv.org/pdf/1502.05477.pdf\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/pdf/1502.05477.pdf</a><br>Proceeding : International Conference on Machine Learning (ICML) 2015<br>정리 : 공민서, 김동민</p>\n<hr>\n<h1 id=\"1-들어가며…\"><a href=\"#1-들어가며…\" class=\"headerlink\" title=\"1. 들어가며…\"></a>1. 들어가며…</h1><p>Trust region policy optimization (TRPO)는 상당히 우수한 성능을 보여주는 policy gradient 기법으로 알려져 있습니다. 높은 차원의 action space를 가진 robot locomotion부터 action은 적지만 화면을 그대로 처리하여 플레이하기 때문에 control parameter가 매우 많은 Atari game까지 각 application에 세부적으로 hyperparameter들을 특화시키지 않아도 두루두루 좋은 성능을 나타내기 때문에 일반화성능이 매우 좋은 기법입니다. 이 TRPO에 대해서 알아보겠습니다.</p>\n<p>※TRPO를 매우 재밌게 설명한 Crazymuse AI의 <a href=\"https://www.youtube.com/watch?v=CKaN5PgkSBc&amp;t=90s\" target=\"_blank\" rel=\"noopener\">Youtube video</a>에서 일부 그림을 차용했습니다. 이 비디오를 시청하시는 것을 강력하게 추천합니다!</p>\n<p><br></p>\n<h2 id=\"1-1-TRPO-흐름-잡기\"><a href=\"#1-1-TRPO-흐름-잡기\" class=\"headerlink\" title=\"1.1 TRPO 흐름 잡기\"></a>1.1 TRPO 흐름 잡기</h2><p>TRPO 논문은 많은 수식이 등장하여 이 수식들을 따라가다보면 큰 그림을 놓칠 수도 있습니다. 세부적인 내용을 살펴보기 전에 기존 연구에서 출발해서 TRPO로 어떻게 발전해나가는지 간략하게 살펴보겠습니다.</p>\n<h3 id=\"1-1-1-Original-Problem\"><a href=\"#1-1-1-Original-Problem\" class=\"headerlink\" title=\"1.1.1 Original Problem\"></a>1.1.1 Original Problem</h3><p>$$\\max_\\pi \\eta(\\pi)$$</p>\n<p>모든 강화학습이 그렇듯이 expected discounted reward를 최대화하는 policy를 찾는 문제로부터 출발합니다.</p>\n<h3 id=\"1-1-2-Conservative-policy-iteration\"><a href=\"#1-1-2-Conservative-policy-iteration\" class=\"headerlink\" title=\"1.1.2 Conservative policy iteration\"></a>1.1.2 Conservative policy iteration</h3><p>$$\\max L_\\pi(\\tilde\\pi) = \\eta(\\pi) + \\sum_s \\rho_\\pi(s)\\sum_a\\tilde\\pi(a\\vert s)A_\\pi(s,a)$$</p>\n<p>$\\eta(\\pi)$를 바로 최대화하는 것은 많은 경우 어렵습니다. $\\eta(\\pi)$의 성능향상을 보장하면서 policy를 update하는 conservative policy iteration 기법이 <a href=\"http://www.cs.cmu.edu/~./jcl/papers/aoarl/Final.pdf\" target=\"_blank\" rel=\"noopener\">Kakade와 Langford</a>에 의하여 제안되었습니다. 이 기법을 이용하면 policy update가 성능을 향상시키는지는 못하더라도 최소한 성능을 악화시키지는 않는다는 것이 이론적으로 보장됩니다.</p>\n<h3 id=\"1-1-3-Theorem-1-of-TRPO\"><a href=\"#1-1-3-Theorem-1-of-TRPO\" class=\"headerlink\" title=\"1.1.3 Theorem 1 of TRPO\"></a>1.1.3 Theorem 1 of TRPO</h3><p>$$\\max L_{\\pi_\\mathrm{old} }\\left(\\pi_\\mathrm{new}\\right) - \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2,\\quad\\left(\\alpha=D_\\mathrm{TV}^\\max\\left(\\pi_\\mathrm{old},\\pi_\\mathrm{new}\\right)\\right)$$</p>\n<p>기존의 conservative policy iteration은 과거 policy와 새로운 policy를 섞어서 사용해서 실용적이지 않다는 단점이 있었는데 이것을 보완하여 온전히 새로운 policy만으로 update할 수 있는 기법을 제안합니다. </p>\n<h3 id=\"1-1-4-KL-divergence-version-of-Theorem-1\"><a href=\"#1-1-4-KL-divergence-version-of-Theorem-1\" class=\"headerlink\" title=\"1.1.4 KL divergence version of Theorem 1\"></a>1.1.4 KL divergence version of Theorem 1</h3><p>$$\\max L_{\\pi}\\left(\\tilde\\pi\\right) - C\\cdot D_\\mathrm{KL}^\\max\\left(\\pi,\\tilde\\pi\\right),\\quad\\left(C = \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\right)$$</p>\n<p>distance metric을 KL divergence로 바꿀 수 있습니다.</p>\n<h3 id=\"1-1-5-Using-parameterized-policy\"><a href=\"#1-1-5-Using-parameterized-policy\" class=\"headerlink\" title=\"1.1.5 Using parameterized policy\"></a>1.1.5 Using parameterized policy</h3><p>$$\\max_\\theta L_{\\theta_\\mathrm{old} }(\\theta) - C\\cdot D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right)$$</p>\n<p>최적화문제를 더욱 편리하게 풀 수 있도록 낮은 dimension을 가지는 parameter들로 parameterized된 policy를 사용할 수 있습니다.</p>\n<h3 id=\"1-1-6-Trust-region-constraint\"><a href=\"#1-1-6-Trust-region-constraint\" class=\"headerlink\" title=\"1.1.6 Trust region constraint\"></a>1.1.6 Trust region constraint</h3><p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;L_{\\theta_\\mathrm{old} }(\\theta) \\\\<br>\\mathrm{s.t.\\ }&amp;D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right) \\leq \\delta<br>\\end{align}$$</p>\n<p>1.1.5까지는 아직 conservative policy iteration을 약간 변형시킨 것입니다. policy를 update할 때 지나치게 많이 변하는 것을 방지하기 위하여 <a href=\"https://en.wikipedia.org/wiki/Trust_region\" target=\"_blank\" rel=\"noopener\">trust region</a>을 constraint로 설정할 수 있습니다. 이 아이디어로 인해서 TRPO라는 명칭을 가지게 됩니다.</p>\n<h3 id=\"1-1-7-Heuristic-approximation\"><a href=\"#1-1-7-Heuristic-approximation\" class=\"headerlink\" title=\"1.1.7 Heuristic approximation\"></a>1.1.7 Heuristic approximation</h3><p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;L_{\\theta_\\mathrm{old} }(\\theta) \\\\<br>\\mathrm{s.t.\\ }&amp;\\overline{D}_\\mathrm{KL}^{\\rho_\\mathrm{old} }\\left(\\theta_\\mathrm{old}, \\theta\\right) \\leq \\delta<br>\\end{align}$$</p>\n<p>사실 1.1.6의 constraint는 모든 state에 대해서 성립해야 하기 때문에 문제를 매우 어렵게 만듭니다. 이것을 좀 더 다루기 쉽게 state distribution에 대한 평균을 취한 것으로 변형합니다.</p>\n<h3 id=\"1-1-8-Monte-Carlo-simulation\"><a href=\"#1-1-8-Monte-Carlo-simulation\" class=\"headerlink\" title=\"1.1.8 Monte Carlo simulation\"></a>1.1.8 Monte Carlo simulation</h3><p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}Q_{\\theta_\\mathrm{old} }(s,a)\\right] \\\\<br>\\mathrm{s.t.\\ }&amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} }}\\left[D_\\mathrm{KL}\\left(\\pi_{\\theta_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta<br>\\end{align}$$</p>\n<p>Sampling을 통한 계산이 가능하도록 식을 다시 표현할 수 있습니다. </p>\n<h3 id=\"1-1-9-Efficiently-solving-TRPO\"><a href=\"#1-1-9-Efficiently-solving-TRPO\" class=\"headerlink\" title=\"1.1.9 Efficiently solving TRPO\"></a>1.1.9 Efficiently solving TRPO</h3><p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;\\nabla_\\theta \\left. L_{\\theta_\\mathrm{old} }(\\theta)\\right\\vert _{\\theta=\\theta_\\mathrm{old} } \\left(\\theta - \\theta_\\mathrm{old}\\right) \\\\<br>\\mathrm{s.t.\\ }&amp;\\frac{1}{2}\\left(\\theta_\\mathrm{old} - \\theta \\right)^T A\\left(\\theta_\\mathrm{old}\\right)\\left(\\theta_\\mathrm{old} - \\theta \\right) \\leq \\delta \\\\<br>&amp;A_{ij} = \\frac{\\partial}{\\partial \\theta_i}\\frac{\\partial}{\\partial \\theta_j}\\overline{D}_\\mathrm{KL}^{\\rho_\\mathrm{old} }\\left(\\theta_\\mathrm{old}, \\theta\\right)<br>\\end{align}$$</p>\n<p>문제를 효율적으로 풀기 위하여 approximation을 적용할 수 있습니다. objective function은 first order approximation, constraint는 quadratic approximation을 취하면 효율적으로 문제를 풀 수 있는 형태로 바뀌는데 이것을 natural gradient로 풀 수도 있고 conjugate gradient로 풀 수도 있습니다.</p>\n<p>TRPO는 이렇게 다양한 방법으로 문제를 변형한 것입니다! 이제 좀 더 자세히 살펴보겠습니다.</p>\n<p><br><br></p>\n<h1 id=\"2-Preliminaries\"><a href=\"#2-Preliminaries\" class=\"headerlink\" title=\"2. Preliminaries\"></a>2. Preliminaries</h1><p>다음과 같은 파라미터를 가지는 infinite-horizon discounted Markov decision process (MDP)를 고려합니다. </p>\n<ul>\n<li>$\\mathcal{S}$: finite set of states</li>\n<li>$\\mathcal{A}$: finite set of actions</li>\n<li>$P: \\mathcal{S}\\times\\mathcal{A}\\rightarrow\\mathbb{R}$: transition probability distribution</li>\n<li>$r: \\mathcal{S}\\rightarrow\\mathbb{R}$: reward function</li>\n<li>$\\rho_0:\\mathcal{S}\\rightarrow\\mathbb{R}$: distribution of the initial state $s_0$</li>\n<li>$\\gamma\\in(0,1)$: discount factor</li>\n<li>$\\eta(\\pi)=E_{s_0,a_0,\\ldots}\\left[\\sum_{t=0}^\\infty\\gamma^t r\\left(s_t\\right)\\right]$, where $s_0\\sim\\rho_0\\left(s_0\\right), a_t\\sim\\pi\\left(\\left.a_t \\right\\vert s_t\\right), s_{t+1}\\sim P\\left(\\left.s_{t+1}\\right\\vert s_t,a_t\\right)$</li>\n<li>$Q_\\pi\\left(s_t,a_t\\right)=E_{ { \\color{red}{s_{t+1}} },a_{t+1},\\ldots}\\left[\\sum_{l=0}^\\infty \\gamma^l r\\left(s_{t+l}\\right)\\right]$: action value function</li>\n<li>$V_\\pi\\left(s_t\\right)=E_{ { \\color{red}{a_t} }, s_{t+1},a_{t+1},\\ldots}\\left[\\sum_{l=0}^\\infty \\gamma^l r\\left(s_{t+l}\\right)\\right]$: value function (action value function과 expectation의 첨자가 다른 부분을 유의하세요.)</li>\n<li>$A_\\pi(s,a) = Q_\\pi(s,a) - V_\\pi(s)$: advantage function</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-1-Useful-identity-Kakade-amp-Langford-2002\"><a href=\"#2-1-Useful-identity-Kakade-amp-Langford-2002\" class=\"headerlink\" title=\"2.1 Useful identity [Kakade &amp; Langford 2002]\"></a>2.1 Useful identity [Kakade &amp; Langford 2002]</h2><p>우리의 목표는 $\\eta(\\pi)$가 최대화되도록 만드는 것입니다. 하지만 $\\pi$의 변화에 따라 $\\eta$가 어떻게 변하는지 알아내는 것도 쉽지 않습니다. $\\pi$는 기존의 policy이고 $\\tilde\\pi$는 새로운 policy를 나타낸다고 할 때 $\\eta$와 policy update 사이에 다음과 같은 관계가 있다는 것이 밝혀졌습니다.</p>\n<p><strong>Lemma 1.</strong> $\\eta\\left(\\tilde\\pi\\right) = \\eta(\\pi) + E_{s_{0},a_{0},\\ldots\\sim\\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t A_\\pi\\left(s_t,a_t\\right)\\right]$</p>\n<p><em>Proof.</em> $A_\\pi(s,a)=E_{s’\\sim P\\left(s’\\vert s,a\\right)}\\left[r(s)+\\gamma V_\\pi\\left(s’\\right)-V_\\pi(s)\\right]$로 다시 표현할 수 있습니다. 표기의 편의를 위하여 $\\tau:=\\left(s_0,a_0,s_1,a_1,\\ldots\\right)$를 정의하겠습니다. 다음과 같은 수식 전개가 가능합니다.<br>$$<br>\\begin{align}<br>&amp;E_{\\tau\\vert \\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t A_\\pi\\left(s_t,a_t\\right)\\right] \\\\<br> &amp;= E_{\\tau\\vert \\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t \\left(r(s_t)+\\gamma V_\\pi\\left(s_{t+1}\\right)-V_\\pi(s_t)\\right)\\right] \\\\<br>&amp;= E_{\\tau\\vert \\tilde\\pi}\\left[\\left(\\sum_{t=0}^\\infty\\gamma^t r(s_t)\\right)+\\gamma V_\\pi\\left(s_{1}\\right)-V_\\pi(s_0)+\\gamma^2 V_\\pi\\left(s_{2}\\right)-\\gamma V_\\pi(s_1)+\\gamma^3 V_\\pi\\left(s_{3}\\right)-\\gamma^2 V_\\pi(s_2)+\\cdots\\right] \\\\<br>&amp;= E_{\\tau\\vert \\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t r(s_t)+\\color{red}{\\gamma V_\\pi\\left(s_{1}\\right)}-V_\\pi(s_0)+\\color{red}{\\gamma^2 V_\\pi\\left(s_{2}\\right)}-\\color{red}{\\gamma V_\\pi(s_1)}+\\cdots\\right] \\\\<br>&amp;= E_{\\tau\\vert \\tilde\\pi}\\left[-V_\\pi(s_0)+\\sum_{t=0}^\\infty\\gamma^t r(s_t)\\right] \\\\<br>&amp;\\overset{\\underset{\\mathrm{(a)} }{} }{=} -E_{\\color{red}{s_0}}\\left[V_\\pi(s_0)\\right]+E_{\\tau\\vert \\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t r(s_t)\\right] \\\\<br>&amp;=-\\eta(\\pi) + \\eta\\left(\\tilde\\pi\\right)\\\\<br>&amp;\\quad\\therefore \\eta\\left(\\tilde\\pi\\right) = \\eta(\\pi) + E_{s_{0},a_{0},\\ldots\\sim\\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t A_\\pi\\left(s_t,a_t\\right)\\right]<br>\\end{align}<br>$$<br>위의 전개의 (a) 부분은 $V_\\pi\\left(s_0\\right)=E_{a_0,a_1,s_1,\\ldots}\\left[\\sum_{t=0}^\\infty\\gamma^l r\\left(s_l\\right)\\right]$이므로 $\\tau$ 중 많은 부분들이 이미 expectation이 취해진 값이므로 무시됩니다. 오직 $s_0$만 expectation이 취해지지 않았기 때문에 남아있습니다.</p>\n<p><strong>Lemma 1</strong>은 새로운 policy와 기존의 policy 사이의 관계를 규정합니다. 다음과 같은 식을 정의하고 이것을 이용해서 <strong>Lemma 1</strong>을 변형시켜 봅시다.</p>\n<ul>\n<li>$\\rho_\\pi(s) = P\\left(s_0=s\\right) + \\gamma P\\left(s_1=s\\right)\\ + \\gamma^2 P\\left(s_2=s\\right) + \\cdots $: (unnormalized) discounted visitation frequencies</li>\n</ul>\n<p>$$<br>\\begin{align}<br>\\eta\\left(\\tilde\\pi\\right) &amp;= \\eta(\\pi) + \\sum_{t=0}^\\infty\\sum_{s}P\\left(\\left.s_t=s\\right\\vert \\tilde\\pi\\right)\\sum_a\\tilde\\pi(a\\vert s)\\gamma^t A_\\pi(s,a) \\\\<br>&amp;= \\eta(\\pi) + \\sum_{s}{\\color{red}{\\sum_{t=0}^\\infty \\gamma^t P\\left(\\left.s_t=s\\right\\vert \\tilde\\pi\\right)} } \\sum_a\\tilde\\pi(a\\vert s) A_\\pi(s,a) \\\\<br>&amp;= \\eta(\\pi) + \\sum_{s}{\\color{red}{\\rho_\\tilde\\pi(s)} }\\sum_a\\tilde\\pi(a\\vert s) A_\\pi(s,a) \\\\<br>\\end{align}<br>$$</p>\n<p>이 수식의 의미가 무엇일까요? 만약 $\\sum_a\\tilde\\pi(a\\vert s) A_\\pi(s,a) \\geq 0$이라면 $\\eta(\\tilde\\pi)$는 항상 $\\eta(\\pi)$보다 큽니다. 즉, policy를 업데이트함으로써 항상 개선됩니다. 즉, 이 수식을 통해서 항상 더 좋은 성능을 내는 policy update가 가져야 할 특징을 알 수 있습니다. 다음과 같은 deterministic policy가 있다고 합시다.</p>\n<p>$$<br>\\tilde\\pi(s) = \\arg\\max_a A_\\pi(s,a)<br>$$</p>\n<p>이 policy는 적어도 하나의 state-action pair에서 0보다 큰 값을 가지는 advantage가 있고 그 때의 확률이 0이 아니라면 항상 성능을 개선시킵니다. 어려운 점은 policy를 바꾸면 $\\rho$도 바뀐다는 점입니다. </p>\n<p>다음 그림을 봅시다. Starting Point에서 여러가지 경로를 거쳐서 Destination으로 갈 수 있습니다. 다른 policy를 이용하는 것은 다른 경로를 이용한 것입니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=16s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/policy_change.png\" alt=\"policy_change\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>이 때 다른 policy를 이용함에 따라 state visitation frequency도 변하게 됩니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=36s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/state_visitation_change.png\" alt=\"state_visitation_change\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>이로 인해서 policy를 최적화하는 것은 어려워집니다. 그래서 이러한 변화를 무시하는 다음과 같은 approximation을 취합니다.</p>\n<p>$$<br>\\begin{align}<br>L_\\pi\\left(\\tilde\\pi\\right) &amp;= \\eta(\\pi) + \\sum_{s}{\\color{red}{\\rho_\\pi(s)} } \\sum_a\\tilde\\pi(a\\vert s) A_\\pi(s,a)<br>\\end{align}<br>$$</p>\n<p>policy가 바뀌었음에도 이전의 state distribution을 계속 이용하는 것입니다. 이것은 policy의 변화가 크지 않다면 어느 정도 허용될 수 있을 것입니다. 그렇지만 얼마나 많은 변화가 허용될까요? 이것을 정하기 위해서 이용하는 것이 trust region입니다.</p>\n<p><br></p>\n<h2 id=\"2-2-Conservative-Policy-Iteration\"><a href=\"#2-2-Conservative-Policy-Iteration\" class=\"headerlink\" title=\"2.2 Conservative Policy Iteration\"></a>2.2 Conservative Policy Iteration</h2><p>policy의 변화를 다루기 용이하게 하기 위해서 policy를 다음과 같이 파라미터를 이용해서 표현합시다.</p>\n<ul>\n<li>$\\pi_\\theta(a\\vert s)$: parameterized policy</li>\n</ul>\n<p>$\\pi_\\theta$는 $\\theta$에 대하여 미분가능한 함수입니다. $L_\\pi\\left(\\tilde\\pi\\right)$을 $\\theta_0$에서 $\\eta(\\pi)$에 대한 first order approximation이라고 하면 다음 식이 성립합니다.</p>\n<p>$$<br>\\begin{align}<br>L_{\\pi_{\\theta_0} }\\left(\\pi_{\\theta_0}\\right) &amp;= \\eta\\left(\\pi_{\\theta_0}\\right) \\\\<br>\\nabla_\\theta \\left.L_{\\pi_{\\theta_0} }\\left(\\pi_{\\theta_0}\\right)\\right\\vert _{\\theta=\\theta_0} &amp;= \\nabla_\\theta\\left.\\eta(\\pi_{\\theta_0})\\right\\vert _{\\theta=\\theta_0}<br>\\end{align}<br>$$</p>\n<p>이것의 의미는 $\\pi_{\\theta_0}$가 매우 작게 변한다면 $L_{\\pi_{\\theta_0} }$를 개선시키는 것이 $\\eta$를 개선시키는 것이라는 것입니다. 그러나 지금까지의 설명만으로는 $\\pi_{\\theta_0}$를 얼마나 작게 변화시켜야 할지에 대해서는 알 수 없습니다.</p>\n<p>Kakade &amp; Langford가 2002년 발표한 논문에서도 이것에 대해서 고민했습니다. 그 논문에서 <em>conservative policy iteration</em>이라는 기법을 제안합니다. 그 논문의 contribution은 다음과 같습니다.</p>\n<ul>\n<li>$\\eta$ 개선의 lower bound를 제공</li>\n<li>기존의 policy를 $\\pi_\\mathrm{old}$라고 하고 $\\pi’$를 $\\pi’=\\arg\\max_{\\pi’}L_{\\pi_\\mathrm{old} }\\left(\\pi’\\right)$과 같이 정의할 때, 새로운 mixture policy $\\pi_\\mathrm{new}$를 다음과 같이 제안<br>$$<br>\\pi_\\mathrm{new}(a\\vert s) = (1-\\alpha)\\pi_\\mathrm{old}(a\\vert s) + \\alpha \\pi’(a\\vert s)<br>$$</li>\n</ul>\n<p>그림으로 표현하면 다음과 같습니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=2m46s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/mixure_policy.png\" alt=\"mixure_policy\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<ul>\n<li>다음과 같은 lower bound를 정의<br>$$<br>\\eta\\left(\\pi_\\mathrm{new}\\right) \\geq L_{\\pi_{\\theta_\\mathrm{old} }}\\left(\\pi_\\mathrm{new}\\right) - \\frac{2\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2,\\\\<br>\\mathrm{where\\ }\\epsilon = \\max_s\\left\\vert E_{a\\sim\\pi’(a\\vert s)}\\left[A_\\pi(s,a)\\right]\\right\\vert<br>$$</li>\n</ul>\n<p>하지만 mixture된 policy라는 것은 실용적이지 않습니다. </p>\n<p><br><br></p>\n<h1 id=\"3-Monotonic-Improvement-Guarantee-for-General-Stochastic-Policies\"><a href=\"#3-Monotonic-Improvement-Guarantee-for-General-Stochastic-Policies\" class=\"headerlink\" title=\"3. Monotonic Improvement Guarantee for General Stochastic Policies\"></a>3. Monotonic Improvement Guarantee for General Stochastic Policies</h1><p>이전 장에서 설명한 lower bound는 오직 mixture policy에 대해서만 성립하고 더 많이 사용되는 stochastic policy에는 적용되지 않습니다. 따라서 stochastic policy를 이용할 수 있도록 개선할 필요가 있습니다. 아래와 같이 기존 수식에서 두 가지를 바꿈으로써 이것이 가능합니다.</p>\n<ul>\n<li>$\\alpha\\rightarrow$ distance measure between $\\pi$ and $\\tilde\\pi$</li>\n<li>constant $\\epsilon\\rightarrow\\max_{s,a}\\left\\vert A_\\pi(s,a)\\right\\vert $</li>\n</ul>\n<p>여기서 distance measure로 total variation divergence를 이용합니다. discrete porbability distribution $p$와 $q$에 대하여 다음과 같이 정의됩니다.<br>$$<br>D_\\mathrm{TV}(p\\parallel q) = \\frac{1}{2}\\sum_i\\left\\vert p_i - q_i\\right\\vert<br>$$</p>\n<p>이것을 이용하여 $D_\\mathrm{TV}^\\max$를 다음과 같이 정의합니다.<br>$$<br>D_\\mathrm{TV}^\\max(\\pi\\parallel \\tilde\\pi) = \\max_s D_\\mathrm{TV}\\left(\\pi(\\cdot\\vert s)\\parallel\\tilde\\pi(\\cdot\\vert s)\\right)<br>$$</p>\n<p>그림으로 표현하면 다음과 같습니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=3m15s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/tvd.png\" alt=\"tvd\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>이것을 이용하여 다음과 같은 관계식을 얻을 수 있습니다.</p>\n<p><strong>Theorem 1.</strong> Let $\\alpha=D_\\mathrm{TV}^\\max(\\pi\\parallel \\tilde\\pi)$. Then the following bound holds:<br>$$<br>\\begin{align}<br>\\eta\\left(\\pi_\\mathrm{new}\\right) &amp;\\geq L_{\\pi_\\mathrm{old} }\\left(\\pi_\\mathrm{new}\\right) - \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2, \\\\<br>\\mathrm{where\\ } \\epsilon&amp;= \\max_{s,a}\\left\\vert A_\\pi(s,a)\\right\\vert<br>\\end{align}<br>$$<br><!--*Proof.* TBD.--></p>\n<p>또다른 distance metric으로 아래 그림과 같은 KL divergence가 있습니다.<br>(그런데 왜 하필 KL divergence로 바꿀까요? 논문의 뒤쪽에서 계산효율을 위해서 conjugate gradient method를 이용하는데 이를 위해서 바꾼게 아닐까 싶습니다. Wasserstein distance 같은 다른 방향으로 발전시킬 수도 있을 것 같습니다. Schulmann이 아마 해봤겠죠?^^a)</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=3m34s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/kld.png\" alt=\"kld\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>total variation divergence와 KL divergence 사이에는 다음과 같은 관계가 있습니다.<br>$$<br>D_\\mathrm{TV}(p\\parallel q)^2 \\leq D_\\mathrm{KL}(p\\parallel q)<br>$$</p>\n<p>다음 수식을 정의합니다.<br>$$<br>D_\\mathrm{KL}^\\max(\\pi\\parallel \\tilde\\pi) = \\max_s D_\\mathrm{KL}\\left(\\pi(\\cdot\\vert s)\\parallel\\tilde\\pi(\\cdot\\vert s)\\right)<br>$$</p>\n<p><strong>Theorem 1</strong>을 이용하여 다음과 같은 수식이 성립함을 알 수 있습니다.<br>$$<br>\\begin{align}<br>\\eta\\left(\\tilde\\pi\\right) &amp;\\geq L_{\\pi}\\left(\\tilde\\pi\\right) - C\\cdot D_\\mathrm{KL}^\\max(\\pi, \\tilde\\pi), \\\\<br>\\mathrm{where\\ } C&amp;= \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}<br>\\end{align}<br>$$</p>\n<p>이러한 policy imporvement bound를 기반으로 다음과 같은 approximate policy iteration 알고리듬을 고안해낼 수 있습니다.</p>\n<p><strong>Algorithm 1</strong> Policy iteration algorithm guaranteeing non-decreasing expected return $\\eta$</p>\n<blockquote>\n<p>Initialize $\\pi_0$<br><strong>for</strong> $i=0,1,2,\\ldots$ until convergence <strong>do</strong><br>$\\quad$Compute all advantage values $A_{\\pi_i}(s,a)$<br>$\\quad$Solve the contstrained optimization problem<br>$\\quad\\pi_{i+1}=\\arg\\max_\\pi\\left[L_{\\pi_i} - CD_\\mathrm{KL}^\\max\\left(\\pi_i,\\pi\\right)\\right]$<br>$\\quad\\quad$where $C = 4\\epsilon\\gamma / (1-\\gamma)^2$<br>$\\quad\\quad$and $L_{\\pi_i}\\left(\\pi\\right) = \\eta(\\pi_i) + \\sum_{s}\\rho_{\\pi_i}(s)\\sum_a\\pi(a\\vert s) A_{\\pi_i}(s,a)$<br><strong>end for</strong></p>\n</blockquote>\n<p><strong>Algorithm 1</strong>은 advantage를 정확하게 계산할 수 있다고 가정하고 있습니다. 이 알고리듬은  monotonical한 성능 증가($\\eta(\\pi_0)\\leq\\eta(\\pi_1)\\leq\\cdots$)를 한다는 것을 다음과 같이 보일 수 있습니다. $M_i(\\pi)=L_{\\pi_i}(\\pi) - CD_\\mathrm{KL}^\\max\\left(\\pi_i,\\pi\\right)$라고 합시다.</p>\n<p>$$<br>\\begin{align}<br>\\eta \\left(\\pi_{i+1}\\right) &amp;\\geq M_i\\left(\\pi_{i+1}\\right)\\\\<br>\\eta \\left(\\pi_{i}\\right) &amp;= M_i\\left(\\pi_{i}\\right) \\\\<br>\\eta \\left(\\pi_{i+1}\\right) - \\eta \\left(\\pi_{i}\\right) &amp;\\geq M_i\\left(\\pi_{i+1}\\right) - M_i\\left(\\pi_{i}\\right)<br>\\end{align}<br>$$</p>\n<p>위 수식과 같이 매 iteration 마다 $M_i$를 최대화함으로써, $\\eta$가 감소하지 않는다는 것을 보장할 수 있습니다. 이와 같은 타입의 알고리듬을 <a href=\"https://www.jstor.org/stable/pdf/27643496.pdf?casa_token=0qHamcl60WoAAAAA:-fkZ9JcA_nrY3-zbCUpqvPOgcAMgw7Gr96MajCZg2byHf8m5GU1KTSxyJJcBy1lPZbBTZVCjHHUXilh4k-iuwF91Wka4B5qdltC1IR2qMWk8q1FoV6__\" target=\"_blank\" rel=\"noopener\">minorization-maximization (MM) algorithm</a>이라고 합니다. EM 알고리듬도 이 타입의 알고리듬 중 하나입니다.</p>\n<p><strong>Algorithm 1</strong>은 아래 그림과 같이 동작합니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=3m52s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/surrogate.png\" alt=\"surrogate\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>$M_i$는 $\\pi_i$일 때 equality가 되는 $\\eta$에 대한  surrogate function입니다. TRPO는 이 surrogate function을 최대화하고 KL divergence를 penalty가 아닌 constraint로 두는 알고리듬입니다.</p>\n<p><br><br></p>\n<h1 id=\"4-Optimization-of-Parameterized-Policies\"><a href=\"#4-Optimization-of-Parameterized-Policies\" class=\"headerlink\" title=\"4. Optimization of Parameterized Policies\"></a>4. Optimization of Parameterized Policies</h1><p>표기의 편의를 위해 다음과 같이 notation들을 더 간략하게 정의합니다.</p>\n<ul>\n<li>$\\eta(\\theta):=\\eta\\left(\\pi_\\theta\\right)$</li>\n<li>$L_\\theta\\left(\\tilde\\theta\\right):=L_{\\pi_\\theta}\\left(\\pi_{\\tilde\\theta}\\right)$</li>\n<li>$D_\\mathrm{KL}\\left(\\theta\\parallel\\tilde\\theta\\right):=D_\\mathrm{KL}\\left(\\pi_\\theta\\parallel\\pi_{\\tilde\\theta}\\right)$</li>\n<li>$\\theta_\\mathrm{old}$: previous policy parameter</li>\n</ul>\n<p><br></p>\n<h2 id=\"4-1-Trust-Region-Policy-Optimization\"><a href=\"#4-1-Trust-Region-Policy-Optimization\" class=\"headerlink\" title=\"4.1 Trust Region Policy Optimization\"></a>4.1 Trust Region Policy Optimization</h2><p>이전 장의 중요 결과를 위의 notation으로 다시 표기하면 아래와 같습니다.</p>\n<p>$$<br>\\eta\\left(\\theta\\right) \\geq L_{\\theta_\\mathrm{old} }\\left(\\theta\\right) - C\\cdot D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right)<br>$$</p>\n<p>$\\eta$의 성능 향상을 보장하기 위해서 그것의 lower bound를 최대화할 수 있습니다.</p>\n<p>$$<br>\\max_\\theta \\left[L_{\\theta_\\mathrm{old} }\\left(\\theta\\right) - C D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right)\\right]<br>$$</p>\n<p>이 최적화 문제는 step size를 매우 작게 해야 올바른 동작을 합니다. 위에서 살펴봤듯이 first order approximation이기 때문입니다. 좀 더 큰 step size를 가질 수 있도록 이 최적화 문제를 trust region constraint를 도입하여 다음과 같이 바꿉니다.</p>\n<p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;L_{\\theta_\\mathrm{old} }(\\theta) \\\\<br>\\mathrm{s.t.\\ }&amp;D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right) \\leq \\delta<br>\\end{align}$$</p>\n<p>이 최적화문제의 constraint는 모든 state space에 대해서 성립해야 합니다. 또한 maximum값을 매번 찾아야 합니다. state가 많은 경우 constraint의 수가 매우 많아져서 문제를 풀기 어렵게 만듭니다. constraint의 수를 줄이기 위하여 다음과 같은 avergae KL divergence를 이용하는 heuristic approximation을 취합니다. 이것이 최선의 방법은 아닐 수 있지만 실용적인 방법입니다.</p>\n<p>$$<br>\\overline D_\\mathrm{KL}^{\\rho}\\left(\\theta_1, \\theta_2\\right):=E_{s\\sim\\rho}\\left[D_\\mathrm{KL}\\left(\\pi_{\\theta_1}(\\cdot\\vert s)\\parallel\\pi_{\\theta_2}(\\cdot\\vert s)\\right)\\right]<br>$$</p>\n<p>이것을 기반으로 다음과 같은 최적화 문제를 풀 수 있습니다.</p>\n<p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;L_{\\theta_\\mathrm{old} }(\\theta) \\\\<br>\\mathrm{s.t.\\ }&amp;\\overline{D}_\\mathrm{KL}^{\\rho_\\mathrm{old} }\\left(\\theta_\\mathrm{old}, \\theta\\right) \\leq \\delta<br>\\end{align}$$</p>\n<p>아래 그림처럼 step size에 대해서 고려할 수 있습니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=4m35s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/heuristic_approx.png\" alt=\"heuristic_approx\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p><br><br></p>\n<h1 id=\"5-Sample-Based-Estimation-of-the-Objective-and-Constraint\"><a href=\"#5-Sample-Based-Estimation-of-the-Objective-and-Constraint\" class=\"headerlink\" title=\"5. Sample-Based Estimation of the Objective and Constraint\"></a>5. Sample-Based Estimation of the Objective and Constraint</h1><p>실용적인 알고리듬을 만들려고 하는 노력은 아직 끝나지 않았습니다. 이제 앞의 알고리듬을 sample-based estimation 즉, Monte Carlo estimation을 할 수 있도록 바꿔보겠습니다. sampling을 편하게 할 수 있도록 아래와 같이 바꿔줍니다.</p>\n<ul>\n<li>$\\sum_s \\rho_{\\theta_\\mathrm{old} }(s)[\\ldots]\\rightarrow\\frac{1}{1-\\gamma}E_{s\\sim\\rho_{\\theta_\\mathrm{old} }}[\\ldots]$</li>\n<li>$A_{\\theta_\\mathrm{old} }\\rightarrow Q_{\\theta_\\mathrm{old} }$</li>\n<li>$\\sum_a\\pi_{\\theta_\\mathrm{old} }(a\\vert s)A_{\\theta_\\mathrm{old} } \\rightarrow E_{a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}A_{\\theta_\\mathrm{old} }\\right]$</li>\n</ul>\n<p>이러한 변화를 그림처럼 도식화할 수 있습니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=5m31s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/sample-based.png\" alt=\"sample-based\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>한 가지 짚고 넘어가야 할 점은 action sampling을 할 때 importance sampling을 사용한다는 것입니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=5m53s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/importance_sampling.png\" alt=\"importance_sampling\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>바뀐 최적화문제는 아래와 같습니다.</p>\n<ul>\n<li><strong>Equation (14).</strong></li>\n</ul>\n<p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}Q_{\\theta_\\mathrm{old} }(s,a)\\right] \\\\<br>\\mathrm{s.t.\\ }&amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} }}\\left[D_\\mathrm{KL}\\left(\\pi_{\\theta_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta<br>\\end{align}<br>$$</p>\n<p>이 때 sampling하는 두 가지 방법이 있습니다.</p>\n<p><br></p>\n<h2 id=\"5-1-Single-Path\"><a href=\"#5-1-Single-Path\" class=\"headerlink\" title=\"5.1 Single Path\"></a>5.1 Single Path</h2><p><em>single path</em>는 개별 trajectory들을 이용하는 방법입니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=6m15s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/single.png\" alt=\"single\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p><br></p>\n<h2 id=\"5-2-Vine\"><a href=\"#5-2-Vine\" class=\"headerlink\" title=\"5.2 Vine\"></a>5.2 Vine</h2><p><em>vine</em>은 한 state에서 rollout을 이용하여 여러 action을 수행하는 방법입니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=6m32s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/vine1.png\" alt=\"vine1\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=6m49s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/vine2.png\" alt=\"vine2\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>estimation의 variance를 낮출 수 있지만 계산량이 많고 한 state에서 여러 action을 수행할 수 있어야 하기 때문에 현실적인 문제에 적용하기에는 어려움이 있습니다.</p>\n<p><br><br></p>\n<h1 id=\"6-Practical-Algorithm\"><a href=\"#6-Practical-Algorithm\" class=\"headerlink\" title=\"6. Practical Algorithm\"></a>6. Practical Algorithm</h1><p>앞서 single-path, vine 샘플링을 사용하는 두가지 방식의 policy optimization 알고리즘을 살펴봤습니다. 실용적인 알고리듬은 아래의 과정을 반복해서 수행합니다.</p>\n<p>1) Q-values의 몬테카를로 추정을 통해 state-action 쌍 집합을 single path 또는 vine 과정을 통해 수집함.</p>\n<p>2) 샘플 평균으로, (14)식의 목적함수와 제약식을 추정함</p>\n<ul>\n<li><strong>Equation (14).</strong><br>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}Q_{\\theta_\\mathrm{old} }(s,a)\\right] \\\\<br>\\mathrm{s.t.\\ }&amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} }}\\left[D_\\mathrm{KL}\\left(\\pi_{\\theta_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta<br>\\end{align}<br>$$</li>\n</ul>\n<p>3) policy parameter vector인 $\\theta$를 업데이트 하면서 제약조건이 있는 최적화 문제를 근사적으로 풂. 본 논문에서는 gradient를 직접 계산하는 것보다는 약간 더 계산량이 있는 line search와 conjugate gradient algorithm을 사용했습니다.</p>\n<p>3)에 대해서,  gradient의 covariance matrix를 사용하지 않고 KL divergence의 Hessian을 해석적으로 계산하여 Fisher Information Matrix를 구성했습니다.</p>\n<p>다시말해서</p>\n<p>$\\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial}{\\partial \\theta_i} \\log \\pi_{\\theta}(a_n\\vert s_n) \\frac{\\partial}{\\partial \\theta_j} \\log \\pi_{\\theta}(a_n\\vert s_n)$ 대신 $\\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_j} D_{KL}(\\pi_{\\theta_{old} }(\\cdot \\vert  s_n) \\vert \\vert  \\pi_{\\theta}(\\cdot \\vert  s_n))$를 계산한 것입니다.</p>\n<p>이 analytic estimator는 Hessian이나 trajectories의 모든 gradient를 저장하지 않아도 되기 때문에 대규모 환경을 고려할 경우 계산 상 이점이 있습니다. 좀 더 자세하게 설명한 Appendix C 부분을 살펴보겠습니다.</p>\n<p><br></p>\n<h2 id=\"Appendix-C-Efficiently-Solving-the-Trust-Region-Constrained-Optimization-Problem\"><a href=\"#Appendix-C-Efficiently-Solving-the-Trust-Region-Constrained-Optimization-Problem\" class=\"headerlink\" title=\"Appendix C. Efficiently Solving the Trust-Region Constrained Optimization Problem\"></a>Appendix C. Efficiently Solving the Trust-Region Constrained Optimization Problem</h2><p>우리가 풀고자 하는 식은 다음과 같습니다.</p>\n<p>$$<br>\\begin{align}<br>\\max\\quad &amp;L(\\theta) \\\\<br>\\mathrm{s.t.\\ } &amp;\\overline{D}_{KL}(\\theta_\\mathrm{old}, \\theta) \\leq \\delta<br>\\end{align}<br>$$</p>\n<p>이 최적화 문제를 풀기 위하여 우리는 gradient 기법을 이용할 것입니다. gradient 기법은 결국 update할 방향과 크기를 결정하는 문제로 생각할 수 있습니다. 이에 따라 아래와 같은 두가지 과정으로 진행합니다.</p>\n<p>1) 탐색 방향을 계산, 목적함수의 first-order approximation(선형근사)와 제약식의 quadratic approximation(2차 근사).</p>\n<p>2) 이동거리 계산을 위해 해당 방향으로 line search 수행.</p>\n<p>탐색 방향은 $Ax = g$ 수식을 근사적으로 풀어서 구합니다. 여기서 $A$ 는 Fisher Information Matrix이고, 이것은 KL divergence 제약식의 2차 근사 $\\overline{D}_\\mathrm{KL} (\\theta_\\mathrm{old}, \\theta) \\approx \\frac{1}{2}(\\theta - \\theta_\\mathrm{old})^T A(\\theta - \\theta_\\mathrm{old})$를 푸는 것입니다. 여기서 $A_{ij} = \\frac{\\partial}{\\partial \\theta_i} \\frac{\\partial}{\\partial \\theta_j} \\overline{D}_\\mathrm{KL}(\\theta_\\mathrm{old}, \\theta)$입니다.</p>\n<p>논문에서 이 부분에 대한 설명이 약간 부족해서 좀 더 내용을 추가하자면, $\\frac{1}{2}x^T A x - x^T g + b = 0$과 같은 quadratic equation의 해는 1차 derivative를 0으로 만드는 $x$를 찾음으로써 알 수 있습니다. 위에서 우리는 목적함수를 1차 근사시키고 제약조건은 2차 근사를 시킴으로써 quadratic equation 형태로 변형했기 때문에 같은 방식으로 풀 수 있습니다. 따라서 $g$는 1차근사시킨 목적함수의 계수가 될 것입니다.</p>\n<p>대규모 문제에서 $A$ 혹은 $A^{-1}$ 을 계산하는 것은 엄청난 계산량이 필요합니다. 하지만 conjugate gradient algorithm을 잘 이용하면 전체 $A$행렬(FIM)을 계산하지 않아도 $Ax=g$ 를 근사적으로 해결할 수 있게 해줍니다.</p>\n<p>탐색 방향 $s \\approx A^{-1}g$과 더불어 최대스텝길이(step size) $\\beta$도 계산할 필요가 있습니다.</p>\n<p>즉, $\\delta = \\overline{D}_\\mathrm{KL} \\approx \\frac{1}{2}(\\beta s)^T A(\\beta s) = \\frac{1}{2}\\beta^2s^TAs$ 이고, </p>\n<p>$\\beta = \\sqrt{2\\delta / s^T As}$가 됩니다. 이때 $\\delta$는 KL divergence의 boundary term 입니다.</p>\n<p>$s^T As$ 항은 Hessian-vector product로 계산할 수 있고, conjugate gradient 과정에서 계산됩니다.</p>\n<p>마지막으로, surrogate objective와 KL divergence 제약식을 위해 다음과 같은 함수를 정의하고 line search를 사용합니다.</p>\n<p>$$<br>L_{\\theta_\\mathrm{old} }(\\theta) - \\chi [\\overline{D}_{KL}(\\theta_\\mathrm{old}, \\theta) \\leq \\delta]<br>$$</p>\n<p>$\\chi[\\cdot]$ 함수는 []안의 조건이 맞으면 0이고 틀리면 무한으로 발산하는 함수입니다.</p>\n<p>위에서 계산한 $\\beta$ 의 최대 값부터 시작해서 목적함수가 개선될때까지 exponential하게 줄여 나갑니다. line search가 없었다면 이 알고리즘은 매우 큰 스텝으로 계산될 것이고 심각한 성능 저하가 발생할 것입니다.</p>\n<p>(여기서 살짝 예고를 하자면, 나중에 이 KL 기반 기법을 개선한 더 단순한 방법이 제안됩니다. PPO라고…)</p>\n<p><br><br></p>\n<h1 id=\"7-Connections-with-Prior-Work\"><a href=\"#7-Connections-with-Prior-Work\" class=\"headerlink\" title=\"7. Connections with Prior Work\"></a>7. Connections with Prior Work</h1><p>Natural Policy Gradient는 $L$의 선형 근사와 $\\overline D_\\mathrm{KL}$ 제약식을 2차근사 하는 아래의 식의 special case입니다.</p>\n<ul>\n<li>Equation (17).</li>\n</ul>\n<p>$L_{\\pi}(\\tilde\\pi) = \\eta(\\pi) + \\underset{s}{\\sum}\\rho_{\\pi}(s) \\underset{a}{\\sum}\\tilde\\pi(a\\vert s)A_\\pi(s,a)$</p>\n<p>$\\underset{\\theta}{\\max}\\quad [\\nabla_{\\theta}L_{\\theta_\\mathrm{old} }(\\theta) \\ \\rvert_{\\theta=\\theta_\\mathrm{old} } \\cdot (\\theta - \\theta_\\mathrm{old})]$</p>\n<p>$\\mathrm{s.t.}\\quad\\frac{1}{2}(\\theta_\\mathrm{old} - \\theta)^{T} A(\\theta_\\mathrm{old})(\\theta_\\mathrm{old}-\\theta) \\leq \\delta$</p>\n<p>$\\mathrm{where\\ }A(\\theta_{old})_{ij} = $</p>\n<p>$\\Large \\frac{\\partial}{\\partial \\theta_{i} } \\frac{\\partial}{\\partial \\theta_{j} }<br>E_{s \\sim \\rho_{\\pi} }[D_\\mathrm{KL}(\\pi(\\cdot\\rvert s, \\theta_\\mathrm{old})  \\rvert\\rvert \\pi(\\cdot \\rvert s, \\theta))] \\rvert_{\\theta=\\theta_\\mathrm{old} }$</p>\n<p>업데이트 식은 다음과 같습니다.</p>\n<p>$\\theta_\\mathrm{new} = \\theta_\\mathrm{old} +  \\color{Red}{\\frac{1}{\\lambda} } A(\\theta_\\mathrm{old})^{-1}\\nabla_{\\theta}L(\\theta) \\rvert_{\\theta=\\theta_\\mathrm{old} }$</p>\n<p>여기서 step size인 $\\frac{1}{\\lambda}$ 일반적으로 알고리즘의 파라미터로 취급되지만 TRPO는 각 업데이트 마다 제약식으로 사용한다는 점이 다릅니다. 사소한 차이처럼 보이지만, 큰 차원을 다루는 실험에서 성능을 크게 향상시켰습니다.</p>\n<p>또한 $L^2$ 제약식(혹은 페널티) 를 사용하면 표준 Policy Gradient 업데이트 식을 얻었습니다.</p>\n<ul>\n<li>Equation (18).</li>\n</ul>\n<p>$\\underset{\\theta}{\\max}\\quad[\\nabla_{\\theta} L_{\\theta_\\mathrm{old} }(\\theta) \\rvert_{\\theta=\\theta_\\mathrm{old} } \\cdot (\\theta - \\theta_\\mathrm{old})]$</p>\n<p>$\\mathrm{s.t.}\\quad \\frac{1}{2} \\vert\\vert \\theta - \\theta_\\mathrm{old} \\vert\\vert^2 \\leq \\delta$</p>\n<p>$L_{\\pi}(\\tilde\\pi) = \\eta(\\pi) + \\underset{s}{\\sum}\\rho_{\\pi}(s) \\underset{a}{\\sum}\\tilde\\pi(a\\vert s)A_\\pi(s,a) $를 이용해서 제약조건없이 $\\underset{\\pi}{\\max}\\quad L_{\\pi_\\mathrm{old} }(\\pi)$를 풀면<br>Policy Iteration update를 하는 것과 같습니다.</p>\n<p><br><br></p>\n<h1 id=\"8-Experiments\"><a href=\"#8-Experiments\" class=\"headerlink\" title=\"8. Experiments\"></a>8. Experiments</h1><ul>\n<li><p>다음과 같은 3 가지 궁금증을 해결하기 위하여 실험을 설계하였습니다.</p>\n<ol>\n<li><p>Single-path 와 vine 의 성능면에서 특징을 알고싶음.</p>\n</li>\n<li><p>Fixed penalty coefficient(NPG) 보다 fixed KL divergence를 사용하는 것(TRPO)으로<br>변경한 것이 얼마나 뚜렷한 차이를 보일지, 성능 면에서 어떤 영향을 미치는지 알고싶음.</p>\n</li>\n<li><p>TRPO가 큰 스케일의 문제를 해결할 수 있는지, 기존에 연구/적용되어왔던 방법들과<br>성능, 계산시간, 샘플 복잡도 면에서 비교하고 싶음.</p>\n</li>\n</ol>\n</li>\n</ul>\n<ul>\n<li><p>1, 2의 궁금증에 대해서 실험 환경에 single path, vine을 비롯한 여러 사전방법들을 함께 실험하였습니다.</p>\n</li>\n<li><p>3에 대해선, robotic locomotion과 Atari game에 TRPO를 실험하였습니다.</p>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"8-1-Simulated-Robotic-Locomotion\"><a href=\"#8-1-Simulated-Robotic-Locomotion\" class=\"headerlink\" title=\"8.1 Simulated Robotic Locomotion\"></a>8.1 Simulated Robotic Locomotion</h2><ul>\n<li>MuJoCo 시뮬레이터를 활용해 로봇 locomotion 실험 진행하였습니다.</li>\n<li>Robot State: positions and velocities, joint torques</li>\n</ul>\n<p><img src=\"https://i.imgur.com/bs5ATS3.png\" alt=\"\"></p>\n<ul>\n<li><p>수영, 점프, 걷기 행동을 학습시키고자 합니다.</p>\n<ul>\n<li><p>수영: 10 dimensional state space , reward: $r(x,u)=v_x - 10^{-5}\\vert \\vert u\\vert \\vert ^2$, quadratic penalty.</p>\n</li>\n<li><p>점프: 12 dimensional state space, episode의 끝은 높이와 각도 threshol로 점프를 판단, non-terminal state에 대해 +1 보상 추가.</p>\n</li>\n<li><p>걷기: 18 dimensional state space, 뜀뛰는 듯한 보법(점프에이전트)보다 부드러운 보법을 유도하기위해 페널티를 추가함.</p>\n</li>\n</ul>\n</li>\n</ul>\n<p><strong>Detailed Experiment Setup and used parameters, used network model</strong><br><img src=\"https://i.imgur.com/zgnsbw6.png\" alt=\"\"></p>\n<p><img src=\"https://i.imgur.com/FqdWC53.png\" alt=\"\"></p>\n<p>equation (12) : $\\max\\quad L_{\\theta_\\mathrm{old}(\\theta)}, \\ \\ \\mathrm{s.t.\\ }\\overline{D}_\\mathrm{KL}^{\\rho_{\\theta_\\mathrm{old} }}(\\theta_\\mathrm{old}, \\theta) \\leq \\delta$</p>\n<p>이 실험에서 $\\delta = 0.01$입니다.</p>\n<ul>\n<li><p>비교에 사용된 모델들</p>\n<ul>\n<li>TRPO Single-path</li>\n<li>TRPO vine</li>\n<li>CEM(cross-entropy method)</li>\n<li>CMA(covariance matrix adaptation)</li>\n<li>NPG (Lagrange Multiplier penalty coefficien를 사용하는 것이 차이점)</li>\n<li>Empirical FIM</li>\n<li>maximum KL(cartpole)</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://i.imgur.com/wSqolvS.png\" alt=\"\"></p>\n<ul>\n<li><p>제안한 single path, vine 모두 task를 잘 학습하는 것을 확인할 수 있습니다.</p>\n</li>\n<li><p>페널티를 고정하는 것보다, KL divergenc를 제약하는 것이 step size를 선택하는 부분에서 Robust합니다.</p>\n</li>\n<li><p>CEM/CMA는 derivative free 알고리즘으로, 샘플 복잡도가 네트워크 파라미터수 만큼 확장되어 high dimension 문제에서는 속도와 성능이 약한 모습을 보입니다.</p>\n</li>\n<li><p>maxKL은 제안방법보다 느린 학습성능을 보입니다.</p>\n</li>\n</ul>\n<p>TRPO video link: <a href=\"https://www.youtube.com/watch?v=jeid0wIrSn4\" target=\"_blank\" rel=\"noopener\">https://www.youtube.com/watch?v=jeid0wIrSn4</a></p>\n<ul>\n<li><p>사전지식이 적은 상태에서 여러 액션을 학습하는 실험으로 TRPO의 성능을 보입니다.</p>\n</li>\n<li><p>지금까지의 균형이나 걸음 같은 개념을 명시적으로 인코딩한 robotic locomotion에서의 사전 연구들과 차이를 보였습니다.</p>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"8-2-Playing-Games-from-Images\"><a href=\"#8-2-Playing-Games-from-Images\" class=\"headerlink\" title=\"8.2 Playing Games from Images\"></a>8.2 Playing Games from Images</h2><ul>\n<li>부분관찰가능하며 복잡한 observation인 환경에서 TRPO를 평가하기위해 raw image를 입력으로 하는 아타리게임 환경에서 실험하였습니다.</li>\n</ul>\n<ul>\n<li>Challenging point :<ul>\n<li>High dimensional space</li>\n<li>delayed reward</li>\n<li>non-stationary image (Enduro는 배경 이미지가 바뀌고, 반전처리가 됨)</li>\n<li>complex action sequence (Q*bert는 21개의 다른 발판에서 점프 해야함)</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li>ALE(Arcade Learning Environment) 에서 테스트</li>\n<li><p>DQN과 전처리 방식은 동일<br>  210 x 160 pixel -&gt; gray-scaling &amp; down-sampling -&gt; 110 x 84  -&gt; crop -&gt; 84 x 84</p>\n</li>\n<li><p>2 convolution layer ( 16 channels) , stride 2, FC layer with 20 units</p>\n</li>\n</ul>\n<p><img src=\"https://i.imgur.com/NJBC69d.png\" alt=\"\"> </p>\n<p><img src=\"https://i.imgur.com/wTe1OEW.png\" alt=\"\"></p>\n<p><img src=\"https://i.imgur.com/j22VtNQ.png\" alt=\"\"></p>\n<ul>\n<li><p>30시간 동안 500 iteration 학습을 하였습니다.</p>\n</li>\n<li><p>DQN과 MCST를 사용한 모델(UCC-I), 사람이 플레이한 것(Human) 과의 비교하였습니다.</p>\n</li>\n<li><p>이전 방법들에 비해 압도적인 점수를 기록하진 못했으나, 다양한 게임에서 적절한 게임 기록하였습니다.</p>\n</li>\n<li><p>이전 방법들과 다른 점은, 특정 task에 초점을 맞춰 설계하지않았는데 robotics나 game playing Task에도 적절한 성능을 내는 점에서 <strong>TRPO의 일반화 성능</strong>을 확인하였습니다.</p>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"9-Discussion\"><a href=\"#9-Discussion\" class=\"headerlink\" title=\"9. Discussion\"></a>9. Discussion</h1><ul>\n<li><p>Trust Region Policy Optimization 을 제안하였습니다.</p>\n</li>\n<li><p>KL Divergence 페널티로 $\\eta(\\pi)$를 최적화하는 알고리즘이 monotonically improve 함을 증명하였습니다.</p>\n</li>\n<li><p>Locomotion 도메인에서 여러 행동(수영, 걷기, 점프)을 제어하는 controller를 학습하였습니다.</p>\n</li>\n<li><p>Robotics와 게임 실험을 결합해, 시각정보와 센서데이터를 사용하는 로봇제어 정책을 학습시킬 수 있는 가능성을 보았습니다.</p>\n</li>\n<li><p>샘플의 복잡도를 상당히 줄일 수 있어 실제상황에 적용가능성을 보았습니다.</p>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"처음으로\"><a href=\"#처음으로\" class=\"headerlink\" title=\"처음으로\"></a>처음으로</h1><h2 id=\"PG-Travel-Guide\"><a href=\"#PG-Travel-Guide\" class=\"headerlink\" title=\"PG Travel Guide\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/\">PG Travel Guide</a></h2><p><br></p>\n<h1 id=\"이전으로\"><a href=\"#이전으로\" class=\"headerlink\" title=\"이전으로\"></a>이전으로</h1><h2 id=\"NPG-여행하기\"><a href=\"#NPG-여행하기\" class=\"headerlink\" title=\"NPG 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/25/4_npg/\">NPG 여행하기</a></h2><h2 id=\"NPG-Code\"><a href=\"#NPG-Code\" class=\"headerlink\" title=\"NPG Code\"></a><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/tnpg.py\" target=\"_blank\" rel=\"noopener\">NPG Code</a></h2><p><br></p>\n<h1 id=\"다음으로\"><a href=\"#다음으로\" class=\"headerlink\" title=\"다음으로\"></a>다음으로</h1><h2 id=\"TRPO-Code\"><a href=\"#TRPO-Code\" class=\"headerlink\" title=\"TRPO Code\"></a><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/trpo_gae.py\" target=\"_blank\" rel=\"noopener\">TRPO Code</a></h2><h2 id=\"GAE-여행하기\"><a href=\"#GAE-여행하기\" class=\"headerlink\" title=\"GAE 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/23/6_gae/\">GAE 여행하기</a></h2>","site":{"data":{}},"excerpt":"","more":"<center> <img src=\"https://www.dropbox.com/s/o7cjcn0e17mpizr/Screen%20Shot%202018-07-18%20at%201.14.22%20AM.png?dl=1\" width=\"700\"> </center>\n\n<p>논문 저자 : John Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, Pieter Abbeel<br>논문 링크 : <a href=\"https://arxiv.org/pdf/1502.05477.pdf\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/pdf/1502.05477.pdf</a><br>Proceeding : International Conference on Machine Learning (ICML) 2015<br>정리 : 공민서, 김동민</p>\n<hr>\n<h1 id=\"1-들어가며…\"><a href=\"#1-들어가며…\" class=\"headerlink\" title=\"1. 들어가며…\"></a>1. 들어가며…</h1><p>Trust region policy optimization (TRPO)는 상당히 우수한 성능을 보여주는 policy gradient 기법으로 알려져 있습니다. 높은 차원의 action space를 가진 robot locomotion부터 action은 적지만 화면을 그대로 처리하여 플레이하기 때문에 control parameter가 매우 많은 Atari game까지 각 application에 세부적으로 hyperparameter들을 특화시키지 않아도 두루두루 좋은 성능을 나타내기 때문에 일반화성능이 매우 좋은 기법입니다. 이 TRPO에 대해서 알아보겠습니다.</p>\n<p>※TRPO를 매우 재밌게 설명한 Crazymuse AI의 <a href=\"https://www.youtube.com/watch?v=CKaN5PgkSBc&amp;t=90s\" target=\"_blank\" rel=\"noopener\">Youtube video</a>에서 일부 그림을 차용했습니다. 이 비디오를 시청하시는 것을 강력하게 추천합니다!</p>\n<p><br></p>\n<h2 id=\"1-1-TRPO-흐름-잡기\"><a href=\"#1-1-TRPO-흐름-잡기\" class=\"headerlink\" title=\"1.1 TRPO 흐름 잡기\"></a>1.1 TRPO 흐름 잡기</h2><p>TRPO 논문은 많은 수식이 등장하여 이 수식들을 따라가다보면 큰 그림을 놓칠 수도 있습니다. 세부적인 내용을 살펴보기 전에 기존 연구에서 출발해서 TRPO로 어떻게 발전해나가는지 간략하게 살펴보겠습니다.</p>\n<h3 id=\"1-1-1-Original-Problem\"><a href=\"#1-1-1-Original-Problem\" class=\"headerlink\" title=\"1.1.1 Original Problem\"></a>1.1.1 Original Problem</h3><p>$$\\max_\\pi \\eta(\\pi)$$</p>\n<p>모든 강화학습이 그렇듯이 expected discounted reward를 최대화하는 policy를 찾는 문제로부터 출발합니다.</p>\n<h3 id=\"1-1-2-Conservative-policy-iteration\"><a href=\"#1-1-2-Conservative-policy-iteration\" class=\"headerlink\" title=\"1.1.2 Conservative policy iteration\"></a>1.1.2 Conservative policy iteration</h3><p>$$\\max L_\\pi(\\tilde\\pi) = \\eta(\\pi) + \\sum_s \\rho_\\pi(s)\\sum_a\\tilde\\pi(a\\vert s)A_\\pi(s,a)$$</p>\n<p>$\\eta(\\pi)$를 바로 최대화하는 것은 많은 경우 어렵습니다. $\\eta(\\pi)$의 성능향상을 보장하면서 policy를 update하는 conservative policy iteration 기법이 <a href=\"http://www.cs.cmu.edu/~./jcl/papers/aoarl/Final.pdf\" target=\"_blank\" rel=\"noopener\">Kakade와 Langford</a>에 의하여 제안되었습니다. 이 기법을 이용하면 policy update가 성능을 향상시키는지는 못하더라도 최소한 성능을 악화시키지는 않는다는 것이 이론적으로 보장됩니다.</p>\n<h3 id=\"1-1-3-Theorem-1-of-TRPO\"><a href=\"#1-1-3-Theorem-1-of-TRPO\" class=\"headerlink\" title=\"1.1.3 Theorem 1 of TRPO\"></a>1.1.3 Theorem 1 of TRPO</h3><p>$$\\max L_{\\pi_\\mathrm{old} }\\left(\\pi_\\mathrm{new}\\right) - \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2,\\quad\\left(\\alpha=D_\\mathrm{TV}^\\max\\left(\\pi_\\mathrm{old},\\pi_\\mathrm{new}\\right)\\right)$$</p>\n<p>기존의 conservative policy iteration은 과거 policy와 새로운 policy를 섞어서 사용해서 실용적이지 않다는 단점이 있었는데 이것을 보완하여 온전히 새로운 policy만으로 update할 수 있는 기법을 제안합니다. </p>\n<h3 id=\"1-1-4-KL-divergence-version-of-Theorem-1\"><a href=\"#1-1-4-KL-divergence-version-of-Theorem-1\" class=\"headerlink\" title=\"1.1.4 KL divergence version of Theorem 1\"></a>1.1.4 KL divergence version of Theorem 1</h3><p>$$\\max L_{\\pi}\\left(\\tilde\\pi\\right) - C\\cdot D_\\mathrm{KL}^\\max\\left(\\pi,\\tilde\\pi\\right),\\quad\\left(C = \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\right)$$</p>\n<p>distance metric을 KL divergence로 바꿀 수 있습니다.</p>\n<h3 id=\"1-1-5-Using-parameterized-policy\"><a href=\"#1-1-5-Using-parameterized-policy\" class=\"headerlink\" title=\"1.1.5 Using parameterized policy\"></a>1.1.5 Using parameterized policy</h3><p>$$\\max_\\theta L_{\\theta_\\mathrm{old} }(\\theta) - C\\cdot D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right)$$</p>\n<p>최적화문제를 더욱 편리하게 풀 수 있도록 낮은 dimension을 가지는 parameter들로 parameterized된 policy를 사용할 수 있습니다.</p>\n<h3 id=\"1-1-6-Trust-region-constraint\"><a href=\"#1-1-6-Trust-region-constraint\" class=\"headerlink\" title=\"1.1.6 Trust region constraint\"></a>1.1.6 Trust region constraint</h3><p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;L_{\\theta_\\mathrm{old} }(\\theta) \\\\<br>\\mathrm{s.t.\\ }&amp;D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right) \\leq \\delta<br>\\end{align}$$</p>\n<p>1.1.5까지는 아직 conservative policy iteration을 약간 변형시킨 것입니다. policy를 update할 때 지나치게 많이 변하는 것을 방지하기 위하여 <a href=\"https://en.wikipedia.org/wiki/Trust_region\" target=\"_blank\" rel=\"noopener\">trust region</a>을 constraint로 설정할 수 있습니다. 이 아이디어로 인해서 TRPO라는 명칭을 가지게 됩니다.</p>\n<h3 id=\"1-1-7-Heuristic-approximation\"><a href=\"#1-1-7-Heuristic-approximation\" class=\"headerlink\" title=\"1.1.7 Heuristic approximation\"></a>1.1.7 Heuristic approximation</h3><p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;L_{\\theta_\\mathrm{old} }(\\theta) \\\\<br>\\mathrm{s.t.\\ }&amp;\\overline{D}_\\mathrm{KL}^{\\rho_\\mathrm{old} }\\left(\\theta_\\mathrm{old}, \\theta\\right) \\leq \\delta<br>\\end{align}$$</p>\n<p>사실 1.1.6의 constraint는 모든 state에 대해서 성립해야 하기 때문에 문제를 매우 어렵게 만듭니다. 이것을 좀 더 다루기 쉽게 state distribution에 대한 평균을 취한 것으로 변형합니다.</p>\n<h3 id=\"1-1-8-Monte-Carlo-simulation\"><a href=\"#1-1-8-Monte-Carlo-simulation\" class=\"headerlink\" title=\"1.1.8 Monte Carlo simulation\"></a>1.1.8 Monte Carlo simulation</h3><p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}Q_{\\theta_\\mathrm{old} }(s,a)\\right] \\\\<br>\\mathrm{s.t.\\ }&amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} }}\\left[D_\\mathrm{KL}\\left(\\pi_{\\theta_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta<br>\\end{align}$$</p>\n<p>Sampling을 통한 계산이 가능하도록 식을 다시 표현할 수 있습니다. </p>\n<h3 id=\"1-1-9-Efficiently-solving-TRPO\"><a href=\"#1-1-9-Efficiently-solving-TRPO\" class=\"headerlink\" title=\"1.1.9 Efficiently solving TRPO\"></a>1.1.9 Efficiently solving TRPO</h3><p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;\\nabla_\\theta \\left. L_{\\theta_\\mathrm{old} }(\\theta)\\right\\vert _{\\theta=\\theta_\\mathrm{old} } \\left(\\theta - \\theta_\\mathrm{old}\\right) \\\\<br>\\mathrm{s.t.\\ }&amp;\\frac{1}{2}\\left(\\theta_\\mathrm{old} - \\theta \\right)^T A\\left(\\theta_\\mathrm{old}\\right)\\left(\\theta_\\mathrm{old} - \\theta \\right) \\leq \\delta \\\\<br>&amp;A_{ij} = \\frac{\\partial}{\\partial \\theta_i}\\frac{\\partial}{\\partial \\theta_j}\\overline{D}_\\mathrm{KL}^{\\rho_\\mathrm{old} }\\left(\\theta_\\mathrm{old}, \\theta\\right)<br>\\end{align}$$</p>\n<p>문제를 효율적으로 풀기 위하여 approximation을 적용할 수 있습니다. objective function은 first order approximation, constraint는 quadratic approximation을 취하면 효율적으로 문제를 풀 수 있는 형태로 바뀌는데 이것을 natural gradient로 풀 수도 있고 conjugate gradient로 풀 수도 있습니다.</p>\n<p>TRPO는 이렇게 다양한 방법으로 문제를 변형한 것입니다! 이제 좀 더 자세히 살펴보겠습니다.</p>\n<p><br><br></p>\n<h1 id=\"2-Preliminaries\"><a href=\"#2-Preliminaries\" class=\"headerlink\" title=\"2. Preliminaries\"></a>2. Preliminaries</h1><p>다음과 같은 파라미터를 가지는 infinite-horizon discounted Markov decision process (MDP)를 고려합니다. </p>\n<ul>\n<li>$\\mathcal{S}$: finite set of states</li>\n<li>$\\mathcal{A}$: finite set of actions</li>\n<li>$P: \\mathcal{S}\\times\\mathcal{A}\\rightarrow\\mathbb{R}$: transition probability distribution</li>\n<li>$r: \\mathcal{S}\\rightarrow\\mathbb{R}$: reward function</li>\n<li>$\\rho_0:\\mathcal{S}\\rightarrow\\mathbb{R}$: distribution of the initial state $s_0$</li>\n<li>$\\gamma\\in(0,1)$: discount factor</li>\n<li>$\\eta(\\pi)=E_{s_0,a_0,\\ldots}\\left[\\sum_{t=0}^\\infty\\gamma^t r\\left(s_t\\right)\\right]$, where $s_0\\sim\\rho_0\\left(s_0\\right), a_t\\sim\\pi\\left(\\left.a_t \\right\\vert s_t\\right), s_{t+1}\\sim P\\left(\\left.s_{t+1}\\right\\vert s_t,a_t\\right)$</li>\n<li>$Q_\\pi\\left(s_t,a_t\\right)=E_{ { \\color{red}{s_{t+1}} },a_{t+1},\\ldots}\\left[\\sum_{l=0}^\\infty \\gamma^l r\\left(s_{t+l}\\right)\\right]$: action value function</li>\n<li>$V_\\pi\\left(s_t\\right)=E_{ { \\color{red}{a_t} }, s_{t+1},a_{t+1},\\ldots}\\left[\\sum_{l=0}^\\infty \\gamma^l r\\left(s_{t+l}\\right)\\right]$: value function (action value function과 expectation의 첨자가 다른 부분을 유의하세요.)</li>\n<li>$A_\\pi(s,a) = Q_\\pi(s,a) - V_\\pi(s)$: advantage function</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-1-Useful-identity-Kakade-amp-Langford-2002\"><a href=\"#2-1-Useful-identity-Kakade-amp-Langford-2002\" class=\"headerlink\" title=\"2.1 Useful identity [Kakade &amp; Langford 2002]\"></a>2.1 Useful identity [Kakade &amp; Langford 2002]</h2><p>우리의 목표는 $\\eta(\\pi)$가 최대화되도록 만드는 것입니다. 하지만 $\\pi$의 변화에 따라 $\\eta$가 어떻게 변하는지 알아내는 것도 쉽지 않습니다. $\\pi$는 기존의 policy이고 $\\tilde\\pi$는 새로운 policy를 나타낸다고 할 때 $\\eta$와 policy update 사이에 다음과 같은 관계가 있다는 것이 밝혀졌습니다.</p>\n<p><strong>Lemma 1.</strong> $\\eta\\left(\\tilde\\pi\\right) = \\eta(\\pi) + E_{s_{0},a_{0},\\ldots\\sim\\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t A_\\pi\\left(s_t,a_t\\right)\\right]$</p>\n<p><em>Proof.</em> $A_\\pi(s,a)=E_{s’\\sim P\\left(s’\\vert s,a\\right)}\\left[r(s)+\\gamma V_\\pi\\left(s’\\right)-V_\\pi(s)\\right]$로 다시 표현할 수 있습니다. 표기의 편의를 위하여 $\\tau:=\\left(s_0,a_0,s_1,a_1,\\ldots\\right)$를 정의하겠습니다. 다음과 같은 수식 전개가 가능합니다.<br>$$<br>\\begin{align}<br>&amp;E_{\\tau\\vert \\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t A_\\pi\\left(s_t,a_t\\right)\\right] \\\\<br> &amp;= E_{\\tau\\vert \\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t \\left(r(s_t)+\\gamma V_\\pi\\left(s_{t+1}\\right)-V_\\pi(s_t)\\right)\\right] \\\\<br>&amp;= E_{\\tau\\vert \\tilde\\pi}\\left[\\left(\\sum_{t=0}^\\infty\\gamma^t r(s_t)\\right)+\\gamma V_\\pi\\left(s_{1}\\right)-V_\\pi(s_0)+\\gamma^2 V_\\pi\\left(s_{2}\\right)-\\gamma V_\\pi(s_1)+\\gamma^3 V_\\pi\\left(s_{3}\\right)-\\gamma^2 V_\\pi(s_2)+\\cdots\\right] \\\\<br>&amp;= E_{\\tau\\vert \\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t r(s_t)+\\color{red}{\\gamma V_\\pi\\left(s_{1}\\right)}-V_\\pi(s_0)+\\color{red}{\\gamma^2 V_\\pi\\left(s_{2}\\right)}-\\color{red}{\\gamma V_\\pi(s_1)}+\\cdots\\right] \\\\<br>&amp;= E_{\\tau\\vert \\tilde\\pi}\\left[-V_\\pi(s_0)+\\sum_{t=0}^\\infty\\gamma^t r(s_t)\\right] \\\\<br>&amp;\\overset{\\underset{\\mathrm{(a)} }{} }{=} -E_{\\color{red}{s_0}}\\left[V_\\pi(s_0)\\right]+E_{\\tau\\vert \\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t r(s_t)\\right] \\\\<br>&amp;=-\\eta(\\pi) + \\eta\\left(\\tilde\\pi\\right)\\\\<br>&amp;\\quad\\therefore \\eta\\left(\\tilde\\pi\\right) = \\eta(\\pi) + E_{s_{0},a_{0},\\ldots\\sim\\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t A_\\pi\\left(s_t,a_t\\right)\\right]<br>\\end{align}<br>$$<br>위의 전개의 (a) 부분은 $V_\\pi\\left(s_0\\right)=E_{a_0,a_1,s_1,\\ldots}\\left[\\sum_{t=0}^\\infty\\gamma^l r\\left(s_l\\right)\\right]$이므로 $\\tau$ 중 많은 부분들이 이미 expectation이 취해진 값이므로 무시됩니다. 오직 $s_0$만 expectation이 취해지지 않았기 때문에 남아있습니다.</p>\n<p><strong>Lemma 1</strong>은 새로운 policy와 기존의 policy 사이의 관계를 규정합니다. 다음과 같은 식을 정의하고 이것을 이용해서 <strong>Lemma 1</strong>을 변형시켜 봅시다.</p>\n<ul>\n<li>$\\rho_\\pi(s) = P\\left(s_0=s\\right) + \\gamma P\\left(s_1=s\\right)\\ + \\gamma^2 P\\left(s_2=s\\right) + \\cdots $: (unnormalized) discounted visitation frequencies</li>\n</ul>\n<p>$$<br>\\begin{align}<br>\\eta\\left(\\tilde\\pi\\right) &amp;= \\eta(\\pi) + \\sum_{t=0}^\\infty\\sum_{s}P\\left(\\left.s_t=s\\right\\vert \\tilde\\pi\\right)\\sum_a\\tilde\\pi(a\\vert s)\\gamma^t A_\\pi(s,a) \\\\<br>&amp;= \\eta(\\pi) + \\sum_{s}{\\color{red}{\\sum_{t=0}^\\infty \\gamma^t P\\left(\\left.s_t=s\\right\\vert \\tilde\\pi\\right)} } \\sum_a\\tilde\\pi(a\\vert s) A_\\pi(s,a) \\\\<br>&amp;= \\eta(\\pi) + \\sum_{s}{\\color{red}{\\rho_\\tilde\\pi(s)} }\\sum_a\\tilde\\pi(a\\vert s) A_\\pi(s,a) \\\\<br>\\end{align}<br>$$</p>\n<p>이 수식의 의미가 무엇일까요? 만약 $\\sum_a\\tilde\\pi(a\\vert s) A_\\pi(s,a) \\geq 0$이라면 $\\eta(\\tilde\\pi)$는 항상 $\\eta(\\pi)$보다 큽니다. 즉, policy를 업데이트함으로써 항상 개선됩니다. 즉, 이 수식을 통해서 항상 더 좋은 성능을 내는 policy update가 가져야 할 특징을 알 수 있습니다. 다음과 같은 deterministic policy가 있다고 합시다.</p>\n<p>$$<br>\\tilde\\pi(s) = \\arg\\max_a A_\\pi(s,a)<br>$$</p>\n<p>이 policy는 적어도 하나의 state-action pair에서 0보다 큰 값을 가지는 advantage가 있고 그 때의 확률이 0이 아니라면 항상 성능을 개선시킵니다. 어려운 점은 policy를 바꾸면 $\\rho$도 바뀐다는 점입니다. </p>\n<p>다음 그림을 봅시다. Starting Point에서 여러가지 경로를 거쳐서 Destination으로 갈 수 있습니다. 다른 policy를 이용하는 것은 다른 경로를 이용한 것입니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=16s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/policy_change.png\" alt=\"policy_change\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>이 때 다른 policy를 이용함에 따라 state visitation frequency도 변하게 됩니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=36s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/state_visitation_change.png\" alt=\"state_visitation_change\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>이로 인해서 policy를 최적화하는 것은 어려워집니다. 그래서 이러한 변화를 무시하는 다음과 같은 approximation을 취합니다.</p>\n<p>$$<br>\\begin{align}<br>L_\\pi\\left(\\tilde\\pi\\right) &amp;= \\eta(\\pi) + \\sum_{s}{\\color{red}{\\rho_\\pi(s)} } \\sum_a\\tilde\\pi(a\\vert s) A_\\pi(s,a)<br>\\end{align}<br>$$</p>\n<p>policy가 바뀌었음에도 이전의 state distribution을 계속 이용하는 것입니다. 이것은 policy의 변화가 크지 않다면 어느 정도 허용될 수 있을 것입니다. 그렇지만 얼마나 많은 변화가 허용될까요? 이것을 정하기 위해서 이용하는 것이 trust region입니다.</p>\n<p><br></p>\n<h2 id=\"2-2-Conservative-Policy-Iteration\"><a href=\"#2-2-Conservative-Policy-Iteration\" class=\"headerlink\" title=\"2.2 Conservative Policy Iteration\"></a>2.2 Conservative Policy Iteration</h2><p>policy의 변화를 다루기 용이하게 하기 위해서 policy를 다음과 같이 파라미터를 이용해서 표현합시다.</p>\n<ul>\n<li>$\\pi_\\theta(a\\vert s)$: parameterized policy</li>\n</ul>\n<p>$\\pi_\\theta$는 $\\theta$에 대하여 미분가능한 함수입니다. $L_\\pi\\left(\\tilde\\pi\\right)$을 $\\theta_0$에서 $\\eta(\\pi)$에 대한 first order approximation이라고 하면 다음 식이 성립합니다.</p>\n<p>$$<br>\\begin{align}<br>L_{\\pi_{\\theta_0} }\\left(\\pi_{\\theta_0}\\right) &amp;= \\eta\\left(\\pi_{\\theta_0}\\right) \\\\<br>\\nabla_\\theta \\left.L_{\\pi_{\\theta_0} }\\left(\\pi_{\\theta_0}\\right)\\right\\vert _{\\theta=\\theta_0} &amp;= \\nabla_\\theta\\left.\\eta(\\pi_{\\theta_0})\\right\\vert _{\\theta=\\theta_0}<br>\\end{align}<br>$$</p>\n<p>이것의 의미는 $\\pi_{\\theta_0}$가 매우 작게 변한다면 $L_{\\pi_{\\theta_0} }$를 개선시키는 것이 $\\eta$를 개선시키는 것이라는 것입니다. 그러나 지금까지의 설명만으로는 $\\pi_{\\theta_0}$를 얼마나 작게 변화시켜야 할지에 대해서는 알 수 없습니다.</p>\n<p>Kakade &amp; Langford가 2002년 발표한 논문에서도 이것에 대해서 고민했습니다. 그 논문에서 <em>conservative policy iteration</em>이라는 기법을 제안합니다. 그 논문의 contribution은 다음과 같습니다.</p>\n<ul>\n<li>$\\eta$ 개선의 lower bound를 제공</li>\n<li>기존의 policy를 $\\pi_\\mathrm{old}$라고 하고 $\\pi’$를 $\\pi’=\\arg\\max_{\\pi’}L_{\\pi_\\mathrm{old} }\\left(\\pi’\\right)$과 같이 정의할 때, 새로운 mixture policy $\\pi_\\mathrm{new}$를 다음과 같이 제안<br>$$<br>\\pi_\\mathrm{new}(a\\vert s) = (1-\\alpha)\\pi_\\mathrm{old}(a\\vert s) + \\alpha \\pi’(a\\vert s)<br>$$</li>\n</ul>\n<p>그림으로 표현하면 다음과 같습니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=2m46s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/mixure_policy.png\" alt=\"mixure_policy\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<ul>\n<li>다음과 같은 lower bound를 정의<br>$$<br>\\eta\\left(\\pi_\\mathrm{new}\\right) \\geq L_{\\pi_{\\theta_\\mathrm{old} }}\\left(\\pi_\\mathrm{new}\\right) - \\frac{2\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2,\\\\<br>\\mathrm{where\\ }\\epsilon = \\max_s\\left\\vert E_{a\\sim\\pi’(a\\vert s)}\\left[A_\\pi(s,a)\\right]\\right\\vert<br>$$</li>\n</ul>\n<p>하지만 mixture된 policy라는 것은 실용적이지 않습니다. </p>\n<p><br><br></p>\n<h1 id=\"3-Monotonic-Improvement-Guarantee-for-General-Stochastic-Policies\"><a href=\"#3-Monotonic-Improvement-Guarantee-for-General-Stochastic-Policies\" class=\"headerlink\" title=\"3. Monotonic Improvement Guarantee for General Stochastic Policies\"></a>3. Monotonic Improvement Guarantee for General Stochastic Policies</h1><p>이전 장에서 설명한 lower bound는 오직 mixture policy에 대해서만 성립하고 더 많이 사용되는 stochastic policy에는 적용되지 않습니다. 따라서 stochastic policy를 이용할 수 있도록 개선할 필요가 있습니다. 아래와 같이 기존 수식에서 두 가지를 바꿈으로써 이것이 가능합니다.</p>\n<ul>\n<li>$\\alpha\\rightarrow$ distance measure between $\\pi$ and $\\tilde\\pi$</li>\n<li>constant $\\epsilon\\rightarrow\\max_{s,a}\\left\\vert A_\\pi(s,a)\\right\\vert $</li>\n</ul>\n<p>여기서 distance measure로 total variation divergence를 이용합니다. discrete porbability distribution $p$와 $q$에 대하여 다음과 같이 정의됩니다.<br>$$<br>D_\\mathrm{TV}(p\\parallel q) = \\frac{1}{2}\\sum_i\\left\\vert p_i - q_i\\right\\vert<br>$$</p>\n<p>이것을 이용하여 $D_\\mathrm{TV}^\\max$를 다음과 같이 정의합니다.<br>$$<br>D_\\mathrm{TV}^\\max(\\pi\\parallel \\tilde\\pi) = \\max_s D_\\mathrm{TV}\\left(\\pi(\\cdot\\vert s)\\parallel\\tilde\\pi(\\cdot\\vert s)\\right)<br>$$</p>\n<p>그림으로 표현하면 다음과 같습니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=3m15s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/tvd.png\" alt=\"tvd\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>이것을 이용하여 다음과 같은 관계식을 얻을 수 있습니다.</p>\n<p><strong>Theorem 1.</strong> Let $\\alpha=D_\\mathrm{TV}^\\max(\\pi\\parallel \\tilde\\pi)$. Then the following bound holds:<br>$$<br>\\begin{align}<br>\\eta\\left(\\pi_\\mathrm{new}\\right) &amp;\\geq L_{\\pi_\\mathrm{old} }\\left(\\pi_\\mathrm{new}\\right) - \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2, \\\\<br>\\mathrm{where\\ } \\epsilon&amp;= \\max_{s,a}\\left\\vert A_\\pi(s,a)\\right\\vert<br>\\end{align}<br>$$<br><!--*Proof.* TBD.--></p>\n<p>또다른 distance metric으로 아래 그림과 같은 KL divergence가 있습니다.<br>(그런데 왜 하필 KL divergence로 바꿀까요? 논문의 뒤쪽에서 계산효율을 위해서 conjugate gradient method를 이용하는데 이를 위해서 바꾼게 아닐까 싶습니다. Wasserstein distance 같은 다른 방향으로 발전시킬 수도 있을 것 같습니다. Schulmann이 아마 해봤겠죠?^^a)</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=3m34s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/kld.png\" alt=\"kld\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>total variation divergence와 KL divergence 사이에는 다음과 같은 관계가 있습니다.<br>$$<br>D_\\mathrm{TV}(p\\parallel q)^2 \\leq D_\\mathrm{KL}(p\\parallel q)<br>$$</p>\n<p>다음 수식을 정의합니다.<br>$$<br>D_\\mathrm{KL}^\\max(\\pi\\parallel \\tilde\\pi) = \\max_s D_\\mathrm{KL}\\left(\\pi(\\cdot\\vert s)\\parallel\\tilde\\pi(\\cdot\\vert s)\\right)<br>$$</p>\n<p><strong>Theorem 1</strong>을 이용하여 다음과 같은 수식이 성립함을 알 수 있습니다.<br>$$<br>\\begin{align}<br>\\eta\\left(\\tilde\\pi\\right) &amp;\\geq L_{\\pi}\\left(\\tilde\\pi\\right) - C\\cdot D_\\mathrm{KL}^\\max(\\pi, \\tilde\\pi), \\\\<br>\\mathrm{where\\ } C&amp;= \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}<br>\\end{align}<br>$$</p>\n<p>이러한 policy imporvement bound를 기반으로 다음과 같은 approximate policy iteration 알고리듬을 고안해낼 수 있습니다.</p>\n<p><strong>Algorithm 1</strong> Policy iteration algorithm guaranteeing non-decreasing expected return $\\eta$</p>\n<blockquote>\n<p>Initialize $\\pi_0$<br><strong>for</strong> $i=0,1,2,\\ldots$ until convergence <strong>do</strong><br>$\\quad$Compute all advantage values $A_{\\pi_i}(s,a)$<br>$\\quad$Solve the contstrained optimization problem<br>$\\quad\\pi_{i+1}=\\arg\\max_\\pi\\left[L_{\\pi_i} - CD_\\mathrm{KL}^\\max\\left(\\pi_i,\\pi\\right)\\right]$<br>$\\quad\\quad$where $C = 4\\epsilon\\gamma / (1-\\gamma)^2$<br>$\\quad\\quad$and $L_{\\pi_i}\\left(\\pi\\right) = \\eta(\\pi_i) + \\sum_{s}\\rho_{\\pi_i}(s)\\sum_a\\pi(a\\vert s) A_{\\pi_i}(s,a)$<br><strong>end for</strong></p>\n</blockquote>\n<p><strong>Algorithm 1</strong>은 advantage를 정확하게 계산할 수 있다고 가정하고 있습니다. 이 알고리듬은  monotonical한 성능 증가($\\eta(\\pi_0)\\leq\\eta(\\pi_1)\\leq\\cdots$)를 한다는 것을 다음과 같이 보일 수 있습니다. $M_i(\\pi)=L_{\\pi_i}(\\pi) - CD_\\mathrm{KL}^\\max\\left(\\pi_i,\\pi\\right)$라고 합시다.</p>\n<p>$$<br>\\begin{align}<br>\\eta \\left(\\pi_{i+1}\\right) &amp;\\geq M_i\\left(\\pi_{i+1}\\right)\\\\<br>\\eta \\left(\\pi_{i}\\right) &amp;= M_i\\left(\\pi_{i}\\right) \\\\<br>\\eta \\left(\\pi_{i+1}\\right) - \\eta \\left(\\pi_{i}\\right) &amp;\\geq M_i\\left(\\pi_{i+1}\\right) - M_i\\left(\\pi_{i}\\right)<br>\\end{align}<br>$$</p>\n<p>위 수식과 같이 매 iteration 마다 $M_i$를 최대화함으로써, $\\eta$가 감소하지 않는다는 것을 보장할 수 있습니다. 이와 같은 타입의 알고리듬을 <a href=\"https://www.jstor.org/stable/pdf/27643496.pdf?casa_token=0qHamcl60WoAAAAA:-fkZ9JcA_nrY3-zbCUpqvPOgcAMgw7Gr96MajCZg2byHf8m5GU1KTSxyJJcBy1lPZbBTZVCjHHUXilh4k-iuwF91Wka4B5qdltC1IR2qMWk8q1FoV6__\" target=\"_blank\" rel=\"noopener\">minorization-maximization (MM) algorithm</a>이라고 합니다. EM 알고리듬도 이 타입의 알고리듬 중 하나입니다.</p>\n<p><strong>Algorithm 1</strong>은 아래 그림과 같이 동작합니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=3m52s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/surrogate.png\" alt=\"surrogate\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>$M_i$는 $\\pi_i$일 때 equality가 되는 $\\eta$에 대한  surrogate function입니다. TRPO는 이 surrogate function을 최대화하고 KL divergence를 penalty가 아닌 constraint로 두는 알고리듬입니다.</p>\n<p><br><br></p>\n<h1 id=\"4-Optimization-of-Parameterized-Policies\"><a href=\"#4-Optimization-of-Parameterized-Policies\" class=\"headerlink\" title=\"4. Optimization of Parameterized Policies\"></a>4. Optimization of Parameterized Policies</h1><p>표기의 편의를 위해 다음과 같이 notation들을 더 간략하게 정의합니다.</p>\n<ul>\n<li>$\\eta(\\theta):=\\eta\\left(\\pi_\\theta\\right)$</li>\n<li>$L_\\theta\\left(\\tilde\\theta\\right):=L_{\\pi_\\theta}\\left(\\pi_{\\tilde\\theta}\\right)$</li>\n<li>$D_\\mathrm{KL}\\left(\\theta\\parallel\\tilde\\theta\\right):=D_\\mathrm{KL}\\left(\\pi_\\theta\\parallel\\pi_{\\tilde\\theta}\\right)$</li>\n<li>$\\theta_\\mathrm{old}$: previous policy parameter</li>\n</ul>\n<p><br></p>\n<h2 id=\"4-1-Trust-Region-Policy-Optimization\"><a href=\"#4-1-Trust-Region-Policy-Optimization\" class=\"headerlink\" title=\"4.1 Trust Region Policy Optimization\"></a>4.1 Trust Region Policy Optimization</h2><p>이전 장의 중요 결과를 위의 notation으로 다시 표기하면 아래와 같습니다.</p>\n<p>$$<br>\\eta\\left(\\theta\\right) \\geq L_{\\theta_\\mathrm{old} }\\left(\\theta\\right) - C\\cdot D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right)<br>$$</p>\n<p>$\\eta$의 성능 향상을 보장하기 위해서 그것의 lower bound를 최대화할 수 있습니다.</p>\n<p>$$<br>\\max_\\theta \\left[L_{\\theta_\\mathrm{old} }\\left(\\theta\\right) - C D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right)\\right]<br>$$</p>\n<p>이 최적화 문제는 step size를 매우 작게 해야 올바른 동작을 합니다. 위에서 살펴봤듯이 first order approximation이기 때문입니다. 좀 더 큰 step size를 가질 수 있도록 이 최적화 문제를 trust region constraint를 도입하여 다음과 같이 바꿉니다.</p>\n<p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;L_{\\theta_\\mathrm{old} }(\\theta) \\\\<br>\\mathrm{s.t.\\ }&amp;D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right) \\leq \\delta<br>\\end{align}$$</p>\n<p>이 최적화문제의 constraint는 모든 state space에 대해서 성립해야 합니다. 또한 maximum값을 매번 찾아야 합니다. state가 많은 경우 constraint의 수가 매우 많아져서 문제를 풀기 어렵게 만듭니다. constraint의 수를 줄이기 위하여 다음과 같은 avergae KL divergence를 이용하는 heuristic approximation을 취합니다. 이것이 최선의 방법은 아닐 수 있지만 실용적인 방법입니다.</p>\n<p>$$<br>\\overline D_\\mathrm{KL}^{\\rho}\\left(\\theta_1, \\theta_2\\right):=E_{s\\sim\\rho}\\left[D_\\mathrm{KL}\\left(\\pi_{\\theta_1}(\\cdot\\vert s)\\parallel\\pi_{\\theta_2}(\\cdot\\vert s)\\right)\\right]<br>$$</p>\n<p>이것을 기반으로 다음과 같은 최적화 문제를 풀 수 있습니다.</p>\n<p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;L_{\\theta_\\mathrm{old} }(\\theta) \\\\<br>\\mathrm{s.t.\\ }&amp;\\overline{D}_\\mathrm{KL}^{\\rho_\\mathrm{old} }\\left(\\theta_\\mathrm{old}, \\theta\\right) \\leq \\delta<br>\\end{align}$$</p>\n<p>아래 그림처럼 step size에 대해서 고려할 수 있습니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=4m35s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/heuristic_approx.png\" alt=\"heuristic_approx\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p><br><br></p>\n<h1 id=\"5-Sample-Based-Estimation-of-the-Objective-and-Constraint\"><a href=\"#5-Sample-Based-Estimation-of-the-Objective-and-Constraint\" class=\"headerlink\" title=\"5. Sample-Based Estimation of the Objective and Constraint\"></a>5. Sample-Based Estimation of the Objective and Constraint</h1><p>실용적인 알고리듬을 만들려고 하는 노력은 아직 끝나지 않았습니다. 이제 앞의 알고리듬을 sample-based estimation 즉, Monte Carlo estimation을 할 수 있도록 바꿔보겠습니다. sampling을 편하게 할 수 있도록 아래와 같이 바꿔줍니다.</p>\n<ul>\n<li>$\\sum_s \\rho_{\\theta_\\mathrm{old} }(s)[\\ldots]\\rightarrow\\frac{1}{1-\\gamma}E_{s\\sim\\rho_{\\theta_\\mathrm{old} }}[\\ldots]$</li>\n<li>$A_{\\theta_\\mathrm{old} }\\rightarrow Q_{\\theta_\\mathrm{old} }$</li>\n<li>$\\sum_a\\pi_{\\theta_\\mathrm{old} }(a\\vert s)A_{\\theta_\\mathrm{old} } \\rightarrow E_{a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}A_{\\theta_\\mathrm{old} }\\right]$</li>\n</ul>\n<p>이러한 변화를 그림처럼 도식화할 수 있습니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=5m31s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/sample-based.png\" alt=\"sample-based\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>한 가지 짚고 넘어가야 할 점은 action sampling을 할 때 importance sampling을 사용한다는 것입니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=5m53s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/importance_sampling.png\" alt=\"importance_sampling\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>바뀐 최적화문제는 아래와 같습니다.</p>\n<ul>\n<li><strong>Equation (14).</strong></li>\n</ul>\n<p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}Q_{\\theta_\\mathrm{old} }(s,a)\\right] \\\\<br>\\mathrm{s.t.\\ }&amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} }}\\left[D_\\mathrm{KL}\\left(\\pi_{\\theta_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta<br>\\end{align}<br>$$</p>\n<p>이 때 sampling하는 두 가지 방법이 있습니다.</p>\n<p><br></p>\n<h2 id=\"5-1-Single-Path\"><a href=\"#5-1-Single-Path\" class=\"headerlink\" title=\"5.1 Single Path\"></a>5.1 Single Path</h2><p><em>single path</em>는 개별 trajectory들을 이용하는 방법입니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=6m15s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/single.png\" alt=\"single\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p><br></p>\n<h2 id=\"5-2-Vine\"><a href=\"#5-2-Vine\" class=\"headerlink\" title=\"5.2 Vine\"></a>5.2 Vine</h2><p><em>vine</em>은 한 state에서 rollout을 이용하여 여러 action을 수행하는 방법입니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=6m32s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/vine1.png\" alt=\"vine1\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=6m49s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/vine2.png\" alt=\"vine2\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>estimation의 variance를 낮출 수 있지만 계산량이 많고 한 state에서 여러 action을 수행할 수 있어야 하기 때문에 현실적인 문제에 적용하기에는 어려움이 있습니다.</p>\n<p><br><br></p>\n<h1 id=\"6-Practical-Algorithm\"><a href=\"#6-Practical-Algorithm\" class=\"headerlink\" title=\"6. Practical Algorithm\"></a>6. Practical Algorithm</h1><p>앞서 single-path, vine 샘플링을 사용하는 두가지 방식의 policy optimization 알고리즘을 살펴봤습니다. 실용적인 알고리듬은 아래의 과정을 반복해서 수행합니다.</p>\n<p>1) Q-values의 몬테카를로 추정을 통해 state-action 쌍 집합을 single path 또는 vine 과정을 통해 수집함.</p>\n<p>2) 샘플 평균으로, (14)식의 목적함수와 제약식을 추정함</p>\n<ul>\n<li><strong>Equation (14).</strong><br>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}Q_{\\theta_\\mathrm{old} }(s,a)\\right] \\\\<br>\\mathrm{s.t.\\ }&amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} }}\\left[D_\\mathrm{KL}\\left(\\pi_{\\theta_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta<br>\\end{align}<br>$$</li>\n</ul>\n<p>3) policy parameter vector인 $\\theta$를 업데이트 하면서 제약조건이 있는 최적화 문제를 근사적으로 풂. 본 논문에서는 gradient를 직접 계산하는 것보다는 약간 더 계산량이 있는 line search와 conjugate gradient algorithm을 사용했습니다.</p>\n<p>3)에 대해서,  gradient의 covariance matrix를 사용하지 않고 KL divergence의 Hessian을 해석적으로 계산하여 Fisher Information Matrix를 구성했습니다.</p>\n<p>다시말해서</p>\n<p>$\\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial}{\\partial \\theta_i} \\log \\pi_{\\theta}(a_n\\vert s_n) \\frac{\\partial}{\\partial \\theta_j} \\log \\pi_{\\theta}(a_n\\vert s_n)$ 대신 $\\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_j} D_{KL}(\\pi_{\\theta_{old} }(\\cdot \\vert  s_n) \\vert \\vert  \\pi_{\\theta}(\\cdot \\vert  s_n))$를 계산한 것입니다.</p>\n<p>이 analytic estimator는 Hessian이나 trajectories의 모든 gradient를 저장하지 않아도 되기 때문에 대규모 환경을 고려할 경우 계산 상 이점이 있습니다. 좀 더 자세하게 설명한 Appendix C 부분을 살펴보겠습니다.</p>\n<p><br></p>\n<h2 id=\"Appendix-C-Efficiently-Solving-the-Trust-Region-Constrained-Optimization-Problem\"><a href=\"#Appendix-C-Efficiently-Solving-the-Trust-Region-Constrained-Optimization-Problem\" class=\"headerlink\" title=\"Appendix C. Efficiently Solving the Trust-Region Constrained Optimization Problem\"></a>Appendix C. Efficiently Solving the Trust-Region Constrained Optimization Problem</h2><p>우리가 풀고자 하는 식은 다음과 같습니다.</p>\n<p>$$<br>\\begin{align}<br>\\max\\quad &amp;L(\\theta) \\\\<br>\\mathrm{s.t.\\ } &amp;\\overline{D}_{KL}(\\theta_\\mathrm{old}, \\theta) \\leq \\delta<br>\\end{align}<br>$$</p>\n<p>이 최적화 문제를 풀기 위하여 우리는 gradient 기법을 이용할 것입니다. gradient 기법은 결국 update할 방향과 크기를 결정하는 문제로 생각할 수 있습니다. 이에 따라 아래와 같은 두가지 과정으로 진행합니다.</p>\n<p>1) 탐색 방향을 계산, 목적함수의 first-order approximation(선형근사)와 제약식의 quadratic approximation(2차 근사).</p>\n<p>2) 이동거리 계산을 위해 해당 방향으로 line search 수행.</p>\n<p>탐색 방향은 $Ax = g$ 수식을 근사적으로 풀어서 구합니다. 여기서 $A$ 는 Fisher Information Matrix이고, 이것은 KL divergence 제약식의 2차 근사 $\\overline{D}_\\mathrm{KL} (\\theta_\\mathrm{old}, \\theta) \\approx \\frac{1}{2}(\\theta - \\theta_\\mathrm{old})^T A(\\theta - \\theta_\\mathrm{old})$를 푸는 것입니다. 여기서 $A_{ij} = \\frac{\\partial}{\\partial \\theta_i} \\frac{\\partial}{\\partial \\theta_j} \\overline{D}_\\mathrm{KL}(\\theta_\\mathrm{old}, \\theta)$입니다.</p>\n<p>논문에서 이 부분에 대한 설명이 약간 부족해서 좀 더 내용을 추가하자면, $\\frac{1}{2}x^T A x - x^T g + b = 0$과 같은 quadratic equation의 해는 1차 derivative를 0으로 만드는 $x$를 찾음으로써 알 수 있습니다. 위에서 우리는 목적함수를 1차 근사시키고 제약조건은 2차 근사를 시킴으로써 quadratic equation 형태로 변형했기 때문에 같은 방식으로 풀 수 있습니다. 따라서 $g$는 1차근사시킨 목적함수의 계수가 될 것입니다.</p>\n<p>대규모 문제에서 $A$ 혹은 $A^{-1}$ 을 계산하는 것은 엄청난 계산량이 필요합니다. 하지만 conjugate gradient algorithm을 잘 이용하면 전체 $A$행렬(FIM)을 계산하지 않아도 $Ax=g$ 를 근사적으로 해결할 수 있게 해줍니다.</p>\n<p>탐색 방향 $s \\approx A^{-1}g$과 더불어 최대스텝길이(step size) $\\beta$도 계산할 필요가 있습니다.</p>\n<p>즉, $\\delta = \\overline{D}_\\mathrm{KL} \\approx \\frac{1}{2}(\\beta s)^T A(\\beta s) = \\frac{1}{2}\\beta^2s^TAs$ 이고, </p>\n<p>$\\beta = \\sqrt{2\\delta / s^T As}$가 됩니다. 이때 $\\delta$는 KL divergence의 boundary term 입니다.</p>\n<p>$s^T As$ 항은 Hessian-vector product로 계산할 수 있고, conjugate gradient 과정에서 계산됩니다.</p>\n<p>마지막으로, surrogate objective와 KL divergence 제약식을 위해 다음과 같은 함수를 정의하고 line search를 사용합니다.</p>\n<p>$$<br>L_{\\theta_\\mathrm{old} }(\\theta) - \\chi [\\overline{D}_{KL}(\\theta_\\mathrm{old}, \\theta) \\leq \\delta]<br>$$</p>\n<p>$\\chi[\\cdot]$ 함수는 []안의 조건이 맞으면 0이고 틀리면 무한으로 발산하는 함수입니다.</p>\n<p>위에서 계산한 $\\beta$ 의 최대 값부터 시작해서 목적함수가 개선될때까지 exponential하게 줄여 나갑니다. line search가 없었다면 이 알고리즘은 매우 큰 스텝으로 계산될 것이고 심각한 성능 저하가 발생할 것입니다.</p>\n<p>(여기서 살짝 예고를 하자면, 나중에 이 KL 기반 기법을 개선한 더 단순한 방법이 제안됩니다. PPO라고…)</p>\n<p><br><br></p>\n<h1 id=\"7-Connections-with-Prior-Work\"><a href=\"#7-Connections-with-Prior-Work\" class=\"headerlink\" title=\"7. Connections with Prior Work\"></a>7. Connections with Prior Work</h1><p>Natural Policy Gradient는 $L$의 선형 근사와 $\\overline D_\\mathrm{KL}$ 제약식을 2차근사 하는 아래의 식의 special case입니다.</p>\n<ul>\n<li>Equation (17).</li>\n</ul>\n<p>$L_{\\pi}(\\tilde\\pi) = \\eta(\\pi) + \\underset{s}{\\sum}\\rho_{\\pi}(s) \\underset{a}{\\sum}\\tilde\\pi(a\\vert s)A_\\pi(s,a)$</p>\n<p>$\\underset{\\theta}{\\max}\\quad [\\nabla_{\\theta}L_{\\theta_\\mathrm{old} }(\\theta) \\ \\rvert_{\\theta=\\theta_\\mathrm{old} } \\cdot (\\theta - \\theta_\\mathrm{old})]$</p>\n<p>$\\mathrm{s.t.}\\quad\\frac{1}{2}(\\theta_\\mathrm{old} - \\theta)^{T} A(\\theta_\\mathrm{old})(\\theta_\\mathrm{old}-\\theta) \\leq \\delta$</p>\n<p>$\\mathrm{where\\ }A(\\theta_{old})_{ij} = $</p>\n<p>$\\Large \\frac{\\partial}{\\partial \\theta_{i} } \\frac{\\partial}{\\partial \\theta_{j} }<br>E_{s \\sim \\rho_{\\pi} }[D_\\mathrm{KL}(\\pi(\\cdot\\rvert s, \\theta_\\mathrm{old})  \\rvert\\rvert \\pi(\\cdot \\rvert s, \\theta))] \\rvert_{\\theta=\\theta_\\mathrm{old} }$</p>\n<p>업데이트 식은 다음과 같습니다.</p>\n<p>$\\theta_\\mathrm{new} = \\theta_\\mathrm{old} +  \\color{Red}{\\frac{1}{\\lambda} } A(\\theta_\\mathrm{old})^{-1}\\nabla_{\\theta}L(\\theta) \\rvert_{\\theta=\\theta_\\mathrm{old} }$</p>\n<p>여기서 step size인 $\\frac{1}{\\lambda}$ 일반적으로 알고리즘의 파라미터로 취급되지만 TRPO는 각 업데이트 마다 제약식으로 사용한다는 점이 다릅니다. 사소한 차이처럼 보이지만, 큰 차원을 다루는 실험에서 성능을 크게 향상시켰습니다.</p>\n<p>또한 $L^2$ 제약식(혹은 페널티) 를 사용하면 표준 Policy Gradient 업데이트 식을 얻었습니다.</p>\n<ul>\n<li>Equation (18).</li>\n</ul>\n<p>$\\underset{\\theta}{\\max}\\quad[\\nabla_{\\theta} L_{\\theta_\\mathrm{old} }(\\theta) \\rvert_{\\theta=\\theta_\\mathrm{old} } \\cdot (\\theta - \\theta_\\mathrm{old})]$</p>\n<p>$\\mathrm{s.t.}\\quad \\frac{1}{2} \\vert\\vert \\theta - \\theta_\\mathrm{old} \\vert\\vert^2 \\leq \\delta$</p>\n<p>$L_{\\pi}(\\tilde\\pi) = \\eta(\\pi) + \\underset{s}{\\sum}\\rho_{\\pi}(s) \\underset{a}{\\sum}\\tilde\\pi(a\\vert s)A_\\pi(s,a) $를 이용해서 제약조건없이 $\\underset{\\pi}{\\max}\\quad L_{\\pi_\\mathrm{old} }(\\pi)$를 풀면<br>Policy Iteration update를 하는 것과 같습니다.</p>\n<p><br><br></p>\n<h1 id=\"8-Experiments\"><a href=\"#8-Experiments\" class=\"headerlink\" title=\"8. Experiments\"></a>8. Experiments</h1><ul>\n<li><p>다음과 같은 3 가지 궁금증을 해결하기 위하여 실험을 설계하였습니다.</p>\n<ol>\n<li><p>Single-path 와 vine 의 성능면에서 특징을 알고싶음.</p>\n</li>\n<li><p>Fixed penalty coefficient(NPG) 보다 fixed KL divergence를 사용하는 것(TRPO)으로<br>변경한 것이 얼마나 뚜렷한 차이를 보일지, 성능 면에서 어떤 영향을 미치는지 알고싶음.</p>\n</li>\n<li><p>TRPO가 큰 스케일의 문제를 해결할 수 있는지, 기존에 연구/적용되어왔던 방법들과<br>성능, 계산시간, 샘플 복잡도 면에서 비교하고 싶음.</p>\n</li>\n</ol>\n</li>\n</ul>\n<ul>\n<li><p>1, 2의 궁금증에 대해서 실험 환경에 single path, vine을 비롯한 여러 사전방법들을 함께 실험하였습니다.</p>\n</li>\n<li><p>3에 대해선, robotic locomotion과 Atari game에 TRPO를 실험하였습니다.</p>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"8-1-Simulated-Robotic-Locomotion\"><a href=\"#8-1-Simulated-Robotic-Locomotion\" class=\"headerlink\" title=\"8.1 Simulated Robotic Locomotion\"></a>8.1 Simulated Robotic Locomotion</h2><ul>\n<li>MuJoCo 시뮬레이터를 활용해 로봇 locomotion 실험 진행하였습니다.</li>\n<li>Robot State: positions and velocities, joint torques</li>\n</ul>\n<p><img src=\"https://i.imgur.com/bs5ATS3.png\" alt=\"\"></p>\n<ul>\n<li><p>수영, 점프, 걷기 행동을 학습시키고자 합니다.</p>\n<ul>\n<li><p>수영: 10 dimensional state space , reward: $r(x,u)=v_x - 10^{-5}\\vert \\vert u\\vert \\vert ^2$, quadratic penalty.</p>\n</li>\n<li><p>점프: 12 dimensional state space, episode의 끝은 높이와 각도 threshol로 점프를 판단, non-terminal state에 대해 +1 보상 추가.</p>\n</li>\n<li><p>걷기: 18 dimensional state space, 뜀뛰는 듯한 보법(점프에이전트)보다 부드러운 보법을 유도하기위해 페널티를 추가함.</p>\n</li>\n</ul>\n</li>\n</ul>\n<p><strong>Detailed Experiment Setup and used parameters, used network model</strong><br><img src=\"https://i.imgur.com/zgnsbw6.png\" alt=\"\"></p>\n<p><img src=\"https://i.imgur.com/FqdWC53.png\" alt=\"\"></p>\n<p>equation (12) : $\\max\\quad L_{\\theta_\\mathrm{old}(\\theta)}, \\ \\ \\mathrm{s.t.\\ }\\overline{D}_\\mathrm{KL}^{\\rho_{\\theta_\\mathrm{old} }}(\\theta_\\mathrm{old}, \\theta) \\leq \\delta$</p>\n<p>이 실험에서 $\\delta = 0.01$입니다.</p>\n<ul>\n<li><p>비교에 사용된 모델들</p>\n<ul>\n<li>TRPO Single-path</li>\n<li>TRPO vine</li>\n<li>CEM(cross-entropy method)</li>\n<li>CMA(covariance matrix adaptation)</li>\n<li>NPG (Lagrange Multiplier penalty coefficien를 사용하는 것이 차이점)</li>\n<li>Empirical FIM</li>\n<li>maximum KL(cartpole)</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://i.imgur.com/wSqolvS.png\" alt=\"\"></p>\n<ul>\n<li><p>제안한 single path, vine 모두 task를 잘 학습하는 것을 확인할 수 있습니다.</p>\n</li>\n<li><p>페널티를 고정하는 것보다, KL divergenc를 제약하는 것이 step size를 선택하는 부분에서 Robust합니다.</p>\n</li>\n<li><p>CEM/CMA는 derivative free 알고리즘으로, 샘플 복잡도가 네트워크 파라미터수 만큼 확장되어 high dimension 문제에서는 속도와 성능이 약한 모습을 보입니다.</p>\n</li>\n<li><p>maxKL은 제안방법보다 느린 학습성능을 보입니다.</p>\n</li>\n</ul>\n<p>TRPO video link: <a href=\"https://www.youtube.com/watch?v=jeid0wIrSn4\" target=\"_blank\" rel=\"noopener\">https://www.youtube.com/watch?v=jeid0wIrSn4</a></p>\n<ul>\n<li><p>사전지식이 적은 상태에서 여러 액션을 학습하는 실험으로 TRPO의 성능을 보입니다.</p>\n</li>\n<li><p>지금까지의 균형이나 걸음 같은 개념을 명시적으로 인코딩한 robotic locomotion에서의 사전 연구들과 차이를 보였습니다.</p>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"8-2-Playing-Games-from-Images\"><a href=\"#8-2-Playing-Games-from-Images\" class=\"headerlink\" title=\"8.2 Playing Games from Images\"></a>8.2 Playing Games from Images</h2><ul>\n<li>부분관찰가능하며 복잡한 observation인 환경에서 TRPO를 평가하기위해 raw image를 입력으로 하는 아타리게임 환경에서 실험하였습니다.</li>\n</ul>\n<ul>\n<li>Challenging point :<ul>\n<li>High dimensional space</li>\n<li>delayed reward</li>\n<li>non-stationary image (Enduro는 배경 이미지가 바뀌고, 반전처리가 됨)</li>\n<li>complex action sequence (Q*bert는 21개의 다른 발판에서 점프 해야함)</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li>ALE(Arcade Learning Environment) 에서 테스트</li>\n<li><p>DQN과 전처리 방식은 동일<br>  210 x 160 pixel -&gt; gray-scaling &amp; down-sampling -&gt; 110 x 84  -&gt; crop -&gt; 84 x 84</p>\n</li>\n<li><p>2 convolution layer ( 16 channels) , stride 2, FC layer with 20 units</p>\n</li>\n</ul>\n<p><img src=\"https://i.imgur.com/NJBC69d.png\" alt=\"\"> </p>\n<p><img src=\"https://i.imgur.com/wTe1OEW.png\" alt=\"\"></p>\n<p><img src=\"https://i.imgur.com/j22VtNQ.png\" alt=\"\"></p>\n<ul>\n<li><p>30시간 동안 500 iteration 학습을 하였습니다.</p>\n</li>\n<li><p>DQN과 MCST를 사용한 모델(UCC-I), 사람이 플레이한 것(Human) 과의 비교하였습니다.</p>\n</li>\n<li><p>이전 방법들에 비해 압도적인 점수를 기록하진 못했으나, 다양한 게임에서 적절한 게임 기록하였습니다.</p>\n</li>\n<li><p>이전 방법들과 다른 점은, 특정 task에 초점을 맞춰 설계하지않았는데 robotics나 game playing Task에도 적절한 성능을 내는 점에서 <strong>TRPO의 일반화 성능</strong>을 확인하였습니다.</p>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"9-Discussion\"><a href=\"#9-Discussion\" class=\"headerlink\" title=\"9. Discussion\"></a>9. Discussion</h1><ul>\n<li><p>Trust Region Policy Optimization 을 제안하였습니다.</p>\n</li>\n<li><p>KL Divergence 페널티로 $\\eta(\\pi)$를 최적화하는 알고리즘이 monotonically improve 함을 증명하였습니다.</p>\n</li>\n<li><p>Locomotion 도메인에서 여러 행동(수영, 걷기, 점프)을 제어하는 controller를 학습하였습니다.</p>\n</li>\n<li><p>Robotics와 게임 실험을 결합해, 시각정보와 센서데이터를 사용하는 로봇제어 정책을 학습시킬 수 있는 가능성을 보았습니다.</p>\n</li>\n<li><p>샘플의 복잡도를 상당히 줄일 수 있어 실제상황에 적용가능성을 보았습니다.</p>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"처음으로\"><a href=\"#처음으로\" class=\"headerlink\" title=\"처음으로\"></a>처음으로</h1><h2 id=\"PG-Travel-Guide\"><a href=\"#PG-Travel-Guide\" class=\"headerlink\" title=\"PG Travel Guide\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/\">PG Travel Guide</a></h2><p><br></p>\n<h1 id=\"이전으로\"><a href=\"#이전으로\" class=\"headerlink\" title=\"이전으로\"></a>이전으로</h1><h2 id=\"NPG-여행하기\"><a href=\"#NPG-여행하기\" class=\"headerlink\" title=\"NPG 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/25/4_npg/\">NPG 여행하기</a></h2><h2 id=\"NPG-Code\"><a href=\"#NPG-Code\" class=\"headerlink\" title=\"NPG Code\"></a><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/tnpg.py\" target=\"_blank\" rel=\"noopener\">NPG Code</a></h2><p><br></p>\n<h1 id=\"다음으로\"><a href=\"#다음으로\" class=\"headerlink\" title=\"다음으로\"></a>다음으로</h1><h2 id=\"TRPO-Code\"><a href=\"#TRPO-Code\" class=\"headerlink\" title=\"TRPO Code\"></a><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/trpo_gae.py\" target=\"_blank\" rel=\"noopener\">TRPO Code</a></h2><h2 id=\"GAE-여행하기\"><a href=\"#GAE-여행하기\" class=\"headerlink\" title=\"GAE 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/23/6_gae/\">GAE 여행하기</a></h2>"},{"title":"PG Travel implementation story","date":"2018-08-23T05:18:32.000Z","author":"이웅원, 장수영, 공민서, 양혁렬","subtitle":"피지여행 구현 이야기","_content":"\n\n# PG Travel implementation story\n\n- 구현 코드 링크 : [https://github.com/reinforcement-learning-kr/pg_travel](https://github.com/reinforcement-learning-kr/pg_travel)\n\n\n피지여행 프로젝트에서는 다음 7개 논문을 살펴보았습니다. 각 논문에 대한 리뷰는 이전 글들에서 다루고 있습니다. \n\n<a name=\"1\"></a>\n\n* [1] R. Sutton, et al., \"Policy Gradient Methods for Reinforcement Learning with Function Approximation\", NIPS 2000.\n<a name=\"2\"></a>\n* [2] D. Silver, et al., \"Deterministic Policy Gradient Algorithms\", ICML 2014.\n<a name=\"3\"></a>\n* [3] T. Lillicrap, et al., \"Continuous Control with Deep Reinforcement Learning\", ICLR 2016.\n<a name=\"4\"></a>\n* [4] S. Kakade, \"A Natural Policy Gradient\", NIPS 2002.\n<a name=\"5\"></a>\n* [5] J. Schulman, et al., \"Trust Region Policy Optimization\", ICML 2015.\n<a name=\"6\"></a>\n* [6] J. Schulman, et al., \"High-Dimensional Continuous Control using Generalized Advantage Estimation\", ICLR 2016.\n<a name=\"7\"></a>\n* [7] J. Schulman, et al., \"Proximal Policy Optimization Algorithms\", arXiv, https://arxiv.org/pdf/1707.06347.pdf.\n\n강화학습 알고리즘을 이해하는데 있어서 논문을 보고 이론적인 부분을 알아가는 것이 좋습니다. 하지만 실제 코드로 돌아가는 것은 논문만 보고는 알 수 없는 경우가 많습니다. 따라서 피지여행 프로젝트에서는 위 7개 논문 중에 DPG와 DDPG를 제외한 알고리즘을 구현해보았습니다. 구현한 알고리즘은 다음 4개입니다. 이 때, TRPO와 PPO 구현에는 GAE(General Advantage Estimator)가 함께 들어갑니다. \n \n* Vanilla Policy Gradient [[1](#1)]\n* TNPG(Truncated Natural Policy Gradient) [[4](#4)]\n* TRPO(Trust Region Policy Optimization) [[5](#5)]\n* PPO(Proximal Policy Optimization) [[7](#7)].\n\n바닥부터 저희가 구현한 것은 아니며 다음 코드들을 참고해서 구현하였습니다. Vanilla PG의 경우 RLCode의 깃헙을 참고하였습니다.\n\n* [OpenAI Baseline](https://github.com/openai/baselines/tree/master/baselines/trpo_mpi)\n* [Pytorch implemetation of TRPO](https://github.com/ikostrikov/pytorch-trpo)\n* [RLCode Actor-Critic](https://github.com/rlcode/reinforcement-learning-kr/tree/master/2-cartpole/2-actor-critic)\n\nGAE와 TRPO, PPO 논문에서는 Mujoco라는 물리 시뮬레이션을 학습 환경으로 사용합니다. 따라서 저희도 Mujoco로 처음 시작을 하였습니다. 하지만 Mujoco는 1달만 무료이고 그 이후부터 유료이며 확장성이 떨어집니다. Unity ml-agent는 기존 Unity를 그대로 사용하면서 쉽게 강화학습 에이전트를 붙일 수 있도록 설계되어 있습니다. Unity ml-agent에서는 저희가 살펴본 알고리즘 중에 가장 최신 알고리즘은 PPO를 적용해봤습니다. 기본적으로 제공하는 환경 이외에 저희가 customize 한 환경에서도 학습해봤습니다.  \n\n* mujoco-py: [https://github.com/openai/mujoco-py](https://github.com/openai/mujoco-py)\n* Unity ml-agent: [https://github.com/Unity-Technologies/ml-agents](https://github.com/Unity-Technologies/ml-agents)\n\n코드를 구현하고 환경에서 학습을 시키면서 여러가지 이슈들이 있었고 해결해내가는 과정이 있었습니다. 그 과정을 간단히 정리해서 공유하면 PG를 공부하는 분들께 도움일 될 것 같습니다. 저희가 구현한 순서대로 1. Mujoco 학습 2. Unity ml-agent 학습 3. Unity Curved Surface 로 이 포스트가 진행됩니다.\n\n<br/>\n## 1. Mujoco 학습\n일명 \"Continuous control\" 문제는 action이 discrete하지 않고 continuous한 경우를 말합니다. Mujoco는 continuous control에 강화학습을 적용한 논문들이 애용하는 시뮬레이터입니다. 저희가 리뷰한 논문 중에서도 TRPO, PPO, GAE에서 Mujoco를 사용합니다. 따라서 저희가 처음 피지여행 알고리즘을 적용한 환경으로 Mujoco를 선택했습니다. \n\nMujoco에는 Ant, HalfCheetah, Hopper, Humanoid, HumanoidStandup, InvertedPendulum, Reacher, Swimmer, Walker2d 과 같은 환경이 있습니다. 그 중에서 Hopper에 맞춰서 학습이 되도록 코드를 구현하였습니다. Mujoco 설치와 관련된 내용은 Wiki에 있습니다.\n\n</br>\n### 1.1 Hopper\nHopper는 외다리로 뛰어가는 것을 학습하는 것이 목표입니다. Hopper는 다음과 같이 생겼습니다. \n<img src=\"https://www.dropbox.com/s/wjxrelxyp014j3g/Screenshot%202018-08-23%2000.55.54.png?dl=1\">\n\n환경을 이해하려면 환경의 상태와 행동, 보상 그리고 학습하고 싶은 목표를 알아야합니다. \n\n- 상태 : 관절의 위치, 각도, 각속도\n- 행동 : 관절의 가해지는 토크\n- 보상 : 앞으로 나아가는 속도\n- 목표 : 최대한 앞으로 많이 나아가기\n\n즉 에이전트는 time step마다 관절의 위치와 각도를 받아와서 그 상태에서 어떻게 움직여야 앞으로 나아갈 수 있는지를 학습해야 합니다. 행동은 discrete action이 아닌 continuous action으로 -1과 1사이의 값을 가집니다. 만약 행동이 -1이라면 해당 관절에 시계반대방향으로 토크를 주는 것이고 행동이 +1이라면 해당 관절에 시계방향으로 토크를 주는 것입니다. \n\ncontinuous action을 주는 방법은 네트워크(Actor)의 output layer에서 activation function으로 tanh와 같은 것을 사용해서 continuous한 값을 출력하는 것이 있습니다. 하지만 피지여행 코드 구현에서는 action을 gaussian distribution에서 sampling 하였습니다. 이렇게 하면 분산을 일정하게 유지하면서 지속적인 exploration을 할 수 있습니다. 간단하게 그림으로 보자면 다음과 같습니다. \n<img src=\"https://www.dropbox.com/s/94g01zdxyf5oxu1/Screenshot%202018-08-23%2001.20.21.png?dl=1\">\n\n네트워크 구조와 행동을 선택하는 부분은 다음과 같습니다. Hidden Layer의 activation function으로 tanh를 사용했으며(ReLU를 테스트해보지는 않았습니다. 기존 TRPO, PPO 구현들과 논문에서 tanh를 사용하기 때문에 저희도 사용했습니다. 뒤에 유니티 환경에서는 Swish라는 것을 사용합니다.) log std를 0으로 고정함으로서 일정한 폭을 가지는 분포를 만들어낼 수 있습니다. 이 분포로부터 action을 sampling 합니다.\n\n<img src=\"https://www.dropbox.com/s/xfl9zxies0lmpm1/Screenshot%202018-08-23%2001.20.44.png?dl=1\">\n\n</br>\n### 1.2 Vanilla PG\nVanilla PG는 Actor-Critic의 가장 간단한 형태입니다. Vanilla PG는 이후의 구현에 대한 baseline이 됩니다. 구현이 가장 간단하면서 학습이 안되는 것은 아닙니다. 따라서 코드 전체 구조를 잡는데 Vanilla PG를 짜는 것이 도움이 됩니다. 전반적인 코드 구조는 다음과 같습니다.\n\n- iteration 마다 일정한 step 수만큼 환경에서 진행해서 샘플을 모은다\n- 모은 샘플로 Actor와 Critic을 학습한다\n- 반복한다\n \n\n```python\nepisodes = 0\nfor iter in range(15000):\n    actor.eval(), critic.eval()\n    memory = deque()\n    \n    while steps < 2048:\n        episodes += 1\n        state = env.reset()\n        state = running_state(state)\n        score = 0\n        for _ in range(10000):\n            mu, std, _ = actor(torch.Tensor(state).unsqueeze(0))\n            action = get_action(mu, std)[0]\n            next_state, reward, done, _ = env.step(action)\n            next_state = running_state(next_state)\n\n            if done:\n                mask = 0\n            else:\n                mask = 1\n\n            memory.append([state, action, reward, mask])\n            state = next_state\n\n            if done:\n                break\n                \n    actor.train(), critic.train()\n    train_model(actor, critic, memory, actor_optim, critic_optim)\n```\n\nmemory에 sample을 저장할 때 sample은 state와 action, reward, mask(마지막 state일 경우 0 나머지 1)입니다. mask의 경우 뒤에서 return이나 advantage를 계산할 때 사용됩니다. 또 하나 염두에 두어야할 것은 running_state 입니다. running_state는 input으로 들어오는 state의 scale이 일정하지 않기 때문에 사용합니다. 즉 state의 각 dimension을 평균 0 분산 1로 standardization 하는 것입니다. 따라서 모델을 저장할 때 각 dimension 마다의 평균과 분산도 같이 저장해서 테스트할 때 불러와서 사용해야 합니다.\n\nVanilla PG의 경우 학습 부분이 상당히 간단합니다. 다음 코드를 보시면 메모리에서 state, action, reward, mask를 꺼냅니다. reward와 mask를 통해 return을 구할 수 있고 이 return을 통해 actor를 업데이트 할 수 있습니다 (REINFORCE 알고리즘을 떠올리시면 됩니다). 여기서 critic이 하는 일은 없지만 뒤의 알고리즘들과 코드의 통일성을 위해 fake로 넣어놨습니다. Return은 평균을 빼고 분산으로 나눠서 standardize 합니다. \n\n```python\ndef train_model(actor, critic, memory, actor_optim, critic_optim):\n    memory = np.array(memory)\n    states = np.vstack(memory[:, 0])\n    actions = list(memory[:, 1])\n    rewards = list(memory[:, 2])\n    masks = list(memory[:, 3])\n\n    returns = get_returns(rewards, masks)\n    train_critic(critic, states, returns, critic_optim)\n    train_actor(actor, returns, states, actions, actor_optim)\n    return returns\n```\n\n이 코드로 Hopper 환경에서 학습한 그래프는 다음과 같습니다. \n<img src=\"https://www.dropbox.com/s/asoysfuk76zs1dk/Screenshot%202018-08-23%2001.30.58.png?dl=1\">\n\n</br>\n### 1.3 TNPG\nNPG를 이용한 parameter update 식은 다음과 같습니다. \n\n$$\\bar{w}=F(\\theta)^{-1}\\nabla\\eta(\\theta)$$\n\nNPG를 구현하려면 KL-divergence의 Hessian의 inverse를 구해야하는 문제가 생깁니다. 현재와 같이 Deep Neural Network를 쓰는 경우에 Hessian의 inverse를 직접적으로 구하는 것은 computationally inefficient 합니다. 따라서 직접 구하지 않고 Conjugate gradient 방법을 사용해서 Fisher Vector Product ($$F^{-1}g$$)를 구합니다. 이러한 알고리즘을 Truncated Natural Policy Gradient(TNPG)라고 부릅니다. \n\nTNPG에서 parameter update를 구하는 과정은 다음과 같습니다. \n1. Return 구하기\n2. Critic 학습하기\n3. logp * return --> loss 구하기\n4. loss의 미분과 kl-divergence의 2차 미분을 통해 step direction 구하기\n5. 구한 step direction으로 parameter update\n\n```python\ndef train_model(actor, critic, memory, actor_optim, critic_optim):\n    memory = np.array(memory)\n    states = np.vstack(memory[:, 0])\n    actions = list(memory[:, 1])\n    rewards = list(memory[:, 2])\n    masks = list(memory[:, 3])\n\n    # ----------------------------\n    # step 1: get returns\n    returns = get_returns(rewards, masks)\n\n    # ----------------------------\n    # step 2: train critic several steps with respect to returns\n    train_critic(critic, states, returns, critic_optim)\n\n    # ----------------------------\n    # step 3: get gradient of loss and hessian of kl\n    loss = get_loss(actor, returns, states, actions)\n    loss_grad = torch.autograd.grad(loss, actor.parameters())\n    loss_grad = flat_grad(loss_grad)\n    step_dir = conjugate_gradient(actor, states, loss_grad.data, nsteps=10)\n\n    # ----------------------------\n    # step 4: get step direction and step size and update actor\n    params = flat_params(actor)\n    new_params = params + 0.5 * step_dir\n    update_model(actor, new_params)\n    \n```\n\nconjugate gradient 코드는 OpenAI baseline에서 가져왔습니다. 이 코드는 원래 John schulmann 개인 repository에 있는 그대로 사용하는 것입니다. nsteps 만큼 iterataion을 반복하며 결국 x를 구하는 것인데 이 x가 step direction 입니다. \n\n```python\n# from openai baseline code\n# https://github.com/openai/baselines/blob/master/baselines/common/cg.py\ndef conjugate_gradient(actor, states, b, nsteps, residual_tol=1e-10):\n    x = torch.zeros(b.size())\n    r = b.clone()\n    p = b.clone()\n    rdotr = torch.dot(r, r)\n    for i in range(nsteps):\n        _Avp = fisher_vector_product(actor, states, p)\n        alpha = rdotr / torch.dot(p, _Avp)\n        x += alpha * p\n        r -= alpha * _Avp\n        new_rdotr = torch.dot(r, r)\n        betta = new_rdotr / rdotr\n        p = r + betta * p\n        rdotr = new_rdotr\n        if rdotr < residual_tol:\n            break\n    return x\n```\n\nfisher_vector_product는 kl-divergence의 2차미분과 어떠한 vector의 곱인데 p는 처음에 gradient 값이었다가 점차 업데이트가 됩니다. kl-divergence의 2차 미분을 구하는 과정은 다음과 같습니다. 일단 kl-divergence를 현재 policy에 대해서 구한 다음에 actor parameter에 대해서 미분합니다. 이렇게 미분한 gradient를 일단 flat하게 핀 다음에 p라는 벡터와 곱해서 하나의 값으로 만듭니다. 그 값을 다시 actor의 parameter로 만듦으로서 따로 KL-divergence의 2차미분을 구하지않고 Fisher vector product를 구할 수 있습니다.\n\n```python\ndef fisher_vector_product(actor, states, p):\n    p.detach()\n    kl = kl_divergence(new_actor=actor, old_actor=actor, states=states)\n    kl = kl.mean()\n    kl_grad = torch.autograd.grad(kl, actor.parameters(), create_graph=True)\n    kl_grad = flat_grad(kl_grad)  # check kl_grad == 0\n\n    kl_grad_p = (kl_grad * p).sum()\n    kl_hessian_p = torch.autograd.grad(kl_grad_p, actor.parameters())\n    kl_hessian_p = flat_hessian(kl_hessian_p)\n\n    return kl_hessian_p + 0.1 * p\n```\n\nTNPG 학습 결과는 다음과 같습니다. \n<img src=\"https://www.dropbox.com/s/uc4c0s00qbs33nr/Screenshot%202018-08-23%2001.53.17.png?dl=1\">\n\n</br>\n### 1.4 TRPO\nTRPO와 NPG가 다른 점은 surrogate loss 사용과 trust region 입니다. 하지만 실제로 구현해서 학습을 시켜본 결과 trust region을 넘어가서 back tracking line search를 하는 경우는 거의 없습니다. 따라서 주된 변화는 surrogate loss에 있다고 보셔도 됩니다. Surrogate loss에서 advantage function을 사용하는데 본 코드 구현에서는 GAE를 사용하였습니다. TRPO 업데이트 식은 다음과 같습니다. Q function 위치에 GAE가 들어갑니다.\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi\\_\\theta(a\\vert s)}{q(a\\vert s)}Q\\_{\\theta\\_\\mathrm{old} }(s,a)\\right] \\\\\\\\\n\\mathrm{s.t.\\ }&E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} }}\\left[D\\_\\mathrm{KL}\\left(\\pi\\_{\\theta\\_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi\\_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta\n\\end{align}\n$$\n\n\nGAE를 구하는 코드는 다음과 같습니다. GAE는 td-error의 discounted summation이라고 볼 수 있습니다. 마지막에 advants를 standardization 하는 것은 return에서 하는 것과 같은 효과를 봅니다. 하지만 standardization을 안하고 실험을 해보지는 않았습니다.\n\n```python\ndef get_gae(rewards, masks, values):\n    rewards = torch.Tensor(rewards)\n    masks = torch.Tensor(masks)\n    returns = torch.zeros_like(rewards)\n    advants = torch.zeros_like(rewards)\n\n    running_returns = 0\n    previous_value = 0\n    running_advants = 0\n\n    for t in reversed(range(0, len(rewards))):\n        running_returns = rewards[t] + hp.gamma * running_returns * masks[t]\n        running_tderror = rewards[t] + hp.gamma * previous_value * masks[t] - \\\n                    values.data[t]\n        running_advants = running_tderror + hp.gamma * hp.lamda * \\\n                          running_advants * masks[t]\n\n        returns[t] = running_returns\n        previous_value = values.data[t]\n        advants[t] = running_advants\n\n    advants = (advants - advants.mean()) / advants.std()\n    return returns, advants\n```\n\nSurrogate loss를 구하는 코드는 다음과 같습니다. Advantage function(GAE)를 구하고 나면 이전 policy와 현재 policy 사이의 ratio를 구해서 advantage function에 곱하면 됩니다. 이 때 사실 old policy와 new policy는 값은 같지만 old policy는 clone()이나 detach()를 사용해서 update가 안되게 만들어줍니다.\n\n```python\ndef surrogate_loss(actor, advants, states, old_policy, actions):\n    mu, std, logstd = actor(torch.Tensor(states))\n    new_policy = log_density(torch.Tensor(actions), mu, std, logstd)\n    advants = advants.unsqueeze(1)\n\n    surrogate = advants * torch.exp(new_policy - old_policy)\n    surrogate = surrogate.mean()\n    return surrogate\n\n```\n\nActor의 step direction을 구하는 것은 TNPG와 동일합니다. TNPG에서는 step direction으로 바로 업데이트 했지만 TRPO는 다음과 같은 작업을 해줍니다. Full step을 구하는 과정이라고 볼 수 있습니다. \n\n```python\n# ----------------------------\n# step 4: get step direction and step size and full step\nparams = flat_params(actor)\nshs = 0.5 * (step_dir * fisher_vector_product(actor, states, step_dir)\n             ).sum(0, keepdim=True)\nstep_size = 1 / torch.sqrt(shs / hp.max_kl)[0]\nfull_step = step_size * step_dir\n\n```\n\n이렇게 full step을 구하고 나면 Trust region optimization 단계에 들어갑니다. expected improvement는 구한 step 만큼 parameter space에서 움직였을 때 예상되는 performance 변화입니다. 이 값은 kl-divergence와 함께 trust region 안에 있는지 밖에 있는지 판단하는 근거가 됩니다. expected improve는 출발점에서의 gradient * full step으로 구합니다. 그리고 10번을 돌아가며 Back-tracking line search를 실시합니다. 처음에는 full step 만큼 가본 다음에 kl-divergence와 emprovement를 통해 trust region 안이면 루프 탈출, 밖이면 full step을 반만큼 쪼개서 다시 이 과정을 반복합니다.  \n\n```python\n# ----------------------------\n# step 5: do backtracking line search for n times\nold_actor = Actor(actor.num_inputs, actor.num_outputs)\nupdate_model(old_actor, params)\nexpected_improve = (loss_grad * full_step).sum(0, keepdim=True)\nexpected_improve = expected_improve.data.numpy()\n\nflag = False\nfraction = 1.0\nfor i in range(10):\n    new_params = params + fraction * full_step\n    update_model(actor, new_params)\n    new_loss = surrogate_loss(actor, advants, states, old_policy.detach(),\n                              actions)\n    new_loss = new_loss.data.numpy()\n    loss_improve = new_loss - loss\n    expected_improve *= fraction\n    kl = kl_divergence(new_actor=actor, old_actor=old_actor, states=states)\n    kl = kl.mean()\n\n    print('kl: {:.4f}  loss improve: {:.4f}  expected improve: {:.4f}  '\n          'number of line search: {}'\n          .format(kl.data.numpy(), loss_improve, expected_improve[0], i))\n\n    # see https: // en.wikipedia.org / wiki / Backtracking_line_search\n    if kl < hp.max_kl and (loss_improve / expected_improve) > 0.5:\n        flag = True\n        break\n\n    fraction *= 0.5\n```\n\nCritic의 학습은 단순히 value function과 return의 MSE error를 계산해서 loss로 잡고 loss를 최소화하도록 학습합니다. TRPO 학습 결과는 다음과 같습니다.\n<img src=\"https://www.dropbox.com/s/rc9hxsx1kvokcrv/Screenshot%202018-08-23%2013.36.51.png?dl=1\">\n\n</br>\n### 1.5 PPO\nPPO의 장점을 꼽으라면 GPU 사용하기 좋고 sample efficiency가 늘어난다는 것입니다. TNPG와 TRPO의 경우 한 번 모은 sample은 모델을 단 한 번 업데이트하는데 사용하지만 PPO의 경우 몇 mini-batch로 epoch를 돌리기 때문입니다. GAE를 사용한다는 것은 같고 Conjugate gradient나 Fisher vector product나 back tracking line search가 다 빠집니다. 대신 loss function clip으로 monotonic improvement를 보장하게 학습합니다. 따라서 코드가 상당히 간단해집니다. \n\n다음 코드 부분이 PPO의 전체라고 봐도 무방합니다. PPO는 다음과 같은 순서로 학습합니다. \n\n- batch를 random suffling하고 mini batch를 추출\n- value function 구하기\n- critic loss 구하기 (clip을 사용해도 되고 TRPO와 같이 그냥 학습시켜도 됌)\n- surrogate loss 구하기\n- surrogate loss clip해서 actor loss 만들기\n- actor와 critic 업데이트\n\nActor의 loss를 구하는 것은 다음 식의 값을 구하는 것입니다. 이 식을 구하려면 ratio에 한 번 클립하고 loss 값을 한 번 min을 취하면 됩니다.\n\n$$L^{CLIP}(\\theta) = \\hat{E}_t [min(r_t(\\theta) \\, \\hat{A}_t,  clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\, \\hat{A}_t)]$$\n\n이 코드 구현에서는 actor와 critic을 따로 모델로 만들어서 따로 따로 업데이트를 하지만 하나로 만든다면 loss로 한 번만 업데이트하면 됩니다. 또한 entropy loss를 최종 loss에 더해서 regularization 효과를 볼 수도 있습니다. Critic loss에 clip 해주는 것은 OpenAI baseline의 ppo2 코드를 참조하였습니다.\n\n```python\n# step 2: get value loss and actor loss and update actor & critic\nfor epoch in range(10):\n    np.random.shuffle(arr)\n\n    for i in range(n // hp.batch_size):\n        batch_index = arr[hp.batch_size * i: hp.batch_size * (i + 1)]\n        batch_index = torch.LongTensor(batch_index)\n        inputs = torch.Tensor(states)[batch_index]\n        returns_samples = returns.unsqueeze(1)[batch_index]\n        advants_samples = advants.unsqueeze(1)[batch_index]\n        actions_samples = torch.Tensor(actions)[batch_index]\n        oldvalue_samples = old_values[batch_index].detach()\n\n        loss, ratio = surrogate_loss(actor, advants_samples, inputs,\n                                     old_policy.detach(), actions_samples,\n                                     batch_index)\n\n        values = critic(inputs)\n        clipped_values = oldvalue_samples + \\\n                         torch.clamp(values - oldvalue_samples,\n                                     -hp.clip_param,\n                                     hp.clip_param)\n        critic_loss1 = criterion(clipped_values, returns_samples)\n        critic_loss2 = criterion(values, returns_samples)\n        critic_loss = torch.max(critic_loss1, critic_loss2).mean()\n\n        clipped_ratio = torch.clamp(ratio,\n                                    1.0 - hp.clip_param,\n                                    1.0 + hp.clip_param)\n        clipped_loss = clipped_ratio * advants_samples\n        actor_loss = -torch.min(loss, clipped_loss).mean()\n\n        loss = actor_loss + 0.5 * critic_loss\n\n        critic_optim.zero_grad()\n        loss.backward(retain_graph=True)\n        critic_optim.step()\n\n        actor_optim.zero_grad()\n        loss.backward()\n        actor_optim.step()\n```\n\nPPO의 학습 결과는 다음과 같습니다. \n<img src=\"https://www.dropbox.com/s/rkxa836ap931kbd/Screenshot%202018-08-23%2013.50.57.png?dl=1\">\n\n\n</br>\n## 2. Unity ml-agent 학습\nMujoco Hopper(half-cheetah와 같은 것도)에 Vanilla PG, TNPG, TRPO, PPO를 구현해서 적용했습니다. Mujoco의 경우 이미 Hyper parameter와 같은 정보들이 논문이나 블로그에 있기 때문에 상대적으로 continuous control로 시작하기에는 좋습니다. 맨 처음에 말했듯이 Mujoco는 1달만 무료이고 그 이후부터 유료이며 확장성이 떨어집니다. 좀 더 general한 agent를 학습시키기에 좋은 환경이 필요합니다. 따라서 Unity ml-agent를 살펴봤습니다. Repository는 다음과 같습니다. \n- [Unity ml-agent repository](https://github.com/Unity-Technologies/ml-agents)\n- [Unity ml-agent homepage](https://unity3d.com/machine-learning/)\n\n<img src=\"https://www.dropbox.com/s/lapholj8r4nxmb1/Screenshot%202018-08-24%2013.41.31.png?dl=1\">\n\n현재 Unity ml-agent에서 기본으로 제공하는 환경은 다음과 같습니다. Unity ml-agent는 기존 Unity를 그대로 사용하면서 쉽게 강화학습 에이전트를 붙일 수 있도록 설계되어 있습니다. Unity ml-agent에서는 Walker 환경에서 저희가 살펴본 알고리즘 중에 가장 최신 알고리즘은 PPO를 적용해봤습니다. 이 포스트를 보시는 분들은 이 많은 다른 환경에 자유롭게 저희 코드를 적용할 수 있습니다.\n- [각 환경에 대한 설명](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md)\n\n<img src=\"https://www.dropbox.com/s/lrbodw5dypxowmw/Screenshot%202018-08-24%2014.06.14.png?dl=1\">\n\nUnity ml-agent를 이용해서 강화학습을 하기 위해서는 다음과 같이 진행됩니다. 단계별로 설명하겠습니다. \n\n- Unity에서 환경 만들기\n- Python에서 unity 환경 불러와서 테스트하기\n- 기존에 하던대로 학습하기\n\n\n</br>\n### 2.1 Walker 환경 만들기\n강화학습을 하는 많은 분들이 Unity를 한 번도 다뤄보지 않은 경우가 많습니다. 저도 그런 경우라서 어떻게 환경을 만들어야할지 처음에는 감이 잡히지 않았습니다. 하지만 Unity ml-agent에서는 상당히 자세한 guide를 제공합니다. 다음은 Unity ml-agent의 가장 기본적인 환경인 3DBall에 대한 tutorial입니다. 설치 guide도 제공하고 있으니 참고하시면 될 것 같습니다.\n- [3DBall 예제 tutorial](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Basic-Guide.md)\n- [Unity ml-agent 설치 guide](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md)\n\nUnity ml-agent에서 제공하는 3DBall tutorial을 참고해서 Walker 환경을 만들었습니다. Walker 환경을 만드는 과정을 간단히 말씀드리겠습니다. 다음 그림의 단계들을 동일하므로 따라하시면 됩니다. Unity를 열고 unity-environment로 들어가시면 됩니다.\n<img src=\"https://www.dropbox.com/s/fbdqg781w46a5mz/Screenshot%202018-08-24%2014.50.50.png?dl=1\">\n\n그러면 화면 하단에서 다음과 같은 것을 볼 수 있습니다. Assets/ML-Agents/Examples로 들어가보면 Walker가 있습니다. Scenes에서 Walker를 더블클릭하면 됩니다.\n<img src=\"https://www.dropbox.com/s/h349xml3faln0wy/Screenshot%202018-08-24%2014.52.14.png?dl=1\">\n\n더블클릭해서 나온 화면에서 오른쪽 상단의 파란색 화살표를 누르면 환경이 실행이 됩니다. 저희가 학습하고자 하는 agent는 바로 이녀석입니다. 왼쪽 리스트를 보면 WalkerPair가 11개가 있는 것을 볼 수 있습니다. Unity ml-agent 환경은 기본적으로 Multi-agent로 학습하도록 설정되어있습니다. 따라서 여러개의 Walker들이 화면에 보이는 것입니다. \n<img src=\"https://www.dropbox.com/s/cy8m5kqdmkhopjo/Screenshot%202018-08-24%2014.54.57.png?dl=1\">\n\n리스트 중에 Walker Academy를 클릭해서 그 하위에 있는 WalkerBrain을 더블클릭합니다. 그러면 화면 오른쪽에 다음과 같은 Brain 설정을 볼 수 있습니다. Brain은 쉽게 말해서 Agent라고 생각하면 됩니다. 이 Agent는 상태로 212차원의 vector가 주어지며 다 continuous한 값을 가집니다. 행동은 39개의 행동을 할 수 있으며 다 Continuous입니다. Mujoco에 비해서 상태나 행동의 차원이 상당히 높습니다. 여기서 중요한 것은 Brain Type입니다. Brain type은 internal, external, player, heuristic이 있습니다. player로 type을 설정하고 화면 상단의 play 버튼을 누르면 여러분이 agent를 움직일 수 있습니다. 하지만 Walker는 사람이 움직이는게 거의 불가능하므로 player 기능은 사용할 수 없습니다. 다른 환경에서는 사용해볼 수 있으니 재미로 한 번 플레이해보시면 좋습니다! \n<center><img src=\"https://www.dropbox.com/s/uxfm162f1scbzo5/Screenshot%202018-08-24%2015.09.04.png?dl=1\" width=\"400px\"></center>\n\n이번에는 WalkerPair에서 WalkerAgent를 더블클릭해보겠습니다. 이 설정을 보아 5000 step이 episode의 max step인 것을 볼 수 있습니다.\n<center><img src=\"https://www.dropbox.com/s/r6gwemlczwic2ma/Screenshot%202018-08-24%2015.16.19.png?dl=1\" width=\"400px\"></center>\n\n이제 상단 file menu에서 build setting에 들어갑니다. 환경을 build해서 python 코드에서 import하기 위해서입니다. 물론 unity 환경과 python 코드를 binding해주는 부분은 ml-agent 코드 안에 있습니다. Build 버튼을 누르면 환경이 build가 됩니다.\n<center><img src=\"https://www.dropbox.com/s/4dtgoz1k8896vxs/Screenshot%202018-08-24%2015.19.07.png?dl=1\" width=\"500px\"></center>\n\n\n</br>\n### 2.2 Python에서 unity 환경 불러와서 테스트하기\n환경을 build 했으면 build한 환경을 python에서 불러와서 random action으로 테스트해봅니다. 환경을 테스트하는 코드는 pg_travel repository에서 unity 폴더 밑에 있습니다. test_env.py라는 코드는 간단하게 다음과 같습니다. Build한 walker 환경은 env라는 폴더 밑에 넣어줍니다. unityagent를 import하는데 ml-agent를 git clone 해서 python 폴더 내에서 \"python setup.py install\"을 실행했다면 문제없이 import 됩니다. UnityEnvironment를 통해 env라는 환경을 선언할 수 있습니다. 이렇게 선언하고 나면 gym과 상당히 유사한 형태로 환경과 상호작용이 가능합니다. \n\n```python\nimport numpy as np\nfrom unityagents import UnityEnvironment\nfrom utils.utils import get_action\n\nif __name__==\"__main__\":\n    env_name = \"./env/walker_test\"\n    train_mode = False\n\n    env = UnityEnvironment(file_name=env_name)\n\n    default_brain = env.brain_names[0]\n    brain = env.brains[default_brain]\n    env_info = env.reset(train_mode=train_mode)[default_brain]\n\n    num_inputs = brain.vector_observation_space_size\n    num_actions = brain.vector_action_space_size\n    num_agent = env._n_agents[default_brain]\n\n    print('the size of input dimension is ', num_inputs)\n    print('the size of action dimension is ', num_actions)\n    print('the number of agents is ', num_agent)\n   \n    score = 0\n    episode = 0\n    actions = [0 for i in range(num_actions)] * num_agent\n    for iter in range(1000):\n        env_info = env.step(actions)[default_brain]\n        rewards = env_info.rewards\n        dones = env_info.local_done\n        score += rewards[0]\n\n        if dones[0]:\n            episode += 1\n            score = 0\n            print('{}th episode : mean score of 1st agent is {:.2f}'.format(\n                episode, score))\n```\n\n위 코드를 실행하면 다음과 같이 실행창에 뜹니다. External brain인 것을 알 수 있고 default_brain은 brain 중에 하나만 가져왔기 때문에 number of brain은 1이라고 출력합니다. input dimension은 212이고 action dimension은 39이고 agent 수는 11인 것으로봐서 제대로 환경이 불러와진 것을 확인할 수 있습니다. \n<img src=\"https://www.dropbox.com/s/cioa9h7qu25vonz/Screenshot%202018-08-24%2015.47.43.png?dl=1\">\n\n이 환경에서 행동하려면 agent 숫자만큼 행동을 줘야합니다. 모두 0로 행동을 주고 실행하면 다음과 같이 뒤로 넘어지는 행동을 반복합니다. env.step(actions)[default_brain]으로 env_info를 받아오면 거기서부터 reward와 done, next_state를 받아올 수 있습니다. 이제 학습하기만 하면 됩니다. \n<img src=\"https://www.dropbox.com/s/8qrmxoski6p4n07/Screenshot%202018-08-24%2016.00.21.png?dl=1\">\n\n</br>\n### 2.3 Walker 학습하기\n기존에 Mujoco에 적용했던 PPO 코드를 그대로 Walker에 적용하니 잘 학습이 안되었습니다. 다음 사진이 저희가 중간 해커톤으로 모여서 이 상황을 공유할 때의 사진입니다.\n<img src=\"https://i.imgur.com/1aR2Z77.png\" width=500px>\n\nUnity ml-agent에서는 PPO를 기본 agent로 제공합니다. 학습 코드도 제공하기 때문에 mujoco에 적용했던 코드와의 차이점을 분석할 수 있었습니다. mujoco 코드와 ml-agent baseline 코드의 차이점은 다음과 같습니다. \n\n- agent 여러개를 이용, 별개의 memory에 저장한 후에 gradient를 합침\n- GAE 및 time horizon 등 hyper parameter가 다름\n- Actor와 Critic의 layer가 1층 더 두꺼우며 hidden layer 자체의 사이즈도 더 큼\n- hidden layer의 activation function이 tanh가 아닌 swish\n\nml-agent baseline 코드리뷰할 때 작성했던 마인드맵은 다음과 같습니다.\n<img src=\"https://i.imgur.com/YeaEntG.png\">\n\n크게는 두 가지를 개선해서 성능이 많이 향상했습니다.\n1. Network 수정\n2. multi-agent를 활용해서 학습\n\nNetwork 코드는 다음과 같습니다. Hidden Layer를 하나 더 늘렸으며 swish activation function을 사용할 수 있도록 변경했습니다. 사실 swish라는 activation function은 처음 들어보는 생소한 함수였습니다. 하지만 ml-agent baseline에서 사용했다는 사실과 구현이 상당히 간단하다는 점에서 저희 코드에 적용했습니다. 단순히 x * sigmoid(x) 를 하면 됩니다. swish는 별거 아닌 것 같지만 상당한 성능 개선을 가져다줬습니다. 사실 ReLU나 ELU 등 여러 다른 activation function을 적용해서 비교해보는게 best긴 하지만 시간 관계상 그렇게까지 테스트해보지는 못했습니다. 기존에 TRPO나 PPO는 왜 tanh를 사용했었는지도 의문인 점입니다.\n\n```python\nclass Actor(nn.Module):\n    def __init__(self, num_inputs, num_outputs, args):\n        self.args = args\n        self.num_inputs = num_inputs\n        self.num_outputs = num_outputs\n        super(Actor, self).__init__()\n        self.fc1 = nn.Linear(num_inputs, args.hidden_size)\n        self.fc2 = nn.Linear(args.hidden_size, args.hidden_size)\n        self.fc3 = nn.Linear(args.hidden_size, args.hidden_size)\n        self.fc4 = nn.Linear(args.hidden_size, num_outputs)\n\n        self.fc4.weight.data.mul_(0.1)\n        self.fc4.bias.data.mul_(0.0)\n\n    def forward(self, x):\n        if self.args.activation == 'tanh':\n            x = F.tanh(self.fc1(x))\n            x = F.tanh(self.fc2(x))\n            x = F.tanh(self.fc3(x))\n            mu = self.fc4(x)\n        elif self.args.activation == 'swish':\n            x = self.fc1(x)\n            x = x * F.sigmoid(x)\n            x = self.fc2(x)\n            x = x * F.sigmoid(x)\n            x = self.fc3(x)\n            x = x * F.sigmoid(x)\n            mu = self.fc4(x)\n        else:\n            raise ValueError\n\n        logstd = torch.zeros_like(mu)\n        std = torch.exp(logstd)\n        return mu, std, logstd\n```\n\nswish와 tanh를 사용한 학습을 비교한 그래프입니다. 하늘색 그래프가 swish를 사용한 결과, 파란색이 tanh를 사용한 결과입니다. score는 episode 마다의 reward의 합입니다.\n<center><img src=\"https://www.dropbox.com/s/3d07c1kql4h5oqk/Screenshot%202018-08-24%2016.33.45.png?dl=1\" width=\"350px\"></center>\n\n이제 multi-agent로 학습하도록 변경하면 됩니다. PPO의 경우 memory에 time horizon 동안의 sample을 시간순서대로 저장하고 GAE를 구한 이후에 minibatch로 추출해서 학습합니다. 따라서 여러개의 agent로 학습하기 위해서는 memory를 따로 만들어서 각각의 GAE를 구해서 학습해야합니다. Unity에서는 Mujoco에서 했던 것처럼 deque로 memory를 만들지 않고 따로 named tuple로 구현한 memory class를 import 해서 사용했습니다. utils 폴더 밑에 memory.py 코드에 구현되어있으며 코드는 https://github.com/pytorch/tutorials/blob/master/Reinforcement%20(Q-)Learning%20with%20PyTorch.ipynb\n에서 가져왔습니다. \n\nstate, action, reward, mask를 저장하는데 불러올 때 각각을 따로 불러올 수 있기 때문에 비효율적 시간을 많이 줄여줍니다. \n```python\nTransition = namedtuple('Transition', ('state', 'action', 'reward', 'mask'))\n\n\nclass Memory(object):\n    def __init__(self):\n        self.memory = []\n\n    def push(self, state, action, reward, mask):\n        \"\"\"Saves a transition.\"\"\"\n        self.memory.append(Transition(state, action, reward, mask))\n\n    def sample(self):\n        return Transition(*zip(*self.memory))\n\n    def __len__(self):\n        return len(self.memory)\n```\n\nmain.py 에서는 이 memory를 agent의 개수만큼 생성합니다. \n\n```python\nmemory = [Memory() for _ in range(num_agent)]\n```\nsample을 저장할 때도 agent마다 따로 따로 저장합니다. \n\n```python\nfor i in range(num_agent):\n    memory[i].push(states[i], actions[i], rewards[i], masks[i])\n\n```\n\ntime horizon이 끝나면 모은 sample 들을 가지고 학습하기 위한 값으로 만드는 과정을 진행합니다. 각각의 memory를 가지고 GAE와 old_policy, old_value 등을 계산해서 하나의 batch로 합칩니다. 그렇게 train_model 메소드에 전달하면 기존과 동일하게 agent를 업데이트합니다.\n\n```python\nsts, ats, returns, advants, old_policy, old_value = [], [], [], [], [], []\n\nfor i in range(num_agent):\n    batch = memory[i].sample()\n    st, at, rt, adv, old_p, old_v = process_memory(actor, critic, batch, args)\n    sts.append(st)\n    ats.append(at)\n    returns.append(rt)\n    advants.append(adv)\n    old_policy.append(old_p)\n    old_value.append(old_v)\n\nsts = torch.cat(sts)\nats = torch.cat(ats)\nreturns = torch.cat(returns)\nadvants = torch.cat(advants)\nold_policy = torch.cat(old_policy)\nold_value = torch.cat(old_value)\n\ntrain_model(actor, critic, actor_optim, critic_optim, sts, ats, returns, advants,\n            old_policy, old_value, args)\n```\n\n이렇게 학습한 에이전트는 다음과 같이 걷습니다. 이렇게 walker를 학습시키고 나니 어떻게 하면 사람처럼 자연스럽게 걷는 것을 agent 스스로 학습할 수 있을까라는 고민을 하게 되었습니다.\n<center><img src=\"https://www.dropbox.com/s/fyz1kn5v92l3rrk/plane-595.gif?dl=1\"></center>\n\nUnity ml-agent에서 제공하는 pretrained된 모델을 다음과 같이 걷습니다. 저희가 학습한 agent와 상당히 다르게 걷는데 왜 그런 차이가 나는지도 분석하고 싶습니다. \n<center><img src=\"https://www.dropbox.com/s/xwz766g7c4eiaia/plane-unity.gif?dl=1\"></center>\n\n\n</br>\n## 3. Unity Curved Surface 제작 및 학습기\nUnity ml-agent에서 제공하는 기본 Walker 환경에서 학습하고 나니 바닥을 조금 울퉁불퉁하게 혹은 경사가 진 곳에서 걷는 것을 학습해보고 싶다라는 생각이 들었습니다. 따라서 간단하게 걷는 배경을 다르게 하는 시도를 해봤습니다. \n\n</br>\n### 3.1 Curved Surface 만들기\nAgent가 걸어갈 배경을 처음부터 만드는 것보다 구할 수 있다면 만들어진 배경을 구하기로 했습니다. Unity를 무료라는 점에서 선택했듯이 배경을 무료로 구할 수 있는 방법을 선택했습니다. \n<center><img src=\"https://www.dropbox.com/s/e0tsp3e3c9uq2zh/Screenshot%202018-08-23%2000.19.14.png?dl=1\"></center>\n\n무료로 공개되어있는 Unity 배경 중에서 Curved Ground 라는 것을 가져와서 작업하기로 했습니다. 이 환경 같은 경우 spline을 그리듯이 중간의 점을 이동시키면서 사용자가 곡면을 수정할 수 있습니다.\n<center><img src=\"https://www.dropbox.com/s/3ppmxotrf6qhzaf/Screenshot%202018-08-23%2000.20.25.png?dl=1\"></center>\n\n간단하게 곡면을 만들어서 공을 굴려보면 다음과 같이 잘 굴러갑니다. \n<center><img src=\"https://www.dropbox.com/s/2e8yqvqj1a4th27/slope_walker_ball.gif?dl=1\"></center>\n\n여러 에이전트가 학습할 수 있도록 오목한 경사면을 제작했습니다. 초반의 모습은 다음과 같았습니다. \n<img src=\"https://www.dropbox.com/s/m492xsfp4bolmz5/Screenshot%202018-08-23%2000.36.06.png?dl=1\">\n\n하지만 최종으로는 다음과 같은 곡면으로 사용했습니다. 위 사진의 배경과 아래 사진의 배경이 다른 점은 slope 길이, 내리막 경사, 오르막 경사입니다. Slope 길이의 경우 길이를 기존 plane 과 동일하게 했더니, 오르막 올라가는 부분이 학습이 잘 안 되었습니다. 따라서 길이를 줄였습니다. 내리막 경사의 경우 너무 경사지면 학습이 잘 안 되고, 너무 완만하니 내리막 티가 잘 안 나기 때문에 적절한 경사를 설정했습니다. 오르막 경사의 경우 내리막보다는 오르막이 더 어려울 것이라고 판단해서 오르막 경사를 낮게 설정했습니다. \n<img src=\"https://www.dropbox.com/s/idbov4wtd6jeqb2/Screenshot%202018-08-23%2000.36.54.png?dl=1\">\n\n</br>\n\n### 3.2 Curved Surface에서 학습하기\n위 환경으로 학습을 할 때, agent가 너무 초반에 빨리 쓰러지는 현상이 발생했습니다. 혹시 발의 각도가 문제일까 싶어서 발 각도를 변경해보았습니다. \n\n<center><img src=\"https://www.dropbox.com/s/znvikbeoj7gku0u/Screenshot%202018-08-23%2000.38.22.png?dl=1\" width=\"400px\"></center>\n\n하지만 역시 평지에서 걷는 것처럼 걷도록 학습이 안되었습니다. 이 환경에서 더 잘 학습하려면 더 여러가지를 시도해봐야할 것 같습니다. (그래도 걷는 게 기특합니다..)\n<center><img src=\"https://www.dropbox.com/s/4fqpsdmnzvnvia0/curved-736.gif?dl=1\"></center>\n\n<img src=\"https://www.dropbox.com/s/t5ngr0io4xeex6y/curved-736-overview.gif?dl=1\">\n\n</br>\n## 4. 구현 후기\n피지여행 구현팀은 총 4명으로 진행했습니다. 각 팀원의 후기를 적어보겠습니다.\n\n- 팀원 장수영: 사랑합니다. 행복합니다.\n- 팀원 공민서: 제가 핵심적인 기능을 구현하지는 못했지만 무조코 설치와 모델 테스트를 맡으면서 딥마인드나 openai의 영상으로만 보던 에이전트의 성장과정을 눈으로 지켜볼 수 있었습니다. 제대로 서있지도 못하던 hopper가 어느정도 훈련이 되고서는 넘어지려하다가도 추진력을 얻기위해 웅크렸다 뛰는 것을 관찰하는 것도 재미있고 육아일기를 보는 아버지의 마음을 조금이나마 이해할 수 있었습니다. 텐서보드를 넣는 걸 깜빡해 일일히 에피소드 별 스코어를 시각화 하면서 텐서보드의 소중함을 알았습니다. 유니티 코드리뷰를 하면서도 시스템 아키텍쳐 설계에 대해서도 배울 점이 있었던 것 같고 swish라는 활성화함수의 존재도 알았었고 curiosity도 알게되었고 역시 다른 사람의 코드를 읽는 것도 많은 공부가 된다고 되새기던 시간이었습니다. 물론 너무 크기가 방대해서 가독성은 많이 떨어졌습니다만 무조코보다 유니티가 훨씬 흥할거라고 생각했습니다. 마지막으로 누구 하나 열정적이지 않은 사람이 없이 치열한 고민을 함께 한 PG여행팀 분들, 저의 부족함과 상생의 기쁨을 알게해주셔서 정말 감사드립니다.\n- 팀원 양혁렬: 여러 에이전트가 함께하면 더 잘하는 걸 보면서 새삼 좋은 분들과 함께 할 수 있어서 행복했습니다\n- 팀원 이웅원: 저희가 직접 바닥부터 다 구현했던 것은 아니지만 구현을 해보면서 논문의 내용을 더 잘 이해할 수 있었습니다. 논문에 나와있지 않은 여러 노하우가 필요한 점들도 많았습니다. 역시 코드로 보고 성능 재현이 되어야 제대로 알고리즘을 이해하게 된다는 것을 다시 느낀 시간이었습니다. 또한 강화학습은 역시 환경세팅이 어렵다는 생각을 했습니다. 하지만 unity ml-agent를 사용해보면서 앞으로 강화학습 환경으로서 가능성이 상당히 크다는 생각을 했습니다. 또한 구현팀과 슬랙, 깃헙으로 협업하면서 온라인 협업에 대해서 더 배워가는 것 같습니다. 아직은 익숙하지 않지만 앞으로는 마치 바로 옆에서 같이 코딩하는 것 같이 될 거라고 생각합니다.","source":"_posts/8_implement.md","raw":"---\ntitle: PG Travel implementation story\ndate: 2018-08-23 14:18:32\ntags: [\"프로젝트\", \"피지여행\"]\ncategories: 프로젝트\nauthor: 이웅원, 장수영, 공민서, 양혁렬\nsubtitle: 피지여행 구현 이야기\n---\n\n\n# PG Travel implementation story\n\n- 구현 코드 링크 : [https://github.com/reinforcement-learning-kr/pg_travel](https://github.com/reinforcement-learning-kr/pg_travel)\n\n\n피지여행 프로젝트에서는 다음 7개 논문을 살펴보았습니다. 각 논문에 대한 리뷰는 이전 글들에서 다루고 있습니다. \n\n<a name=\"1\"></a>\n\n* [1] R. Sutton, et al., \"Policy Gradient Methods for Reinforcement Learning with Function Approximation\", NIPS 2000.\n<a name=\"2\"></a>\n* [2] D. Silver, et al., \"Deterministic Policy Gradient Algorithms\", ICML 2014.\n<a name=\"3\"></a>\n* [3] T. Lillicrap, et al., \"Continuous Control with Deep Reinforcement Learning\", ICLR 2016.\n<a name=\"4\"></a>\n* [4] S. Kakade, \"A Natural Policy Gradient\", NIPS 2002.\n<a name=\"5\"></a>\n* [5] J. Schulman, et al., \"Trust Region Policy Optimization\", ICML 2015.\n<a name=\"6\"></a>\n* [6] J. Schulman, et al., \"High-Dimensional Continuous Control using Generalized Advantage Estimation\", ICLR 2016.\n<a name=\"7\"></a>\n* [7] J. Schulman, et al., \"Proximal Policy Optimization Algorithms\", arXiv, https://arxiv.org/pdf/1707.06347.pdf.\n\n강화학습 알고리즘을 이해하는데 있어서 논문을 보고 이론적인 부분을 알아가는 것이 좋습니다. 하지만 실제 코드로 돌아가는 것은 논문만 보고는 알 수 없는 경우가 많습니다. 따라서 피지여행 프로젝트에서는 위 7개 논문 중에 DPG와 DDPG를 제외한 알고리즘을 구현해보았습니다. 구현한 알고리즘은 다음 4개입니다. 이 때, TRPO와 PPO 구현에는 GAE(General Advantage Estimator)가 함께 들어갑니다. \n \n* Vanilla Policy Gradient [[1](#1)]\n* TNPG(Truncated Natural Policy Gradient) [[4](#4)]\n* TRPO(Trust Region Policy Optimization) [[5](#5)]\n* PPO(Proximal Policy Optimization) [[7](#7)].\n\n바닥부터 저희가 구현한 것은 아니며 다음 코드들을 참고해서 구현하였습니다. Vanilla PG의 경우 RLCode의 깃헙을 참고하였습니다.\n\n* [OpenAI Baseline](https://github.com/openai/baselines/tree/master/baselines/trpo_mpi)\n* [Pytorch implemetation of TRPO](https://github.com/ikostrikov/pytorch-trpo)\n* [RLCode Actor-Critic](https://github.com/rlcode/reinforcement-learning-kr/tree/master/2-cartpole/2-actor-critic)\n\nGAE와 TRPO, PPO 논문에서는 Mujoco라는 물리 시뮬레이션을 학습 환경으로 사용합니다. 따라서 저희도 Mujoco로 처음 시작을 하였습니다. 하지만 Mujoco는 1달만 무료이고 그 이후부터 유료이며 확장성이 떨어집니다. Unity ml-agent는 기존 Unity를 그대로 사용하면서 쉽게 강화학습 에이전트를 붙일 수 있도록 설계되어 있습니다. Unity ml-agent에서는 저희가 살펴본 알고리즘 중에 가장 최신 알고리즘은 PPO를 적용해봤습니다. 기본적으로 제공하는 환경 이외에 저희가 customize 한 환경에서도 학습해봤습니다.  \n\n* mujoco-py: [https://github.com/openai/mujoco-py](https://github.com/openai/mujoco-py)\n* Unity ml-agent: [https://github.com/Unity-Technologies/ml-agents](https://github.com/Unity-Technologies/ml-agents)\n\n코드를 구현하고 환경에서 학습을 시키면서 여러가지 이슈들이 있었고 해결해내가는 과정이 있었습니다. 그 과정을 간단히 정리해서 공유하면 PG를 공부하는 분들께 도움일 될 것 같습니다. 저희가 구현한 순서대로 1. Mujoco 학습 2. Unity ml-agent 학습 3. Unity Curved Surface 로 이 포스트가 진행됩니다.\n\n<br/>\n## 1. Mujoco 학습\n일명 \"Continuous control\" 문제는 action이 discrete하지 않고 continuous한 경우를 말합니다. Mujoco는 continuous control에 강화학습을 적용한 논문들이 애용하는 시뮬레이터입니다. 저희가 리뷰한 논문 중에서도 TRPO, PPO, GAE에서 Mujoco를 사용합니다. 따라서 저희가 처음 피지여행 알고리즘을 적용한 환경으로 Mujoco를 선택했습니다. \n\nMujoco에는 Ant, HalfCheetah, Hopper, Humanoid, HumanoidStandup, InvertedPendulum, Reacher, Swimmer, Walker2d 과 같은 환경이 있습니다. 그 중에서 Hopper에 맞춰서 학습이 되도록 코드를 구현하였습니다. Mujoco 설치와 관련된 내용은 Wiki에 있습니다.\n\n</br>\n### 1.1 Hopper\nHopper는 외다리로 뛰어가는 것을 학습하는 것이 목표입니다. Hopper는 다음과 같이 생겼습니다. \n<img src=\"https://www.dropbox.com/s/wjxrelxyp014j3g/Screenshot%202018-08-23%2000.55.54.png?dl=1\">\n\n환경을 이해하려면 환경의 상태와 행동, 보상 그리고 학습하고 싶은 목표를 알아야합니다. \n\n- 상태 : 관절의 위치, 각도, 각속도\n- 행동 : 관절의 가해지는 토크\n- 보상 : 앞으로 나아가는 속도\n- 목표 : 최대한 앞으로 많이 나아가기\n\n즉 에이전트는 time step마다 관절의 위치와 각도를 받아와서 그 상태에서 어떻게 움직여야 앞으로 나아갈 수 있는지를 학습해야 합니다. 행동은 discrete action이 아닌 continuous action으로 -1과 1사이의 값을 가집니다. 만약 행동이 -1이라면 해당 관절에 시계반대방향으로 토크를 주는 것이고 행동이 +1이라면 해당 관절에 시계방향으로 토크를 주는 것입니다. \n\ncontinuous action을 주는 방법은 네트워크(Actor)의 output layer에서 activation function으로 tanh와 같은 것을 사용해서 continuous한 값을 출력하는 것이 있습니다. 하지만 피지여행 코드 구현에서는 action을 gaussian distribution에서 sampling 하였습니다. 이렇게 하면 분산을 일정하게 유지하면서 지속적인 exploration을 할 수 있습니다. 간단하게 그림으로 보자면 다음과 같습니다. \n<img src=\"https://www.dropbox.com/s/94g01zdxyf5oxu1/Screenshot%202018-08-23%2001.20.21.png?dl=1\">\n\n네트워크 구조와 행동을 선택하는 부분은 다음과 같습니다. Hidden Layer의 activation function으로 tanh를 사용했으며(ReLU를 테스트해보지는 않았습니다. 기존 TRPO, PPO 구현들과 논문에서 tanh를 사용하기 때문에 저희도 사용했습니다. 뒤에 유니티 환경에서는 Swish라는 것을 사용합니다.) log std를 0으로 고정함으로서 일정한 폭을 가지는 분포를 만들어낼 수 있습니다. 이 분포로부터 action을 sampling 합니다.\n\n<img src=\"https://www.dropbox.com/s/xfl9zxies0lmpm1/Screenshot%202018-08-23%2001.20.44.png?dl=1\">\n\n</br>\n### 1.2 Vanilla PG\nVanilla PG는 Actor-Critic의 가장 간단한 형태입니다. Vanilla PG는 이후의 구현에 대한 baseline이 됩니다. 구현이 가장 간단하면서 학습이 안되는 것은 아닙니다. 따라서 코드 전체 구조를 잡는데 Vanilla PG를 짜는 것이 도움이 됩니다. 전반적인 코드 구조는 다음과 같습니다.\n\n- iteration 마다 일정한 step 수만큼 환경에서 진행해서 샘플을 모은다\n- 모은 샘플로 Actor와 Critic을 학습한다\n- 반복한다\n \n\n```python\nepisodes = 0\nfor iter in range(15000):\n    actor.eval(), critic.eval()\n    memory = deque()\n    \n    while steps < 2048:\n        episodes += 1\n        state = env.reset()\n        state = running_state(state)\n        score = 0\n        for _ in range(10000):\n            mu, std, _ = actor(torch.Tensor(state).unsqueeze(0))\n            action = get_action(mu, std)[0]\n            next_state, reward, done, _ = env.step(action)\n            next_state = running_state(next_state)\n\n            if done:\n                mask = 0\n            else:\n                mask = 1\n\n            memory.append([state, action, reward, mask])\n            state = next_state\n\n            if done:\n                break\n                \n    actor.train(), critic.train()\n    train_model(actor, critic, memory, actor_optim, critic_optim)\n```\n\nmemory에 sample을 저장할 때 sample은 state와 action, reward, mask(마지막 state일 경우 0 나머지 1)입니다. mask의 경우 뒤에서 return이나 advantage를 계산할 때 사용됩니다. 또 하나 염두에 두어야할 것은 running_state 입니다. running_state는 input으로 들어오는 state의 scale이 일정하지 않기 때문에 사용합니다. 즉 state의 각 dimension을 평균 0 분산 1로 standardization 하는 것입니다. 따라서 모델을 저장할 때 각 dimension 마다의 평균과 분산도 같이 저장해서 테스트할 때 불러와서 사용해야 합니다.\n\nVanilla PG의 경우 학습 부분이 상당히 간단합니다. 다음 코드를 보시면 메모리에서 state, action, reward, mask를 꺼냅니다. reward와 mask를 통해 return을 구할 수 있고 이 return을 통해 actor를 업데이트 할 수 있습니다 (REINFORCE 알고리즘을 떠올리시면 됩니다). 여기서 critic이 하는 일은 없지만 뒤의 알고리즘들과 코드의 통일성을 위해 fake로 넣어놨습니다. Return은 평균을 빼고 분산으로 나눠서 standardize 합니다. \n\n```python\ndef train_model(actor, critic, memory, actor_optim, critic_optim):\n    memory = np.array(memory)\n    states = np.vstack(memory[:, 0])\n    actions = list(memory[:, 1])\n    rewards = list(memory[:, 2])\n    masks = list(memory[:, 3])\n\n    returns = get_returns(rewards, masks)\n    train_critic(critic, states, returns, critic_optim)\n    train_actor(actor, returns, states, actions, actor_optim)\n    return returns\n```\n\n이 코드로 Hopper 환경에서 학습한 그래프는 다음과 같습니다. \n<img src=\"https://www.dropbox.com/s/asoysfuk76zs1dk/Screenshot%202018-08-23%2001.30.58.png?dl=1\">\n\n</br>\n### 1.3 TNPG\nNPG를 이용한 parameter update 식은 다음과 같습니다. \n\n$$\\bar{w}=F(\\theta)^{-1}\\nabla\\eta(\\theta)$$\n\nNPG를 구현하려면 KL-divergence의 Hessian의 inverse를 구해야하는 문제가 생깁니다. 현재와 같이 Deep Neural Network를 쓰는 경우에 Hessian의 inverse를 직접적으로 구하는 것은 computationally inefficient 합니다. 따라서 직접 구하지 않고 Conjugate gradient 방법을 사용해서 Fisher Vector Product ($$F^{-1}g$$)를 구합니다. 이러한 알고리즘을 Truncated Natural Policy Gradient(TNPG)라고 부릅니다. \n\nTNPG에서 parameter update를 구하는 과정은 다음과 같습니다. \n1. Return 구하기\n2. Critic 학습하기\n3. logp * return --> loss 구하기\n4. loss의 미분과 kl-divergence의 2차 미분을 통해 step direction 구하기\n5. 구한 step direction으로 parameter update\n\n```python\ndef train_model(actor, critic, memory, actor_optim, critic_optim):\n    memory = np.array(memory)\n    states = np.vstack(memory[:, 0])\n    actions = list(memory[:, 1])\n    rewards = list(memory[:, 2])\n    masks = list(memory[:, 3])\n\n    # ----------------------------\n    # step 1: get returns\n    returns = get_returns(rewards, masks)\n\n    # ----------------------------\n    # step 2: train critic several steps with respect to returns\n    train_critic(critic, states, returns, critic_optim)\n\n    # ----------------------------\n    # step 3: get gradient of loss and hessian of kl\n    loss = get_loss(actor, returns, states, actions)\n    loss_grad = torch.autograd.grad(loss, actor.parameters())\n    loss_grad = flat_grad(loss_grad)\n    step_dir = conjugate_gradient(actor, states, loss_grad.data, nsteps=10)\n\n    # ----------------------------\n    # step 4: get step direction and step size and update actor\n    params = flat_params(actor)\n    new_params = params + 0.5 * step_dir\n    update_model(actor, new_params)\n    \n```\n\nconjugate gradient 코드는 OpenAI baseline에서 가져왔습니다. 이 코드는 원래 John schulmann 개인 repository에 있는 그대로 사용하는 것입니다. nsteps 만큼 iterataion을 반복하며 결국 x를 구하는 것인데 이 x가 step direction 입니다. \n\n```python\n# from openai baseline code\n# https://github.com/openai/baselines/blob/master/baselines/common/cg.py\ndef conjugate_gradient(actor, states, b, nsteps, residual_tol=1e-10):\n    x = torch.zeros(b.size())\n    r = b.clone()\n    p = b.clone()\n    rdotr = torch.dot(r, r)\n    for i in range(nsteps):\n        _Avp = fisher_vector_product(actor, states, p)\n        alpha = rdotr / torch.dot(p, _Avp)\n        x += alpha * p\n        r -= alpha * _Avp\n        new_rdotr = torch.dot(r, r)\n        betta = new_rdotr / rdotr\n        p = r + betta * p\n        rdotr = new_rdotr\n        if rdotr < residual_tol:\n            break\n    return x\n```\n\nfisher_vector_product는 kl-divergence의 2차미분과 어떠한 vector의 곱인데 p는 처음에 gradient 값이었다가 점차 업데이트가 됩니다. kl-divergence의 2차 미분을 구하는 과정은 다음과 같습니다. 일단 kl-divergence를 현재 policy에 대해서 구한 다음에 actor parameter에 대해서 미분합니다. 이렇게 미분한 gradient를 일단 flat하게 핀 다음에 p라는 벡터와 곱해서 하나의 값으로 만듭니다. 그 값을 다시 actor의 parameter로 만듦으로서 따로 KL-divergence의 2차미분을 구하지않고 Fisher vector product를 구할 수 있습니다.\n\n```python\ndef fisher_vector_product(actor, states, p):\n    p.detach()\n    kl = kl_divergence(new_actor=actor, old_actor=actor, states=states)\n    kl = kl.mean()\n    kl_grad = torch.autograd.grad(kl, actor.parameters(), create_graph=True)\n    kl_grad = flat_grad(kl_grad)  # check kl_grad == 0\n\n    kl_grad_p = (kl_grad * p).sum()\n    kl_hessian_p = torch.autograd.grad(kl_grad_p, actor.parameters())\n    kl_hessian_p = flat_hessian(kl_hessian_p)\n\n    return kl_hessian_p + 0.1 * p\n```\n\nTNPG 학습 결과는 다음과 같습니다. \n<img src=\"https://www.dropbox.com/s/uc4c0s00qbs33nr/Screenshot%202018-08-23%2001.53.17.png?dl=1\">\n\n</br>\n### 1.4 TRPO\nTRPO와 NPG가 다른 점은 surrogate loss 사용과 trust region 입니다. 하지만 실제로 구현해서 학습을 시켜본 결과 trust region을 넘어가서 back tracking line search를 하는 경우는 거의 없습니다. 따라서 주된 변화는 surrogate loss에 있다고 보셔도 됩니다. Surrogate loss에서 advantage function을 사용하는데 본 코드 구현에서는 GAE를 사용하였습니다. TRPO 업데이트 식은 다음과 같습니다. Q function 위치에 GAE가 들어갑니다.\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi\\_\\theta(a\\vert s)}{q(a\\vert s)}Q\\_{\\theta\\_\\mathrm{old} }(s,a)\\right] \\\\\\\\\n\\mathrm{s.t.\\ }&E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} }}\\left[D\\_\\mathrm{KL}\\left(\\pi\\_{\\theta\\_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi\\_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta\n\\end{align}\n$$\n\n\nGAE를 구하는 코드는 다음과 같습니다. GAE는 td-error의 discounted summation이라고 볼 수 있습니다. 마지막에 advants를 standardization 하는 것은 return에서 하는 것과 같은 효과를 봅니다. 하지만 standardization을 안하고 실험을 해보지는 않았습니다.\n\n```python\ndef get_gae(rewards, masks, values):\n    rewards = torch.Tensor(rewards)\n    masks = torch.Tensor(masks)\n    returns = torch.zeros_like(rewards)\n    advants = torch.zeros_like(rewards)\n\n    running_returns = 0\n    previous_value = 0\n    running_advants = 0\n\n    for t in reversed(range(0, len(rewards))):\n        running_returns = rewards[t] + hp.gamma * running_returns * masks[t]\n        running_tderror = rewards[t] + hp.gamma * previous_value * masks[t] - \\\n                    values.data[t]\n        running_advants = running_tderror + hp.gamma * hp.lamda * \\\n                          running_advants * masks[t]\n\n        returns[t] = running_returns\n        previous_value = values.data[t]\n        advants[t] = running_advants\n\n    advants = (advants - advants.mean()) / advants.std()\n    return returns, advants\n```\n\nSurrogate loss를 구하는 코드는 다음과 같습니다. Advantage function(GAE)를 구하고 나면 이전 policy와 현재 policy 사이의 ratio를 구해서 advantage function에 곱하면 됩니다. 이 때 사실 old policy와 new policy는 값은 같지만 old policy는 clone()이나 detach()를 사용해서 update가 안되게 만들어줍니다.\n\n```python\ndef surrogate_loss(actor, advants, states, old_policy, actions):\n    mu, std, logstd = actor(torch.Tensor(states))\n    new_policy = log_density(torch.Tensor(actions), mu, std, logstd)\n    advants = advants.unsqueeze(1)\n\n    surrogate = advants * torch.exp(new_policy - old_policy)\n    surrogate = surrogate.mean()\n    return surrogate\n\n```\n\nActor의 step direction을 구하는 것은 TNPG와 동일합니다. TNPG에서는 step direction으로 바로 업데이트 했지만 TRPO는 다음과 같은 작업을 해줍니다. Full step을 구하는 과정이라고 볼 수 있습니다. \n\n```python\n# ----------------------------\n# step 4: get step direction and step size and full step\nparams = flat_params(actor)\nshs = 0.5 * (step_dir * fisher_vector_product(actor, states, step_dir)\n             ).sum(0, keepdim=True)\nstep_size = 1 / torch.sqrt(shs / hp.max_kl)[0]\nfull_step = step_size * step_dir\n\n```\n\n이렇게 full step을 구하고 나면 Trust region optimization 단계에 들어갑니다. expected improvement는 구한 step 만큼 parameter space에서 움직였을 때 예상되는 performance 변화입니다. 이 값은 kl-divergence와 함께 trust region 안에 있는지 밖에 있는지 판단하는 근거가 됩니다. expected improve는 출발점에서의 gradient * full step으로 구합니다. 그리고 10번을 돌아가며 Back-tracking line search를 실시합니다. 처음에는 full step 만큼 가본 다음에 kl-divergence와 emprovement를 통해 trust region 안이면 루프 탈출, 밖이면 full step을 반만큼 쪼개서 다시 이 과정을 반복합니다.  \n\n```python\n# ----------------------------\n# step 5: do backtracking line search for n times\nold_actor = Actor(actor.num_inputs, actor.num_outputs)\nupdate_model(old_actor, params)\nexpected_improve = (loss_grad * full_step).sum(0, keepdim=True)\nexpected_improve = expected_improve.data.numpy()\n\nflag = False\nfraction = 1.0\nfor i in range(10):\n    new_params = params + fraction * full_step\n    update_model(actor, new_params)\n    new_loss = surrogate_loss(actor, advants, states, old_policy.detach(),\n                              actions)\n    new_loss = new_loss.data.numpy()\n    loss_improve = new_loss - loss\n    expected_improve *= fraction\n    kl = kl_divergence(new_actor=actor, old_actor=old_actor, states=states)\n    kl = kl.mean()\n\n    print('kl: {:.4f}  loss improve: {:.4f}  expected improve: {:.4f}  '\n          'number of line search: {}'\n          .format(kl.data.numpy(), loss_improve, expected_improve[0], i))\n\n    # see https: // en.wikipedia.org / wiki / Backtracking_line_search\n    if kl < hp.max_kl and (loss_improve / expected_improve) > 0.5:\n        flag = True\n        break\n\n    fraction *= 0.5\n```\n\nCritic의 학습은 단순히 value function과 return의 MSE error를 계산해서 loss로 잡고 loss를 최소화하도록 학습합니다. TRPO 학습 결과는 다음과 같습니다.\n<img src=\"https://www.dropbox.com/s/rc9hxsx1kvokcrv/Screenshot%202018-08-23%2013.36.51.png?dl=1\">\n\n</br>\n### 1.5 PPO\nPPO의 장점을 꼽으라면 GPU 사용하기 좋고 sample efficiency가 늘어난다는 것입니다. TNPG와 TRPO의 경우 한 번 모은 sample은 모델을 단 한 번 업데이트하는데 사용하지만 PPO의 경우 몇 mini-batch로 epoch를 돌리기 때문입니다. GAE를 사용한다는 것은 같고 Conjugate gradient나 Fisher vector product나 back tracking line search가 다 빠집니다. 대신 loss function clip으로 monotonic improvement를 보장하게 학습합니다. 따라서 코드가 상당히 간단해집니다. \n\n다음 코드 부분이 PPO의 전체라고 봐도 무방합니다. PPO는 다음과 같은 순서로 학습합니다. \n\n- batch를 random suffling하고 mini batch를 추출\n- value function 구하기\n- critic loss 구하기 (clip을 사용해도 되고 TRPO와 같이 그냥 학습시켜도 됌)\n- surrogate loss 구하기\n- surrogate loss clip해서 actor loss 만들기\n- actor와 critic 업데이트\n\nActor의 loss를 구하는 것은 다음 식의 값을 구하는 것입니다. 이 식을 구하려면 ratio에 한 번 클립하고 loss 값을 한 번 min을 취하면 됩니다.\n\n$$L^{CLIP}(\\theta) = \\hat{E}_t [min(r_t(\\theta) \\, \\hat{A}_t,  clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\, \\hat{A}_t)]$$\n\n이 코드 구현에서는 actor와 critic을 따로 모델로 만들어서 따로 따로 업데이트를 하지만 하나로 만든다면 loss로 한 번만 업데이트하면 됩니다. 또한 entropy loss를 최종 loss에 더해서 regularization 효과를 볼 수도 있습니다. Critic loss에 clip 해주는 것은 OpenAI baseline의 ppo2 코드를 참조하였습니다.\n\n```python\n# step 2: get value loss and actor loss and update actor & critic\nfor epoch in range(10):\n    np.random.shuffle(arr)\n\n    for i in range(n // hp.batch_size):\n        batch_index = arr[hp.batch_size * i: hp.batch_size * (i + 1)]\n        batch_index = torch.LongTensor(batch_index)\n        inputs = torch.Tensor(states)[batch_index]\n        returns_samples = returns.unsqueeze(1)[batch_index]\n        advants_samples = advants.unsqueeze(1)[batch_index]\n        actions_samples = torch.Tensor(actions)[batch_index]\n        oldvalue_samples = old_values[batch_index].detach()\n\n        loss, ratio = surrogate_loss(actor, advants_samples, inputs,\n                                     old_policy.detach(), actions_samples,\n                                     batch_index)\n\n        values = critic(inputs)\n        clipped_values = oldvalue_samples + \\\n                         torch.clamp(values - oldvalue_samples,\n                                     -hp.clip_param,\n                                     hp.clip_param)\n        critic_loss1 = criterion(clipped_values, returns_samples)\n        critic_loss2 = criterion(values, returns_samples)\n        critic_loss = torch.max(critic_loss1, critic_loss2).mean()\n\n        clipped_ratio = torch.clamp(ratio,\n                                    1.0 - hp.clip_param,\n                                    1.0 + hp.clip_param)\n        clipped_loss = clipped_ratio * advants_samples\n        actor_loss = -torch.min(loss, clipped_loss).mean()\n\n        loss = actor_loss + 0.5 * critic_loss\n\n        critic_optim.zero_grad()\n        loss.backward(retain_graph=True)\n        critic_optim.step()\n\n        actor_optim.zero_grad()\n        loss.backward()\n        actor_optim.step()\n```\n\nPPO의 학습 결과는 다음과 같습니다. \n<img src=\"https://www.dropbox.com/s/rkxa836ap931kbd/Screenshot%202018-08-23%2013.50.57.png?dl=1\">\n\n\n</br>\n## 2. Unity ml-agent 학습\nMujoco Hopper(half-cheetah와 같은 것도)에 Vanilla PG, TNPG, TRPO, PPO를 구현해서 적용했습니다. Mujoco의 경우 이미 Hyper parameter와 같은 정보들이 논문이나 블로그에 있기 때문에 상대적으로 continuous control로 시작하기에는 좋습니다. 맨 처음에 말했듯이 Mujoco는 1달만 무료이고 그 이후부터 유료이며 확장성이 떨어집니다. 좀 더 general한 agent를 학습시키기에 좋은 환경이 필요합니다. 따라서 Unity ml-agent를 살펴봤습니다. Repository는 다음과 같습니다. \n- [Unity ml-agent repository](https://github.com/Unity-Technologies/ml-agents)\n- [Unity ml-agent homepage](https://unity3d.com/machine-learning/)\n\n<img src=\"https://www.dropbox.com/s/lapholj8r4nxmb1/Screenshot%202018-08-24%2013.41.31.png?dl=1\">\n\n현재 Unity ml-agent에서 기본으로 제공하는 환경은 다음과 같습니다. Unity ml-agent는 기존 Unity를 그대로 사용하면서 쉽게 강화학습 에이전트를 붙일 수 있도록 설계되어 있습니다. Unity ml-agent에서는 Walker 환경에서 저희가 살펴본 알고리즘 중에 가장 최신 알고리즘은 PPO를 적용해봤습니다. 이 포스트를 보시는 분들은 이 많은 다른 환경에 자유롭게 저희 코드를 적용할 수 있습니다.\n- [각 환경에 대한 설명](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md)\n\n<img src=\"https://www.dropbox.com/s/lrbodw5dypxowmw/Screenshot%202018-08-24%2014.06.14.png?dl=1\">\n\nUnity ml-agent를 이용해서 강화학습을 하기 위해서는 다음과 같이 진행됩니다. 단계별로 설명하겠습니다. \n\n- Unity에서 환경 만들기\n- Python에서 unity 환경 불러와서 테스트하기\n- 기존에 하던대로 학습하기\n\n\n</br>\n### 2.1 Walker 환경 만들기\n강화학습을 하는 많은 분들이 Unity를 한 번도 다뤄보지 않은 경우가 많습니다. 저도 그런 경우라서 어떻게 환경을 만들어야할지 처음에는 감이 잡히지 않았습니다. 하지만 Unity ml-agent에서는 상당히 자세한 guide를 제공합니다. 다음은 Unity ml-agent의 가장 기본적인 환경인 3DBall에 대한 tutorial입니다. 설치 guide도 제공하고 있으니 참고하시면 될 것 같습니다.\n- [3DBall 예제 tutorial](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Basic-Guide.md)\n- [Unity ml-agent 설치 guide](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md)\n\nUnity ml-agent에서 제공하는 3DBall tutorial을 참고해서 Walker 환경을 만들었습니다. Walker 환경을 만드는 과정을 간단히 말씀드리겠습니다. 다음 그림의 단계들을 동일하므로 따라하시면 됩니다. Unity를 열고 unity-environment로 들어가시면 됩니다.\n<img src=\"https://www.dropbox.com/s/fbdqg781w46a5mz/Screenshot%202018-08-24%2014.50.50.png?dl=1\">\n\n그러면 화면 하단에서 다음과 같은 것을 볼 수 있습니다. Assets/ML-Agents/Examples로 들어가보면 Walker가 있습니다. Scenes에서 Walker를 더블클릭하면 됩니다.\n<img src=\"https://www.dropbox.com/s/h349xml3faln0wy/Screenshot%202018-08-24%2014.52.14.png?dl=1\">\n\n더블클릭해서 나온 화면에서 오른쪽 상단의 파란색 화살표를 누르면 환경이 실행이 됩니다. 저희가 학습하고자 하는 agent는 바로 이녀석입니다. 왼쪽 리스트를 보면 WalkerPair가 11개가 있는 것을 볼 수 있습니다. Unity ml-agent 환경은 기본적으로 Multi-agent로 학습하도록 설정되어있습니다. 따라서 여러개의 Walker들이 화면에 보이는 것입니다. \n<img src=\"https://www.dropbox.com/s/cy8m5kqdmkhopjo/Screenshot%202018-08-24%2014.54.57.png?dl=1\">\n\n리스트 중에 Walker Academy를 클릭해서 그 하위에 있는 WalkerBrain을 더블클릭합니다. 그러면 화면 오른쪽에 다음과 같은 Brain 설정을 볼 수 있습니다. Brain은 쉽게 말해서 Agent라고 생각하면 됩니다. 이 Agent는 상태로 212차원의 vector가 주어지며 다 continuous한 값을 가집니다. 행동은 39개의 행동을 할 수 있으며 다 Continuous입니다. Mujoco에 비해서 상태나 행동의 차원이 상당히 높습니다. 여기서 중요한 것은 Brain Type입니다. Brain type은 internal, external, player, heuristic이 있습니다. player로 type을 설정하고 화면 상단의 play 버튼을 누르면 여러분이 agent를 움직일 수 있습니다. 하지만 Walker는 사람이 움직이는게 거의 불가능하므로 player 기능은 사용할 수 없습니다. 다른 환경에서는 사용해볼 수 있으니 재미로 한 번 플레이해보시면 좋습니다! \n<center><img src=\"https://www.dropbox.com/s/uxfm162f1scbzo5/Screenshot%202018-08-24%2015.09.04.png?dl=1\" width=\"400px\"></center>\n\n이번에는 WalkerPair에서 WalkerAgent를 더블클릭해보겠습니다. 이 설정을 보아 5000 step이 episode의 max step인 것을 볼 수 있습니다.\n<center><img src=\"https://www.dropbox.com/s/r6gwemlczwic2ma/Screenshot%202018-08-24%2015.16.19.png?dl=1\" width=\"400px\"></center>\n\n이제 상단 file menu에서 build setting에 들어갑니다. 환경을 build해서 python 코드에서 import하기 위해서입니다. 물론 unity 환경과 python 코드를 binding해주는 부분은 ml-agent 코드 안에 있습니다. Build 버튼을 누르면 환경이 build가 됩니다.\n<center><img src=\"https://www.dropbox.com/s/4dtgoz1k8896vxs/Screenshot%202018-08-24%2015.19.07.png?dl=1\" width=\"500px\"></center>\n\n\n</br>\n### 2.2 Python에서 unity 환경 불러와서 테스트하기\n환경을 build 했으면 build한 환경을 python에서 불러와서 random action으로 테스트해봅니다. 환경을 테스트하는 코드는 pg_travel repository에서 unity 폴더 밑에 있습니다. test_env.py라는 코드는 간단하게 다음과 같습니다. Build한 walker 환경은 env라는 폴더 밑에 넣어줍니다. unityagent를 import하는데 ml-agent를 git clone 해서 python 폴더 내에서 \"python setup.py install\"을 실행했다면 문제없이 import 됩니다. UnityEnvironment를 통해 env라는 환경을 선언할 수 있습니다. 이렇게 선언하고 나면 gym과 상당히 유사한 형태로 환경과 상호작용이 가능합니다. \n\n```python\nimport numpy as np\nfrom unityagents import UnityEnvironment\nfrom utils.utils import get_action\n\nif __name__==\"__main__\":\n    env_name = \"./env/walker_test\"\n    train_mode = False\n\n    env = UnityEnvironment(file_name=env_name)\n\n    default_brain = env.brain_names[0]\n    brain = env.brains[default_brain]\n    env_info = env.reset(train_mode=train_mode)[default_brain]\n\n    num_inputs = brain.vector_observation_space_size\n    num_actions = brain.vector_action_space_size\n    num_agent = env._n_agents[default_brain]\n\n    print('the size of input dimension is ', num_inputs)\n    print('the size of action dimension is ', num_actions)\n    print('the number of agents is ', num_agent)\n   \n    score = 0\n    episode = 0\n    actions = [0 for i in range(num_actions)] * num_agent\n    for iter in range(1000):\n        env_info = env.step(actions)[default_brain]\n        rewards = env_info.rewards\n        dones = env_info.local_done\n        score += rewards[0]\n\n        if dones[0]:\n            episode += 1\n            score = 0\n            print('{}th episode : mean score of 1st agent is {:.2f}'.format(\n                episode, score))\n```\n\n위 코드를 실행하면 다음과 같이 실행창에 뜹니다. External brain인 것을 알 수 있고 default_brain은 brain 중에 하나만 가져왔기 때문에 number of brain은 1이라고 출력합니다. input dimension은 212이고 action dimension은 39이고 agent 수는 11인 것으로봐서 제대로 환경이 불러와진 것을 확인할 수 있습니다. \n<img src=\"https://www.dropbox.com/s/cioa9h7qu25vonz/Screenshot%202018-08-24%2015.47.43.png?dl=1\">\n\n이 환경에서 행동하려면 agent 숫자만큼 행동을 줘야합니다. 모두 0로 행동을 주고 실행하면 다음과 같이 뒤로 넘어지는 행동을 반복합니다. env.step(actions)[default_brain]으로 env_info를 받아오면 거기서부터 reward와 done, next_state를 받아올 수 있습니다. 이제 학습하기만 하면 됩니다. \n<img src=\"https://www.dropbox.com/s/8qrmxoski6p4n07/Screenshot%202018-08-24%2016.00.21.png?dl=1\">\n\n</br>\n### 2.3 Walker 학습하기\n기존에 Mujoco에 적용했던 PPO 코드를 그대로 Walker에 적용하니 잘 학습이 안되었습니다. 다음 사진이 저희가 중간 해커톤으로 모여서 이 상황을 공유할 때의 사진입니다.\n<img src=\"https://i.imgur.com/1aR2Z77.png\" width=500px>\n\nUnity ml-agent에서는 PPO를 기본 agent로 제공합니다. 학습 코드도 제공하기 때문에 mujoco에 적용했던 코드와의 차이점을 분석할 수 있었습니다. mujoco 코드와 ml-agent baseline 코드의 차이점은 다음과 같습니다. \n\n- agent 여러개를 이용, 별개의 memory에 저장한 후에 gradient를 합침\n- GAE 및 time horizon 등 hyper parameter가 다름\n- Actor와 Critic의 layer가 1층 더 두꺼우며 hidden layer 자체의 사이즈도 더 큼\n- hidden layer의 activation function이 tanh가 아닌 swish\n\nml-agent baseline 코드리뷰할 때 작성했던 마인드맵은 다음과 같습니다.\n<img src=\"https://i.imgur.com/YeaEntG.png\">\n\n크게는 두 가지를 개선해서 성능이 많이 향상했습니다.\n1. Network 수정\n2. multi-agent를 활용해서 학습\n\nNetwork 코드는 다음과 같습니다. Hidden Layer를 하나 더 늘렸으며 swish activation function을 사용할 수 있도록 변경했습니다. 사실 swish라는 activation function은 처음 들어보는 생소한 함수였습니다. 하지만 ml-agent baseline에서 사용했다는 사실과 구현이 상당히 간단하다는 점에서 저희 코드에 적용했습니다. 단순히 x * sigmoid(x) 를 하면 됩니다. swish는 별거 아닌 것 같지만 상당한 성능 개선을 가져다줬습니다. 사실 ReLU나 ELU 등 여러 다른 activation function을 적용해서 비교해보는게 best긴 하지만 시간 관계상 그렇게까지 테스트해보지는 못했습니다. 기존에 TRPO나 PPO는 왜 tanh를 사용했었는지도 의문인 점입니다.\n\n```python\nclass Actor(nn.Module):\n    def __init__(self, num_inputs, num_outputs, args):\n        self.args = args\n        self.num_inputs = num_inputs\n        self.num_outputs = num_outputs\n        super(Actor, self).__init__()\n        self.fc1 = nn.Linear(num_inputs, args.hidden_size)\n        self.fc2 = nn.Linear(args.hidden_size, args.hidden_size)\n        self.fc3 = nn.Linear(args.hidden_size, args.hidden_size)\n        self.fc4 = nn.Linear(args.hidden_size, num_outputs)\n\n        self.fc4.weight.data.mul_(0.1)\n        self.fc4.bias.data.mul_(0.0)\n\n    def forward(self, x):\n        if self.args.activation == 'tanh':\n            x = F.tanh(self.fc1(x))\n            x = F.tanh(self.fc2(x))\n            x = F.tanh(self.fc3(x))\n            mu = self.fc4(x)\n        elif self.args.activation == 'swish':\n            x = self.fc1(x)\n            x = x * F.sigmoid(x)\n            x = self.fc2(x)\n            x = x * F.sigmoid(x)\n            x = self.fc3(x)\n            x = x * F.sigmoid(x)\n            mu = self.fc4(x)\n        else:\n            raise ValueError\n\n        logstd = torch.zeros_like(mu)\n        std = torch.exp(logstd)\n        return mu, std, logstd\n```\n\nswish와 tanh를 사용한 학습을 비교한 그래프입니다. 하늘색 그래프가 swish를 사용한 결과, 파란색이 tanh를 사용한 결과입니다. score는 episode 마다의 reward의 합입니다.\n<center><img src=\"https://www.dropbox.com/s/3d07c1kql4h5oqk/Screenshot%202018-08-24%2016.33.45.png?dl=1\" width=\"350px\"></center>\n\n이제 multi-agent로 학습하도록 변경하면 됩니다. PPO의 경우 memory에 time horizon 동안의 sample을 시간순서대로 저장하고 GAE를 구한 이후에 minibatch로 추출해서 학습합니다. 따라서 여러개의 agent로 학습하기 위해서는 memory를 따로 만들어서 각각의 GAE를 구해서 학습해야합니다. Unity에서는 Mujoco에서 했던 것처럼 deque로 memory를 만들지 않고 따로 named tuple로 구현한 memory class를 import 해서 사용했습니다. utils 폴더 밑에 memory.py 코드에 구현되어있으며 코드는 https://github.com/pytorch/tutorials/blob/master/Reinforcement%20(Q-)Learning%20with%20PyTorch.ipynb\n에서 가져왔습니다. \n\nstate, action, reward, mask를 저장하는데 불러올 때 각각을 따로 불러올 수 있기 때문에 비효율적 시간을 많이 줄여줍니다. \n```python\nTransition = namedtuple('Transition', ('state', 'action', 'reward', 'mask'))\n\n\nclass Memory(object):\n    def __init__(self):\n        self.memory = []\n\n    def push(self, state, action, reward, mask):\n        \"\"\"Saves a transition.\"\"\"\n        self.memory.append(Transition(state, action, reward, mask))\n\n    def sample(self):\n        return Transition(*zip(*self.memory))\n\n    def __len__(self):\n        return len(self.memory)\n```\n\nmain.py 에서는 이 memory를 agent의 개수만큼 생성합니다. \n\n```python\nmemory = [Memory() for _ in range(num_agent)]\n```\nsample을 저장할 때도 agent마다 따로 따로 저장합니다. \n\n```python\nfor i in range(num_agent):\n    memory[i].push(states[i], actions[i], rewards[i], masks[i])\n\n```\n\ntime horizon이 끝나면 모은 sample 들을 가지고 학습하기 위한 값으로 만드는 과정을 진행합니다. 각각의 memory를 가지고 GAE와 old_policy, old_value 등을 계산해서 하나의 batch로 합칩니다. 그렇게 train_model 메소드에 전달하면 기존과 동일하게 agent를 업데이트합니다.\n\n```python\nsts, ats, returns, advants, old_policy, old_value = [], [], [], [], [], []\n\nfor i in range(num_agent):\n    batch = memory[i].sample()\n    st, at, rt, adv, old_p, old_v = process_memory(actor, critic, batch, args)\n    sts.append(st)\n    ats.append(at)\n    returns.append(rt)\n    advants.append(adv)\n    old_policy.append(old_p)\n    old_value.append(old_v)\n\nsts = torch.cat(sts)\nats = torch.cat(ats)\nreturns = torch.cat(returns)\nadvants = torch.cat(advants)\nold_policy = torch.cat(old_policy)\nold_value = torch.cat(old_value)\n\ntrain_model(actor, critic, actor_optim, critic_optim, sts, ats, returns, advants,\n            old_policy, old_value, args)\n```\n\n이렇게 학습한 에이전트는 다음과 같이 걷습니다. 이렇게 walker를 학습시키고 나니 어떻게 하면 사람처럼 자연스럽게 걷는 것을 agent 스스로 학습할 수 있을까라는 고민을 하게 되었습니다.\n<center><img src=\"https://www.dropbox.com/s/fyz1kn5v92l3rrk/plane-595.gif?dl=1\"></center>\n\nUnity ml-agent에서 제공하는 pretrained된 모델을 다음과 같이 걷습니다. 저희가 학습한 agent와 상당히 다르게 걷는데 왜 그런 차이가 나는지도 분석하고 싶습니다. \n<center><img src=\"https://www.dropbox.com/s/xwz766g7c4eiaia/plane-unity.gif?dl=1\"></center>\n\n\n</br>\n## 3. Unity Curved Surface 제작 및 학습기\nUnity ml-agent에서 제공하는 기본 Walker 환경에서 학습하고 나니 바닥을 조금 울퉁불퉁하게 혹은 경사가 진 곳에서 걷는 것을 학습해보고 싶다라는 생각이 들었습니다. 따라서 간단하게 걷는 배경을 다르게 하는 시도를 해봤습니다. \n\n</br>\n### 3.1 Curved Surface 만들기\nAgent가 걸어갈 배경을 처음부터 만드는 것보다 구할 수 있다면 만들어진 배경을 구하기로 했습니다. Unity를 무료라는 점에서 선택했듯이 배경을 무료로 구할 수 있는 방법을 선택했습니다. \n<center><img src=\"https://www.dropbox.com/s/e0tsp3e3c9uq2zh/Screenshot%202018-08-23%2000.19.14.png?dl=1\"></center>\n\n무료로 공개되어있는 Unity 배경 중에서 Curved Ground 라는 것을 가져와서 작업하기로 했습니다. 이 환경 같은 경우 spline을 그리듯이 중간의 점을 이동시키면서 사용자가 곡면을 수정할 수 있습니다.\n<center><img src=\"https://www.dropbox.com/s/3ppmxotrf6qhzaf/Screenshot%202018-08-23%2000.20.25.png?dl=1\"></center>\n\n간단하게 곡면을 만들어서 공을 굴려보면 다음과 같이 잘 굴러갑니다. \n<center><img src=\"https://www.dropbox.com/s/2e8yqvqj1a4th27/slope_walker_ball.gif?dl=1\"></center>\n\n여러 에이전트가 학습할 수 있도록 오목한 경사면을 제작했습니다. 초반의 모습은 다음과 같았습니다. \n<img src=\"https://www.dropbox.com/s/m492xsfp4bolmz5/Screenshot%202018-08-23%2000.36.06.png?dl=1\">\n\n하지만 최종으로는 다음과 같은 곡면으로 사용했습니다. 위 사진의 배경과 아래 사진의 배경이 다른 점은 slope 길이, 내리막 경사, 오르막 경사입니다. Slope 길이의 경우 길이를 기존 plane 과 동일하게 했더니, 오르막 올라가는 부분이 학습이 잘 안 되었습니다. 따라서 길이를 줄였습니다. 내리막 경사의 경우 너무 경사지면 학습이 잘 안 되고, 너무 완만하니 내리막 티가 잘 안 나기 때문에 적절한 경사를 설정했습니다. 오르막 경사의 경우 내리막보다는 오르막이 더 어려울 것이라고 판단해서 오르막 경사를 낮게 설정했습니다. \n<img src=\"https://www.dropbox.com/s/idbov4wtd6jeqb2/Screenshot%202018-08-23%2000.36.54.png?dl=1\">\n\n</br>\n\n### 3.2 Curved Surface에서 학습하기\n위 환경으로 학습을 할 때, agent가 너무 초반에 빨리 쓰러지는 현상이 발생했습니다. 혹시 발의 각도가 문제일까 싶어서 발 각도를 변경해보았습니다. \n\n<center><img src=\"https://www.dropbox.com/s/znvikbeoj7gku0u/Screenshot%202018-08-23%2000.38.22.png?dl=1\" width=\"400px\"></center>\n\n하지만 역시 평지에서 걷는 것처럼 걷도록 학습이 안되었습니다. 이 환경에서 더 잘 학습하려면 더 여러가지를 시도해봐야할 것 같습니다. (그래도 걷는 게 기특합니다..)\n<center><img src=\"https://www.dropbox.com/s/4fqpsdmnzvnvia0/curved-736.gif?dl=1\"></center>\n\n<img src=\"https://www.dropbox.com/s/t5ngr0io4xeex6y/curved-736-overview.gif?dl=1\">\n\n</br>\n## 4. 구현 후기\n피지여행 구현팀은 총 4명으로 진행했습니다. 각 팀원의 후기를 적어보겠습니다.\n\n- 팀원 장수영: 사랑합니다. 행복합니다.\n- 팀원 공민서: 제가 핵심적인 기능을 구현하지는 못했지만 무조코 설치와 모델 테스트를 맡으면서 딥마인드나 openai의 영상으로만 보던 에이전트의 성장과정을 눈으로 지켜볼 수 있었습니다. 제대로 서있지도 못하던 hopper가 어느정도 훈련이 되고서는 넘어지려하다가도 추진력을 얻기위해 웅크렸다 뛰는 것을 관찰하는 것도 재미있고 육아일기를 보는 아버지의 마음을 조금이나마 이해할 수 있었습니다. 텐서보드를 넣는 걸 깜빡해 일일히 에피소드 별 스코어를 시각화 하면서 텐서보드의 소중함을 알았습니다. 유니티 코드리뷰를 하면서도 시스템 아키텍쳐 설계에 대해서도 배울 점이 있었던 것 같고 swish라는 활성화함수의 존재도 알았었고 curiosity도 알게되었고 역시 다른 사람의 코드를 읽는 것도 많은 공부가 된다고 되새기던 시간이었습니다. 물론 너무 크기가 방대해서 가독성은 많이 떨어졌습니다만 무조코보다 유니티가 훨씬 흥할거라고 생각했습니다. 마지막으로 누구 하나 열정적이지 않은 사람이 없이 치열한 고민을 함께 한 PG여행팀 분들, 저의 부족함과 상생의 기쁨을 알게해주셔서 정말 감사드립니다.\n- 팀원 양혁렬: 여러 에이전트가 함께하면 더 잘하는 걸 보면서 새삼 좋은 분들과 함께 할 수 있어서 행복했습니다\n- 팀원 이웅원: 저희가 직접 바닥부터 다 구현했던 것은 아니지만 구현을 해보면서 논문의 내용을 더 잘 이해할 수 있었습니다. 논문에 나와있지 않은 여러 노하우가 필요한 점들도 많았습니다. 역시 코드로 보고 성능 재현이 되어야 제대로 알고리즘을 이해하게 된다는 것을 다시 느낀 시간이었습니다. 또한 강화학습은 역시 환경세팅이 어렵다는 생각을 했습니다. 하지만 unity ml-agent를 사용해보면서 앞으로 강화학습 환경으로서 가능성이 상당히 크다는 생각을 했습니다. 또한 구현팀과 슬랙, 깃헙으로 협업하면서 온라인 협업에 대해서 더 배워가는 것 같습니다. 아직은 익숙하지 않지만 앞으로는 마치 바로 옆에서 같이 코딩하는 것 같이 될 거라고 생각합니다.","slug":"8_implement","published":1,"updated":"2018-09-08T11:34:55.269Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjltduyuz0000f8xxn1w8mu2c","content":"<h1 id=\"PG-Travel-implementation-story\"><a href=\"#PG-Travel-implementation-story\" class=\"headerlink\" title=\"PG Travel implementation story\"></a>PG Travel implementation story</h1><ul>\n<li>구현 코드 링크 : <a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">https://github.com/reinforcement-learning-kr/pg_travel</a></li>\n</ul>\n<p>피지여행 프로젝트에서는 다음 7개 논문을 살펴보았습니다. 각 논문에 대한 리뷰는 이전 글들에서 다루고 있습니다. </p>\n<p><a name=\"1\"></a></p>\n<ul>\n<li>[1] R. Sutton, et al., “Policy Gradient Methods for Reinforcement Learning with Function Approximation”, NIPS 2000.<br><a name=\"2\"></a></li>\n<li>[2] D. Silver, et al., “Deterministic Policy Gradient Algorithms”, ICML 2014.<br><a name=\"3\"></a></li>\n<li>[3] T. Lillicrap, et al., “Continuous Control with Deep Reinforcement Learning”, ICLR 2016.<br><a name=\"4\"></a></li>\n<li>[4] S. Kakade, “A Natural Policy Gradient”, NIPS 2002.<br><a name=\"5\"></a></li>\n<li>[5] J. Schulman, et al., “Trust Region Policy Optimization”, ICML 2015.<br><a name=\"6\"></a></li>\n<li>[6] J. Schulman, et al., “High-Dimensional Continuous Control using Generalized Advantage Estimation”, ICLR 2016.<br><a name=\"7\"></a></li>\n<li>[7] J. Schulman, et al., “Proximal Policy Optimization Algorithms”, arXiv, <a href=\"https://arxiv.org/pdf/1707.06347.pdf\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/pdf/1707.06347.pdf</a>.</li>\n</ul>\n<p>강화학습 알고리즘을 이해하는데 있어서 논문을 보고 이론적인 부분을 알아가는 것이 좋습니다. 하지만 실제 코드로 돌아가는 것은 논문만 보고는 알 수 없는 경우가 많습니다. 따라서 피지여행 프로젝트에서는 위 7개 논문 중에 DPG와 DDPG를 제외한 알고리즘을 구현해보았습니다. 구현한 알고리즘은 다음 4개입니다. 이 때, TRPO와 PPO 구현에는 GAE(General Advantage Estimator)가 함께 들어갑니다. </p>\n<ul>\n<li>Vanilla Policy Gradient [<a href=\"#1\">1</a>]</li>\n<li>TNPG(Truncated Natural Policy Gradient) [<a href=\"#4\">4</a>]</li>\n<li>TRPO(Trust Region Policy Optimization) [<a href=\"#5\">5</a>]</li>\n<li>PPO(Proximal Policy Optimization) [<a href=\"#7\">7</a>].</li>\n</ul>\n<p>바닥부터 저희가 구현한 것은 아니며 다음 코드들을 참고해서 구현하였습니다. Vanilla PG의 경우 RLCode의 깃헙을 참고하였습니다.</p>\n<ul>\n<li><a href=\"https://github.com/openai/baselines/tree/master/baselines/trpo_mpi\" target=\"_blank\" rel=\"noopener\">OpenAI Baseline</a></li>\n<li><a href=\"https://github.com/ikostrikov/pytorch-trpo\" target=\"_blank\" rel=\"noopener\">Pytorch implemetation of TRPO</a></li>\n<li><a href=\"https://github.com/rlcode/reinforcement-learning-kr/tree/master/2-cartpole/2-actor-critic\" target=\"_blank\" rel=\"noopener\">RLCode Actor-Critic</a></li>\n</ul>\n<p>GAE와 TRPO, PPO 논문에서는 Mujoco라는 물리 시뮬레이션을 학습 환경으로 사용합니다. 따라서 저희도 Mujoco로 처음 시작을 하였습니다. 하지만 Mujoco는 1달만 무료이고 그 이후부터 유료이며 확장성이 떨어집니다. Unity ml-agent는 기존 Unity를 그대로 사용하면서 쉽게 강화학습 에이전트를 붙일 수 있도록 설계되어 있습니다. Unity ml-agent에서는 저희가 살펴본 알고리즘 중에 가장 최신 알고리즘은 PPO를 적용해봤습니다. 기본적으로 제공하는 환경 이외에 저희가 customize 한 환경에서도 학습해봤습니다.  </p>\n<ul>\n<li>mujoco-py: <a href=\"https://github.com/openai/mujoco-py\" target=\"_blank\" rel=\"noopener\">https://github.com/openai/mujoco-py</a></li>\n<li>Unity ml-agent: <a href=\"https://github.com/Unity-Technologies/ml-agents\" target=\"_blank\" rel=\"noopener\">https://github.com/Unity-Technologies/ml-agents</a></li>\n</ul>\n<p>코드를 구현하고 환경에서 학습을 시키면서 여러가지 이슈들이 있었고 해결해내가는 과정이 있었습니다. 그 과정을 간단히 정리해서 공유하면 PG를 공부하는 분들께 도움일 될 것 같습니다. 저희가 구현한 순서대로 1. Mujoco 학습 2. Unity ml-agent 학습 3. Unity Curved Surface 로 이 포스트가 진행됩니다.</p>\n<p><br></p>\n<h2 id=\"1-Mujoco-학습\"><a href=\"#1-Mujoco-학습\" class=\"headerlink\" title=\"1. Mujoco 학습\"></a>1. Mujoco 학습</h2><p>일명 “Continuous control” 문제는 action이 discrete하지 않고 continuous한 경우를 말합니다. Mujoco는 continuous control에 강화학습을 적용한 논문들이 애용하는 시뮬레이터입니다. 저희가 리뷰한 논문 중에서도 TRPO, PPO, GAE에서 Mujoco를 사용합니다. 따라서 저희가 처음 피지여행 알고리즘을 적용한 환경으로 Mujoco를 선택했습니다. </p>\n<p>Mujoco에는 Ant, HalfCheetah, Hopper, Humanoid, HumanoidStandup, InvertedPendulum, Reacher, Swimmer, Walker2d 과 같은 환경이 있습니다. 그 중에서 Hopper에 맞춰서 학습이 되도록 코드를 구현하였습니다. Mujoco 설치와 관련된 내용은 Wiki에 있습니다.</p>\n<p><br></p>\n<h3 id=\"1-1-Hopper\"><a href=\"#1-1-Hopper\" class=\"headerlink\" title=\"1.1 Hopper\"></a>1.1 Hopper</h3><p>Hopper는 외다리로 뛰어가는 것을 학습하는 것이 목표입니다. Hopper는 다음과 같이 생겼습니다.<br><img src=\"https://www.dropbox.com/s/wjxrelxyp014j3g/Screenshot%202018-08-23%2000.55.54.png?dl=1\"></p>\n<p>환경을 이해하려면 환경의 상태와 행동, 보상 그리고 학습하고 싶은 목표를 알아야합니다. </p>\n<ul>\n<li>상태 : 관절의 위치, 각도, 각속도</li>\n<li>행동 : 관절의 가해지는 토크</li>\n<li>보상 : 앞으로 나아가는 속도</li>\n<li>목표 : 최대한 앞으로 많이 나아가기</li>\n</ul>\n<p>즉 에이전트는 time step마다 관절의 위치와 각도를 받아와서 그 상태에서 어떻게 움직여야 앞으로 나아갈 수 있는지를 학습해야 합니다. 행동은 discrete action이 아닌 continuous action으로 -1과 1사이의 값을 가집니다. 만약 행동이 -1이라면 해당 관절에 시계반대방향으로 토크를 주는 것이고 행동이 +1이라면 해당 관절에 시계방향으로 토크를 주는 것입니다. </p>\n<p>continuous action을 주는 방법은 네트워크(Actor)의 output layer에서 activation function으로 tanh와 같은 것을 사용해서 continuous한 값을 출력하는 것이 있습니다. 하지만 피지여행 코드 구현에서는 action을 gaussian distribution에서 sampling 하였습니다. 이렇게 하면 분산을 일정하게 유지하면서 지속적인 exploration을 할 수 있습니다. 간단하게 그림으로 보자면 다음과 같습니다.<br><img src=\"https://www.dropbox.com/s/94g01zdxyf5oxu1/Screenshot%202018-08-23%2001.20.21.png?dl=1\"></p>\n<p>네트워크 구조와 행동을 선택하는 부분은 다음과 같습니다. Hidden Layer의 activation function으로 tanh를 사용했으며(ReLU를 테스트해보지는 않았습니다. 기존 TRPO, PPO 구현들과 논문에서 tanh를 사용하기 때문에 저희도 사용했습니다. 뒤에 유니티 환경에서는 Swish라는 것을 사용합니다.) log std를 0으로 고정함으로서 일정한 폭을 가지는 분포를 만들어낼 수 있습니다. 이 분포로부터 action을 sampling 합니다.</p>\n<p><img src=\"https://www.dropbox.com/s/xfl9zxies0lmpm1/Screenshot%202018-08-23%2001.20.44.png?dl=1\"></p>\n<p><br></p>\n<h3 id=\"1-2-Vanilla-PG\"><a href=\"#1-2-Vanilla-PG\" class=\"headerlink\" title=\"1.2 Vanilla PG\"></a>1.2 Vanilla PG</h3><p>Vanilla PG는 Actor-Critic의 가장 간단한 형태입니다. Vanilla PG는 이후의 구현에 대한 baseline이 됩니다. 구현이 가장 간단하면서 학습이 안되는 것은 아닙니다. 따라서 코드 전체 구조를 잡는데 Vanilla PG를 짜는 것이 도움이 됩니다. 전반적인 코드 구조는 다음과 같습니다.</p>\n<ul>\n<li>iteration 마다 일정한 step 수만큼 환경에서 진행해서 샘플을 모은다</li>\n<li>모은 샘플로 Actor와 Critic을 학습한다</li>\n<li>반복한다</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">episodes = <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> iter <span class=\"keyword\">in</span> range(<span class=\"number\">15000</span>):</span><br><span class=\"line\">    actor.eval(), critic.eval()</span><br><span class=\"line\">    memory = deque()</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">while</span> steps &lt; <span class=\"number\">2048</span>:</span><br><span class=\"line\">        episodes += <span class=\"number\">1</span></span><br><span class=\"line\">        state = env.reset()</span><br><span class=\"line\">        state = running_state(state)</span><br><span class=\"line\">        score = <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> range(<span class=\"number\">10000</span>):</span><br><span class=\"line\">            mu, std, _ = actor(torch.Tensor(state).unsqueeze(<span class=\"number\">0</span>))</span><br><span class=\"line\">            action = get_action(mu, std)[<span class=\"number\">0</span>]</span><br><span class=\"line\">            next_state, reward, done, _ = env.step(action)</span><br><span class=\"line\">            next_state = running_state(next_state)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">if</span> done:</span><br><span class=\"line\">                mask = <span class=\"number\">0</span></span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                mask = <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">            memory.append([state, action, reward, mask])</span><br><span class=\"line\">            state = next_state</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">if</span> done:</span><br><span class=\"line\">                <span class=\"keyword\">break</span></span><br><span class=\"line\">                </span><br><span class=\"line\">    actor.train(), critic.train()</span><br><span class=\"line\">    train_model(actor, critic, memory, actor_optim, critic_optim)</span><br></pre></td></tr></table></figure>\n<p>memory에 sample을 저장할 때 sample은 state와 action, reward, mask(마지막 state일 경우 0 나머지 1)입니다. mask의 경우 뒤에서 return이나 advantage를 계산할 때 사용됩니다. 또 하나 염두에 두어야할 것은 running_state 입니다. running_state는 input으로 들어오는 state의 scale이 일정하지 않기 때문에 사용합니다. 즉 state의 각 dimension을 평균 0 분산 1로 standardization 하는 것입니다. 따라서 모델을 저장할 때 각 dimension 마다의 평균과 분산도 같이 저장해서 테스트할 때 불러와서 사용해야 합니다.</p>\n<p>Vanilla PG의 경우 학습 부분이 상당히 간단합니다. 다음 코드를 보시면 메모리에서 state, action, reward, mask를 꺼냅니다. reward와 mask를 통해 return을 구할 수 있고 이 return을 통해 actor를 업데이트 할 수 있습니다 (REINFORCE 알고리즘을 떠올리시면 됩니다). 여기서 critic이 하는 일은 없지만 뒤의 알고리즘들과 코드의 통일성을 위해 fake로 넣어놨습니다. Return은 평균을 빼고 분산으로 나눠서 standardize 합니다. </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">train_model</span><span class=\"params\">(actor, critic, memory, actor_optim, critic_optim)</span>:</span></span><br><span class=\"line\">    memory = np.array(memory)</span><br><span class=\"line\">    states = np.vstack(memory[:, <span class=\"number\">0</span>])</span><br><span class=\"line\">    actions = list(memory[:, <span class=\"number\">1</span>])</span><br><span class=\"line\">    rewards = list(memory[:, <span class=\"number\">2</span>])</span><br><span class=\"line\">    masks = list(memory[:, <span class=\"number\">3</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">    returns = get_returns(rewards, masks)</span><br><span class=\"line\">    train_critic(critic, states, returns, critic_optim)</span><br><span class=\"line\">    train_actor(actor, returns, states, actions, actor_optim)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> returns</span><br></pre></td></tr></table></figure>\n<p>이 코드로 Hopper 환경에서 학습한 그래프는 다음과 같습니다.<br><img src=\"https://www.dropbox.com/s/asoysfuk76zs1dk/Screenshot%202018-08-23%2001.30.58.png?dl=1\"></p>\n<p><br></p>\n<h3 id=\"1-3-TNPG\"><a href=\"#1-3-TNPG\" class=\"headerlink\" title=\"1.3 TNPG\"></a>1.3 TNPG</h3><p>NPG를 이용한 parameter update 식은 다음과 같습니다. </p>\n<p>$$\\bar{w}=F(\\theta)^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>NPG를 구현하려면 KL-divergence의 Hessian의 inverse를 구해야하는 문제가 생깁니다. 현재와 같이 Deep Neural Network를 쓰는 경우에 Hessian의 inverse를 직접적으로 구하는 것은 computationally inefficient 합니다. 따라서 직접 구하지 않고 Conjugate gradient 방법을 사용해서 Fisher Vector Product ($$F^{-1}g$$)를 구합니다. 이러한 알고리즘을 Truncated Natural Policy Gradient(TNPG)라고 부릅니다. </p>\n<p>TNPG에서 parameter update를 구하는 과정은 다음과 같습니다. </p>\n<ol>\n<li>Return 구하기</li>\n<li>Critic 학습하기</li>\n<li>logp * return –&gt; loss 구하기</li>\n<li>loss의 미분과 kl-divergence의 2차 미분을 통해 step direction 구하기</li>\n<li>구한 step direction으로 parameter update</li>\n</ol>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">train_model</span><span class=\"params\">(actor, critic, memory, actor_optim, critic_optim)</span>:</span></span><br><span class=\"line\">    memory = np.array(memory)</span><br><span class=\"line\">    states = np.vstack(memory[:, <span class=\"number\">0</span>])</span><br><span class=\"line\">    actions = list(memory[:, <span class=\"number\">1</span>])</span><br><span class=\"line\">    rewards = list(memory[:, <span class=\"number\">2</span>])</span><br><span class=\"line\">    masks = list(memory[:, <span class=\"number\">3</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># ----------------------------</span></span><br><span class=\"line\">    <span class=\"comment\"># step 1: get returns</span></span><br><span class=\"line\">    returns = get_returns(rewards, masks)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># ----------------------------</span></span><br><span class=\"line\">    <span class=\"comment\"># step 2: train critic several steps with respect to returns</span></span><br><span class=\"line\">    train_critic(critic, states, returns, critic_optim)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># ----------------------------</span></span><br><span class=\"line\">    <span class=\"comment\"># step 3: get gradient of loss and hessian of kl</span></span><br><span class=\"line\">    loss = get_loss(actor, returns, states, actions)</span><br><span class=\"line\">    loss_grad = torch.autograd.grad(loss, actor.parameters())</span><br><span class=\"line\">    loss_grad = flat_grad(loss_grad)</span><br><span class=\"line\">    step_dir = conjugate_gradient(actor, states, loss_grad.data, nsteps=<span class=\"number\">10</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># ----------------------------</span></span><br><span class=\"line\">    <span class=\"comment\"># step 4: get step direction and step size and update actor</span></span><br><span class=\"line\">    params = flat_params(actor)</span><br><span class=\"line\">    new_params = params + <span class=\"number\">0.5</span> * step_dir</span><br><span class=\"line\">    update_model(actor, new_params)</span><br></pre></td></tr></table></figure>\n<p>conjugate gradient 코드는 OpenAI baseline에서 가져왔습니다. 이 코드는 원래 John schulmann 개인 repository에 있는 그대로 사용하는 것입니다. nsteps 만큼 iterataion을 반복하며 결국 x를 구하는 것인데 이 x가 step direction 입니다. </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># from openai baseline code</span></span><br><span class=\"line\"><span class=\"comment\"># https://github.com/openai/baselines/blob/master/baselines/common/cg.py</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">conjugate_gradient</span><span class=\"params\">(actor, states, b, nsteps, residual_tol=<span class=\"number\">1e-10</span>)</span>:</span></span><br><span class=\"line\">    x = torch.zeros(b.size())</span><br><span class=\"line\">    r = b.clone()</span><br><span class=\"line\">    p = b.clone()</span><br><span class=\"line\">    rdotr = torch.dot(r, r)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(nsteps):</span><br><span class=\"line\">        _Avp = fisher_vector_product(actor, states, p)</span><br><span class=\"line\">        alpha = rdotr / torch.dot(p, _Avp)</span><br><span class=\"line\">        x += alpha * p</span><br><span class=\"line\">        r -= alpha * _Avp</span><br><span class=\"line\">        new_rdotr = torch.dot(r, r)</span><br><span class=\"line\">        betta = new_rdotr / rdotr</span><br><span class=\"line\">        p = r + betta * p</span><br><span class=\"line\">        rdotr = new_rdotr</span><br><span class=\"line\">        <span class=\"keyword\">if</span> rdotr &lt; residual_tol:</span><br><span class=\"line\">            <span class=\"keyword\">break</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure>\n<p>fisher_vector_product는 kl-divergence의 2차미분과 어떠한 vector의 곱인데 p는 처음에 gradient 값이었다가 점차 업데이트가 됩니다. kl-divergence의 2차 미분을 구하는 과정은 다음과 같습니다. 일단 kl-divergence를 현재 policy에 대해서 구한 다음에 actor parameter에 대해서 미분합니다. 이렇게 미분한 gradient를 일단 flat하게 핀 다음에 p라는 벡터와 곱해서 하나의 값으로 만듭니다. 그 값을 다시 actor의 parameter로 만듦으로서 따로 KL-divergence의 2차미분을 구하지않고 Fisher vector product를 구할 수 있습니다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">fisher_vector_product</span><span class=\"params\">(actor, states, p)</span>:</span></span><br><span class=\"line\">    p.detach()</span><br><span class=\"line\">    kl = kl_divergence(new_actor=actor, old_actor=actor, states=states)</span><br><span class=\"line\">    kl = kl.mean()</span><br><span class=\"line\">    kl_grad = torch.autograd.grad(kl, actor.parameters(), create_graph=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">    kl_grad = flat_grad(kl_grad)  <span class=\"comment\"># check kl_grad == 0</span></span><br><span class=\"line\"></span><br><span class=\"line\">    kl_grad_p = (kl_grad * p).sum()</span><br><span class=\"line\">    kl_hessian_p = torch.autograd.grad(kl_grad_p, actor.parameters())</span><br><span class=\"line\">    kl_hessian_p = flat_hessian(kl_hessian_p)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> kl_hessian_p + <span class=\"number\">0.1</span> * p</span><br></pre></td></tr></table></figure>\n<p>TNPG 학습 결과는 다음과 같습니다.<br><img src=\"https://www.dropbox.com/s/uc4c0s00qbs33nr/Screenshot%202018-08-23%2001.53.17.png?dl=1\"></p>\n<p><br></p>\n<h3 id=\"1-4-TRPO\"><a href=\"#1-4-TRPO\" class=\"headerlink\" title=\"1.4 TRPO\"></a>1.4 TRPO</h3><p>TRPO와 NPG가 다른 점은 surrogate loss 사용과 trust region 입니다. 하지만 실제로 구현해서 학습을 시켜본 결과 trust region을 넘어가서 back tracking line search를 하는 경우는 거의 없습니다. 따라서 주된 변화는 surrogate loss에 있다고 보셔도 됩니다. Surrogate loss에서 advantage function을 사용하는데 본 코드 구현에서는 GAE를 사용하였습니다. TRPO 업데이트 식은 다음과 같습니다. Q function 위치에 GAE가 들어갑니다.</p>\n<p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}Q_{\\theta_\\mathrm{old} }(s,a)\\right] \\\\<br>\\mathrm{s.t.\\ }&amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} }}\\left[D_\\mathrm{KL}\\left(\\pi_{\\theta_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta<br>\\end{align}<br>$$</p>\n<p>GAE를 구하는 코드는 다음과 같습니다. GAE는 td-error의 discounted summation이라고 볼 수 있습니다. 마지막에 advants를 standardization 하는 것은 return에서 하는 것과 같은 효과를 봅니다. 하지만 standardization을 안하고 실험을 해보지는 않았습니다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_gae</span><span class=\"params\">(rewards, masks, values)</span>:</span></span><br><span class=\"line\">    rewards = torch.Tensor(rewards)</span><br><span class=\"line\">    masks = torch.Tensor(masks)</span><br><span class=\"line\">    returns = torch.zeros_like(rewards)</span><br><span class=\"line\">    advants = torch.zeros_like(rewards)</span><br><span class=\"line\"></span><br><span class=\"line\">    running_returns = <span class=\"number\">0</span></span><br><span class=\"line\">    previous_value = <span class=\"number\">0</span></span><br><span class=\"line\">    running_advants = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> t <span class=\"keyword\">in</span> reversed(range(<span class=\"number\">0</span>, len(rewards))):</span><br><span class=\"line\">        running_returns = rewards[t] + hp.gamma * running_returns * masks[t]</span><br><span class=\"line\">        running_tderror = rewards[t] + hp.gamma * previous_value * masks[t] - \\</span><br><span class=\"line\">                    values.data[t]</span><br><span class=\"line\">        running_advants = running_tderror + hp.gamma * hp.lamda * \\</span><br><span class=\"line\">                          running_advants * masks[t]</span><br><span class=\"line\"></span><br><span class=\"line\">        returns[t] = running_returns</span><br><span class=\"line\">        previous_value = values.data[t]</span><br><span class=\"line\">        advants[t] = running_advants</span><br><span class=\"line\"></span><br><span class=\"line\">    advants = (advants - advants.mean()) / advants.std()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> returns, advants</span><br></pre></td></tr></table></figure>\n<p>Surrogate loss를 구하는 코드는 다음과 같습니다. Advantage function(GAE)를 구하고 나면 이전 policy와 현재 policy 사이의 ratio를 구해서 advantage function에 곱하면 됩니다. 이 때 사실 old policy와 new policy는 값은 같지만 old policy는 clone()이나 detach()를 사용해서 update가 안되게 만들어줍니다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">surrogate_loss</span><span class=\"params\">(actor, advants, states, old_policy, actions)</span>:</span></span><br><span class=\"line\">    mu, std, logstd = actor(torch.Tensor(states))</span><br><span class=\"line\">    new_policy = log_density(torch.Tensor(actions), mu, std, logstd)</span><br><span class=\"line\">    advants = advants.unsqueeze(<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    surrogate = advants * torch.exp(new_policy - old_policy)</span><br><span class=\"line\">    surrogate = surrogate.mean()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> surrogate</span><br></pre></td></tr></table></figure>\n<p>Actor의 step direction을 구하는 것은 TNPG와 동일합니다. TNPG에서는 step direction으로 바로 업데이트 했지만 TRPO는 다음과 같은 작업을 해줍니다. Full step을 구하는 과정이라고 볼 수 있습니다. </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># ----------------------------</span></span><br><span class=\"line\"><span class=\"comment\"># step 4: get step direction and step size and full step</span></span><br><span class=\"line\">params = flat_params(actor)</span><br><span class=\"line\">shs = <span class=\"number\">0.5</span> * (step_dir * fisher_vector_product(actor, states, step_dir)</span><br><span class=\"line\">             ).sum(<span class=\"number\">0</span>, keepdim=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">step_size = <span class=\"number\">1</span> / torch.sqrt(shs / hp.max_kl)[<span class=\"number\">0</span>]</span><br><span class=\"line\">full_step = step_size * step_dir</span><br></pre></td></tr></table></figure>\n<p>이렇게 full step을 구하고 나면 Trust region optimization 단계에 들어갑니다. expected improvement는 구한 step 만큼 parameter space에서 움직였을 때 예상되는 performance 변화입니다. 이 값은 kl-divergence와 함께 trust region 안에 있는지 밖에 있는지 판단하는 근거가 됩니다. expected improve는 출발점에서의 gradient * full step으로 구합니다. 그리고 10번을 돌아가며 Back-tracking line search를 실시합니다. 처음에는 full step 만큼 가본 다음에 kl-divergence와 emprovement를 통해 trust region 안이면 루프 탈출, 밖이면 full step을 반만큼 쪼개서 다시 이 과정을 반복합니다.  </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># ----------------------------</span></span><br><span class=\"line\"><span class=\"comment\"># step 5: do backtracking line search for n times</span></span><br><span class=\"line\">old_actor = Actor(actor.num_inputs, actor.num_outputs)</span><br><span class=\"line\">update_model(old_actor, params)</span><br><span class=\"line\">expected_improve = (loss_grad * full_step).sum(<span class=\"number\">0</span>, keepdim=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">expected_improve = expected_improve.data.numpy()</span><br><span class=\"line\"></span><br><span class=\"line\">flag = <span class=\"keyword\">False</span></span><br><span class=\"line\">fraction = <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">10</span>):</span><br><span class=\"line\">    new_params = params + fraction * full_step</span><br><span class=\"line\">    update_model(actor, new_params)</span><br><span class=\"line\">    new_loss = surrogate_loss(actor, advants, states, old_policy.detach(),</span><br><span class=\"line\">                              actions)</span><br><span class=\"line\">    new_loss = new_loss.data.numpy()</span><br><span class=\"line\">    loss_improve = new_loss - loss</span><br><span class=\"line\">    expected_improve *= fraction</span><br><span class=\"line\">    kl = kl_divergence(new_actor=actor, old_actor=old_actor, states=states)</span><br><span class=\"line\">    kl = kl.mean()</span><br><span class=\"line\"></span><br><span class=\"line\">    print(<span class=\"string\">'kl: &#123;:.4f&#125;  loss improve: &#123;:.4f&#125;  expected improve: &#123;:.4f&#125;  '</span></span><br><span class=\"line\">          <span class=\"string\">'number of line search: &#123;&#125;'</span></span><br><span class=\"line\">          .format(kl.data.numpy(), loss_improve, expected_improve[<span class=\"number\">0</span>], i))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># see https: // en.wikipedia.org / wiki / Backtracking_line_search</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> kl &lt; hp.max_kl <span class=\"keyword\">and</span> (loss_improve / expected_improve) &gt; <span class=\"number\">0.5</span>:</span><br><span class=\"line\">        flag = <span class=\"keyword\">True</span></span><br><span class=\"line\">        <span class=\"keyword\">break</span></span><br><span class=\"line\"></span><br><span class=\"line\">    fraction *= <span class=\"number\">0.5</span></span><br></pre></td></tr></table></figure>\n<p>Critic의 학습은 단순히 value function과 return의 MSE error를 계산해서 loss로 잡고 loss를 최소화하도록 학습합니다. TRPO 학습 결과는 다음과 같습니다.<br><img src=\"https://www.dropbox.com/s/rc9hxsx1kvokcrv/Screenshot%202018-08-23%2013.36.51.png?dl=1\"></p>\n<p><br></p>\n<h3 id=\"1-5-PPO\"><a href=\"#1-5-PPO\" class=\"headerlink\" title=\"1.5 PPO\"></a>1.5 PPO</h3><p>PPO의 장점을 꼽으라면 GPU 사용하기 좋고 sample efficiency가 늘어난다는 것입니다. TNPG와 TRPO의 경우 한 번 모은 sample은 모델을 단 한 번 업데이트하는데 사용하지만 PPO의 경우 몇 mini-batch로 epoch를 돌리기 때문입니다. GAE를 사용한다는 것은 같고 Conjugate gradient나 Fisher vector product나 back tracking line search가 다 빠집니다. 대신 loss function clip으로 monotonic improvement를 보장하게 학습합니다. 따라서 코드가 상당히 간단해집니다. </p>\n<p>다음 코드 부분이 PPO의 전체라고 봐도 무방합니다. PPO는 다음과 같은 순서로 학습합니다. </p>\n<ul>\n<li>batch를 random suffling하고 mini batch를 추출</li>\n<li>value function 구하기</li>\n<li>critic loss 구하기 (clip을 사용해도 되고 TRPO와 같이 그냥 학습시켜도 됌)</li>\n<li>surrogate loss 구하기</li>\n<li>surrogate loss clip해서 actor loss 만들기</li>\n<li>actor와 critic 업데이트</li>\n</ul>\n<p>Actor의 loss를 구하는 것은 다음 식의 값을 구하는 것입니다. 이 식을 구하려면 ratio에 한 번 클립하고 loss 값을 한 번 min을 취하면 됩니다.</p>\n<p>$$L^{CLIP}(\\theta) = \\hat{E}_t [min(r_t(\\theta) \\, \\hat{A}_t,  clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\, \\hat{A}_t)]$$</p>\n<p>이 코드 구현에서는 actor와 critic을 따로 모델로 만들어서 따로 따로 업데이트를 하지만 하나로 만든다면 loss로 한 번만 업데이트하면 됩니다. 또한 entropy loss를 최종 loss에 더해서 regularization 효과를 볼 수도 있습니다. Critic loss에 clip 해주는 것은 OpenAI baseline의 ppo2 코드를 참조하였습니다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># step 2: get value loss and actor loss and update actor &amp; critic</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> range(<span class=\"number\">10</span>):</span><br><span class=\"line\">    np.random.shuffle(arr)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(n // hp.batch_size):</span><br><span class=\"line\">        batch_index = arr[hp.batch_size * i: hp.batch_size * (i + <span class=\"number\">1</span>)]</span><br><span class=\"line\">        batch_index = torch.LongTensor(batch_index)</span><br><span class=\"line\">        inputs = torch.Tensor(states)[batch_index]</span><br><span class=\"line\">        returns_samples = returns.unsqueeze(<span class=\"number\">1</span>)[batch_index]</span><br><span class=\"line\">        advants_samples = advants.unsqueeze(<span class=\"number\">1</span>)[batch_index]</span><br><span class=\"line\">        actions_samples = torch.Tensor(actions)[batch_index]</span><br><span class=\"line\">        oldvalue_samples = old_values[batch_index].detach()</span><br><span class=\"line\"></span><br><span class=\"line\">        loss, ratio = surrogate_loss(actor, advants_samples, inputs,</span><br><span class=\"line\">                                     old_policy.detach(), actions_samples,</span><br><span class=\"line\">                                     batch_index)</span><br><span class=\"line\"></span><br><span class=\"line\">        values = critic(inputs)</span><br><span class=\"line\">        clipped_values = oldvalue_samples + \\</span><br><span class=\"line\">                         torch.clamp(values - oldvalue_samples,</span><br><span class=\"line\">                                     -hp.clip_param,</span><br><span class=\"line\">                                     hp.clip_param)</span><br><span class=\"line\">        critic_loss1 = criterion(clipped_values, returns_samples)</span><br><span class=\"line\">        critic_loss2 = criterion(values, returns_samples)</span><br><span class=\"line\">        critic_loss = torch.max(critic_loss1, critic_loss2).mean()</span><br><span class=\"line\"></span><br><span class=\"line\">        clipped_ratio = torch.clamp(ratio,</span><br><span class=\"line\">                                    <span class=\"number\">1.0</span> - hp.clip_param,</span><br><span class=\"line\">                                    <span class=\"number\">1.0</span> + hp.clip_param)</span><br><span class=\"line\">        clipped_loss = clipped_ratio * advants_samples</span><br><span class=\"line\">        actor_loss = -torch.min(loss, clipped_loss).mean()</span><br><span class=\"line\"></span><br><span class=\"line\">        loss = actor_loss + <span class=\"number\">0.5</span> * critic_loss</span><br><span class=\"line\"></span><br><span class=\"line\">        critic_optim.zero_grad()</span><br><span class=\"line\">        loss.backward(retain_graph=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">        critic_optim.step()</span><br><span class=\"line\"></span><br><span class=\"line\">        actor_optim.zero_grad()</span><br><span class=\"line\">        loss.backward()</span><br><span class=\"line\">        actor_optim.step()</span><br></pre></td></tr></table></figure>\n<p>PPO의 학습 결과는 다음과 같습니다.<br><img src=\"https://www.dropbox.com/s/rkxa836ap931kbd/Screenshot%202018-08-23%2013.50.57.png?dl=1\"></p>\n<p><br></p>\n<h2 id=\"2-Unity-ml-agent-학습\"><a href=\"#2-Unity-ml-agent-학습\" class=\"headerlink\" title=\"2. Unity ml-agent 학습\"></a>2. Unity ml-agent 학습</h2><p>Mujoco Hopper(half-cheetah와 같은 것도)에 Vanilla PG, TNPG, TRPO, PPO를 구현해서 적용했습니다. Mujoco의 경우 이미 Hyper parameter와 같은 정보들이 논문이나 블로그에 있기 때문에 상대적으로 continuous control로 시작하기에는 좋습니다. 맨 처음에 말했듯이 Mujoco는 1달만 무료이고 그 이후부터 유료이며 확장성이 떨어집니다. 좀 더 general한 agent를 학습시키기에 좋은 환경이 필요합니다. 따라서 Unity ml-agent를 살펴봤습니다. Repository는 다음과 같습니다. </p>\n<ul>\n<li><a href=\"https://github.com/Unity-Technologies/ml-agents\" target=\"_blank\" rel=\"noopener\">Unity ml-agent repository</a></li>\n<li><a href=\"https://unity3d.com/machine-learning/\" target=\"_blank\" rel=\"noopener\">Unity ml-agent homepage</a></li>\n</ul>\n<p><img src=\"https://www.dropbox.com/s/lapholj8r4nxmb1/Screenshot%202018-08-24%2013.41.31.png?dl=1\"></p>\n<p>현재 Unity ml-agent에서 기본으로 제공하는 환경은 다음과 같습니다. Unity ml-agent는 기존 Unity를 그대로 사용하면서 쉽게 강화학습 에이전트를 붙일 수 있도록 설계되어 있습니다. Unity ml-agent에서는 Walker 환경에서 저희가 살펴본 알고리즘 중에 가장 최신 알고리즘은 PPO를 적용해봤습니다. 이 포스트를 보시는 분들은 이 많은 다른 환경에 자유롭게 저희 코드를 적용할 수 있습니다.</p>\n<ul>\n<li><a href=\"https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md\" target=\"_blank\" rel=\"noopener\">각 환경에 대한 설명</a></li>\n</ul>\n<p><img src=\"https://www.dropbox.com/s/lrbodw5dypxowmw/Screenshot%202018-08-24%2014.06.14.png?dl=1\"></p>\n<p>Unity ml-agent를 이용해서 강화학습을 하기 위해서는 다음과 같이 진행됩니다. 단계별로 설명하겠습니다. </p>\n<ul>\n<li>Unity에서 환경 만들기</li>\n<li>Python에서 unity 환경 불러와서 테스트하기</li>\n<li>기존에 하던대로 학습하기</li>\n</ul>\n<p><br></p>\n<h3 id=\"2-1-Walker-환경-만들기\"><a href=\"#2-1-Walker-환경-만들기\" class=\"headerlink\" title=\"2.1 Walker 환경 만들기\"></a>2.1 Walker 환경 만들기</h3><p>강화학습을 하는 많은 분들이 Unity를 한 번도 다뤄보지 않은 경우가 많습니다. 저도 그런 경우라서 어떻게 환경을 만들어야할지 처음에는 감이 잡히지 않았습니다. 하지만 Unity ml-agent에서는 상당히 자세한 guide를 제공합니다. 다음은 Unity ml-agent의 가장 기본적인 환경인 3DBall에 대한 tutorial입니다. 설치 guide도 제공하고 있으니 참고하시면 될 것 같습니다.</p>\n<ul>\n<li><a href=\"https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Basic-Guide.md\" target=\"_blank\" rel=\"noopener\">3DBall 예제 tutorial</a></li>\n<li><a href=\"https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md\" target=\"_blank\" rel=\"noopener\">Unity ml-agent 설치 guide</a></li>\n</ul>\n<p>Unity ml-agent에서 제공하는 3DBall tutorial을 참고해서 Walker 환경을 만들었습니다. Walker 환경을 만드는 과정을 간단히 말씀드리겠습니다. 다음 그림의 단계들을 동일하므로 따라하시면 됩니다. Unity를 열고 unity-environment로 들어가시면 됩니다.<br><img src=\"https://www.dropbox.com/s/fbdqg781w46a5mz/Screenshot%202018-08-24%2014.50.50.png?dl=1\"></p>\n<p>그러면 화면 하단에서 다음과 같은 것을 볼 수 있습니다. Assets/ML-Agents/Examples로 들어가보면 Walker가 있습니다. Scenes에서 Walker를 더블클릭하면 됩니다.<br><img src=\"https://www.dropbox.com/s/h349xml3faln0wy/Screenshot%202018-08-24%2014.52.14.png?dl=1\"></p>\n<p>더블클릭해서 나온 화면에서 오른쪽 상단의 파란색 화살표를 누르면 환경이 실행이 됩니다. 저희가 학습하고자 하는 agent는 바로 이녀석입니다. 왼쪽 리스트를 보면 WalkerPair가 11개가 있는 것을 볼 수 있습니다. Unity ml-agent 환경은 기본적으로 Multi-agent로 학습하도록 설정되어있습니다. 따라서 여러개의 Walker들이 화면에 보이는 것입니다.<br><img src=\"https://www.dropbox.com/s/cy8m5kqdmkhopjo/Screenshot%202018-08-24%2014.54.57.png?dl=1\"></p>\n<p>리스트 중에 Walker Academy를 클릭해서 그 하위에 있는 WalkerBrain을 더블클릭합니다. 그러면 화면 오른쪽에 다음과 같은 Brain 설정을 볼 수 있습니다. Brain은 쉽게 말해서 Agent라고 생각하면 됩니다. 이 Agent는 상태로 212차원의 vector가 주어지며 다 continuous한 값을 가집니다. 행동은 39개의 행동을 할 수 있으며 다 Continuous입니다. Mujoco에 비해서 상태나 행동의 차원이 상당히 높습니다. 여기서 중요한 것은 Brain Type입니다. Brain type은 internal, external, player, heuristic이 있습니다. player로 type을 설정하고 화면 상단의 play 버튼을 누르면 여러분이 agent를 움직일 수 있습니다. 하지만 Walker는 사람이 움직이는게 거의 불가능하므로 player 기능은 사용할 수 없습니다. 다른 환경에서는 사용해볼 수 있으니 재미로 한 번 플레이해보시면 좋습니다! </p>\n<center><img src=\"https://www.dropbox.com/s/uxfm162f1scbzo5/Screenshot%202018-08-24%2015.09.04.png?dl=1\" width=\"400px\"></center>\n\n<p>이번에는 WalkerPair에서 WalkerAgent를 더블클릭해보겠습니다. 이 설정을 보아 5000 step이 episode의 max step인 것을 볼 수 있습니다.</p>\n<center><img src=\"https://www.dropbox.com/s/r6gwemlczwic2ma/Screenshot%202018-08-24%2015.16.19.png?dl=1\" width=\"400px\"></center>\n\n<p>이제 상단 file menu에서 build setting에 들어갑니다. 환경을 build해서 python 코드에서 import하기 위해서입니다. 물론 unity 환경과 python 코드를 binding해주는 부분은 ml-agent 코드 안에 있습니다. Build 버튼을 누르면 환경이 build가 됩니다.</p>\n<center><img src=\"https://www.dropbox.com/s/4dtgoz1k8896vxs/Screenshot%202018-08-24%2015.19.07.png?dl=1\" width=\"500px\"></center>\n\n\n<p><br></p>\n<h3 id=\"2-2-Python에서-unity-환경-불러와서-테스트하기\"><a href=\"#2-2-Python에서-unity-환경-불러와서-테스트하기\" class=\"headerlink\" title=\"2.2 Python에서 unity 환경 불러와서 테스트하기\"></a>2.2 Python에서 unity 환경 불러와서 테스트하기</h3><p>환경을 build 했으면 build한 환경을 python에서 불러와서 random action으로 테스트해봅니다. 환경을 테스트하는 코드는 pg_travel repository에서 unity 폴더 밑에 있습니다. test_env.py라는 코드는 간단하게 다음과 같습니다. Build한 walker 환경은 env라는 폴더 밑에 넣어줍니다. unityagent를 import하는데 ml-agent를 git clone 해서 python 폴더 내에서 “python setup.py install”을 실행했다면 문제없이 import 됩니다. UnityEnvironment를 통해 env라는 환경을 선언할 수 있습니다. 이렇게 선언하고 나면 gym과 상당히 유사한 형태로 환경과 상호작용이 가능합니다. </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">from</span> unityagents <span class=\"keyword\">import</span> UnityEnvironment</span><br><span class=\"line\"><span class=\"keyword\">from</span> utils.utils <span class=\"keyword\">import</span> get_action</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">\"__main__\"</span>:</span><br><span class=\"line\">    env_name = <span class=\"string\">\"./env/walker_test\"</span></span><br><span class=\"line\">    train_mode = <span class=\"keyword\">False</span></span><br><span class=\"line\"></span><br><span class=\"line\">    env = UnityEnvironment(file_name=env_name)</span><br><span class=\"line\"></span><br><span class=\"line\">    default_brain = env.brain_names[<span class=\"number\">0</span>]</span><br><span class=\"line\">    brain = env.brains[default_brain]</span><br><span class=\"line\">    env_info = env.reset(train_mode=train_mode)[default_brain]</span><br><span class=\"line\"></span><br><span class=\"line\">    num_inputs = brain.vector_observation_space_size</span><br><span class=\"line\">    num_actions = brain.vector_action_space_size</span><br><span class=\"line\">    num_agent = env._n_agents[default_brain]</span><br><span class=\"line\"></span><br><span class=\"line\">    print(<span class=\"string\">'the size of input dimension is '</span>, num_inputs)</span><br><span class=\"line\">    print(<span class=\"string\">'the size of action dimension is '</span>, num_actions)</span><br><span class=\"line\">    print(<span class=\"string\">'the number of agents is '</span>, num_agent)</span><br><span class=\"line\">   </span><br><span class=\"line\">    score = <span class=\"number\">0</span></span><br><span class=\"line\">    episode = <span class=\"number\">0</span></span><br><span class=\"line\">    actions = [<span class=\"number\">0</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(num_actions)] * num_agent</span><br><span class=\"line\">    <span class=\"keyword\">for</span> iter <span class=\"keyword\">in</span> range(<span class=\"number\">1000</span>):</span><br><span class=\"line\">        env_info = env.step(actions)[default_brain]</span><br><span class=\"line\">        rewards = env_info.rewards</span><br><span class=\"line\">        dones = env_info.local_done</span><br><span class=\"line\">        score += rewards[<span class=\"number\">0</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> dones[<span class=\"number\">0</span>]:</span><br><span class=\"line\">            episode += <span class=\"number\">1</span></span><br><span class=\"line\">            score = <span class=\"number\">0</span></span><br><span class=\"line\">            print(<span class=\"string\">'&#123;&#125;th episode : mean score of 1st agent is &#123;:.2f&#125;'</span>.format(</span><br><span class=\"line\">                episode, score))</span><br></pre></td></tr></table></figure>\n<p>위 코드를 실행하면 다음과 같이 실행창에 뜹니다. External brain인 것을 알 수 있고 default_brain은 brain 중에 하나만 가져왔기 때문에 number of brain은 1이라고 출력합니다. input dimension은 212이고 action dimension은 39이고 agent 수는 11인 것으로봐서 제대로 환경이 불러와진 것을 확인할 수 있습니다.<br><img src=\"https://www.dropbox.com/s/cioa9h7qu25vonz/Screenshot%202018-08-24%2015.47.43.png?dl=1\"></p>\n<p>이 환경에서 행동하려면 agent 숫자만큼 행동을 줘야합니다. 모두 0로 행동을 주고 실행하면 다음과 같이 뒤로 넘어지는 행동을 반복합니다. env.step(actions)[default_brain]으로 env_info를 받아오면 거기서부터 reward와 done, next_state를 받아올 수 있습니다. 이제 학습하기만 하면 됩니다.<br><img src=\"https://www.dropbox.com/s/8qrmxoski6p4n07/Screenshot%202018-08-24%2016.00.21.png?dl=1\"></p>\n<p><br></p>\n<h3 id=\"2-3-Walker-학습하기\"><a href=\"#2-3-Walker-학습하기\" class=\"headerlink\" title=\"2.3 Walker 학습하기\"></a>2.3 Walker 학습하기</h3><p>기존에 Mujoco에 적용했던 PPO 코드를 그대로 Walker에 적용하니 잘 학습이 안되었습니다. 다음 사진이 저희가 중간 해커톤으로 모여서 이 상황을 공유할 때의 사진입니다.<br><img src=\"https://i.imgur.com/1aR2Z77.png\" width=\"500px\"></p>\n<p>Unity ml-agent에서는 PPO를 기본 agent로 제공합니다. 학습 코드도 제공하기 때문에 mujoco에 적용했던 코드와의 차이점을 분석할 수 있었습니다. mujoco 코드와 ml-agent baseline 코드의 차이점은 다음과 같습니다. </p>\n<ul>\n<li>agent 여러개를 이용, 별개의 memory에 저장한 후에 gradient를 합침</li>\n<li>GAE 및 time horizon 등 hyper parameter가 다름</li>\n<li>Actor와 Critic의 layer가 1층 더 두꺼우며 hidden layer 자체의 사이즈도 더 큼</li>\n<li>hidden layer의 activation function이 tanh가 아닌 swish</li>\n</ul>\n<p>ml-agent baseline 코드리뷰할 때 작성했던 마인드맵은 다음과 같습니다.<br><img src=\"https://i.imgur.com/YeaEntG.png\"></p>\n<p>크게는 두 가지를 개선해서 성능이 많이 향상했습니다.</p>\n<ol>\n<li>Network 수정</li>\n<li>multi-agent를 활용해서 학습</li>\n</ol>\n<p>Network 코드는 다음과 같습니다. Hidden Layer를 하나 더 늘렸으며 swish activation function을 사용할 수 있도록 변경했습니다. 사실 swish라는 activation function은 처음 들어보는 생소한 함수였습니다. 하지만 ml-agent baseline에서 사용했다는 사실과 구현이 상당히 간단하다는 점에서 저희 코드에 적용했습니다. 단순히 x * sigmoid(x) 를 하면 됩니다. swish는 별거 아닌 것 같지만 상당한 성능 개선을 가져다줬습니다. 사실 ReLU나 ELU 등 여러 다른 activation function을 적용해서 비교해보는게 best긴 하지만 시간 관계상 그렇게까지 테스트해보지는 못했습니다. 기존에 TRPO나 PPO는 왜 tanh를 사용했었는지도 의문인 점입니다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Actor</span><span class=\"params\">(nn.Module)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, num_inputs, num_outputs, args)</span>:</span></span><br><span class=\"line\">        self.args = args</span><br><span class=\"line\">        self.num_inputs = num_inputs</span><br><span class=\"line\">        self.num_outputs = num_outputs</span><br><span class=\"line\">        super(Actor, self).__init__()</span><br><span class=\"line\">        self.fc1 = nn.Linear(num_inputs, args.hidden_size)</span><br><span class=\"line\">        self.fc2 = nn.Linear(args.hidden_size, args.hidden_size)</span><br><span class=\"line\">        self.fc3 = nn.Linear(args.hidden_size, args.hidden_size)</span><br><span class=\"line\">        self.fc4 = nn.Linear(args.hidden_size, num_outputs)</span><br><span class=\"line\"></span><br><span class=\"line\">        self.fc4.weight.data.mul_(<span class=\"number\">0.1</span>)</span><br><span class=\"line\">        self.fc4.bias.data.mul_(<span class=\"number\">0.0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.args.activation == <span class=\"string\">'tanh'</span>:</span><br><span class=\"line\">            x = F.tanh(self.fc1(x))</span><br><span class=\"line\">            x = F.tanh(self.fc2(x))</span><br><span class=\"line\">            x = F.tanh(self.fc3(x))</span><br><span class=\"line\">            mu = self.fc4(x)</span><br><span class=\"line\">        <span class=\"keyword\">elif</span> self.args.activation == <span class=\"string\">'swish'</span>:</span><br><span class=\"line\">            x = self.fc1(x)</span><br><span class=\"line\">            x = x * F.sigmoid(x)</span><br><span class=\"line\">            x = self.fc2(x)</span><br><span class=\"line\">            x = x * F.sigmoid(x)</span><br><span class=\"line\">            x = self.fc3(x)</span><br><span class=\"line\">            x = x * F.sigmoid(x)</span><br><span class=\"line\">            mu = self.fc4(x)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">raise</span> ValueError</span><br><span class=\"line\"></span><br><span class=\"line\">        logstd = torch.zeros_like(mu)</span><br><span class=\"line\">        std = torch.exp(logstd)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> mu, std, logstd</span><br></pre></td></tr></table></figure>\n<p>swish와 tanh를 사용한 학습을 비교한 그래프입니다. 하늘색 그래프가 swish를 사용한 결과, 파란색이 tanh를 사용한 결과입니다. score는 episode 마다의 reward의 합입니다.</p>\n<center><img src=\"https://www.dropbox.com/s/3d07c1kql4h5oqk/Screenshot%202018-08-24%2016.33.45.png?dl=1\" width=\"350px\"></center>\n\n<p>이제 multi-agent로 학습하도록 변경하면 됩니다. PPO의 경우 memory에 time horizon 동안의 sample을 시간순서대로 저장하고 GAE를 구한 이후에 minibatch로 추출해서 학습합니다. 따라서 여러개의 agent로 학습하기 위해서는 memory를 따로 만들어서 각각의 GAE를 구해서 학습해야합니다. Unity에서는 Mujoco에서 했던 것처럼 deque로 memory를 만들지 않고 따로 named tuple로 구현한 memory class를 import 해서 사용했습니다. utils 폴더 밑에 memory.py 코드에 구현되어있으며 코드는 <a href=\"https://github.com/pytorch/tutorials/blob/master/Reinforcement%20(Q-)Learning%20with%20PyTorch.ipynb\" target=\"_blank\" rel=\"noopener\">https://github.com/pytorch/tutorials/blob/master/Reinforcement%20(Q-)Learning%20with%20PyTorch.ipynb</a><br>에서 가져왔습니다. </p>\n<p>state, action, reward, mask를 저장하는데 불러올 때 각각을 따로 불러올 수 있기 때문에 비효율적 시간을 많이 줄여줍니다.<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Transition = namedtuple(<span class=\"string\">'Transition'</span>, (<span class=\"string\">'state'</span>, <span class=\"string\">'action'</span>, <span class=\"string\">'reward'</span>, <span class=\"string\">'mask'</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Memory</span><span class=\"params\">(object)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        self.memory = []</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">push</span><span class=\"params\">(self, state, action, reward, mask)</span>:</span></span><br><span class=\"line\">        <span class=\"string\">\"\"\"Saves a transition.\"\"\"</span></span><br><span class=\"line\">        self.memory.append(Transition(state, action, reward, mask))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sample</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> Transition(*zip(*self.memory))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__len__</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> len(self.memory)</span><br></pre></td></tr></table></figure></p>\n<p>main.py 에서는 이 memory를 agent의 개수만큼 생성합니다. </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">memory = [Memory() <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> range(num_agent)]</span><br></pre></td></tr></table></figure>\n<p>sample을 저장할 때도 agent마다 따로 따로 저장합니다. </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(num_agent):</span><br><span class=\"line\">    memory[i].push(states[i], actions[i], rewards[i], masks[i])</span><br></pre></td></tr></table></figure>\n<p>time horizon이 끝나면 모은 sample 들을 가지고 학습하기 위한 값으로 만드는 과정을 진행합니다. 각각의 memory를 가지고 GAE와 old_policy, old_value 등을 계산해서 하나의 batch로 합칩니다. 그렇게 train_model 메소드에 전달하면 기존과 동일하게 agent를 업데이트합니다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sts, ats, returns, advants, old_policy, old_value = [], [], [], [], [], []</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(num_agent):</span><br><span class=\"line\">    batch = memory[i].sample()</span><br><span class=\"line\">    st, at, rt, adv, old_p, old_v = process_memory(actor, critic, batch, args)</span><br><span class=\"line\">    sts.append(st)</span><br><span class=\"line\">    ats.append(at)</span><br><span class=\"line\">    returns.append(rt)</span><br><span class=\"line\">    advants.append(adv)</span><br><span class=\"line\">    old_policy.append(old_p)</span><br><span class=\"line\">    old_value.append(old_v)</span><br><span class=\"line\"></span><br><span class=\"line\">sts = torch.cat(sts)</span><br><span class=\"line\">ats = torch.cat(ats)</span><br><span class=\"line\">returns = torch.cat(returns)</span><br><span class=\"line\">advants = torch.cat(advants)</span><br><span class=\"line\">old_policy = torch.cat(old_policy)</span><br><span class=\"line\">old_value = torch.cat(old_value)</span><br><span class=\"line\"></span><br><span class=\"line\">train_model(actor, critic, actor_optim, critic_optim, sts, ats, returns, advants,</span><br><span class=\"line\">            old_policy, old_value, args)</span><br></pre></td></tr></table></figure>\n<p>이렇게 학습한 에이전트는 다음과 같이 걷습니다. 이렇게 walker를 학습시키고 나니 어떻게 하면 사람처럼 자연스럽게 걷는 것을 agent 스스로 학습할 수 있을까라는 고민을 하게 되었습니다.</p>\n<center><img src=\"https://www.dropbox.com/s/fyz1kn5v92l3rrk/plane-595.gif?dl=1\"></center>\n\n<p>Unity ml-agent에서 제공하는 pretrained된 모델을 다음과 같이 걷습니다. 저희가 학습한 agent와 상당히 다르게 걷는데 왜 그런 차이가 나는지도 분석하고 싶습니다. </p>\n<center><img src=\"https://www.dropbox.com/s/xwz766g7c4eiaia/plane-unity.gif?dl=1\"></center>\n\n\n<p><br></p>\n<h2 id=\"3-Unity-Curved-Surface-제작-및-학습기\"><a href=\"#3-Unity-Curved-Surface-제작-및-학습기\" class=\"headerlink\" title=\"3. Unity Curved Surface 제작 및 학습기\"></a>3. Unity Curved Surface 제작 및 학습기</h2><p>Unity ml-agent에서 제공하는 기본 Walker 환경에서 학습하고 나니 바닥을 조금 울퉁불퉁하게 혹은 경사가 진 곳에서 걷는 것을 학습해보고 싶다라는 생각이 들었습니다. 따라서 간단하게 걷는 배경을 다르게 하는 시도를 해봤습니다. </p>\n<p><br></p>\n<h3 id=\"3-1-Curved-Surface-만들기\"><a href=\"#3-1-Curved-Surface-만들기\" class=\"headerlink\" title=\"3.1 Curved Surface 만들기\"></a>3.1 Curved Surface 만들기</h3><p>Agent가 걸어갈 배경을 처음부터 만드는 것보다 구할 수 있다면 만들어진 배경을 구하기로 했습니다. Unity를 무료라는 점에서 선택했듯이 배경을 무료로 구할 수 있는 방법을 선택했습니다. </p>\n<center><img src=\"https://www.dropbox.com/s/e0tsp3e3c9uq2zh/Screenshot%202018-08-23%2000.19.14.png?dl=1\"></center>\n\n<p>무료로 공개되어있는 Unity 배경 중에서 Curved Ground 라는 것을 가져와서 작업하기로 했습니다. 이 환경 같은 경우 spline을 그리듯이 중간의 점을 이동시키면서 사용자가 곡면을 수정할 수 있습니다.</p>\n<center><img src=\"https://www.dropbox.com/s/3ppmxotrf6qhzaf/Screenshot%202018-08-23%2000.20.25.png?dl=1\"></center>\n\n<p>간단하게 곡면을 만들어서 공을 굴려보면 다음과 같이 잘 굴러갑니다. </p>\n<center><img src=\"https://www.dropbox.com/s/2e8yqvqj1a4th27/slope_walker_ball.gif?dl=1\"></center>\n\n<p>여러 에이전트가 학습할 수 있도록 오목한 경사면을 제작했습니다. 초반의 모습은 다음과 같았습니다.<br><img src=\"https://www.dropbox.com/s/m492xsfp4bolmz5/Screenshot%202018-08-23%2000.36.06.png?dl=1\"></p>\n<p>하지만 최종으로는 다음과 같은 곡면으로 사용했습니다. 위 사진의 배경과 아래 사진의 배경이 다른 점은 slope 길이, 내리막 경사, 오르막 경사입니다. Slope 길이의 경우 길이를 기존 plane 과 동일하게 했더니, 오르막 올라가는 부분이 학습이 잘 안 되었습니다. 따라서 길이를 줄였습니다. 내리막 경사의 경우 너무 경사지면 학습이 잘 안 되고, 너무 완만하니 내리막 티가 잘 안 나기 때문에 적절한 경사를 설정했습니다. 오르막 경사의 경우 내리막보다는 오르막이 더 어려울 것이라고 판단해서 오르막 경사를 낮게 설정했습니다.<br><img src=\"https://www.dropbox.com/s/idbov4wtd6jeqb2/Screenshot%202018-08-23%2000.36.54.png?dl=1\"></p>\n<p><br></p>\n<h3 id=\"3-2-Curved-Surface에서-학습하기\"><a href=\"#3-2-Curved-Surface에서-학습하기\" class=\"headerlink\" title=\"3.2 Curved Surface에서 학습하기\"></a>3.2 Curved Surface에서 학습하기</h3><p>위 환경으로 학습을 할 때, agent가 너무 초반에 빨리 쓰러지는 현상이 발생했습니다. 혹시 발의 각도가 문제일까 싶어서 발 각도를 변경해보았습니다. </p>\n<center><img src=\"https://www.dropbox.com/s/znvikbeoj7gku0u/Screenshot%202018-08-23%2000.38.22.png?dl=1\" width=\"400px\"></center>\n\n<p>하지만 역시 평지에서 걷는 것처럼 걷도록 학습이 안되었습니다. 이 환경에서 더 잘 학습하려면 더 여러가지를 시도해봐야할 것 같습니다. (그래도 걷는 게 기특합니다..)</p>\n<center><img src=\"https://www.dropbox.com/s/4fqpsdmnzvnvia0/curved-736.gif?dl=1\"></center>\n\n<p><img src=\"https://www.dropbox.com/s/t5ngr0io4xeex6y/curved-736-overview.gif?dl=1\"></p>\n<p><br></p>\n<h2 id=\"4-구현-후기\"><a href=\"#4-구현-후기\" class=\"headerlink\" title=\"4. 구현 후기\"></a>4. 구현 후기</h2><p>피지여행 구현팀은 총 4명으로 진행했습니다. 각 팀원의 후기를 적어보겠습니다.</p>\n<ul>\n<li>팀원 장수영: 사랑합니다. 행복합니다.</li>\n<li>팀원 공민서: 제가 핵심적인 기능을 구현하지는 못했지만 무조코 설치와 모델 테스트를 맡으면서 딥마인드나 openai의 영상으로만 보던 에이전트의 성장과정을 눈으로 지켜볼 수 있었습니다. 제대로 서있지도 못하던 hopper가 어느정도 훈련이 되고서는 넘어지려하다가도 추진력을 얻기위해 웅크렸다 뛰는 것을 관찰하는 것도 재미있고 육아일기를 보는 아버지의 마음을 조금이나마 이해할 수 있었습니다. 텐서보드를 넣는 걸 깜빡해 일일히 에피소드 별 스코어를 시각화 하면서 텐서보드의 소중함을 알았습니다. 유니티 코드리뷰를 하면서도 시스템 아키텍쳐 설계에 대해서도 배울 점이 있었던 것 같고 swish라는 활성화함수의 존재도 알았었고 curiosity도 알게되었고 역시 다른 사람의 코드를 읽는 것도 많은 공부가 된다고 되새기던 시간이었습니다. 물론 너무 크기가 방대해서 가독성은 많이 떨어졌습니다만 무조코보다 유니티가 훨씬 흥할거라고 생각했습니다. 마지막으로 누구 하나 열정적이지 않은 사람이 없이 치열한 고민을 함께 한 PG여행팀 분들, 저의 부족함과 상생의 기쁨을 알게해주셔서 정말 감사드립니다.</li>\n<li>팀원 양혁렬: 여러 에이전트가 함께하면 더 잘하는 걸 보면서 새삼 좋은 분들과 함께 할 수 있어서 행복했습니다</li>\n<li>팀원 이웅원: 저희가 직접 바닥부터 다 구현했던 것은 아니지만 구현을 해보면서 논문의 내용을 더 잘 이해할 수 있었습니다. 논문에 나와있지 않은 여러 노하우가 필요한 점들도 많았습니다. 역시 코드로 보고 성능 재현이 되어야 제대로 알고리즘을 이해하게 된다는 것을 다시 느낀 시간이었습니다. 또한 강화학습은 역시 환경세팅이 어렵다는 생각을 했습니다. 하지만 unity ml-agent를 사용해보면서 앞으로 강화학습 환경으로서 가능성이 상당히 크다는 생각을 했습니다. 또한 구현팀과 슬랙, 깃헙으로 협업하면서 온라인 협업에 대해서 더 배워가는 것 같습니다. 아직은 익숙하지 않지만 앞으로는 마치 바로 옆에서 같이 코딩하는 것 같이 될 거라고 생각합니다.</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"PG-Travel-implementation-story\"><a href=\"#PG-Travel-implementation-story\" class=\"headerlink\" title=\"PG Travel implementation story\"></a>PG Travel implementation story</h1><ul>\n<li>구현 코드 링크 : <a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">https://github.com/reinforcement-learning-kr/pg_travel</a></li>\n</ul>\n<p>피지여행 프로젝트에서는 다음 7개 논문을 살펴보았습니다. 각 논문에 대한 리뷰는 이전 글들에서 다루고 있습니다. </p>\n<p><a name=\"1\"></a></p>\n<ul>\n<li>[1] R. Sutton, et al., “Policy Gradient Methods for Reinforcement Learning with Function Approximation”, NIPS 2000.<br><a name=\"2\"></a></li>\n<li>[2] D. Silver, et al., “Deterministic Policy Gradient Algorithms”, ICML 2014.<br><a name=\"3\"></a></li>\n<li>[3] T. Lillicrap, et al., “Continuous Control with Deep Reinforcement Learning”, ICLR 2016.<br><a name=\"4\"></a></li>\n<li>[4] S. Kakade, “A Natural Policy Gradient”, NIPS 2002.<br><a name=\"5\"></a></li>\n<li>[5] J. Schulman, et al., “Trust Region Policy Optimization”, ICML 2015.<br><a name=\"6\"></a></li>\n<li>[6] J. Schulman, et al., “High-Dimensional Continuous Control using Generalized Advantage Estimation”, ICLR 2016.<br><a name=\"7\"></a></li>\n<li>[7] J. Schulman, et al., “Proximal Policy Optimization Algorithms”, arXiv, <a href=\"https://arxiv.org/pdf/1707.06347.pdf\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/pdf/1707.06347.pdf</a>.</li>\n</ul>\n<p>강화학습 알고리즘을 이해하는데 있어서 논문을 보고 이론적인 부분을 알아가는 것이 좋습니다. 하지만 실제 코드로 돌아가는 것은 논문만 보고는 알 수 없는 경우가 많습니다. 따라서 피지여행 프로젝트에서는 위 7개 논문 중에 DPG와 DDPG를 제외한 알고리즘을 구현해보았습니다. 구현한 알고리즘은 다음 4개입니다. 이 때, TRPO와 PPO 구현에는 GAE(General Advantage Estimator)가 함께 들어갑니다. </p>\n<ul>\n<li>Vanilla Policy Gradient [<a href=\"#1\">1</a>]</li>\n<li>TNPG(Truncated Natural Policy Gradient) [<a href=\"#4\">4</a>]</li>\n<li>TRPO(Trust Region Policy Optimization) [<a href=\"#5\">5</a>]</li>\n<li>PPO(Proximal Policy Optimization) [<a href=\"#7\">7</a>].</li>\n</ul>\n<p>바닥부터 저희가 구현한 것은 아니며 다음 코드들을 참고해서 구현하였습니다. Vanilla PG의 경우 RLCode의 깃헙을 참고하였습니다.</p>\n<ul>\n<li><a href=\"https://github.com/openai/baselines/tree/master/baselines/trpo_mpi\" target=\"_blank\" rel=\"noopener\">OpenAI Baseline</a></li>\n<li><a href=\"https://github.com/ikostrikov/pytorch-trpo\" target=\"_blank\" rel=\"noopener\">Pytorch implemetation of TRPO</a></li>\n<li><a href=\"https://github.com/rlcode/reinforcement-learning-kr/tree/master/2-cartpole/2-actor-critic\" target=\"_blank\" rel=\"noopener\">RLCode Actor-Critic</a></li>\n</ul>\n<p>GAE와 TRPO, PPO 논문에서는 Mujoco라는 물리 시뮬레이션을 학습 환경으로 사용합니다. 따라서 저희도 Mujoco로 처음 시작을 하였습니다. 하지만 Mujoco는 1달만 무료이고 그 이후부터 유료이며 확장성이 떨어집니다. Unity ml-agent는 기존 Unity를 그대로 사용하면서 쉽게 강화학습 에이전트를 붙일 수 있도록 설계되어 있습니다. Unity ml-agent에서는 저희가 살펴본 알고리즘 중에 가장 최신 알고리즘은 PPO를 적용해봤습니다. 기본적으로 제공하는 환경 이외에 저희가 customize 한 환경에서도 학습해봤습니다.  </p>\n<ul>\n<li>mujoco-py: <a href=\"https://github.com/openai/mujoco-py\" target=\"_blank\" rel=\"noopener\">https://github.com/openai/mujoco-py</a></li>\n<li>Unity ml-agent: <a href=\"https://github.com/Unity-Technologies/ml-agents\" target=\"_blank\" rel=\"noopener\">https://github.com/Unity-Technologies/ml-agents</a></li>\n</ul>\n<p>코드를 구현하고 환경에서 학습을 시키면서 여러가지 이슈들이 있었고 해결해내가는 과정이 있었습니다. 그 과정을 간단히 정리해서 공유하면 PG를 공부하는 분들께 도움일 될 것 같습니다. 저희가 구현한 순서대로 1. Mujoco 학습 2. Unity ml-agent 학습 3. Unity Curved Surface 로 이 포스트가 진행됩니다.</p>\n<p><br></p>\n<h2 id=\"1-Mujoco-학습\"><a href=\"#1-Mujoco-학습\" class=\"headerlink\" title=\"1. Mujoco 학습\"></a>1. Mujoco 학습</h2><p>일명 “Continuous control” 문제는 action이 discrete하지 않고 continuous한 경우를 말합니다. Mujoco는 continuous control에 강화학습을 적용한 논문들이 애용하는 시뮬레이터입니다. 저희가 리뷰한 논문 중에서도 TRPO, PPO, GAE에서 Mujoco를 사용합니다. 따라서 저희가 처음 피지여행 알고리즘을 적용한 환경으로 Mujoco를 선택했습니다. </p>\n<p>Mujoco에는 Ant, HalfCheetah, Hopper, Humanoid, HumanoidStandup, InvertedPendulum, Reacher, Swimmer, Walker2d 과 같은 환경이 있습니다. 그 중에서 Hopper에 맞춰서 학습이 되도록 코드를 구현하였습니다. Mujoco 설치와 관련된 내용은 Wiki에 있습니다.</p>\n<p><br></p>\n<h3 id=\"1-1-Hopper\"><a href=\"#1-1-Hopper\" class=\"headerlink\" title=\"1.1 Hopper\"></a>1.1 Hopper</h3><p>Hopper는 외다리로 뛰어가는 것을 학습하는 것이 목표입니다. Hopper는 다음과 같이 생겼습니다.<br><img src=\"https://www.dropbox.com/s/wjxrelxyp014j3g/Screenshot%202018-08-23%2000.55.54.png?dl=1\"></p>\n<p>환경을 이해하려면 환경의 상태와 행동, 보상 그리고 학습하고 싶은 목표를 알아야합니다. </p>\n<ul>\n<li>상태 : 관절의 위치, 각도, 각속도</li>\n<li>행동 : 관절의 가해지는 토크</li>\n<li>보상 : 앞으로 나아가는 속도</li>\n<li>목표 : 최대한 앞으로 많이 나아가기</li>\n</ul>\n<p>즉 에이전트는 time step마다 관절의 위치와 각도를 받아와서 그 상태에서 어떻게 움직여야 앞으로 나아갈 수 있는지를 학습해야 합니다. 행동은 discrete action이 아닌 continuous action으로 -1과 1사이의 값을 가집니다. 만약 행동이 -1이라면 해당 관절에 시계반대방향으로 토크를 주는 것이고 행동이 +1이라면 해당 관절에 시계방향으로 토크를 주는 것입니다. </p>\n<p>continuous action을 주는 방법은 네트워크(Actor)의 output layer에서 activation function으로 tanh와 같은 것을 사용해서 continuous한 값을 출력하는 것이 있습니다. 하지만 피지여행 코드 구현에서는 action을 gaussian distribution에서 sampling 하였습니다. 이렇게 하면 분산을 일정하게 유지하면서 지속적인 exploration을 할 수 있습니다. 간단하게 그림으로 보자면 다음과 같습니다.<br><img src=\"https://www.dropbox.com/s/94g01zdxyf5oxu1/Screenshot%202018-08-23%2001.20.21.png?dl=1\"></p>\n<p>네트워크 구조와 행동을 선택하는 부분은 다음과 같습니다. Hidden Layer의 activation function으로 tanh를 사용했으며(ReLU를 테스트해보지는 않았습니다. 기존 TRPO, PPO 구현들과 논문에서 tanh를 사용하기 때문에 저희도 사용했습니다. 뒤에 유니티 환경에서는 Swish라는 것을 사용합니다.) log std를 0으로 고정함으로서 일정한 폭을 가지는 분포를 만들어낼 수 있습니다. 이 분포로부터 action을 sampling 합니다.</p>\n<p><img src=\"https://www.dropbox.com/s/xfl9zxies0lmpm1/Screenshot%202018-08-23%2001.20.44.png?dl=1\"></p>\n<p><br></p>\n<h3 id=\"1-2-Vanilla-PG\"><a href=\"#1-2-Vanilla-PG\" class=\"headerlink\" title=\"1.2 Vanilla PG\"></a>1.2 Vanilla PG</h3><p>Vanilla PG는 Actor-Critic의 가장 간단한 형태입니다. Vanilla PG는 이후의 구현에 대한 baseline이 됩니다. 구현이 가장 간단하면서 학습이 안되는 것은 아닙니다. 따라서 코드 전체 구조를 잡는데 Vanilla PG를 짜는 것이 도움이 됩니다. 전반적인 코드 구조는 다음과 같습니다.</p>\n<ul>\n<li>iteration 마다 일정한 step 수만큼 환경에서 진행해서 샘플을 모은다</li>\n<li>모은 샘플로 Actor와 Critic을 학습한다</li>\n<li>반복한다</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">episodes = <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> iter <span class=\"keyword\">in</span> range(<span class=\"number\">15000</span>):</span><br><span class=\"line\">    actor.eval(), critic.eval()</span><br><span class=\"line\">    memory = deque()</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">while</span> steps &lt; <span class=\"number\">2048</span>:</span><br><span class=\"line\">        episodes += <span class=\"number\">1</span></span><br><span class=\"line\">        state = env.reset()</span><br><span class=\"line\">        state = running_state(state)</span><br><span class=\"line\">        score = <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> range(<span class=\"number\">10000</span>):</span><br><span class=\"line\">            mu, std, _ = actor(torch.Tensor(state).unsqueeze(<span class=\"number\">0</span>))</span><br><span class=\"line\">            action = get_action(mu, std)[<span class=\"number\">0</span>]</span><br><span class=\"line\">            next_state, reward, done, _ = env.step(action)</span><br><span class=\"line\">            next_state = running_state(next_state)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">if</span> done:</span><br><span class=\"line\">                mask = <span class=\"number\">0</span></span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                mask = <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">            memory.append([state, action, reward, mask])</span><br><span class=\"line\">            state = next_state</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">if</span> done:</span><br><span class=\"line\">                <span class=\"keyword\">break</span></span><br><span class=\"line\">                </span><br><span class=\"line\">    actor.train(), critic.train()</span><br><span class=\"line\">    train_model(actor, critic, memory, actor_optim, critic_optim)</span><br></pre></td></tr></table></figure>\n<p>memory에 sample을 저장할 때 sample은 state와 action, reward, mask(마지막 state일 경우 0 나머지 1)입니다. mask의 경우 뒤에서 return이나 advantage를 계산할 때 사용됩니다. 또 하나 염두에 두어야할 것은 running_state 입니다. running_state는 input으로 들어오는 state의 scale이 일정하지 않기 때문에 사용합니다. 즉 state의 각 dimension을 평균 0 분산 1로 standardization 하는 것입니다. 따라서 모델을 저장할 때 각 dimension 마다의 평균과 분산도 같이 저장해서 테스트할 때 불러와서 사용해야 합니다.</p>\n<p>Vanilla PG의 경우 학습 부분이 상당히 간단합니다. 다음 코드를 보시면 메모리에서 state, action, reward, mask를 꺼냅니다. reward와 mask를 통해 return을 구할 수 있고 이 return을 통해 actor를 업데이트 할 수 있습니다 (REINFORCE 알고리즘을 떠올리시면 됩니다). 여기서 critic이 하는 일은 없지만 뒤의 알고리즘들과 코드의 통일성을 위해 fake로 넣어놨습니다. Return은 평균을 빼고 분산으로 나눠서 standardize 합니다. </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">train_model</span><span class=\"params\">(actor, critic, memory, actor_optim, critic_optim)</span>:</span></span><br><span class=\"line\">    memory = np.array(memory)</span><br><span class=\"line\">    states = np.vstack(memory[:, <span class=\"number\">0</span>])</span><br><span class=\"line\">    actions = list(memory[:, <span class=\"number\">1</span>])</span><br><span class=\"line\">    rewards = list(memory[:, <span class=\"number\">2</span>])</span><br><span class=\"line\">    masks = list(memory[:, <span class=\"number\">3</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">    returns = get_returns(rewards, masks)</span><br><span class=\"line\">    train_critic(critic, states, returns, critic_optim)</span><br><span class=\"line\">    train_actor(actor, returns, states, actions, actor_optim)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> returns</span><br></pre></td></tr></table></figure>\n<p>이 코드로 Hopper 환경에서 학습한 그래프는 다음과 같습니다.<br><img src=\"https://www.dropbox.com/s/asoysfuk76zs1dk/Screenshot%202018-08-23%2001.30.58.png?dl=1\"></p>\n<p><br></p>\n<h3 id=\"1-3-TNPG\"><a href=\"#1-3-TNPG\" class=\"headerlink\" title=\"1.3 TNPG\"></a>1.3 TNPG</h3><p>NPG를 이용한 parameter update 식은 다음과 같습니다. </p>\n<p>$$\\bar{w}=F(\\theta)^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>NPG를 구현하려면 KL-divergence의 Hessian의 inverse를 구해야하는 문제가 생깁니다. 현재와 같이 Deep Neural Network를 쓰는 경우에 Hessian의 inverse를 직접적으로 구하는 것은 computationally inefficient 합니다. 따라서 직접 구하지 않고 Conjugate gradient 방법을 사용해서 Fisher Vector Product ($$F^{-1}g$$)를 구합니다. 이러한 알고리즘을 Truncated Natural Policy Gradient(TNPG)라고 부릅니다. </p>\n<p>TNPG에서 parameter update를 구하는 과정은 다음과 같습니다. </p>\n<ol>\n<li>Return 구하기</li>\n<li>Critic 학습하기</li>\n<li>logp * return –&gt; loss 구하기</li>\n<li>loss의 미분과 kl-divergence의 2차 미분을 통해 step direction 구하기</li>\n<li>구한 step direction으로 parameter update</li>\n</ol>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">train_model</span><span class=\"params\">(actor, critic, memory, actor_optim, critic_optim)</span>:</span></span><br><span class=\"line\">    memory = np.array(memory)</span><br><span class=\"line\">    states = np.vstack(memory[:, <span class=\"number\">0</span>])</span><br><span class=\"line\">    actions = list(memory[:, <span class=\"number\">1</span>])</span><br><span class=\"line\">    rewards = list(memory[:, <span class=\"number\">2</span>])</span><br><span class=\"line\">    masks = list(memory[:, <span class=\"number\">3</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># ----------------------------</span></span><br><span class=\"line\">    <span class=\"comment\"># step 1: get returns</span></span><br><span class=\"line\">    returns = get_returns(rewards, masks)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># ----------------------------</span></span><br><span class=\"line\">    <span class=\"comment\"># step 2: train critic several steps with respect to returns</span></span><br><span class=\"line\">    train_critic(critic, states, returns, critic_optim)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># ----------------------------</span></span><br><span class=\"line\">    <span class=\"comment\"># step 3: get gradient of loss and hessian of kl</span></span><br><span class=\"line\">    loss = get_loss(actor, returns, states, actions)</span><br><span class=\"line\">    loss_grad = torch.autograd.grad(loss, actor.parameters())</span><br><span class=\"line\">    loss_grad = flat_grad(loss_grad)</span><br><span class=\"line\">    step_dir = conjugate_gradient(actor, states, loss_grad.data, nsteps=<span class=\"number\">10</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># ----------------------------</span></span><br><span class=\"line\">    <span class=\"comment\"># step 4: get step direction and step size and update actor</span></span><br><span class=\"line\">    params = flat_params(actor)</span><br><span class=\"line\">    new_params = params + <span class=\"number\">0.5</span> * step_dir</span><br><span class=\"line\">    update_model(actor, new_params)</span><br></pre></td></tr></table></figure>\n<p>conjugate gradient 코드는 OpenAI baseline에서 가져왔습니다. 이 코드는 원래 John schulmann 개인 repository에 있는 그대로 사용하는 것입니다. nsteps 만큼 iterataion을 반복하며 결국 x를 구하는 것인데 이 x가 step direction 입니다. </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># from openai baseline code</span></span><br><span class=\"line\"><span class=\"comment\"># https://github.com/openai/baselines/blob/master/baselines/common/cg.py</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">conjugate_gradient</span><span class=\"params\">(actor, states, b, nsteps, residual_tol=<span class=\"number\">1e-10</span>)</span>:</span></span><br><span class=\"line\">    x = torch.zeros(b.size())</span><br><span class=\"line\">    r = b.clone()</span><br><span class=\"line\">    p = b.clone()</span><br><span class=\"line\">    rdotr = torch.dot(r, r)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(nsteps):</span><br><span class=\"line\">        _Avp = fisher_vector_product(actor, states, p)</span><br><span class=\"line\">        alpha = rdotr / torch.dot(p, _Avp)</span><br><span class=\"line\">        x += alpha * p</span><br><span class=\"line\">        r -= alpha * _Avp</span><br><span class=\"line\">        new_rdotr = torch.dot(r, r)</span><br><span class=\"line\">        betta = new_rdotr / rdotr</span><br><span class=\"line\">        p = r + betta * p</span><br><span class=\"line\">        rdotr = new_rdotr</span><br><span class=\"line\">        <span class=\"keyword\">if</span> rdotr &lt; residual_tol:</span><br><span class=\"line\">            <span class=\"keyword\">break</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure>\n<p>fisher_vector_product는 kl-divergence의 2차미분과 어떠한 vector의 곱인데 p는 처음에 gradient 값이었다가 점차 업데이트가 됩니다. kl-divergence의 2차 미분을 구하는 과정은 다음과 같습니다. 일단 kl-divergence를 현재 policy에 대해서 구한 다음에 actor parameter에 대해서 미분합니다. 이렇게 미분한 gradient를 일단 flat하게 핀 다음에 p라는 벡터와 곱해서 하나의 값으로 만듭니다. 그 값을 다시 actor의 parameter로 만듦으로서 따로 KL-divergence의 2차미분을 구하지않고 Fisher vector product를 구할 수 있습니다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">fisher_vector_product</span><span class=\"params\">(actor, states, p)</span>:</span></span><br><span class=\"line\">    p.detach()</span><br><span class=\"line\">    kl = kl_divergence(new_actor=actor, old_actor=actor, states=states)</span><br><span class=\"line\">    kl = kl.mean()</span><br><span class=\"line\">    kl_grad = torch.autograd.grad(kl, actor.parameters(), create_graph=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">    kl_grad = flat_grad(kl_grad)  <span class=\"comment\"># check kl_grad == 0</span></span><br><span class=\"line\"></span><br><span class=\"line\">    kl_grad_p = (kl_grad * p).sum()</span><br><span class=\"line\">    kl_hessian_p = torch.autograd.grad(kl_grad_p, actor.parameters())</span><br><span class=\"line\">    kl_hessian_p = flat_hessian(kl_hessian_p)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> kl_hessian_p + <span class=\"number\">0.1</span> * p</span><br></pre></td></tr></table></figure>\n<p>TNPG 학습 결과는 다음과 같습니다.<br><img src=\"https://www.dropbox.com/s/uc4c0s00qbs33nr/Screenshot%202018-08-23%2001.53.17.png?dl=1\"></p>\n<p><br></p>\n<h3 id=\"1-4-TRPO\"><a href=\"#1-4-TRPO\" class=\"headerlink\" title=\"1.4 TRPO\"></a>1.4 TRPO</h3><p>TRPO와 NPG가 다른 점은 surrogate loss 사용과 trust region 입니다. 하지만 실제로 구현해서 학습을 시켜본 결과 trust region을 넘어가서 back tracking line search를 하는 경우는 거의 없습니다. 따라서 주된 변화는 surrogate loss에 있다고 보셔도 됩니다. Surrogate loss에서 advantage function을 사용하는데 본 코드 구현에서는 GAE를 사용하였습니다. TRPO 업데이트 식은 다음과 같습니다. Q function 위치에 GAE가 들어갑니다.</p>\n<p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}Q_{\\theta_\\mathrm{old} }(s,a)\\right] \\\\<br>\\mathrm{s.t.\\ }&amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} }}\\left[D_\\mathrm{KL}\\left(\\pi_{\\theta_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta<br>\\end{align}<br>$$</p>\n<p>GAE를 구하는 코드는 다음과 같습니다. GAE는 td-error의 discounted summation이라고 볼 수 있습니다. 마지막에 advants를 standardization 하는 것은 return에서 하는 것과 같은 효과를 봅니다. 하지만 standardization을 안하고 실험을 해보지는 않았습니다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_gae</span><span class=\"params\">(rewards, masks, values)</span>:</span></span><br><span class=\"line\">    rewards = torch.Tensor(rewards)</span><br><span class=\"line\">    masks = torch.Tensor(masks)</span><br><span class=\"line\">    returns = torch.zeros_like(rewards)</span><br><span class=\"line\">    advants = torch.zeros_like(rewards)</span><br><span class=\"line\"></span><br><span class=\"line\">    running_returns = <span class=\"number\">0</span></span><br><span class=\"line\">    previous_value = <span class=\"number\">0</span></span><br><span class=\"line\">    running_advants = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> t <span class=\"keyword\">in</span> reversed(range(<span class=\"number\">0</span>, len(rewards))):</span><br><span class=\"line\">        running_returns = rewards[t] + hp.gamma * running_returns * masks[t]</span><br><span class=\"line\">        running_tderror = rewards[t] + hp.gamma * previous_value * masks[t] - \\</span><br><span class=\"line\">                    values.data[t]</span><br><span class=\"line\">        running_advants = running_tderror + hp.gamma * hp.lamda * \\</span><br><span class=\"line\">                          running_advants * masks[t]</span><br><span class=\"line\"></span><br><span class=\"line\">        returns[t] = running_returns</span><br><span class=\"line\">        previous_value = values.data[t]</span><br><span class=\"line\">        advants[t] = running_advants</span><br><span class=\"line\"></span><br><span class=\"line\">    advants = (advants - advants.mean()) / advants.std()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> returns, advants</span><br></pre></td></tr></table></figure>\n<p>Surrogate loss를 구하는 코드는 다음과 같습니다. Advantage function(GAE)를 구하고 나면 이전 policy와 현재 policy 사이의 ratio를 구해서 advantage function에 곱하면 됩니다. 이 때 사실 old policy와 new policy는 값은 같지만 old policy는 clone()이나 detach()를 사용해서 update가 안되게 만들어줍니다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">surrogate_loss</span><span class=\"params\">(actor, advants, states, old_policy, actions)</span>:</span></span><br><span class=\"line\">    mu, std, logstd = actor(torch.Tensor(states))</span><br><span class=\"line\">    new_policy = log_density(torch.Tensor(actions), mu, std, logstd)</span><br><span class=\"line\">    advants = advants.unsqueeze(<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    surrogate = advants * torch.exp(new_policy - old_policy)</span><br><span class=\"line\">    surrogate = surrogate.mean()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> surrogate</span><br></pre></td></tr></table></figure>\n<p>Actor의 step direction을 구하는 것은 TNPG와 동일합니다. TNPG에서는 step direction으로 바로 업데이트 했지만 TRPO는 다음과 같은 작업을 해줍니다. Full step을 구하는 과정이라고 볼 수 있습니다. </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># ----------------------------</span></span><br><span class=\"line\"><span class=\"comment\"># step 4: get step direction and step size and full step</span></span><br><span class=\"line\">params = flat_params(actor)</span><br><span class=\"line\">shs = <span class=\"number\">0.5</span> * (step_dir * fisher_vector_product(actor, states, step_dir)</span><br><span class=\"line\">             ).sum(<span class=\"number\">0</span>, keepdim=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">step_size = <span class=\"number\">1</span> / torch.sqrt(shs / hp.max_kl)[<span class=\"number\">0</span>]</span><br><span class=\"line\">full_step = step_size * step_dir</span><br></pre></td></tr></table></figure>\n<p>이렇게 full step을 구하고 나면 Trust region optimization 단계에 들어갑니다. expected improvement는 구한 step 만큼 parameter space에서 움직였을 때 예상되는 performance 변화입니다. 이 값은 kl-divergence와 함께 trust region 안에 있는지 밖에 있는지 판단하는 근거가 됩니다. expected improve는 출발점에서의 gradient * full step으로 구합니다. 그리고 10번을 돌아가며 Back-tracking line search를 실시합니다. 처음에는 full step 만큼 가본 다음에 kl-divergence와 emprovement를 통해 trust region 안이면 루프 탈출, 밖이면 full step을 반만큼 쪼개서 다시 이 과정을 반복합니다.  </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># ----------------------------</span></span><br><span class=\"line\"><span class=\"comment\"># step 5: do backtracking line search for n times</span></span><br><span class=\"line\">old_actor = Actor(actor.num_inputs, actor.num_outputs)</span><br><span class=\"line\">update_model(old_actor, params)</span><br><span class=\"line\">expected_improve = (loss_grad * full_step).sum(<span class=\"number\">0</span>, keepdim=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">expected_improve = expected_improve.data.numpy()</span><br><span class=\"line\"></span><br><span class=\"line\">flag = <span class=\"keyword\">False</span></span><br><span class=\"line\">fraction = <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">10</span>):</span><br><span class=\"line\">    new_params = params + fraction * full_step</span><br><span class=\"line\">    update_model(actor, new_params)</span><br><span class=\"line\">    new_loss = surrogate_loss(actor, advants, states, old_policy.detach(),</span><br><span class=\"line\">                              actions)</span><br><span class=\"line\">    new_loss = new_loss.data.numpy()</span><br><span class=\"line\">    loss_improve = new_loss - loss</span><br><span class=\"line\">    expected_improve *= fraction</span><br><span class=\"line\">    kl = kl_divergence(new_actor=actor, old_actor=old_actor, states=states)</span><br><span class=\"line\">    kl = kl.mean()</span><br><span class=\"line\"></span><br><span class=\"line\">    print(<span class=\"string\">'kl: &#123;:.4f&#125;  loss improve: &#123;:.4f&#125;  expected improve: &#123;:.4f&#125;  '</span></span><br><span class=\"line\">          <span class=\"string\">'number of line search: &#123;&#125;'</span></span><br><span class=\"line\">          .format(kl.data.numpy(), loss_improve, expected_improve[<span class=\"number\">0</span>], i))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># see https: // en.wikipedia.org / wiki / Backtracking_line_search</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> kl &lt; hp.max_kl <span class=\"keyword\">and</span> (loss_improve / expected_improve) &gt; <span class=\"number\">0.5</span>:</span><br><span class=\"line\">        flag = <span class=\"keyword\">True</span></span><br><span class=\"line\">        <span class=\"keyword\">break</span></span><br><span class=\"line\"></span><br><span class=\"line\">    fraction *= <span class=\"number\">0.5</span></span><br></pre></td></tr></table></figure>\n<p>Critic의 학습은 단순히 value function과 return의 MSE error를 계산해서 loss로 잡고 loss를 최소화하도록 학습합니다. TRPO 학습 결과는 다음과 같습니다.<br><img src=\"https://www.dropbox.com/s/rc9hxsx1kvokcrv/Screenshot%202018-08-23%2013.36.51.png?dl=1\"></p>\n<p><br></p>\n<h3 id=\"1-5-PPO\"><a href=\"#1-5-PPO\" class=\"headerlink\" title=\"1.5 PPO\"></a>1.5 PPO</h3><p>PPO의 장점을 꼽으라면 GPU 사용하기 좋고 sample efficiency가 늘어난다는 것입니다. TNPG와 TRPO의 경우 한 번 모은 sample은 모델을 단 한 번 업데이트하는데 사용하지만 PPO의 경우 몇 mini-batch로 epoch를 돌리기 때문입니다. GAE를 사용한다는 것은 같고 Conjugate gradient나 Fisher vector product나 back tracking line search가 다 빠집니다. 대신 loss function clip으로 monotonic improvement를 보장하게 학습합니다. 따라서 코드가 상당히 간단해집니다. </p>\n<p>다음 코드 부분이 PPO의 전체라고 봐도 무방합니다. PPO는 다음과 같은 순서로 학습합니다. </p>\n<ul>\n<li>batch를 random suffling하고 mini batch를 추출</li>\n<li>value function 구하기</li>\n<li>critic loss 구하기 (clip을 사용해도 되고 TRPO와 같이 그냥 학습시켜도 됌)</li>\n<li>surrogate loss 구하기</li>\n<li>surrogate loss clip해서 actor loss 만들기</li>\n<li>actor와 critic 업데이트</li>\n</ul>\n<p>Actor의 loss를 구하는 것은 다음 식의 값을 구하는 것입니다. 이 식을 구하려면 ratio에 한 번 클립하고 loss 값을 한 번 min을 취하면 됩니다.</p>\n<p>$$L^{CLIP}(\\theta) = \\hat{E}_t [min(r_t(\\theta) \\, \\hat{A}_t,  clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\, \\hat{A}_t)]$$</p>\n<p>이 코드 구현에서는 actor와 critic을 따로 모델로 만들어서 따로 따로 업데이트를 하지만 하나로 만든다면 loss로 한 번만 업데이트하면 됩니다. 또한 entropy loss를 최종 loss에 더해서 regularization 효과를 볼 수도 있습니다. Critic loss에 clip 해주는 것은 OpenAI baseline의 ppo2 코드를 참조하였습니다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># step 2: get value loss and actor loss and update actor &amp; critic</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> range(<span class=\"number\">10</span>):</span><br><span class=\"line\">    np.random.shuffle(arr)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(n // hp.batch_size):</span><br><span class=\"line\">        batch_index = arr[hp.batch_size * i: hp.batch_size * (i + <span class=\"number\">1</span>)]</span><br><span class=\"line\">        batch_index = torch.LongTensor(batch_index)</span><br><span class=\"line\">        inputs = torch.Tensor(states)[batch_index]</span><br><span class=\"line\">        returns_samples = returns.unsqueeze(<span class=\"number\">1</span>)[batch_index]</span><br><span class=\"line\">        advants_samples = advants.unsqueeze(<span class=\"number\">1</span>)[batch_index]</span><br><span class=\"line\">        actions_samples = torch.Tensor(actions)[batch_index]</span><br><span class=\"line\">        oldvalue_samples = old_values[batch_index].detach()</span><br><span class=\"line\"></span><br><span class=\"line\">        loss, ratio = surrogate_loss(actor, advants_samples, inputs,</span><br><span class=\"line\">                                     old_policy.detach(), actions_samples,</span><br><span class=\"line\">                                     batch_index)</span><br><span class=\"line\"></span><br><span class=\"line\">        values = critic(inputs)</span><br><span class=\"line\">        clipped_values = oldvalue_samples + \\</span><br><span class=\"line\">                         torch.clamp(values - oldvalue_samples,</span><br><span class=\"line\">                                     -hp.clip_param,</span><br><span class=\"line\">                                     hp.clip_param)</span><br><span class=\"line\">        critic_loss1 = criterion(clipped_values, returns_samples)</span><br><span class=\"line\">        critic_loss2 = criterion(values, returns_samples)</span><br><span class=\"line\">        critic_loss = torch.max(critic_loss1, critic_loss2).mean()</span><br><span class=\"line\"></span><br><span class=\"line\">        clipped_ratio = torch.clamp(ratio,</span><br><span class=\"line\">                                    <span class=\"number\">1.0</span> - hp.clip_param,</span><br><span class=\"line\">                                    <span class=\"number\">1.0</span> + hp.clip_param)</span><br><span class=\"line\">        clipped_loss = clipped_ratio * advants_samples</span><br><span class=\"line\">        actor_loss = -torch.min(loss, clipped_loss).mean()</span><br><span class=\"line\"></span><br><span class=\"line\">        loss = actor_loss + <span class=\"number\">0.5</span> * critic_loss</span><br><span class=\"line\"></span><br><span class=\"line\">        critic_optim.zero_grad()</span><br><span class=\"line\">        loss.backward(retain_graph=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">        critic_optim.step()</span><br><span class=\"line\"></span><br><span class=\"line\">        actor_optim.zero_grad()</span><br><span class=\"line\">        loss.backward()</span><br><span class=\"line\">        actor_optim.step()</span><br></pre></td></tr></table></figure>\n<p>PPO의 학습 결과는 다음과 같습니다.<br><img src=\"https://www.dropbox.com/s/rkxa836ap931kbd/Screenshot%202018-08-23%2013.50.57.png?dl=1\"></p>\n<p><br></p>\n<h2 id=\"2-Unity-ml-agent-학습\"><a href=\"#2-Unity-ml-agent-학습\" class=\"headerlink\" title=\"2. Unity ml-agent 학습\"></a>2. Unity ml-agent 학습</h2><p>Mujoco Hopper(half-cheetah와 같은 것도)에 Vanilla PG, TNPG, TRPO, PPO를 구현해서 적용했습니다. Mujoco의 경우 이미 Hyper parameter와 같은 정보들이 논문이나 블로그에 있기 때문에 상대적으로 continuous control로 시작하기에는 좋습니다. 맨 처음에 말했듯이 Mujoco는 1달만 무료이고 그 이후부터 유료이며 확장성이 떨어집니다. 좀 더 general한 agent를 학습시키기에 좋은 환경이 필요합니다. 따라서 Unity ml-agent를 살펴봤습니다. Repository는 다음과 같습니다. </p>\n<ul>\n<li><a href=\"https://github.com/Unity-Technologies/ml-agents\" target=\"_blank\" rel=\"noopener\">Unity ml-agent repository</a></li>\n<li><a href=\"https://unity3d.com/machine-learning/\" target=\"_blank\" rel=\"noopener\">Unity ml-agent homepage</a></li>\n</ul>\n<p><img src=\"https://www.dropbox.com/s/lapholj8r4nxmb1/Screenshot%202018-08-24%2013.41.31.png?dl=1\"></p>\n<p>현재 Unity ml-agent에서 기본으로 제공하는 환경은 다음과 같습니다. Unity ml-agent는 기존 Unity를 그대로 사용하면서 쉽게 강화학습 에이전트를 붙일 수 있도록 설계되어 있습니다. Unity ml-agent에서는 Walker 환경에서 저희가 살펴본 알고리즘 중에 가장 최신 알고리즘은 PPO를 적용해봤습니다. 이 포스트를 보시는 분들은 이 많은 다른 환경에 자유롭게 저희 코드를 적용할 수 있습니다.</p>\n<ul>\n<li><a href=\"https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md\" target=\"_blank\" rel=\"noopener\">각 환경에 대한 설명</a></li>\n</ul>\n<p><img src=\"https://www.dropbox.com/s/lrbodw5dypxowmw/Screenshot%202018-08-24%2014.06.14.png?dl=1\"></p>\n<p>Unity ml-agent를 이용해서 강화학습을 하기 위해서는 다음과 같이 진행됩니다. 단계별로 설명하겠습니다. </p>\n<ul>\n<li>Unity에서 환경 만들기</li>\n<li>Python에서 unity 환경 불러와서 테스트하기</li>\n<li>기존에 하던대로 학습하기</li>\n</ul>\n<p><br></p>\n<h3 id=\"2-1-Walker-환경-만들기\"><a href=\"#2-1-Walker-환경-만들기\" class=\"headerlink\" title=\"2.1 Walker 환경 만들기\"></a>2.1 Walker 환경 만들기</h3><p>강화학습을 하는 많은 분들이 Unity를 한 번도 다뤄보지 않은 경우가 많습니다. 저도 그런 경우라서 어떻게 환경을 만들어야할지 처음에는 감이 잡히지 않았습니다. 하지만 Unity ml-agent에서는 상당히 자세한 guide를 제공합니다. 다음은 Unity ml-agent의 가장 기본적인 환경인 3DBall에 대한 tutorial입니다. 설치 guide도 제공하고 있으니 참고하시면 될 것 같습니다.</p>\n<ul>\n<li><a href=\"https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Basic-Guide.md\" target=\"_blank\" rel=\"noopener\">3DBall 예제 tutorial</a></li>\n<li><a href=\"https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md\" target=\"_blank\" rel=\"noopener\">Unity ml-agent 설치 guide</a></li>\n</ul>\n<p>Unity ml-agent에서 제공하는 3DBall tutorial을 참고해서 Walker 환경을 만들었습니다. Walker 환경을 만드는 과정을 간단히 말씀드리겠습니다. 다음 그림의 단계들을 동일하므로 따라하시면 됩니다. Unity를 열고 unity-environment로 들어가시면 됩니다.<br><img src=\"https://www.dropbox.com/s/fbdqg781w46a5mz/Screenshot%202018-08-24%2014.50.50.png?dl=1\"></p>\n<p>그러면 화면 하단에서 다음과 같은 것을 볼 수 있습니다. Assets/ML-Agents/Examples로 들어가보면 Walker가 있습니다. Scenes에서 Walker를 더블클릭하면 됩니다.<br><img src=\"https://www.dropbox.com/s/h349xml3faln0wy/Screenshot%202018-08-24%2014.52.14.png?dl=1\"></p>\n<p>더블클릭해서 나온 화면에서 오른쪽 상단의 파란색 화살표를 누르면 환경이 실행이 됩니다. 저희가 학습하고자 하는 agent는 바로 이녀석입니다. 왼쪽 리스트를 보면 WalkerPair가 11개가 있는 것을 볼 수 있습니다. Unity ml-agent 환경은 기본적으로 Multi-agent로 학습하도록 설정되어있습니다. 따라서 여러개의 Walker들이 화면에 보이는 것입니다.<br><img src=\"https://www.dropbox.com/s/cy8m5kqdmkhopjo/Screenshot%202018-08-24%2014.54.57.png?dl=1\"></p>\n<p>리스트 중에 Walker Academy를 클릭해서 그 하위에 있는 WalkerBrain을 더블클릭합니다. 그러면 화면 오른쪽에 다음과 같은 Brain 설정을 볼 수 있습니다. Brain은 쉽게 말해서 Agent라고 생각하면 됩니다. 이 Agent는 상태로 212차원의 vector가 주어지며 다 continuous한 값을 가집니다. 행동은 39개의 행동을 할 수 있으며 다 Continuous입니다. Mujoco에 비해서 상태나 행동의 차원이 상당히 높습니다. 여기서 중요한 것은 Brain Type입니다. Brain type은 internal, external, player, heuristic이 있습니다. player로 type을 설정하고 화면 상단의 play 버튼을 누르면 여러분이 agent를 움직일 수 있습니다. 하지만 Walker는 사람이 움직이는게 거의 불가능하므로 player 기능은 사용할 수 없습니다. 다른 환경에서는 사용해볼 수 있으니 재미로 한 번 플레이해보시면 좋습니다! </p>\n<center><img src=\"https://www.dropbox.com/s/uxfm162f1scbzo5/Screenshot%202018-08-24%2015.09.04.png?dl=1\" width=\"400px\"></center>\n\n<p>이번에는 WalkerPair에서 WalkerAgent를 더블클릭해보겠습니다. 이 설정을 보아 5000 step이 episode의 max step인 것을 볼 수 있습니다.</p>\n<center><img src=\"https://www.dropbox.com/s/r6gwemlczwic2ma/Screenshot%202018-08-24%2015.16.19.png?dl=1\" width=\"400px\"></center>\n\n<p>이제 상단 file menu에서 build setting에 들어갑니다. 환경을 build해서 python 코드에서 import하기 위해서입니다. 물론 unity 환경과 python 코드를 binding해주는 부분은 ml-agent 코드 안에 있습니다. Build 버튼을 누르면 환경이 build가 됩니다.</p>\n<center><img src=\"https://www.dropbox.com/s/4dtgoz1k8896vxs/Screenshot%202018-08-24%2015.19.07.png?dl=1\" width=\"500px\"></center>\n\n\n<p><br></p>\n<h3 id=\"2-2-Python에서-unity-환경-불러와서-테스트하기\"><a href=\"#2-2-Python에서-unity-환경-불러와서-테스트하기\" class=\"headerlink\" title=\"2.2 Python에서 unity 환경 불러와서 테스트하기\"></a>2.2 Python에서 unity 환경 불러와서 테스트하기</h3><p>환경을 build 했으면 build한 환경을 python에서 불러와서 random action으로 테스트해봅니다. 환경을 테스트하는 코드는 pg_travel repository에서 unity 폴더 밑에 있습니다. test_env.py라는 코드는 간단하게 다음과 같습니다. Build한 walker 환경은 env라는 폴더 밑에 넣어줍니다. unityagent를 import하는데 ml-agent를 git clone 해서 python 폴더 내에서 “python setup.py install”을 실행했다면 문제없이 import 됩니다. UnityEnvironment를 통해 env라는 환경을 선언할 수 있습니다. 이렇게 선언하고 나면 gym과 상당히 유사한 형태로 환경과 상호작용이 가능합니다. </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">from</span> unityagents <span class=\"keyword\">import</span> UnityEnvironment</span><br><span class=\"line\"><span class=\"keyword\">from</span> utils.utils <span class=\"keyword\">import</span> get_action</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">\"__main__\"</span>:</span><br><span class=\"line\">    env_name = <span class=\"string\">\"./env/walker_test\"</span></span><br><span class=\"line\">    train_mode = <span class=\"keyword\">False</span></span><br><span class=\"line\"></span><br><span class=\"line\">    env = UnityEnvironment(file_name=env_name)</span><br><span class=\"line\"></span><br><span class=\"line\">    default_brain = env.brain_names[<span class=\"number\">0</span>]</span><br><span class=\"line\">    brain = env.brains[default_brain]</span><br><span class=\"line\">    env_info = env.reset(train_mode=train_mode)[default_brain]</span><br><span class=\"line\"></span><br><span class=\"line\">    num_inputs = brain.vector_observation_space_size</span><br><span class=\"line\">    num_actions = brain.vector_action_space_size</span><br><span class=\"line\">    num_agent = env._n_agents[default_brain]</span><br><span class=\"line\"></span><br><span class=\"line\">    print(<span class=\"string\">'the size of input dimension is '</span>, num_inputs)</span><br><span class=\"line\">    print(<span class=\"string\">'the size of action dimension is '</span>, num_actions)</span><br><span class=\"line\">    print(<span class=\"string\">'the number of agents is '</span>, num_agent)</span><br><span class=\"line\">   </span><br><span class=\"line\">    score = <span class=\"number\">0</span></span><br><span class=\"line\">    episode = <span class=\"number\">0</span></span><br><span class=\"line\">    actions = [<span class=\"number\">0</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(num_actions)] * num_agent</span><br><span class=\"line\">    <span class=\"keyword\">for</span> iter <span class=\"keyword\">in</span> range(<span class=\"number\">1000</span>):</span><br><span class=\"line\">        env_info = env.step(actions)[default_brain]</span><br><span class=\"line\">        rewards = env_info.rewards</span><br><span class=\"line\">        dones = env_info.local_done</span><br><span class=\"line\">        score += rewards[<span class=\"number\">0</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> dones[<span class=\"number\">0</span>]:</span><br><span class=\"line\">            episode += <span class=\"number\">1</span></span><br><span class=\"line\">            score = <span class=\"number\">0</span></span><br><span class=\"line\">            print(<span class=\"string\">'&#123;&#125;th episode : mean score of 1st agent is &#123;:.2f&#125;'</span>.format(</span><br><span class=\"line\">                episode, score))</span><br></pre></td></tr></table></figure>\n<p>위 코드를 실행하면 다음과 같이 실행창에 뜹니다. External brain인 것을 알 수 있고 default_brain은 brain 중에 하나만 가져왔기 때문에 number of brain은 1이라고 출력합니다. input dimension은 212이고 action dimension은 39이고 agent 수는 11인 것으로봐서 제대로 환경이 불러와진 것을 확인할 수 있습니다.<br><img src=\"https://www.dropbox.com/s/cioa9h7qu25vonz/Screenshot%202018-08-24%2015.47.43.png?dl=1\"></p>\n<p>이 환경에서 행동하려면 agent 숫자만큼 행동을 줘야합니다. 모두 0로 행동을 주고 실행하면 다음과 같이 뒤로 넘어지는 행동을 반복합니다. env.step(actions)[default_brain]으로 env_info를 받아오면 거기서부터 reward와 done, next_state를 받아올 수 있습니다. 이제 학습하기만 하면 됩니다.<br><img src=\"https://www.dropbox.com/s/8qrmxoski6p4n07/Screenshot%202018-08-24%2016.00.21.png?dl=1\"></p>\n<p><br></p>\n<h3 id=\"2-3-Walker-학습하기\"><a href=\"#2-3-Walker-학습하기\" class=\"headerlink\" title=\"2.3 Walker 학습하기\"></a>2.3 Walker 학습하기</h3><p>기존에 Mujoco에 적용했던 PPO 코드를 그대로 Walker에 적용하니 잘 학습이 안되었습니다. 다음 사진이 저희가 중간 해커톤으로 모여서 이 상황을 공유할 때의 사진입니다.<br><img src=\"https://i.imgur.com/1aR2Z77.png\" width=\"500px\"></p>\n<p>Unity ml-agent에서는 PPO를 기본 agent로 제공합니다. 학습 코드도 제공하기 때문에 mujoco에 적용했던 코드와의 차이점을 분석할 수 있었습니다. mujoco 코드와 ml-agent baseline 코드의 차이점은 다음과 같습니다. </p>\n<ul>\n<li>agent 여러개를 이용, 별개의 memory에 저장한 후에 gradient를 합침</li>\n<li>GAE 및 time horizon 등 hyper parameter가 다름</li>\n<li>Actor와 Critic의 layer가 1층 더 두꺼우며 hidden layer 자체의 사이즈도 더 큼</li>\n<li>hidden layer의 activation function이 tanh가 아닌 swish</li>\n</ul>\n<p>ml-agent baseline 코드리뷰할 때 작성했던 마인드맵은 다음과 같습니다.<br><img src=\"https://i.imgur.com/YeaEntG.png\"></p>\n<p>크게는 두 가지를 개선해서 성능이 많이 향상했습니다.</p>\n<ol>\n<li>Network 수정</li>\n<li>multi-agent를 활용해서 학습</li>\n</ol>\n<p>Network 코드는 다음과 같습니다. Hidden Layer를 하나 더 늘렸으며 swish activation function을 사용할 수 있도록 변경했습니다. 사실 swish라는 activation function은 처음 들어보는 생소한 함수였습니다. 하지만 ml-agent baseline에서 사용했다는 사실과 구현이 상당히 간단하다는 점에서 저희 코드에 적용했습니다. 단순히 x * sigmoid(x) 를 하면 됩니다. swish는 별거 아닌 것 같지만 상당한 성능 개선을 가져다줬습니다. 사실 ReLU나 ELU 등 여러 다른 activation function을 적용해서 비교해보는게 best긴 하지만 시간 관계상 그렇게까지 테스트해보지는 못했습니다. 기존에 TRPO나 PPO는 왜 tanh를 사용했었는지도 의문인 점입니다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Actor</span><span class=\"params\">(nn.Module)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, num_inputs, num_outputs, args)</span>:</span></span><br><span class=\"line\">        self.args = args</span><br><span class=\"line\">        self.num_inputs = num_inputs</span><br><span class=\"line\">        self.num_outputs = num_outputs</span><br><span class=\"line\">        super(Actor, self).__init__()</span><br><span class=\"line\">        self.fc1 = nn.Linear(num_inputs, args.hidden_size)</span><br><span class=\"line\">        self.fc2 = nn.Linear(args.hidden_size, args.hidden_size)</span><br><span class=\"line\">        self.fc3 = nn.Linear(args.hidden_size, args.hidden_size)</span><br><span class=\"line\">        self.fc4 = nn.Linear(args.hidden_size, num_outputs)</span><br><span class=\"line\"></span><br><span class=\"line\">        self.fc4.weight.data.mul_(<span class=\"number\">0.1</span>)</span><br><span class=\"line\">        self.fc4.bias.data.mul_(<span class=\"number\">0.0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.args.activation == <span class=\"string\">'tanh'</span>:</span><br><span class=\"line\">            x = F.tanh(self.fc1(x))</span><br><span class=\"line\">            x = F.tanh(self.fc2(x))</span><br><span class=\"line\">            x = F.tanh(self.fc3(x))</span><br><span class=\"line\">            mu = self.fc4(x)</span><br><span class=\"line\">        <span class=\"keyword\">elif</span> self.args.activation == <span class=\"string\">'swish'</span>:</span><br><span class=\"line\">            x = self.fc1(x)</span><br><span class=\"line\">            x = x * F.sigmoid(x)</span><br><span class=\"line\">            x = self.fc2(x)</span><br><span class=\"line\">            x = x * F.sigmoid(x)</span><br><span class=\"line\">            x = self.fc3(x)</span><br><span class=\"line\">            x = x * F.sigmoid(x)</span><br><span class=\"line\">            mu = self.fc4(x)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">raise</span> ValueError</span><br><span class=\"line\"></span><br><span class=\"line\">        logstd = torch.zeros_like(mu)</span><br><span class=\"line\">        std = torch.exp(logstd)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> mu, std, logstd</span><br></pre></td></tr></table></figure>\n<p>swish와 tanh를 사용한 학습을 비교한 그래프입니다. 하늘색 그래프가 swish를 사용한 결과, 파란색이 tanh를 사용한 결과입니다. score는 episode 마다의 reward의 합입니다.</p>\n<center><img src=\"https://www.dropbox.com/s/3d07c1kql4h5oqk/Screenshot%202018-08-24%2016.33.45.png?dl=1\" width=\"350px\"></center>\n\n<p>이제 multi-agent로 학습하도록 변경하면 됩니다. PPO의 경우 memory에 time horizon 동안의 sample을 시간순서대로 저장하고 GAE를 구한 이후에 minibatch로 추출해서 학습합니다. 따라서 여러개의 agent로 학습하기 위해서는 memory를 따로 만들어서 각각의 GAE를 구해서 학습해야합니다. Unity에서는 Mujoco에서 했던 것처럼 deque로 memory를 만들지 않고 따로 named tuple로 구현한 memory class를 import 해서 사용했습니다. utils 폴더 밑에 memory.py 코드에 구현되어있으며 코드는 <a href=\"https://github.com/pytorch/tutorials/blob/master/Reinforcement%20(Q-)Learning%20with%20PyTorch.ipynb\" target=\"_blank\" rel=\"noopener\">https://github.com/pytorch/tutorials/blob/master/Reinforcement%20(Q-)Learning%20with%20PyTorch.ipynb</a><br>에서 가져왔습니다. </p>\n<p>state, action, reward, mask를 저장하는데 불러올 때 각각을 따로 불러올 수 있기 때문에 비효율적 시간을 많이 줄여줍니다.<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Transition = namedtuple(<span class=\"string\">'Transition'</span>, (<span class=\"string\">'state'</span>, <span class=\"string\">'action'</span>, <span class=\"string\">'reward'</span>, <span class=\"string\">'mask'</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Memory</span><span class=\"params\">(object)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        self.memory = []</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">push</span><span class=\"params\">(self, state, action, reward, mask)</span>:</span></span><br><span class=\"line\">        <span class=\"string\">\"\"\"Saves a transition.\"\"\"</span></span><br><span class=\"line\">        self.memory.append(Transition(state, action, reward, mask))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sample</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> Transition(*zip(*self.memory))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__len__</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> len(self.memory)</span><br></pre></td></tr></table></figure></p>\n<p>main.py 에서는 이 memory를 agent의 개수만큼 생성합니다. </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">memory = [Memory() <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> range(num_agent)]</span><br></pre></td></tr></table></figure>\n<p>sample을 저장할 때도 agent마다 따로 따로 저장합니다. </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(num_agent):</span><br><span class=\"line\">    memory[i].push(states[i], actions[i], rewards[i], masks[i])</span><br></pre></td></tr></table></figure>\n<p>time horizon이 끝나면 모은 sample 들을 가지고 학습하기 위한 값으로 만드는 과정을 진행합니다. 각각의 memory를 가지고 GAE와 old_policy, old_value 등을 계산해서 하나의 batch로 합칩니다. 그렇게 train_model 메소드에 전달하면 기존과 동일하게 agent를 업데이트합니다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sts, ats, returns, advants, old_policy, old_value = [], [], [], [], [], []</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(num_agent):</span><br><span class=\"line\">    batch = memory[i].sample()</span><br><span class=\"line\">    st, at, rt, adv, old_p, old_v = process_memory(actor, critic, batch, args)</span><br><span class=\"line\">    sts.append(st)</span><br><span class=\"line\">    ats.append(at)</span><br><span class=\"line\">    returns.append(rt)</span><br><span class=\"line\">    advants.append(adv)</span><br><span class=\"line\">    old_policy.append(old_p)</span><br><span class=\"line\">    old_value.append(old_v)</span><br><span class=\"line\"></span><br><span class=\"line\">sts = torch.cat(sts)</span><br><span class=\"line\">ats = torch.cat(ats)</span><br><span class=\"line\">returns = torch.cat(returns)</span><br><span class=\"line\">advants = torch.cat(advants)</span><br><span class=\"line\">old_policy = torch.cat(old_policy)</span><br><span class=\"line\">old_value = torch.cat(old_value)</span><br><span class=\"line\"></span><br><span class=\"line\">train_model(actor, critic, actor_optim, critic_optim, sts, ats, returns, advants,</span><br><span class=\"line\">            old_policy, old_value, args)</span><br></pre></td></tr></table></figure>\n<p>이렇게 학습한 에이전트는 다음과 같이 걷습니다. 이렇게 walker를 학습시키고 나니 어떻게 하면 사람처럼 자연스럽게 걷는 것을 agent 스스로 학습할 수 있을까라는 고민을 하게 되었습니다.</p>\n<center><img src=\"https://www.dropbox.com/s/fyz1kn5v92l3rrk/plane-595.gif?dl=1\"></center>\n\n<p>Unity ml-agent에서 제공하는 pretrained된 모델을 다음과 같이 걷습니다. 저희가 학습한 agent와 상당히 다르게 걷는데 왜 그런 차이가 나는지도 분석하고 싶습니다. </p>\n<center><img src=\"https://www.dropbox.com/s/xwz766g7c4eiaia/plane-unity.gif?dl=1\"></center>\n\n\n<p><br></p>\n<h2 id=\"3-Unity-Curved-Surface-제작-및-학습기\"><a href=\"#3-Unity-Curved-Surface-제작-및-학습기\" class=\"headerlink\" title=\"3. Unity Curved Surface 제작 및 학습기\"></a>3. Unity Curved Surface 제작 및 학습기</h2><p>Unity ml-agent에서 제공하는 기본 Walker 환경에서 학습하고 나니 바닥을 조금 울퉁불퉁하게 혹은 경사가 진 곳에서 걷는 것을 학습해보고 싶다라는 생각이 들었습니다. 따라서 간단하게 걷는 배경을 다르게 하는 시도를 해봤습니다. </p>\n<p><br></p>\n<h3 id=\"3-1-Curved-Surface-만들기\"><a href=\"#3-1-Curved-Surface-만들기\" class=\"headerlink\" title=\"3.1 Curved Surface 만들기\"></a>3.1 Curved Surface 만들기</h3><p>Agent가 걸어갈 배경을 처음부터 만드는 것보다 구할 수 있다면 만들어진 배경을 구하기로 했습니다. Unity를 무료라는 점에서 선택했듯이 배경을 무료로 구할 수 있는 방법을 선택했습니다. </p>\n<center><img src=\"https://www.dropbox.com/s/e0tsp3e3c9uq2zh/Screenshot%202018-08-23%2000.19.14.png?dl=1\"></center>\n\n<p>무료로 공개되어있는 Unity 배경 중에서 Curved Ground 라는 것을 가져와서 작업하기로 했습니다. 이 환경 같은 경우 spline을 그리듯이 중간의 점을 이동시키면서 사용자가 곡면을 수정할 수 있습니다.</p>\n<center><img src=\"https://www.dropbox.com/s/3ppmxotrf6qhzaf/Screenshot%202018-08-23%2000.20.25.png?dl=1\"></center>\n\n<p>간단하게 곡면을 만들어서 공을 굴려보면 다음과 같이 잘 굴러갑니다. </p>\n<center><img src=\"https://www.dropbox.com/s/2e8yqvqj1a4th27/slope_walker_ball.gif?dl=1\"></center>\n\n<p>여러 에이전트가 학습할 수 있도록 오목한 경사면을 제작했습니다. 초반의 모습은 다음과 같았습니다.<br><img src=\"https://www.dropbox.com/s/m492xsfp4bolmz5/Screenshot%202018-08-23%2000.36.06.png?dl=1\"></p>\n<p>하지만 최종으로는 다음과 같은 곡면으로 사용했습니다. 위 사진의 배경과 아래 사진의 배경이 다른 점은 slope 길이, 내리막 경사, 오르막 경사입니다. Slope 길이의 경우 길이를 기존 plane 과 동일하게 했더니, 오르막 올라가는 부분이 학습이 잘 안 되었습니다. 따라서 길이를 줄였습니다. 내리막 경사의 경우 너무 경사지면 학습이 잘 안 되고, 너무 완만하니 내리막 티가 잘 안 나기 때문에 적절한 경사를 설정했습니다. 오르막 경사의 경우 내리막보다는 오르막이 더 어려울 것이라고 판단해서 오르막 경사를 낮게 설정했습니다.<br><img src=\"https://www.dropbox.com/s/idbov4wtd6jeqb2/Screenshot%202018-08-23%2000.36.54.png?dl=1\"></p>\n<p><br></p>\n<h3 id=\"3-2-Curved-Surface에서-학습하기\"><a href=\"#3-2-Curved-Surface에서-학습하기\" class=\"headerlink\" title=\"3.2 Curved Surface에서 학습하기\"></a>3.2 Curved Surface에서 학습하기</h3><p>위 환경으로 학습을 할 때, agent가 너무 초반에 빨리 쓰러지는 현상이 발생했습니다. 혹시 발의 각도가 문제일까 싶어서 발 각도를 변경해보았습니다. </p>\n<center><img src=\"https://www.dropbox.com/s/znvikbeoj7gku0u/Screenshot%202018-08-23%2000.38.22.png?dl=1\" width=\"400px\"></center>\n\n<p>하지만 역시 평지에서 걷는 것처럼 걷도록 학습이 안되었습니다. 이 환경에서 더 잘 학습하려면 더 여러가지를 시도해봐야할 것 같습니다. (그래도 걷는 게 기특합니다..)</p>\n<center><img src=\"https://www.dropbox.com/s/4fqpsdmnzvnvia0/curved-736.gif?dl=1\"></center>\n\n<p><img src=\"https://www.dropbox.com/s/t5ngr0io4xeex6y/curved-736-overview.gif?dl=1\"></p>\n<p><br></p>\n<h2 id=\"4-구현-후기\"><a href=\"#4-구현-후기\" class=\"headerlink\" title=\"4. 구현 후기\"></a>4. 구현 후기</h2><p>피지여행 구현팀은 총 4명으로 진행했습니다. 각 팀원의 후기를 적어보겠습니다.</p>\n<ul>\n<li>팀원 장수영: 사랑합니다. 행복합니다.</li>\n<li>팀원 공민서: 제가 핵심적인 기능을 구현하지는 못했지만 무조코 설치와 모델 테스트를 맡으면서 딥마인드나 openai의 영상으로만 보던 에이전트의 성장과정을 눈으로 지켜볼 수 있었습니다. 제대로 서있지도 못하던 hopper가 어느정도 훈련이 되고서는 넘어지려하다가도 추진력을 얻기위해 웅크렸다 뛰는 것을 관찰하는 것도 재미있고 육아일기를 보는 아버지의 마음을 조금이나마 이해할 수 있었습니다. 텐서보드를 넣는 걸 깜빡해 일일히 에피소드 별 스코어를 시각화 하면서 텐서보드의 소중함을 알았습니다. 유니티 코드리뷰를 하면서도 시스템 아키텍쳐 설계에 대해서도 배울 점이 있었던 것 같고 swish라는 활성화함수의 존재도 알았었고 curiosity도 알게되었고 역시 다른 사람의 코드를 읽는 것도 많은 공부가 된다고 되새기던 시간이었습니다. 물론 너무 크기가 방대해서 가독성은 많이 떨어졌습니다만 무조코보다 유니티가 훨씬 흥할거라고 생각했습니다. 마지막으로 누구 하나 열정적이지 않은 사람이 없이 치열한 고민을 함께 한 PG여행팀 분들, 저의 부족함과 상생의 기쁨을 알게해주셔서 정말 감사드립니다.</li>\n<li>팀원 양혁렬: 여러 에이전트가 함께하면 더 잘하는 걸 보면서 새삼 좋은 분들과 함께 할 수 있어서 행복했습니다</li>\n<li>팀원 이웅원: 저희가 직접 바닥부터 다 구현했던 것은 아니지만 구현을 해보면서 논문의 내용을 더 잘 이해할 수 있었습니다. 논문에 나와있지 않은 여러 노하우가 필요한 점들도 많았습니다. 역시 코드로 보고 성능 재현이 되어야 제대로 알고리즘을 이해하게 된다는 것을 다시 느낀 시간이었습니다. 또한 강화학습은 역시 환경세팅이 어렵다는 생각을 했습니다. 하지만 unity ml-agent를 사용해보면서 앞으로 강화학습 환경으로서 가능성이 상당히 크다는 생각을 했습니다. 또한 구현팀과 슬랙, 깃헙으로 협업하면서 온라인 협업에 대해서 더 배워가는 것 같습니다. 아직은 익숙하지 않지만 앞으로는 마치 바로 옆에서 같이 코딩하는 것 같이 될 거라고 생각합니다.</li>\n</ul>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"cjkj6tyra0000j715ow4qussb","category_id":"cjiinnj6o0003j28aoi038xjq","_id":"cjkj6tyrg0003j715c7cboixr"},{"post_id":"cjkj6tyrj0004j7150bs8twv4","category_id":"cjiinnj6o0003j28aoi038xjq","_id":"cjkj6tyrm0009j715tps3pxt5"},{"post_id":"cjkj6tyrl0007j715h3xhmg1v","category_id":"cjiinnj6o0003j28aoi038xjq","_id":"cjkj6tyrn000dj715eyeu3llt"},{"post_id":"cjkj6tyrt000gj715smm2b4il","category_id":"cjiinnj6o0003j28aoi038xjq","_id":"cjkj6tyrx000kj7155hylfm2h"},{"post_id":"cjkj6tyru000hj715be3owc0l","category_id":"cjiinnj6o0003j28aoi038xjq","_id":"cjkj6tyrx000mj715fdqsbo38"},{"post_id":"cjkj6tysm000oj715gn62q8pn","category_id":"cjiinnj6o0003j28aoi038xjq","_id":"cjkj6tysp000sj7157uuw667i"},{"post_id":"cjkj6tysn000pj715uqgp4pw0","category_id":"cjiinnj6o0003j28aoi038xjq","_id":"cjkj6tysp000uj7155mqtv0xl"},{"post_id":"cjkj6tyrk0005j715nj1djy8z","category_id":"cjiinnj6o0003j28aoi038xjq","_id":"cjksx7wwh0002s1156zzkag5q"},{"post_id":"cjltduyuz0000f8xxn1w8mu2c","category_id":"cjiinnj6o0003j28aoi038xjq","_id":"cjltduyv10003f8xx6ada9z6x"}],"PostTag":[{"post_id":"cjkj6tyra0000j715ow4qussb","tag_id":"cjiinnj6q0004j28ajv2qofj6","_id":"cjkj6tyrf0001j715whd99u9w"},{"post_id":"cjkj6tyra0000j715ow4qussb","tag_id":"cjiinnj6r0007j28a9ifjaq6c","_id":"cjkj6tyrg0002j71546h4nv0u"},{"post_id":"cjkj6tyrj0004j7150bs8twv4","tag_id":"cjiinnj6q0004j28ajv2qofj6","_id":"cjkj6tyrl0006j715adg8rqs5"},{"post_id":"cjkj6tyrj0004j7150bs8twv4","tag_id":"cjiinnj6r0007j28a9ifjaq6c","_id":"cjkj6tyrm0008j715h2hpllx8"},{"post_id":"cjkj6tyrl0007j715h3xhmg1v","tag_id":"cjiinnj6q0004j28ajv2qofj6","_id":"cjkj6tyrn000ej7152otx51e2"},{"post_id":"cjkj6tyrl0007j715h3xhmg1v","tag_id":"cjiinnj6r0007j28a9ifjaq6c","_id":"cjkj6tyrn000fj715bpgua9sq"},{"post_id":"cjkj6tyrt000gj715smm2b4il","tag_id":"cjiinnj6q0004j28ajv2qofj6","_id":"cjkj6tyrw000ij7155l45pn4y"},{"post_id":"cjkj6tyrt000gj715smm2b4il","tag_id":"cjiinnj6r0007j28a9ifjaq6c","_id":"cjkj6tyrx000jj715epv4fkhp"},{"post_id":"cjkj6tyru000hj715be3owc0l","tag_id":"cjiinnj6q0004j28ajv2qofj6","_id":"cjkj6tyrx000lj715u1neohgy"},{"post_id":"cjkj6tyru000hj715be3owc0l","tag_id":"cjiinnj6r0007j28a9ifjaq6c","_id":"cjkj6tyrx000nj7152r385gm9"},{"post_id":"cjkj6tysm000oj715gn62q8pn","tag_id":"cjiinnj6q0004j28ajv2qofj6","_id":"cjkj6tyso000qj715x7aa8xha"},{"post_id":"cjkj6tysm000oj715gn62q8pn","tag_id":"cjiinnj6r0007j28a9ifjaq6c","_id":"cjkj6tysp000rj715hmao443e"},{"post_id":"cjkj6tysn000pj715uqgp4pw0","tag_id":"cjiinnj6q0004j28ajv2qofj6","_id":"cjkj6tysp000tj715u54hpj49"},{"post_id":"cjkj6tysn000pj715uqgp4pw0","tag_id":"cjiinnj6r0007j28a9ifjaq6c","_id":"cjkj6tysp000vj715mme6vq6k"},{"post_id":"cjkj6tyrk0005j715nj1djy8z","tag_id":"cjiinnj6q0004j28ajv2qofj6","_id":"cjksx7wwg0000s115xaglrkx2"},{"post_id":"cjkj6tyrk0005j715nj1djy8z","tag_id":"cjiinnj6r0007j28a9ifjaq6c","_id":"cjksx7wwh0001s115813l7b8c"},{"post_id":"cjltduyuz0000f8xxn1w8mu2c","tag_id":"cjiinnj6q0004j28ajv2qofj6","_id":"cjltduyv10001f8xxn1tmka5y"},{"post_id":"cjltduyuz0000f8xxn1w8mu2c","tag_id":"cjiinnj6r0007j28a9ifjaq6c","_id":"cjltduyv10002f8xxrp7fss5z"}],"Tag":[{"name":"프로젝트","_id":"cjiinnj6q0004j28ajv2qofj6"},{"name":"피지여행","_id":"cjiinnj6r0007j28a9ifjaq6c"}]}}