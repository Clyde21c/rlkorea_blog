{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/img/Exp_OctopusArm.png","path":"img/Exp_OctopusArm.png","modified":0,"renderable":0},{"_id":"source/img/Exp_ContinuousBandit.png","path":"img/Exp_ContinuousBandit.png","modified":0,"renderable":0},{"_id":"source/img/Ref_Degris2012b_Theorem1.png","path":"img/Ref_Degris2012b_Theorem1.png","modified":0,"renderable":0},{"_id":"source/img/Exp_OctopusArm_Ref.png","path":"img/Exp_OctopusArm_Ref.png","modified":0,"renderable":0},{"_id":"source/img/Exp_ContinuousRL.png","path":"img/Exp_ContinuousRL.png","modified":0,"renderable":0},{"_id":"source/img/Ref_Degris2012b_offpolicygradient.png","path":"img/Ref_Degris2012b_offpolicygradient.png","modified":0,"renderable":0},{"_id":"themes/clean-blog/source/css/article.styl","path":"css/article.styl","modified":0,"renderable":1},{"_id":"themes/clean-blog/source/css/base.styl","path":"css/base.styl","modified":0,"renderable":1},{"_id":"themes/clean-blog/source/css/mixins.styl","path":"css/mixins.styl","modified":0,"renderable":1},{"_id":"themes/clean-blog/source/css/style.styl","path":"css/style.styl","modified":0,"renderable":1},{"_id":"themes/clean-blog/source/css/variables.styl","path":"css/variables.styl","modified":0,"renderable":1},{"_id":"themes/clean-blog/source/img/about-bg.jpg","path":"img/about-bg.jpg","modified":0,"renderable":1},{"_id":"themes/clean-blog/source/img/favicon.ico","path":"img/favicon.ico","modified":0,"renderable":1},{"_id":"themes/clean-blog/source/img/home-bg.jpg","path":"img/home-bg.jpg","modified":0,"renderable":1},{"_id":"themes/clean-blog/source/img/contact-bg.jpg","path":"img/contact-bg.jpg","modified":0,"renderable":1},{"_id":"source/img/1.jpg","path":"img/1.jpg","modified":0,"renderable":0},{"_id":"source/img/mixure_policy.png","path":"img/mixure_policy.png","modified":0,"renderable":0},{"_id":"source/img/tvd.png","path":"img/tvd.png","modified":0,"renderable":0},{"_id":"source/img/importance_sampling.png","path":"img/importance_sampling.png","modified":0,"renderable":0},{"_id":"source/img/heuristic_approx.png","path":"img/heuristic_approx.png","modified":0,"renderable":0},{"_id":"source/img/kld.png","path":"img/kld.png","modified":0,"renderable":0},{"_id":"source/img/sample-based.png","path":"img/sample-based.png","modified":0,"renderable":0},{"_id":"source/img/single.png","path":"img/single.png","modified":0,"renderable":0},{"_id":"source/img/surrogate.png","path":"img/surrogate.png","modified":0,"renderable":0},{"_id":"source/img/vine1.png","path":"img/vine1.png","modified":0,"renderable":0},{"_id":"source/img/vine2.png","path":"img/vine2.png","modified":0,"renderable":0},{"_id":"source/img/state_visitation_change.png","path":"img/state_visitation_change.png","modified":0,"renderable":0},{"_id":"source/img/policy_change.png","path":"img/policy_change.png","modified":0,"renderable":0}],"Cache":[{"_id":"source/.DS_Store","hash":"cc7d1323fdf1fb465d97d2547c601caa064dcf9a","modified":1531911524266},{"_id":"themes/clean-blog/.DS_Store","hash":"18b40c0a074fb756ef76fd437c21c2205a08a585","modified":1529045736390},{"_id":"themes/clean-blog/LICENSE","hash":"8726b416df4f067cff579e859f05c4b594b8be09","modified":1529043522273},{"_id":"themes/clean-blog/README.md","hash":"861dd2f959ab75d121226f4f3e2f61f4bc95fddb","modified":1529043522273},{"_id":"themes/clean-blog/_config.yml","hash":"c0ee3fcf1841410d07402e8d6e50e8847f31ae4b","modified":1529047469272},{"_id":"source/_posts/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1529228886819},{"_id":"source/_posts/sutton-pg.md","hash":"3d59908082236ba60e2ea21bb9a26113e983e539","modified":1532002605271},{"_id":"source/_posts/피지여행-소개.md","hash":"9ee06938369d11302335856c2728bee3f367eafe","modified":1532002582289},{"_id":"source/_posts/2018-06-15-npg.md","hash":"ad0e79383db0552d94078ca181c5e7c764b81f57","modified":1529150062936},{"_id":"source/_posts/dpg.md","hash":"7a1716cfe1264c0e04d60ba2f9c98a35f3452afd","modified":1532002642105},{"_id":"source/img/Exp_OctopusArm.png","hash":"f0ae59d0c2a6600ef961fd0b2ea8903dcfdbd4d9","modified":1529117349098},{"_id":"source/img/Exp_ContinuousBandit.png","hash":"72eddde3b296070075706688038f1267bfd04d31","modified":1529113799292},{"_id":"source/img/Ref_Degris2012b_Theorem1.png","hash":"154f42239538c3dd0508e815014d84685e18f89b","modified":1529106775328},{"_id":"themes/clean-blog/languages/default.yml","hash":"97326c9e6518d9f379778178b3b8f9a58434725d","modified":1529043522274},{"_id":"themes/clean-blog/languages/en.yml","hash":"97326c9e6518d9f379778178b3b8f9a58434725d","modified":1529043522274},{"_id":"themes/clean-blog/languages/de.yml","hash":"424a9c1e6ab69334d7873f6574da02ca960aa572","modified":1529043522273},{"_id":"themes/clean-blog/languages/no.yml","hash":"8ca475a3b4f8efe6603030f0013aae39668230e1","modified":1529043522274},{"_id":"themes/clean-blog/languages/es.yml","hash":"cb4eeca0ed3768a77e0cd216300f2b2549628b1b","modified":1529043522274},{"_id":"themes/clean-blog/languages/fr.yml","hash":"e9e6f7cb362ebb7997f11027498a2748fe3bac95","modified":1529043522274},{"_id":"themes/clean-blog/languages/pt.yml","hash":"1d0c3689eb32fe13f37f8f6f303af7624ebfbaf0","modified":1529043522275},{"_id":"themes/clean-blog/languages/pl.yml","hash":"de7eb5850ae65ba7638e907c805fea90617a988c","modified":1529043522274},{"_id":"themes/clean-blog/languages/ru.yml","hash":"42df7afeb7a35dc46d272b7f4fb880a9d9ebcaa5","modified":1529043522275},{"_id":"themes/clean-blog/languages/zh-TW.yml","hash":"9acac6cc4f8002c3fa53ff69fb8cf66c915bd016","modified":1529043522275},{"_id":"themes/clean-blog/languages/zh-CN.yml","hash":"7bfcb0b8e97d7e5edcfca8ab26d55d9da2573c1c","modified":1529043522275},{"_id":"themes/clean-blog/layout/index.ejs","hash":"425da730d537805046040c5070571d9e739d4b19","modified":1529229786757},{"_id":"themes/clean-blog/layout/archive.ejs","hash":"f2ef73afc3d275333329bb30b9369b82e119da76","modified":1529043522279},{"_id":"themes/clean-blog/layout/layout.ejs","hash":"da2f9018047924ddaf376aee5996c7ddc06cebc1","modified":1529043522279},{"_id":"themes/clean-blog/layout/page.ejs","hash":"591af587e1aae962950de7e79bd25c1f060c69ac","modified":1529043522279},{"_id":"themes/clean-blog/source/.DS_Store","hash":"84f35e390633eadc3c78584a28f5f5f8ce7f43a5","modified":1529045731387},{"_id":"themes/clean-blog/layout/post.ejs","hash":"38382e9bbeb6b8d2eafbd53fff2984111f524c1a","modified":1529043522279},{"_id":"source/img/Exp_OctopusArm_Ref.png","hash":"ebf69a16fc09e447442808f8cc46d770bed0cf10","modified":1529116470075},{"_id":"source/img/Exp_ContinuousRL.png","hash":"85b9ebfb164631804d2fb6be0164e49cd1db746b","modified":1529115804602},{"_id":"source/img/Ref_Degris2012b_offpolicygradient.png","hash":"46f0f17392ff40927579899fd10bdaf6bf562ebd","modified":1529106760142},{"_id":"themes/clean-blog/layout/_partial/article-categories.ejs","hash":"5a0bf5a20f670621d8013c9b9d7976b45c8aa80f","modified":1529043522276},{"_id":"themes/clean-blog/layout/_partial/article-archive.ejs","hash":"3d8d98c6545b8332a6d6ed4f8b00327df03ea945","modified":1529043522276},{"_id":"themes/clean-blog/layout/_partial/after-footer.ejs","hash":"a6ad079ded70024d35264fae798ae73bdbcb0ae6","modified":1529149982621},{"_id":"themes/clean-blog/layout/_partial/article-full.ejs","hash":"6cf24bd7785d57cb7198b3f1ed4fa6a86c84a502","modified":1529043522276},{"_id":"themes/clean-blog/layout/_partial/article-index.ejs","hash":"e433df4e245e2d4c628052c6e59966563542d94d","modified":1529043522276},{"_id":"themes/clean-blog/layout/_partial/article-tags.ejs","hash":"6136434be09056c1466149cecb3cc2e80d107999","modified":1529043522277},{"_id":"themes/clean-blog/layout/_partial/comments.ejs","hash":"3fedb75436439d1d6979b7e4d20d48a593e12be4","modified":1529043522277},{"_id":"themes/clean-blog/layout/_partial/footer.ejs","hash":"a92f5168c006193c3d964fd293ad3c38aae69419","modified":1529043522277},{"_id":"themes/clean-blog/layout/_partial/gallery.ejs","hash":"21e4f28909f4a79ff7d9f10bdfef6a8cb11632bf","modified":1529043522277},{"_id":"themes/clean-blog/layout/_partial/google-analytics.ejs","hash":"4e6e8de9becea5a1636a4dcadcf7a10c06e2426e","modified":1529043522277},{"_id":"themes/clean-blog/layout/_partial/head.ejs","hash":"f8ddbced1627704ab35993e8fc6d6e34cc6f2ba9","modified":1529043522278},{"_id":"themes/clean-blog/layout/_partial/menu_origin.ejs","hash":"cfc30e6b1ef9487cff3ce594d403d1e7c4d9cdf4","modified":1529043522278},{"_id":"themes/clean-blog/layout/_partial/pagination.ejs","hash":"557d6bb069a1d48af49ae912994653f44b32a570","modified":1529043522278},{"_id":"themes/clean-blog/layout/_partial/menu.ejs","hash":"bf186aa55623a77e18c6b1748e6af429ca7e7d67","modified":1529230461987},{"_id":"themes/clean-blog/layout/_partial/tag-category-index.ejs","hash":"10cdc1b7866999c714a666557c150d2c79c1fba9","modified":1529043522278},{"_id":"themes/clean-blog/source/css/article.styl","hash":"f5294d7a3d6127fcb287de3ff0c12aebb1766c7b","modified":1529043522280},{"_id":"themes/clean-blog/source/css/base.styl","hash":"29b54c63060bd2d7f5c501d403d9db5a552ad10c","modified":1529043522280},{"_id":"themes/clean-blog/source/css/mixins.styl","hash":"14264bf86b4e3194a3156447f7b7bce2fd0db5bd","modified":1529043522280},{"_id":"themes/clean-blog/source/css/style.styl","hash":"c40dc495a41007d21c59f342ee42b2d31d7b5ff4","modified":1529043522280},{"_id":"themes/clean-blog/source/css/variables.styl","hash":"cd82df5ca8dfbcfec12d833f01adfac00878e835","modified":1529043522280},{"_id":"themes/clean-blog/source/img/about-bg.jpg","hash":"d39126a6456f2bac0169d1779304725f179c9900","modified":1529043522281},{"_id":"themes/clean-blog/source/img/favicon.ico","hash":"3412e0d657aa5a6cfbbfcf4ef398572c24035565","modified":1529045668405},{"_id":"themes/clean-blog/source/img/home-bg.jpg","hash":"990f6f9dd0ecb5348bfcc47305553d58c0d8f326","modified":1529043522283},{"_id":"themes/clean-blog/source/img/contact-bg.jpg","hash":"6af63305c923899017e727b5ca968a2703bc08cf","modified":1529043522282},{"_id":"public/index.html","hash":"7ec571191d829b2fae80a2b201d50cec50d4236b","modified":1531306829477},{"_id":"public/archives/index.html","hash":"84f1f075bb5ca773dad2dfce11a2e38cf63335af","modified":1531306829469},{"_id":"public/archives/2018/index.html","hash":"03977bb2ca137aff0121b32db030332fe4dac5ea","modified":1531306829469},{"_id":"public/archives/2018/06/index.html","hash":"229f2f899c2eec800479d40d719cd69f660bfdb8","modified":1531306829477},{"_id":"public/categories/프로젝트/index.html","hash":"9189730fe1800b3a637ad2e3a30b07857c7a19ac","modified":1531306829478},{"_id":"public/tags/프로젝트/index.html","hash":"5ab827553d9304271cecd7fda7f5b9ec31f2e385","modified":1531306829478},{"_id":"public/tags/피지여행/index.html","hash":"067bbb31fe0bbd11a30ea5db37bb8bce5c217f41","modified":1531306829478},{"_id":"public/2018/06/16/dpg/index.html","hash":"7bc394c8fcb2dca3d19acc45af754700fb5389e7","modified":1529230754824},{"_id":"public/2018/06/15/sutton-pg/index.html","hash":"2a84a7e4ea8168f22bebf2b1150355e14e9efea1","modified":1529230754824},{"_id":"public/2018/06/14/2018-06-15-npg/index.html","hash":"63865f511c586719f341629a55a4354cf9e39371","modified":1529230754824},{"_id":"public/2018/06/17/피지여행-소개/index.html","hash":"4444d94a395bd894dc96f4c1968ca4d457493924","modified":1529230754823},{"_id":"source/img/Screen Shot 2018-07-10 at 3.41.06 PM.png","hash":"c20c1e539d1b1fe5801de009e693492158ac961f","modified":1531204874000},{"_id":"source/_posts/ddpg.md","hash":"33a85a803d4a37f1577690ff3f4929a4ce1fd036","modified":1532002649265},{"_id":"source/_posts/npg.md","hash":"4819bfbb7dcdf3e8857423df860f53278e7eabe2","modified":1532002657422},{"_id":"source/_posts/gae.md","hash":"fe2911596acbf2bc9bdf6c64ba35929c3ac81313","modified":1532238686158},{"_id":"source/_posts/pg-travel-guide.md","hash":"996c58fbc529b7d2affc527d6fa0a2fd2d2e8368","modified":1532340141420},{"_id":"source/images/Exp_ContinuousBandit.png","hash":"72eddde3b296070075706688038f1267bfd04d31","modified":1531290478254},{"_id":"source/images/Exp_OctopusArm.png","hash":"f0ae59d0c2a6600ef961fd0b2ea8903dcfdbd4d9","modified":1531290478256},{"_id":"source/images/Ref_Degris2012b_Theorem1.png","hash":"154f42239538c3dd0508e815014d84685e18f89b","modified":1531290478257},{"_id":"source/images/figure.png","hash":"c20c1e539d1b1fe5801de009e693492158ac961f","modified":1531204874000},{"_id":"source/images/Exp_ContinuousRL.png","hash":"85b9ebfb164631804d2fb6be0164e49cd1db746b","modified":1531290478255},{"_id":"source/images/Exp_OctopusArm_Ref.png","hash":"ebf69a16fc09e447442808f8cc46d770bed0cf10","modified":1531290478256},{"_id":"source/images/Ref_Degris2012b_offpolicygradient.png","hash":"46f0f17392ff40927579899fd10bdaf6bf562ebd","modified":1531290478257},{"_id":"source/img/figure.png","hash":"c20c1e539d1b1fe5801de009e693492158ac961f","modified":1531204874000},{"_id":"source/img/1.jpg","hash":"179e50805da05b9a2417192bff85de448f5de998","modified":1531306513731},{"_id":"public/2018/07/11/gae/index.html","hash":"27ec6523b1572a0fe8e9bd806926005578ec9067","modified":1531306829478},{"_id":"public/2018/07/11/pg-travel-guide/index.html","hash":"5a90be92767db785e6e33ef1b213cceb48882f71","modified":1531306829479},{"_id":"public/2018/07/10/sutton-pg/index.html","hash":"709c482d1075fdee66a7cc080add172ff3892017","modified":1531306829479},{"_id":"public/archives/2018/07/index.html","hash":"9713c99afe7263b1f69f4bc88c757bda9ea22a4b","modified":1531306829480},{"_id":"public/2018/06/23/ddpg/index.html","hash":"c8278826e2c5a127debc6847b215143da26c7a0d","modified":1531306829481},{"_id":"public/2018/06/14/npg/index.html","hash":"63865f511c586719f341629a55a4354cf9e39371","modified":1531306829481},{"_id":"public/img/figure.png","hash":"c20c1e539d1b1fe5801de009e693492158ac961f","modified":1531306829481},{"_id":"public/img/1.jpg","hash":"179e50805da05b9a2417192bff85de448f5de998","modified":1531306829481},{"_id":"source/img/figure10.jpg","hash":"5ac3e77e51f235f1ff01913f60b39b0d4c15c702","modified":1531380515987},{"_id":"source/img/figure2.jpg","hash":"be090f33c5c52aac5affb1a5141b7bb59df95feb","modified":1531378668469},{"_id":"source/img/figure4.jpg","hash":"4f3a5da99d582cfaeef680bcad5ffe04b9f77b2e","modified":1531378999629},{"_id":"source/img/figure6.jpg","hash":"dc10abad67ca9b7dc1e1f2f5a50979b597cff957","modified":1531379789001},{"_id":"source/img/figure9.jpg","hash":"a5b762eedea6af2e0f842f6c8171cf8600d0f066","modified":1531380074106},{"_id":"source/img/figure5.jpg","hash":"6ff807873a00811aebdf9dbd3d7a2780ddac3719","modified":1531379639148},{"_id":"source/img/figure3.jpg","hash":"b48ff63551254da2f6ef90124ca63435e3768ed7","modified":1531378674752},{"_id":"source/img/figure1.jpg","hash":"9e0b12cbbedcc3e1d4ac20552fcbad8bb3e8071a","modified":1531378413225},{"_id":"source/img/figure8.jpg","hash":"74a82b06007bcbe9ea602275785247bd250c4341","modified":1531380066337},{"_id":"source/img/figure7.jpg","hash":"a72367c5b094841966473e3d3be983413b486933","modified":1531380059406},{"_id":"source/_posts/trpo.md","hash":"7d2d875ba6ade487c6ea2d58409eb1f75da6ac32","modified":1532400779553},{"_id":"source/img/mixure_policy.png","hash":"a29de731fa31bb01bfe2d32714d2c330b62ebf1c","modified":1531555460393},{"_id":"source/img/tvd.png","hash":"48832e9dfdc030c0eec69f4eb3a6df6f79e443b2","modified":1531555460408},{"_id":"source/img/importance_sampling.png","hash":"557cb910e6cbaa52570749c971f30a9ac99f5b90","modified":1531555460389},{"_id":"source/img/heuristic_approx.png","hash":"5f2474685413fe5220cabe92f8efd55541c19654","modified":1531555460387},{"_id":"source/img/kld.png","hash":"ec5b1eebc409fbe8b5a2a1d6ab8cdbb921b39d42","modified":1531555460392},{"_id":"source/img/sample-based.png","hash":"c4b79bd8d64eaa2ce36fb0a3ee0981b5455cce30","modified":1531555460399},{"_id":"source/img/single.png","hash":"52a63dd842c7448ebf20fa53d5485bd0364250b9","modified":1531555460401},{"_id":"source/img/surrogate.png","hash":"a0f501e64f45b6edde6282688416c513a27b6dff","modified":1531555460407},{"_id":"source/img/vine1.png","hash":"acea47e2739665b82e467660d2f1e382ef0307b6","modified":1531555460410},{"_id":"source/img/vine2.png","hash":"80103e41a869da947827726b5719bb30a33de414","modified":1531555460413},{"_id":"source/img/state_visitation_change.png","hash":"81c9b7879e5cb69e65519415b5ba72dbf09c19ee","modified":1531555460404},{"_id":"source/img/policy_change.png","hash":"e1fd4af3c5e8236d14eed042b9eec7358479626d","modified":1531555460397},{"_id":"source/img/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1531628462724},{"_id":"source/_posts/ppo.md","hash":"8b8844cb0c04b02fe7f9fdaef81731095bce3d2a","modified":1532400999024}],"Category":[{"name":"프로젝트","_id":"cjiinnj6o0003j28aoi038xjq"}],"Data":[],"Page":[],"Post":[{"title":"피지여행 소개","date":"2018-06-30T09:50:06.000Z","_content":"","source":"_posts/피지여행-소개.md","raw":"---\ntitle: 피지여행 소개\ndate: 2018-06-30 18:50:06\ntags:\n---\n","slug":"피지여행-소개","published":1,"updated":"2018-07-19T12:16:22.289Z","_id":"cjiinnj690000j28ai14rugme","comments":1,"layout":"post","photos":[],"link":"","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"Policy Gradient Methods for Reinforcement Learning with Function Approximation","date":"2018-06-28T05:18:32.000Z","author":"김동민, 이동민","subtitle":"피지여행 1번째 논문","_content":"\n<center> <img src=\"https://www.dropbox.com/s/83i4atz733ali0w/Screen%20Shot%202018-07-18%20at%2012.10.52%20AM.png?dl=1\" width=\"600\"> </center>\n\n논문 저자 : Richard S. Sutton, David McAllester, Satinder Singh, Yishay Mansour    \n논문 링크 : [NIPS](http://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf)\nProceeding : Advances in Neural Information Processing Systems (NIPS) 2000        \n정리 : 김동민, 이동민\n\n---\n\n# 1. Intro to Policy Gradient\n\n이 논문은 policy gradient (PG) 기법의 효시와도 같으며 향후 많은 파생연구를 낳은 중요한 논문입니다. 7페이지의 짧은 논문이지만 읽기에 만만한 논문은 아닙니다. 이 논문을 이해하기 위해 필요한 배경지식을 먼저 설명하고 논문을 차근차근 살펴보도록 하겠습니다.\n\n<br>\n## 1.1 Value Function Approach\n\n전통적으로 강화학습 기법은 value function을 기반으로 동작하였습니다. 특정 state에서 vaue function 또는 value function을 근사하는 함수(function approximation)를 최대화하는 action을 찾는 greedy action-selection policy가  대표적입니다.\n\n논문에서는 이러한 방법은 deterministic한 policy를 찾는 쪽으로 나아가게 되지만 종종 최적의 policy는 stochastic한 성질을 가지기 때문에 이 방법으로는 최적의 policy를 찾을 수 없다고 언급하고 있습니다. (그러나 이 논문이 나오고 나서 한참 후 David Silver를 필두로한 DeepMind의 연구진들은 high-dimensional action space를 가지는 application에서 보다 빠르게 동작하는 [deterministic policy gradient](../../../06/16/dpg/)을 개발하였습니다.) 아마도 이러한 부분은 $\\epsilon$-greedy action-selection policy로 개선될 수 있을 것입니다. 또 하나의 문제는 value function의 작은 변화로 인해서 action이 크게 변할 수도 있다는 것입니다. 이것은 알고리듬의 수렴성에 문제를 야기할 수 있습니다. 이러한 문제점을 해결하기 위하여 policy search라는 새로운 기법이 고안됩니다.\n\n<br>\n## 1.2 Policy Search\n\npolicy search는 최적의 policy $\\pi^*$를 reward로부터 직접 찾습니다. policy search 방법은 크게 두 가지로 분류할 수 있습니다. 첫 번째는 gradient-based optimization으로 policy gradient method가 이에 속합니다. 두 번째는 gradient-free optimization으로 진화(evolutionary) 연산을 이용하는 것이 이에 속합니다. 이 논문에서는 제목에서 알 수 있듯이 gradient-based optimization을 다룹니다. gradient는 변화량을 의미합니다. 특정 policy x와 또 다른 policy y가 있을 때 policy가 얼마나 많이 변화했는지 어떻게 모델링할 수 있을까요? policy의 변화를 제어하는 어떤 파라미터가 있다면 이 파라미터를 조정하여 policy를 변화시킬 수 있습니다. policy x에 해당하는 파라미터값과 policy y에 해당하는 파라미터값의 차이가 policy의 변화량이라고 할 수 있습니다. 이와 같은 방식으로 policy의 변화를 모델링하기 위하여 파라미터 $\\theta$를 이용하여 policy를 $\\pi_{\\theta}$로 표현할 수 있습니다. 최적의 policy를 찾기 위하여 expected return $E\\left[R|\\theta\\right]$이 최대화되도록 parameter를 조정합니다. 이를 간단한 수식으로 정리하면 다음과 같습니다.\n\n$$\n\\pi^* = \\arg \\max\\limits_\\pi  E\\left[ {\\left. R \\right|\\pi } \\right] \\to {\\text{original problem} } \n\\\\\\\\  {\\Downarrow} \\\\\\\\\n{\\text{policy parameterization by } } \\pi_{\\theta}: \\Theta  \\to \\Pi \\\\\\\\ {\\Downarrow} $$\n\n$$\\pi^* = \\arg \\max\\limits_{\\theta}  E\\left[ {\\left. R \\right|\\theta } \\right] \\to {\\text{policy search problem} }\n$$\n\n<br>\n## 1.3 How to Obtain the Expected Return\nexpected return을 최대화하는 방향으로 policy를 update한다고 하였습니다. 그렇다면 expected return은 어떻게 구할 수 있을까요? 여기에도 크게 두 가지 방법이 있습니다. 첫 번째는 deterministic approximation으로 Markov decision process의 dynamics를 모델링한 후 수식을 통해 구하는 것입니다. 두 번째 방법은 monte carlo estimation으로 dynamics에 대한 모델을 하지 않고 많은 sample들을 얻은 후 empirical하게 expected return을 계산하는 방법입니다. 어느 방법이 더 좋을지는 풀고자 하는 문제의 특성에 따라 다를 것입니다. dynamics에 대한 모델이 어렵거나 변화가 큰 경우에는 두 번째 방법이 좀 더 현실적이지만 gradient를 구하는 것은 더 어렵습니다. 결국 gradient를 esimate하는방법을 고안해야 합니다. 가장 유명한 gradient estimation 방법이 1992년 R. J. Williams에 의해서 제안된 [REINFORCE](https://link.springer.com/content/pdf/10.1007/BF00992696.pdf) 기법입니다. REINFORCE는 Monte Carlo estimate 또는 likelihood-ratio estimate라고 부르는 방법을 이용합니다. 이 방법에 대해서 좀 더 알아볼까요?\n\n### 1.3.1 Monte Carlo Gradient Estimation\n다음과 같은 parameter $\\theta$를 가지는 random variable $X$가 있습니다: $X:\\Omega\\mapsto\\mathcal{X}$. 그리고 이 $x$에 대한 함수 $f$가 있습니다: $f:\\mathcal{X}\\mapsto\\mathbb{R}$. expected return처럼 $E[f(x)]$를 최대화하고자 합니다. 이를 위해서는 $\\nabla_{\\theta}E[f(x)]$를 구해야 합니다. 이 때 log derivate trick을 이용하여 다음과 같이 수식을 변형시킬 수 있습니다.\n\n$$\n\\begin{align}\n\\nabla_{\\theta} E_{p(x;\\theta)}\\left[ {f(x)} \\right] \n&= \\nabla_\\theta\\int{f\\left( x \\right)p\\left( {x;\\theta} \\right) dx}\n\\\\\\\\\n&= \\int { {\\nabla_\\theta }p\\left( {x;\\theta } \\right)f\\left( x \\right)dx} \n\\\\\\\\\n&= \\int {\\frac{ {p\\left( {x;\\theta } \\right)} }{ {p\\left( {x;\\theta } \\right)} }{\\nabla_\\theta }p\\left( {x;\\theta } \\right)f\\left( x \\right)dx} \n\\\\\\\\\n&= \\int {p\\left( {x;\\theta } \\right){\\nabla_\\theta }\\log p\\left( {x;\\theta } \\right)f\\left( x \\right)dx} \n\\\\\\\\\n&= E_{p(x;\\theta)}[f(x)\\nabla_\\theta \\log p({x;\\theta})]\n\\end{align}\n$$\n\n이제 expectation은 많은 샘플들을 모아서 평균을 취하는 Monte Carlo 기법으로 근사화할 수 있습니다.\n\n$${\\nabla_{\\theta} }{E_{p\\left( {x;\\theta } \\right)} }\\left[ {f\\left( x \\right)} \\right] = \\frac{1}{N}\\sum_{n=1}^N f\\left(X_n\\right)\\nabla_\\theta\\log p\\left(X_n;\\theta\\right)$$\n\n이 방법을 사용하기 위해서 필요한 조건은 $\\log p\\left(X_n;\\theta\\right)$가 미분가능해야 한다는 것 뿐입니다. 그러나 이 방법은 얻은 샘플들에 의존하기 때문에 경우에 따라서 큰 variance를 가질 수 있습니다.\n\n<br><br>\n\n# 2. Policy Gradient Methods\n\n이제 본격적으로 policy gradient 기법에 대해서 알아보겠습니다.\n\n<br>\n## 2.1 System Model\n\n논문에서 사용하는 수학기호(notation)와 가정들을 설명하는 시스템모델은 다음과 같습니다.\n\n* Markov decision process (MDP) at each time $t\\in{0,1,2,...}$\n* $S_t\\in\\mathcal{S}$: state\n* $A_t\\in\\mathcal{A}$: action\n* $r_t\\in \\mathcal{R}$: reward\n* $\\theta\\in\\mathbb{R}^l$: vector of policy parameters, for $l\\ll\\left|\\mathcal{S}\\right|$\n* $\\mathcal{P} _{s s'}^a = \\Pr[S _{t+1}=s' \\vert S_t=s,A_t=a]$: state transition probabilities\n* $\\mathcal{R} _{s s'}^a = E[R _{t+1}\\vert S_t=s,A_t=a]$: expected reward\n* $\\pi(s,a,\\theta)=\\Pr[A_t=a|S_t=s,\\theta]$: policy, shortened as $\\pi(s,a)$ or $\\pi(a|s)$\n* $\\theta$: policy parameter들의 vector  \n* $\\rho$: 해당 policy들의 성능을 나타내는 척도 (예. average reward per step)  \n* $\\alpha$: positive-definite한 step size \n\n<br>\n## 2.2 Policy Gradent Approach\npolicy gradient는 stochastic policy를 자체적인 파리미터를 가진 function approximator를 이용해서 근사화시킵니다.\n\n\npolicy parameter는 gradient에 비례하여 업데이트됩니다.\n\n$$\\Delta\\theta \\approx \\alpha\\frac{\\partial\\rho}{\\partial\\theta}$$\n\n$\\rho$는 $\\theta$에 대하여 미분가능해야겠죠? 위와 같이 업데이트하면 local optimal policy로 수렴합니다. 논문의 가장 중요한 contribution은 특정 조건을 만족하는 function approximator를 이용하여 경험(experience, sample)을 축적하고 이것들을 이용하여 위의 gradient를 unbiased estimate할 수 있음을 증명한 것입니다.\n\n먼저 $\\rho$, 즉, reward를 표현하는 두 가지 방법에 대해서 알아보겠습니다.\n\n<br>\n## 2.3 Average Reward Formulation\nAverage reward formulation은 시간의 흐름에 따른 reward를 표현한다기보다는 모든 시간의 reward를 평균을 내서 표현하는 방법입니다.\n\n* Long‐term expected reward per step\n$$\\rho(\\pi)=\\lim_{n\\to\\infty}\\frac{1}{n}E[r_1+r_2+\\cdots+r_n|\\pi] = \\sum_s d^\\pi(s)\\sum_a\\pi(s,a)\\mathcal{R}_s^a$$\n\n    - $d^\\pi(s): \\lim_{t\\to\\infty}\\Pr[S_t=s|s_0,\\pi]$: stationary distribution of states under $\\pi$\n    - $d^\\pi(s)$가 존재한다고 가정합니다. 그리고 모든 policy에 대해서 $d^\\pi(s)$는 $s_0$와 independent합니다.\n    - 위의 (limit수식과 summation수식 사이의) 등식은 왜 성립할까요? Time average와 Ensemble average가 같다는 뜻으로 [ergodic](https://en.wikipedia.org/wiki/Ergodicity)한 system에서 성립합니다.\n\n* Value of a state-action pair given a policy\n$$Q^{\\pi(s,a)} = \\sum_{t=1}^\\infty E[r_t - \\rho(\\pi)|S_0=s, A_0=a, \\pi]$$\n    - 위의 Q-function의 표현식이 정의인지 유도된 것인지는 이 포스트를 작성하는 저희도 아직 확실히 모르겠습니다. 다만, $r_t$들을 더하면 무한대로 발산할 가능성이 매우 높은데 $\\rho$를 빼줌으로해서 어찌보면 평균 reward로부터의 차이를 더하는 것이라고 할 수 있고 이것은 bound된 값일 가능성이 더 높기 때문에 Q-function을 이렇게 표현하는 것이 더 안전하다고 할 수 있습니다. 이 부분에 대해서 좋은 의견이 있으시면 피드백을 주시면 감사하겠습니다!\n\n* State-value function\n$$V^\\pi(s) = \\sum_{a} \\pi(s,a)Q^\\pi(s,a)$$\n\n<br>\n## 2.4 Start-State Formulation\nstart-state formulation은 시간의 흐름에 따라 감소하는 reward들을 표현합니다.\n\n* Long-term expected reward per step with a designated start state $S_0$\n$$\\rho(\\pi) = E\\left[\\left.\\sum_{t=1}^\\infty \\gamma^{t-1}r_t\\right|S_0,\\pi\\right]$$\n    - $\\gamma\\in\\left[0,1\\right]$: discount rate\n\n* Value of a state‐action pair given a policy\n$$Q^\\pi(s,a) = E\\left[\\left.\\sum_{k=1}^\\infty \\gamma^{k-1} r_{t+k}\\right|S_t=s, A_t=a, \\pi\\right]$$\n\n* Alternative form of $Q^\\pi(s,a)$\n$$\nQ^\\pi(s,a) = R_s^a + \\gamma\\sum _{s'} \\mathcal{P} _{s s'}^a V^{\\pi}(s')\n$$\n\n<br>\n## 2.5 Policy Gradient Theorem\n논문의 중요한 결과인 Theorem 1은 다음과 같습니다.\n\n**Theorem 1 (Policy Gradient)** *For any MDP, in either the average‐reward or start‐state formulations,*\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)$$\n\n놀라운 부분은 expected return의 gradient를 취할 때 $\\frac{\\partial d^\\pi(s)}{\\partial\\theta}$를 구하지 않아도 된다는 점입니다. 즉, policy의 변화가 state distribution에 영향을 주지 않는다는 것입니다. 이것은 다시 말하면 아래 수식과 같이 우리는 $\\frac{\\partial\\rho}{\\partial\\theta}$에 대한 unbiased estimator를 구할 수 있다는 뜻입니다.\n\n$$E_s\\left[\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)\\right] = \\frac{\\partial\\rho}{\\partial\\theta}$$\n\n이로인해서 우리는 gradient를 sampling을 통해서 추정할 수 있습니다. 그렇지만 이것은 sample이 아주 많을 때만 성립합니다!\n\n또 한 가지 문제는 우리는 $Q^\\pi(s,a)$의 정확한 값을 알 수 없다는 것 입니다. 이것을 estimate하기 위해서 현재의 return값 $R_t$를 이용할 수 있습니다.\n\n<br>\n## 2.6 Proof of Policy Gradient Theorem\n증명을 살펴보겠습니다.\n\n* average-reward formulation을 이용할 때의 증명은 아래와 같습니다.\n\n$$\n\\begin{align}\n\\frac{\\partial V^\\pi(s)}{\\partial\\theta} &\\overset{\\underset{\\mathrm{def} }{} }{=}\\frac{\\partial}{\\partial\\theta}\\sum_a \\pi(s,a)Q^\\pi(s,a)\n\\\\\\\\\n&=\\sum_a \\frac{\\partial}{\\partial\\theta}\\left[\\pi(s,a)Q^\\pi(s,a)\\right]\\\\\\\\\n&=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\frac{\\partial Q^\\pi(s,a)}{\\partial\\theta}\\right]\\\\\\\\\n&=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\frac{\\partial}{\\partial\\theta}\\left[R_s^a -\\rho(\\pi) + \\sum_{s'}\\mathcal{P}_{s s'}^a V^{\\pi}(s')\\right]\\right]\\\\\\\\\n&=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\left[-\\frac{\\partial\\rho}{\\partial\\theta} + \\sum_{s'}\\mathcal{P}_{s s'}^a \\frac{\\partial V^{\\pi}(s')}{\\partial\\theta}\\right]\\right]\\\\\\\\\n\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\sum_{s'}\\mathcal{P}_{s s'}^a \\frac{\\partial V^{\\pi}(s')}{\\partial\\theta}\\right] - \\frac{\\partial V^\\pi(s)}{\\partial\\theta}\n\\end{align}\n$$\n\n양변에 stationary distribution에 대한 평균을 취합니다.\n$$\n\\begin{align}\n\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+ \\sum_s d^\\pi(s)\\sum_a\\pi(s,a)\\sum_{s'}\\mathcal{P}_{s s'}^a \\frac{\\partial V^{\\pi}(s')}{\\partial\\theta} \n\\\\\\\\\n&- \\sum_s d^\\pi(s)\\frac{\\partial V^\\pi(s)}{\\partial\\theta}\n\\end{align}\n$$\n\n모든 state $s$에서 모든 state $s'$으로 이동하는 것은 state $s'$에 존재할 확률이므로 다음과 같이 표현할 수 있습니다.\n\n$$\n\\begin{align}\n\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+ \\sum_s d^\\pi(s') \\frac{\\partial V^{\\pi}(s')}{\\partial\\theta} - \\sum_s d^\\pi(s)\\frac{\\partial V^\\pi(s)}{\\partial\\theta}\n\\end{align}\n$$\n\n뒤의 두 항은 같은 식이므로 0이 됩니다. 따라서 다음과 같이 표현됩니다.\n$$\n\\begin{align}\n\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)\n\\end{align}\n$$\n\n$\\frac{\\partial\\rho}{\\partial\\theta}$는 reward의 파라미터에 대한 미분값입니다. reward 자체가 이미 모든 state에 대한 평균값이므로 $s$의 함수가 아닙니다.  $s$에 대한 함수가 아니므로 $\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}=\\frac{\\partial\\rho}{\\partial\\theta}\\sum_s d^\\pi(s)=\\frac{\\partial\\rho}{\\partial\\theta}$입니다. 여기서 $d^\\pi(s)$는 state에 대한 확률값이죠. 모든 state에 대해서 이 값을 다 더하면 1이 됩니다.\n이를 이용해서 위의 식을 다시 표현하면 아래와 같고 이를 통해 증명이 완성됩니다.\n\n$$\n\\begin{align}\n\\therefore\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)\n\\end{align}\n$$\n\n* 다음으로 start-state formulation에 대한 증명입니다.\n\nstart-state formulation은 이 논문에서도 나오고 서튼책에서도 나오는데, 서튼책의 증명이 더 설명이 자세합니다. 그래서 논문에 있는 증명이 아닌 서튼책에 있는 증명으로 설명드리겠습니다. 다만, notation이 약간 다른데, 큰 문제 없이 이해하실 수 있으므로 별도의 설명은 하지 않겠습니다. (서튼책 [Link](https://drive.google.com/file/d/1xeUDVGWGUUv1-ccUMAZHJLej2C7aAFWY/view))\n\n$$\\nabla{v_\\pi}(s)=\\nabla\\big[\\sum_a\\pi(a|s)q_\\pi(s,a)\\big]$$ \n\n다음으로 $(f\\cdot g)'=f'\\cdot g+f\\cdot g'$ (product rule of calculus)에 의해 다음과 같이 나타낼 수 있습니다.\n$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\nabla q_\\pi(s,a)\\big]$$  \n\n이어서 $p(s',r|s,a) := Pr[S_t=s', R_t=r|S_{t-1}=s,A_{t-1}=a ]$ (서튼책 3.2 공식) 에 의해 다음과 같이 나타낼 수 있습니다.\n\n$$=\\sum_a\\big[\\nabla\\pi(a|s)q_{\\pi}(s,a)+\\pi(a|s)\\nabla\\big[\\sum_{s', r}p(s',r|s,a)(r+v_{\\pi}(s'))\\big]\\big]$$  \n\n계속해서 $p(s'|s,a)=\\sum_{r\\in R}p(s',r|s,a)$ (서튼책 3.4 공식) 에 의해 다음과 같이 나타낼 수 있습니다.\n$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\sum_{s'}p(s'|s,a)\\nabla v_\\pi(s') \\big]$$ \n\n여기서 $\\nabla v_\\pi(s')$ 부분을 unrolling을 하면 다음과 같이 나타낼 수 있습니다.\n$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\sum_{s'}p(s'|s,a)\\sum_{a'}\\big[\\nabla\\pi(a'|s')q_\\pi(s',a')\\\\\\\\+\\pi(a'|s')\\sum_{s''}p(s''|s',a')\\nabla v_\\pi(s''))\\big] \\big]$$ \n\n$\\nabla v_\\pi(s'')$에 대해서 unrolling을 하고 또 나오는 다른 항에 대해서도 계속 반복하다보면 다음과 같이 나타낼 수 있습니다.\n$$=\\sum_{x\\in S}\\sum_{k=0}^{\\infty}\\Pr(s\\rightarrow x,k,\\pi)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$\n\n여기에서 $\\Pr(s\\rightarrow x,k,\\pi)$ 는 policy $\\pi$ 에 대해 state $s$에서 state $x$까지 $k$ step만큼 움직일 때의 변환 확률입니다. 따라서 위의 수식을 아래처럼 다시 나타낼 수 있습니다.\n <!-- \\begin{align}  -->\n$$\\nabla J(\\theta)=\\nabla v_\\pi (s_0)$$ \n$$=\\sum_s(\\sum_{k=0}^{\\infty}\\Pr(s_0\\rightarrow s,k,\\pi))\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$\n$$=\\sum_s \\eta(s) \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$ \n<!-- \\end{align} -->\n- 서튼책에서 $\\eta(s)$는 다음과 같이 표현할 수 있습니다.\n    - $\\eta(s)$ = 평균적으로 하나의 episode마다 state에서 머무른 time step의 수 (page 199)\n- 논문에서는 $\\eta(s)$를 다음과 같이 표현합니다.\n    - $d^\\pi(s)=\\sum_{t=0}^{\\infty}\\gamma^t Pr(s_t=s|s_0,\\pi)$ \n    ($\\gamma=1$ is allowed only in episodic tasks = discounted weighting of states)\n\n여기서부터가 논문에는 없는 내용입니다. 위의 최종 수식을 다시 한 번 적으면 아래와 같습니다.\n$$\\sum_s \\eta(s) \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$\n\n위의 수식을 아래와 같이 바꿀 수 있습니다.\n$$\\sum_{s'} \\eta(s') \\cdot\\sum_s \\frac{\\eta(s)}{\\sum_{s'}\\eta(s')} \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$\n\n그러면 이것을 새로운 기호로 다시 나타낼 수 있습니다.\n$$\\sum_{s'} \\eta(s') \\cdot\\sum_s \\mu(s)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$ \n- 여기서 $\\mu(s)$ 는 state distribution이라고 하며, weight를 state distribution으로 바꿔주는 과정이라고 생각할 수 있습니다. 쉽게 말해 어떠한 state에 agent가 머무르는 확률입니다.\n\n위의 수식은 아래의 수식과 비례합니다. 따라서 최종 형태는 다음과 같습니다.\n$$\\propto \\sum_s \\mu(s)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$\n\n<br><br>\n\n# 3. Policy Gradient with Approximation\n이 장에서는 앞서 다뤘던 Theorem 1 중에 $Q^\\pi$에 대해서 중점적으로 다룹니다. 어떠한 $Q^\\pi$가 학습된 function approximator로 근사된다고 합시다. \n\n$f_w$ ($f_w$는 $S × A \\rightarrow \\Re$)는 parameter $w$를 가지는 어떠한 $Q^\\pi$ 에 대한 근사치입니다. 학습된 function approximator에 대해서 생각하기 때문에, 학습된 $f_w$는 error를 최소화하는 방향으로 parameter $w$를 업데이트합니다. 그러면 다음과 같은 관계식을 생각할 수 있습니다.\n$$\\Delta w_t \\propto \\frac{\\partial}{\\partial w}\\big[ {\\hat Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]^2 \\propto \\big[{\\hat Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s_t,a_t)}{\\partial w}$$ \n(여기서 ${\\hat Q}^\\pi(s_t,a_t)$ 는 $Q^\\pi(s_t,a_t)$의 unbiased estimator $R_t$입니다.)\n\n그러면 위와 같은 수식이 local optimum에 수렴을 했을 때, 다음과 같이 나타낼 수 있습니다.\n$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$\n\n- 위의 수식에 대한 추가 설명\n    - local optimum으로 수렴을 하기 때문에 당연히 위의 수식은 0이 됩니다.\n    - 또한 stochastic policy이기 때문에 local optimum으로 수렴하려면 모든 state, action에 대해 expectation을 해야합니다. 따라서 $\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)$이 붙게 됩니다.\n\n<br>\n## 3.1 Theorem 2: Policy Gradient with Function Approximation\n\n만약 $f_w$가 아래의 등식을 만족한다고 합시다.\n$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$\n\n또한 $f_w$가 policy parameterization과 양립할 수 있다고 합시다. policy parameterization에 대한 여러가지 의미가 있는데 이 논문에서는 다음을 의미합니다.\n$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}$$\n- 위의 수식에 대한 추가 설명\n    - 이 수식은 Monte-Calro Estimation을 이용하기 위한 조건입니다.\n    - Compatibility Condition이라고 부릅니다.\n    - 우변(= $\\nabla\\log\\pi(s,a)$)으로 유도함으로써 아래의 수식과 같이 sampling을 이용한 추정이 가능해집니다.\n\n따라서 위의 두 수식을 이용하여 아래와 같이 나타낼 수 있습니다.\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}f_w(s,a)$$\n\n<br>\n## 3.2 Proof of Theorem 2\n\n$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$\n\n$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}$$\n\n위의 두 수식을 합치면 다음과 같이 나타낼 수 있습니다.\n$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}=0$$\n\n분자와 분모에 있는 $\\pi(s,a)$를 지우면 아래와 같습니다.\n$$\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]=0$$\n\n이어서 위의 수식이 0이기 때문에 policy gradient theorem(앞서 다뤘던 Theorem 1)에서 위의 수식을 뺄 수 있습니다.\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)-\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ Q^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]$$\n\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ Q^\\pi(s_t,a_t)-Q^\\pi(s_t,a_t)+f_w(s_t,a_t) \\big]$$\n\n$$\\therefore\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}f_w(s_t,a_t)$$\n\n<br><br>\n\n# 4. Application to Deriving Algorithms and Advantages\n\n<br>\n## 4.1 Application to Deriving Algorithms\nfeature의 linear combination에서 Gibbs distribution (softmax)를 하나의 policy로 생각해볼 수 있습니다.\n$$\\pi(s,a)=\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_b e^{\\theta^{T}\\phi_{sb} }}$$\n- $\\phi_{sa}$: state-action pair $s$, $a$를 나타내는 $l$-dimensional feature vector\n\ncompatible condition을 적용하면 다음과 같습니다. (4.2 Proof of application of compatible condition 참고)\n$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\frac{1}{\\pi(s,a)}=\\phi_{sa}-\\sum_b \\pi(s,b)\\phi_{sb}$$ \n\n이어서 $f_w$을 적분한 natural parameterization은 다음과 같습니다.\n$$f_w(s,a)=w^T\\big[ \\phi_{sa}-\\sum_b \\pi(s,b)\\phi_{sb}\\big]$$\n\n- 위의 수식에 대한 추가 설명\n    - $f_w$는 policy로서 같은 feature들에 대해 linear합니다.\n    - $f_w$는 각각의 state에 대해서 평균이 0입니다. ($\\sum_a\\pi(s,a)f_w(s,a)=0$)\n    - advantage function $A^\\pi(s,a)=Q^\\pi(s,a)-V^\\pi(s)$의 하나의 근사치로서 $f_w$를 생각해도 좋습니다.\n\n<br>\n## 4.2 Proof of application of compatible condition\n증명을 하기 전에 다음을 가정합니다.\n1. $(\\frac{f(x)}{g(x)})'=f'(x)\\cdot \\frac{1}{g(x)}-f(x)\\cdot \\frac{g'(x)}{g(x)^{2} }=\\frac{f'(x)\\cdot g(x)-f(x)\\cdot g'(x)}{g(x)^{2} }$\n2. $f(\\theta)=e^{\\theta^{T}\\phi_{sa} }$, $g(\\theta)=\\sum_be^{\\theta^{T}\\phi_{sb} }$\n\n$$\n\\begin{align}\n\\frac{\\partial f_w(s,a)}{\\partial w}&=\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\frac{1}{\\pi(s,a)}\n\\\\\\\\\n&=\\big(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }}\\big)'\\frac{1}{\\pi(s,a)}\n\\\\\\\\\n&=\\big[ \\phi_{sa}(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }})-(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }})\\frac{\\sum_b\\phi_{sb}e^{ {\\theta^T}\\phi_{sb} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }}\\big]\\frac{1}{\\pi(s,a)}\\\\\\\\\n&=\\big[ \\phi_{sa}\\pi(s,a)-\\pi(s,a)\\sum_b\\phi_{sb}\\pi(s,b)\\big]\\frac{1}{\\pi(s,a)}\n\\\\\\\\\n&=\\pi(s,a)\\big[ \\phi_{sa}-\\sum_b\\pi(s,b)\\phi_{sb}\\big]\\frac{1}{\\pi(s,a)}\n\\\\\\\\\n&\\therefore\\phi_{sa}-\\sum_b\\pi(s,b)\\phi_{sb}\n\\end{align}\n$$\n\n<br>\n## 4.3 Application to Advantages\nPolicy Gradient with Function Approximation Theorem (Theorem 2)는 advantage function으로 확장될 수 있습니다.\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_sd^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ f_w(s,a)+v(s) \\big]$$\n- 위의 수식에 대한 추가설명\n    - $v$ ($v$는 $S\\rightarrow\\Re$) 는 arbitrary function입니다.\n    - 이 수식은 $\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}=0$이기 때문에 가능해집니다.\n    - 즉, 이 수식은 $\\pi(s,a)$의 gradient에만 dependent하기 때문에 advantage 역할을 하는 함수들을 넣어도 아무런 상관이 없습니다.\n    - $v$의 선택은 Theorem들에 영향을 미치지 못하지만, 실질적으로 gradient estimator의 variance에 영향을 미칩니다.\n    - 이러한 문제는 전체적으로 이전의 연구에 reinforcement baseline의 사용에 있어서 유사합니다.\n    - (comment) 위의 수식에서 $f_w(s,a)+v(s)$와 Application to Deriving Algorithms의 $f_w(s,a)$와는 다른 것입니다. Application to Deriving Algorithms의 $f_w(s,a)$은 softmax에 의해 스스로 advantage function의 역할을 할 수 있습니다. 하지만 보통의 경우에는 그러지 못할 수도 있기 때문에 위의 수식처럼 $f_w(s,a)+v(s)$을 추가하여 zero mean만들어서 variance를 줄일 수 있습니다.\n\n<br><br>\n\n# 5. Convergence of Policy Iteration with Function Approximation\n\n<br>\n## 5.1 Theorem 3: Convergence of Policy Iteration with Function Approximation\n\npolicy iteration with function approximation은 locally optimal policy에 수렴합니다. $\\pi$와 $f_w$를 \n\n1. compatibility condition을 만족하는 policy와 value function에 대한\n2. 그리고 $\\max_{\\theta, s, a, i, j}\\big| \\frac{\\partial^2\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}\\big|<B<\\infty$에 대한\n\n어떠한 미분가능한 function approximator라고 합시다.\n\n- (comment) $\\max_{\\theta, s, a, i, j}\\big| \\frac{\\partial^2\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}\\big|<B<\\infty$에서 $\\pi(s,a)$라는 function은 이계미분값이 존재하고, 임의의 상수인 (Bound) B에 bound되어 있기 때문에 function의 그래프는 smooth하다고 볼 수 있습니다. (아래의 그림 중 빨간색 그래프 참고)\n\n<center> <img src=\"https://www.dropbox.com/s/xx02ejfg5ao19ps/Screen%20Shot%202018-07-10%20at%203.41.06%20PM.png?dl=1\" width=\"200\"> </center>\n\n이어서 ${\\alpha_k}$는 $\\lim_{k\\rightarrow\\infty}\\alpha_k=0$이며 $\\sum_k\\alpha_k=\\infty$를 만족하는 step-size sequence라고 합시다.\n\n그 때, bounded reward를 가진 MDP에 대해\n1. 어떠한 $\\theta_0, \\pi_k=\\pi(\\cdot,\\cdot,\\theta_k)$\n2. 그리고 $\\sum_sd^{\\pi_{k} }(s)\\sum_a\\pi_k(s,a)\\big[ Q^{\\pi_{k} }(s,a)-f_w(s,a)\\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$로 인하여 $w_k=w$, $\\theta_{k+1}=\\theta_k+\\alpha_k\\sum_sd^{\\pi_{k} }(s)\\sum_a\\frac{\\partial\\pi_k(s,a)}{\\partial\\theta}f_{w_{k} }(s,a)$\n\n으로 정의된 sequence $\\rho(\\pi_k)$는 $\\lim_{k\\rightarrow\\infty}\\frac{\\partial\\rho(\\pi_k)}{\\partial\\theta}=0$이기 때문에 수렴합니다.\n- sequence $\\rho(\\pi_k)_{k=0}^\\infty$에 대한 추가 설명\n    - $\\theta_{k+1}=\\theta_k+\\alpha_k\\sum_sd^{\\pi_{k} }(s)\\sum_a\\frac{\\partial\\pi_k(s,a)}{\\partial\\theta}f_{w_{k} }(s,a)$ 에 따라 $\\theta$가 1, 2, ..., $\\infty$로 갈텐데, 거기에 따른 objective function or performance의 sequence입니다.\n    - (comment) 굳이 sequence라는 표현이 없어도 될 것 같습니다. 어짜피 k가 $\\infty$로 가면 $\\rho(\\pi_k)$가 수렴한다는 의미이기 때문에 불필요해보입니다.\n\n<br>\n## 5.2 Proof of Theorem 3\n- Theorem 2는 $\\theta_k$ update가 gradient의 error를 최소화한다는 것을 증명했습니다.\n- $\\frac{\\partial^{2}\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}$와 MDP의 reward에서의 bound는, $\\frac{\\partial^{2}\\rho}{\\partial\\theta_i\\partial\\theta_j}$ 또한 bound된다는 것을 증명합니다.\n- step-size 필요조건 때문에 이러한 bound된 것들은 Proposition 3.5 from page 96 of Bertsekas and Tsitsiklis (1996)에 적용하기 위해 필요한 조건입니다.\n- Proposition 3.5 from page 96 of Bertsekas and Tsitsiklis (1996)은 local optimum으로 수렴한다는 것을 증명했습니다.\n- Proposition 3.5를 자세하게 설명하지는 않겠습니다. 일반적인 gradient method의 수렴성을 증명한 것이며, 이 논문의 policy gradient method도 gradient method의 일종이므로 특정 조건을 만족할 때 수렴한다는 것입니다. 수렴하는 원리는 소위 말하는 Lipschitz continuity condition을 만족하기 때문인데 다음과 같습니다. $L>0$인 임의의 상수입니다.\n$$\n\\parallel \\nabla f(r) - \\nabla f(\\bar{r}) \\parallel \\leq L \\parallel r - \\bar{r} \\parallel\n$$\n즉, 업데이트가 진행되어 나감에 따라서 gradient의 차이가 점점 줄어들게 되므로 언젠가는 0으로 수렴하게 됩니다.\n\n\n<br><br>\n\n# 6. Summary \n\n논문에서 설명한 policy gradient 기법을 요약하면 다음과 같습니다.\n\n* Original maximization problem: $\\max\\rho_{\\theta}=E\\left[R\\right]$\n* gradient ascent method를 이용한 parameter update: $\\theta_{t+1} = \\theta_{t} + \\alpha\\left.\\frac{\\partial E[R]}{\\partial\\theta}\\right|_{\\theta_t}$\n    - 그러나 우리는 gradient를 모르고, 추정하기도 어렵습니다. 왜냐하면 expectation이 안에 있기 때문입니다.\n    - 그렇다면 이것을 추정 가능한 형태로 바꿔야합니다. 다시 말해 expectation이 밖에 있는 형태로 바꾸는 것입니다.\n    - Expectation이 밖에 있으면 왜 추정이 유리할까요? 바로 Sample mean을 취하면 되기 때문입니다.\n* Approximate gradient\n    - 방법1: 논문의 Theorem 1을 이용하면 gradient와 expectation의 위치를 바꿀 수 있습니다.\n    $$\\frac{\\partial E[R]}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)$$\n    - 방법2: Log derivative trick을 이용하면 gradient와 expectation의 위치를 바꿀 수 있습니다. (Theorem 2)\n    $$\\theta_{t+1} = \\theta_{t} + \\alpha E\\left[ \\left. R\\frac{\\partial}{\\partial\\theta}\\log p_{\\theta} \\right\\vert \\theta_t\\right]$$\n    - 그리고 특정 trajectory를 따라가면서 return값을 구하고 이것을 여러 번 수행하여 sample mean을 취하면 gradient를 추정하는 것이 가능합니다.\n    - 이러한 기법을 제시한 기존의 연구가 REINFORCE입니다.\n    - 여러 Trajectory를 이용하므로 variance가 높을 수 밖에 없습니다.\n    - Advantage를 활용하거나 하는 방식으로 이후 여러 연구가 진행되었습니다.\n\n","source":"_posts/sutton-pg.md","raw":"---\ntitle: Policy Gradient Methods for Reinforcement Learning with Function Approximation\ndate: 2018-06-28 14:18:32\ntags: [\"프로젝트\", \"피지여행\"]\ncategories: 프로젝트\nauthor: 김동민, 이동민\nsubtitle: 피지여행 1번째 논문\n---\n\n<center> <img src=\"https://www.dropbox.com/s/83i4atz733ali0w/Screen%20Shot%202018-07-18%20at%2012.10.52%20AM.png?dl=1\" width=\"600\"> </center>\n\n논문 저자 : Richard S. Sutton, David McAllester, Satinder Singh, Yishay Mansour    \n논문 링크 : [NIPS](http://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf)\nProceeding : Advances in Neural Information Processing Systems (NIPS) 2000        \n정리 : 김동민, 이동민\n\n---\n\n# 1. Intro to Policy Gradient\n\n이 논문은 policy gradient (PG) 기법의 효시와도 같으며 향후 많은 파생연구를 낳은 중요한 논문입니다. 7페이지의 짧은 논문이지만 읽기에 만만한 논문은 아닙니다. 이 논문을 이해하기 위해 필요한 배경지식을 먼저 설명하고 논문을 차근차근 살펴보도록 하겠습니다.\n\n<br>\n## 1.1 Value Function Approach\n\n전통적으로 강화학습 기법은 value function을 기반으로 동작하였습니다. 특정 state에서 vaue function 또는 value function을 근사하는 함수(function approximation)를 최대화하는 action을 찾는 greedy action-selection policy가  대표적입니다.\n\n논문에서는 이러한 방법은 deterministic한 policy를 찾는 쪽으로 나아가게 되지만 종종 최적의 policy는 stochastic한 성질을 가지기 때문에 이 방법으로는 최적의 policy를 찾을 수 없다고 언급하고 있습니다. (그러나 이 논문이 나오고 나서 한참 후 David Silver를 필두로한 DeepMind의 연구진들은 high-dimensional action space를 가지는 application에서 보다 빠르게 동작하는 [deterministic policy gradient](../../../06/16/dpg/)을 개발하였습니다.) 아마도 이러한 부분은 $\\epsilon$-greedy action-selection policy로 개선될 수 있을 것입니다. 또 하나의 문제는 value function의 작은 변화로 인해서 action이 크게 변할 수도 있다는 것입니다. 이것은 알고리듬의 수렴성에 문제를 야기할 수 있습니다. 이러한 문제점을 해결하기 위하여 policy search라는 새로운 기법이 고안됩니다.\n\n<br>\n## 1.2 Policy Search\n\npolicy search는 최적의 policy $\\pi^*$를 reward로부터 직접 찾습니다. policy search 방법은 크게 두 가지로 분류할 수 있습니다. 첫 번째는 gradient-based optimization으로 policy gradient method가 이에 속합니다. 두 번째는 gradient-free optimization으로 진화(evolutionary) 연산을 이용하는 것이 이에 속합니다. 이 논문에서는 제목에서 알 수 있듯이 gradient-based optimization을 다룹니다. gradient는 변화량을 의미합니다. 특정 policy x와 또 다른 policy y가 있을 때 policy가 얼마나 많이 변화했는지 어떻게 모델링할 수 있을까요? policy의 변화를 제어하는 어떤 파라미터가 있다면 이 파라미터를 조정하여 policy를 변화시킬 수 있습니다. policy x에 해당하는 파라미터값과 policy y에 해당하는 파라미터값의 차이가 policy의 변화량이라고 할 수 있습니다. 이와 같은 방식으로 policy의 변화를 모델링하기 위하여 파라미터 $\\theta$를 이용하여 policy를 $\\pi_{\\theta}$로 표현할 수 있습니다. 최적의 policy를 찾기 위하여 expected return $E\\left[R|\\theta\\right]$이 최대화되도록 parameter를 조정합니다. 이를 간단한 수식으로 정리하면 다음과 같습니다.\n\n$$\n\\pi^* = \\arg \\max\\limits_\\pi  E\\left[ {\\left. R \\right|\\pi } \\right] \\to {\\text{original problem} } \n\\\\\\\\  {\\Downarrow} \\\\\\\\\n{\\text{policy parameterization by } } \\pi_{\\theta}: \\Theta  \\to \\Pi \\\\\\\\ {\\Downarrow} $$\n\n$$\\pi^* = \\arg \\max\\limits_{\\theta}  E\\left[ {\\left. R \\right|\\theta } \\right] \\to {\\text{policy search problem} }\n$$\n\n<br>\n## 1.3 How to Obtain the Expected Return\nexpected return을 최대화하는 방향으로 policy를 update한다고 하였습니다. 그렇다면 expected return은 어떻게 구할 수 있을까요? 여기에도 크게 두 가지 방법이 있습니다. 첫 번째는 deterministic approximation으로 Markov decision process의 dynamics를 모델링한 후 수식을 통해 구하는 것입니다. 두 번째 방법은 monte carlo estimation으로 dynamics에 대한 모델을 하지 않고 많은 sample들을 얻은 후 empirical하게 expected return을 계산하는 방법입니다. 어느 방법이 더 좋을지는 풀고자 하는 문제의 특성에 따라 다를 것입니다. dynamics에 대한 모델이 어렵거나 변화가 큰 경우에는 두 번째 방법이 좀 더 현실적이지만 gradient를 구하는 것은 더 어렵습니다. 결국 gradient를 esimate하는방법을 고안해야 합니다. 가장 유명한 gradient estimation 방법이 1992년 R. J. Williams에 의해서 제안된 [REINFORCE](https://link.springer.com/content/pdf/10.1007/BF00992696.pdf) 기법입니다. REINFORCE는 Monte Carlo estimate 또는 likelihood-ratio estimate라고 부르는 방법을 이용합니다. 이 방법에 대해서 좀 더 알아볼까요?\n\n### 1.3.1 Monte Carlo Gradient Estimation\n다음과 같은 parameter $\\theta$를 가지는 random variable $X$가 있습니다: $X:\\Omega\\mapsto\\mathcal{X}$. 그리고 이 $x$에 대한 함수 $f$가 있습니다: $f:\\mathcal{X}\\mapsto\\mathbb{R}$. expected return처럼 $E[f(x)]$를 최대화하고자 합니다. 이를 위해서는 $\\nabla_{\\theta}E[f(x)]$를 구해야 합니다. 이 때 log derivate trick을 이용하여 다음과 같이 수식을 변형시킬 수 있습니다.\n\n$$\n\\begin{align}\n\\nabla_{\\theta} E_{p(x;\\theta)}\\left[ {f(x)} \\right] \n&= \\nabla_\\theta\\int{f\\left( x \\right)p\\left( {x;\\theta} \\right) dx}\n\\\\\\\\\n&= \\int { {\\nabla_\\theta }p\\left( {x;\\theta } \\right)f\\left( x \\right)dx} \n\\\\\\\\\n&= \\int {\\frac{ {p\\left( {x;\\theta } \\right)} }{ {p\\left( {x;\\theta } \\right)} }{\\nabla_\\theta }p\\left( {x;\\theta } \\right)f\\left( x \\right)dx} \n\\\\\\\\\n&= \\int {p\\left( {x;\\theta } \\right){\\nabla_\\theta }\\log p\\left( {x;\\theta } \\right)f\\left( x \\right)dx} \n\\\\\\\\\n&= E_{p(x;\\theta)}[f(x)\\nabla_\\theta \\log p({x;\\theta})]\n\\end{align}\n$$\n\n이제 expectation은 많은 샘플들을 모아서 평균을 취하는 Monte Carlo 기법으로 근사화할 수 있습니다.\n\n$${\\nabla_{\\theta} }{E_{p\\left( {x;\\theta } \\right)} }\\left[ {f\\left( x \\right)} \\right] = \\frac{1}{N}\\sum_{n=1}^N f\\left(X_n\\right)\\nabla_\\theta\\log p\\left(X_n;\\theta\\right)$$\n\n이 방법을 사용하기 위해서 필요한 조건은 $\\log p\\left(X_n;\\theta\\right)$가 미분가능해야 한다는 것 뿐입니다. 그러나 이 방법은 얻은 샘플들에 의존하기 때문에 경우에 따라서 큰 variance를 가질 수 있습니다.\n\n<br><br>\n\n# 2. Policy Gradient Methods\n\n이제 본격적으로 policy gradient 기법에 대해서 알아보겠습니다.\n\n<br>\n## 2.1 System Model\n\n논문에서 사용하는 수학기호(notation)와 가정들을 설명하는 시스템모델은 다음과 같습니다.\n\n* Markov decision process (MDP) at each time $t\\in{0,1,2,...}$\n* $S_t\\in\\mathcal{S}$: state\n* $A_t\\in\\mathcal{A}$: action\n* $r_t\\in \\mathcal{R}$: reward\n* $\\theta\\in\\mathbb{R}^l$: vector of policy parameters, for $l\\ll\\left|\\mathcal{S}\\right|$\n* $\\mathcal{P} _{s s'}^a = \\Pr[S _{t+1}=s' \\vert S_t=s,A_t=a]$: state transition probabilities\n* $\\mathcal{R} _{s s'}^a = E[R _{t+1}\\vert S_t=s,A_t=a]$: expected reward\n* $\\pi(s,a,\\theta)=\\Pr[A_t=a|S_t=s,\\theta]$: policy, shortened as $\\pi(s,a)$ or $\\pi(a|s)$\n* $\\theta$: policy parameter들의 vector  \n* $\\rho$: 해당 policy들의 성능을 나타내는 척도 (예. average reward per step)  \n* $\\alpha$: positive-definite한 step size \n\n<br>\n## 2.2 Policy Gradent Approach\npolicy gradient는 stochastic policy를 자체적인 파리미터를 가진 function approximator를 이용해서 근사화시킵니다.\n\n\npolicy parameter는 gradient에 비례하여 업데이트됩니다.\n\n$$\\Delta\\theta \\approx \\alpha\\frac{\\partial\\rho}{\\partial\\theta}$$\n\n$\\rho$는 $\\theta$에 대하여 미분가능해야겠죠? 위와 같이 업데이트하면 local optimal policy로 수렴합니다. 논문의 가장 중요한 contribution은 특정 조건을 만족하는 function approximator를 이용하여 경험(experience, sample)을 축적하고 이것들을 이용하여 위의 gradient를 unbiased estimate할 수 있음을 증명한 것입니다.\n\n먼저 $\\rho$, 즉, reward를 표현하는 두 가지 방법에 대해서 알아보겠습니다.\n\n<br>\n## 2.3 Average Reward Formulation\nAverage reward formulation은 시간의 흐름에 따른 reward를 표현한다기보다는 모든 시간의 reward를 평균을 내서 표현하는 방법입니다.\n\n* Long‐term expected reward per step\n$$\\rho(\\pi)=\\lim_{n\\to\\infty}\\frac{1}{n}E[r_1+r_2+\\cdots+r_n|\\pi] = \\sum_s d^\\pi(s)\\sum_a\\pi(s,a)\\mathcal{R}_s^a$$\n\n    - $d^\\pi(s): \\lim_{t\\to\\infty}\\Pr[S_t=s|s_0,\\pi]$: stationary distribution of states under $\\pi$\n    - $d^\\pi(s)$가 존재한다고 가정합니다. 그리고 모든 policy에 대해서 $d^\\pi(s)$는 $s_0$와 independent합니다.\n    - 위의 (limit수식과 summation수식 사이의) 등식은 왜 성립할까요? Time average와 Ensemble average가 같다는 뜻으로 [ergodic](https://en.wikipedia.org/wiki/Ergodicity)한 system에서 성립합니다.\n\n* Value of a state-action pair given a policy\n$$Q^{\\pi(s,a)} = \\sum_{t=1}^\\infty E[r_t - \\rho(\\pi)|S_0=s, A_0=a, \\pi]$$\n    - 위의 Q-function의 표현식이 정의인지 유도된 것인지는 이 포스트를 작성하는 저희도 아직 확실히 모르겠습니다. 다만, $r_t$들을 더하면 무한대로 발산할 가능성이 매우 높은데 $\\rho$를 빼줌으로해서 어찌보면 평균 reward로부터의 차이를 더하는 것이라고 할 수 있고 이것은 bound된 값일 가능성이 더 높기 때문에 Q-function을 이렇게 표현하는 것이 더 안전하다고 할 수 있습니다. 이 부분에 대해서 좋은 의견이 있으시면 피드백을 주시면 감사하겠습니다!\n\n* State-value function\n$$V^\\pi(s) = \\sum_{a} \\pi(s,a)Q^\\pi(s,a)$$\n\n<br>\n## 2.4 Start-State Formulation\nstart-state formulation은 시간의 흐름에 따라 감소하는 reward들을 표현합니다.\n\n* Long-term expected reward per step with a designated start state $S_0$\n$$\\rho(\\pi) = E\\left[\\left.\\sum_{t=1}^\\infty \\gamma^{t-1}r_t\\right|S_0,\\pi\\right]$$\n    - $\\gamma\\in\\left[0,1\\right]$: discount rate\n\n* Value of a state‐action pair given a policy\n$$Q^\\pi(s,a) = E\\left[\\left.\\sum_{k=1}^\\infty \\gamma^{k-1} r_{t+k}\\right|S_t=s, A_t=a, \\pi\\right]$$\n\n* Alternative form of $Q^\\pi(s,a)$\n$$\nQ^\\pi(s,a) = R_s^a + \\gamma\\sum _{s'} \\mathcal{P} _{s s'}^a V^{\\pi}(s')\n$$\n\n<br>\n## 2.5 Policy Gradient Theorem\n논문의 중요한 결과인 Theorem 1은 다음과 같습니다.\n\n**Theorem 1 (Policy Gradient)** *For any MDP, in either the average‐reward or start‐state formulations,*\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)$$\n\n놀라운 부분은 expected return의 gradient를 취할 때 $\\frac{\\partial d^\\pi(s)}{\\partial\\theta}$를 구하지 않아도 된다는 점입니다. 즉, policy의 변화가 state distribution에 영향을 주지 않는다는 것입니다. 이것은 다시 말하면 아래 수식과 같이 우리는 $\\frac{\\partial\\rho}{\\partial\\theta}$에 대한 unbiased estimator를 구할 수 있다는 뜻입니다.\n\n$$E_s\\left[\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)\\right] = \\frac{\\partial\\rho}{\\partial\\theta}$$\n\n이로인해서 우리는 gradient를 sampling을 통해서 추정할 수 있습니다. 그렇지만 이것은 sample이 아주 많을 때만 성립합니다!\n\n또 한 가지 문제는 우리는 $Q^\\pi(s,a)$의 정확한 값을 알 수 없다는 것 입니다. 이것을 estimate하기 위해서 현재의 return값 $R_t$를 이용할 수 있습니다.\n\n<br>\n## 2.6 Proof of Policy Gradient Theorem\n증명을 살펴보겠습니다.\n\n* average-reward formulation을 이용할 때의 증명은 아래와 같습니다.\n\n$$\n\\begin{align}\n\\frac{\\partial V^\\pi(s)}{\\partial\\theta} &\\overset{\\underset{\\mathrm{def} }{} }{=}\\frac{\\partial}{\\partial\\theta}\\sum_a \\pi(s,a)Q^\\pi(s,a)\n\\\\\\\\\n&=\\sum_a \\frac{\\partial}{\\partial\\theta}\\left[\\pi(s,a)Q^\\pi(s,a)\\right]\\\\\\\\\n&=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\frac{\\partial Q^\\pi(s,a)}{\\partial\\theta}\\right]\\\\\\\\\n&=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\frac{\\partial}{\\partial\\theta}\\left[R_s^a -\\rho(\\pi) + \\sum_{s'}\\mathcal{P}_{s s'}^a V^{\\pi}(s')\\right]\\right]\\\\\\\\\n&=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\left[-\\frac{\\partial\\rho}{\\partial\\theta} + \\sum_{s'}\\mathcal{P}_{s s'}^a \\frac{\\partial V^{\\pi}(s')}{\\partial\\theta}\\right]\\right]\\\\\\\\\n\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\sum_{s'}\\mathcal{P}_{s s'}^a \\frac{\\partial V^{\\pi}(s')}{\\partial\\theta}\\right] - \\frac{\\partial V^\\pi(s)}{\\partial\\theta}\n\\end{align}\n$$\n\n양변에 stationary distribution에 대한 평균을 취합니다.\n$$\n\\begin{align}\n\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+ \\sum_s d^\\pi(s)\\sum_a\\pi(s,a)\\sum_{s'}\\mathcal{P}_{s s'}^a \\frac{\\partial V^{\\pi}(s')}{\\partial\\theta} \n\\\\\\\\\n&- \\sum_s d^\\pi(s)\\frac{\\partial V^\\pi(s)}{\\partial\\theta}\n\\end{align}\n$$\n\n모든 state $s$에서 모든 state $s'$으로 이동하는 것은 state $s'$에 존재할 확률이므로 다음과 같이 표현할 수 있습니다.\n\n$$\n\\begin{align}\n\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+ \\sum_s d^\\pi(s') \\frac{\\partial V^{\\pi}(s')}{\\partial\\theta} - \\sum_s d^\\pi(s)\\frac{\\partial V^\\pi(s)}{\\partial\\theta}\n\\end{align}\n$$\n\n뒤의 두 항은 같은 식이므로 0이 됩니다. 따라서 다음과 같이 표현됩니다.\n$$\n\\begin{align}\n\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)\n\\end{align}\n$$\n\n$\\frac{\\partial\\rho}{\\partial\\theta}$는 reward의 파라미터에 대한 미분값입니다. reward 자체가 이미 모든 state에 대한 평균값이므로 $s$의 함수가 아닙니다.  $s$에 대한 함수가 아니므로 $\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}=\\frac{\\partial\\rho}{\\partial\\theta}\\sum_s d^\\pi(s)=\\frac{\\partial\\rho}{\\partial\\theta}$입니다. 여기서 $d^\\pi(s)$는 state에 대한 확률값이죠. 모든 state에 대해서 이 값을 다 더하면 1이 됩니다.\n이를 이용해서 위의 식을 다시 표현하면 아래와 같고 이를 통해 증명이 완성됩니다.\n\n$$\n\\begin{align}\n\\therefore\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)\n\\end{align}\n$$\n\n* 다음으로 start-state formulation에 대한 증명입니다.\n\nstart-state formulation은 이 논문에서도 나오고 서튼책에서도 나오는데, 서튼책의 증명이 더 설명이 자세합니다. 그래서 논문에 있는 증명이 아닌 서튼책에 있는 증명으로 설명드리겠습니다. 다만, notation이 약간 다른데, 큰 문제 없이 이해하실 수 있으므로 별도의 설명은 하지 않겠습니다. (서튼책 [Link](https://drive.google.com/file/d/1xeUDVGWGUUv1-ccUMAZHJLej2C7aAFWY/view))\n\n$$\\nabla{v_\\pi}(s)=\\nabla\\big[\\sum_a\\pi(a|s)q_\\pi(s,a)\\big]$$ \n\n다음으로 $(f\\cdot g)'=f'\\cdot g+f\\cdot g'$ (product rule of calculus)에 의해 다음과 같이 나타낼 수 있습니다.\n$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\nabla q_\\pi(s,a)\\big]$$  \n\n이어서 $p(s',r|s,a) := Pr[S_t=s', R_t=r|S_{t-1}=s,A_{t-1}=a ]$ (서튼책 3.2 공식) 에 의해 다음과 같이 나타낼 수 있습니다.\n\n$$=\\sum_a\\big[\\nabla\\pi(a|s)q_{\\pi}(s,a)+\\pi(a|s)\\nabla\\big[\\sum_{s', r}p(s',r|s,a)(r+v_{\\pi}(s'))\\big]\\big]$$  \n\n계속해서 $p(s'|s,a)=\\sum_{r\\in R}p(s',r|s,a)$ (서튼책 3.4 공식) 에 의해 다음과 같이 나타낼 수 있습니다.\n$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\sum_{s'}p(s'|s,a)\\nabla v_\\pi(s') \\big]$$ \n\n여기서 $\\nabla v_\\pi(s')$ 부분을 unrolling을 하면 다음과 같이 나타낼 수 있습니다.\n$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\sum_{s'}p(s'|s,a)\\sum_{a'}\\big[\\nabla\\pi(a'|s')q_\\pi(s',a')\\\\\\\\+\\pi(a'|s')\\sum_{s''}p(s''|s',a')\\nabla v_\\pi(s''))\\big] \\big]$$ \n\n$\\nabla v_\\pi(s'')$에 대해서 unrolling을 하고 또 나오는 다른 항에 대해서도 계속 반복하다보면 다음과 같이 나타낼 수 있습니다.\n$$=\\sum_{x\\in S}\\sum_{k=0}^{\\infty}\\Pr(s\\rightarrow x,k,\\pi)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$\n\n여기에서 $\\Pr(s\\rightarrow x,k,\\pi)$ 는 policy $\\pi$ 에 대해 state $s$에서 state $x$까지 $k$ step만큼 움직일 때의 변환 확률입니다. 따라서 위의 수식을 아래처럼 다시 나타낼 수 있습니다.\n <!-- \\begin{align}  -->\n$$\\nabla J(\\theta)=\\nabla v_\\pi (s_0)$$ \n$$=\\sum_s(\\sum_{k=0}^{\\infty}\\Pr(s_0\\rightarrow s,k,\\pi))\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$\n$$=\\sum_s \\eta(s) \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$ \n<!-- \\end{align} -->\n- 서튼책에서 $\\eta(s)$는 다음과 같이 표현할 수 있습니다.\n    - $\\eta(s)$ = 평균적으로 하나의 episode마다 state에서 머무른 time step의 수 (page 199)\n- 논문에서는 $\\eta(s)$를 다음과 같이 표현합니다.\n    - $d^\\pi(s)=\\sum_{t=0}^{\\infty}\\gamma^t Pr(s_t=s|s_0,\\pi)$ \n    ($\\gamma=1$ is allowed only in episodic tasks = discounted weighting of states)\n\n여기서부터가 논문에는 없는 내용입니다. 위의 최종 수식을 다시 한 번 적으면 아래와 같습니다.\n$$\\sum_s \\eta(s) \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$\n\n위의 수식을 아래와 같이 바꿀 수 있습니다.\n$$\\sum_{s'} \\eta(s') \\cdot\\sum_s \\frac{\\eta(s)}{\\sum_{s'}\\eta(s')} \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$\n\n그러면 이것을 새로운 기호로 다시 나타낼 수 있습니다.\n$$\\sum_{s'} \\eta(s') \\cdot\\sum_s \\mu(s)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$ \n- 여기서 $\\mu(s)$ 는 state distribution이라고 하며, weight를 state distribution으로 바꿔주는 과정이라고 생각할 수 있습니다. 쉽게 말해 어떠한 state에 agent가 머무르는 확률입니다.\n\n위의 수식은 아래의 수식과 비례합니다. 따라서 최종 형태는 다음과 같습니다.\n$$\\propto \\sum_s \\mu(s)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$\n\n<br><br>\n\n# 3. Policy Gradient with Approximation\n이 장에서는 앞서 다뤘던 Theorem 1 중에 $Q^\\pi$에 대해서 중점적으로 다룹니다. 어떠한 $Q^\\pi$가 학습된 function approximator로 근사된다고 합시다. \n\n$f_w$ ($f_w$는 $S × A \\rightarrow \\Re$)는 parameter $w$를 가지는 어떠한 $Q^\\pi$ 에 대한 근사치입니다. 학습된 function approximator에 대해서 생각하기 때문에, 학습된 $f_w$는 error를 최소화하는 방향으로 parameter $w$를 업데이트합니다. 그러면 다음과 같은 관계식을 생각할 수 있습니다.\n$$\\Delta w_t \\propto \\frac{\\partial}{\\partial w}\\big[ {\\hat Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]^2 \\propto \\big[{\\hat Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s_t,a_t)}{\\partial w}$$ \n(여기서 ${\\hat Q}^\\pi(s_t,a_t)$ 는 $Q^\\pi(s_t,a_t)$의 unbiased estimator $R_t$입니다.)\n\n그러면 위와 같은 수식이 local optimum에 수렴을 했을 때, 다음과 같이 나타낼 수 있습니다.\n$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$\n\n- 위의 수식에 대한 추가 설명\n    - local optimum으로 수렴을 하기 때문에 당연히 위의 수식은 0이 됩니다.\n    - 또한 stochastic policy이기 때문에 local optimum으로 수렴하려면 모든 state, action에 대해 expectation을 해야합니다. 따라서 $\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)$이 붙게 됩니다.\n\n<br>\n## 3.1 Theorem 2: Policy Gradient with Function Approximation\n\n만약 $f_w$가 아래의 등식을 만족한다고 합시다.\n$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$\n\n또한 $f_w$가 policy parameterization과 양립할 수 있다고 합시다. policy parameterization에 대한 여러가지 의미가 있는데 이 논문에서는 다음을 의미합니다.\n$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}$$\n- 위의 수식에 대한 추가 설명\n    - 이 수식은 Monte-Calro Estimation을 이용하기 위한 조건입니다.\n    - Compatibility Condition이라고 부릅니다.\n    - 우변(= $\\nabla\\log\\pi(s,a)$)으로 유도함으로써 아래의 수식과 같이 sampling을 이용한 추정이 가능해집니다.\n\n따라서 위의 두 수식을 이용하여 아래와 같이 나타낼 수 있습니다.\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}f_w(s,a)$$\n\n<br>\n## 3.2 Proof of Theorem 2\n\n$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$\n\n$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}$$\n\n위의 두 수식을 합치면 다음과 같이 나타낼 수 있습니다.\n$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}=0$$\n\n분자와 분모에 있는 $\\pi(s,a)$를 지우면 아래와 같습니다.\n$$\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]=0$$\n\n이어서 위의 수식이 0이기 때문에 policy gradient theorem(앞서 다뤘던 Theorem 1)에서 위의 수식을 뺄 수 있습니다.\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)-\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ Q^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]$$\n\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ Q^\\pi(s_t,a_t)-Q^\\pi(s_t,a_t)+f_w(s_t,a_t) \\big]$$\n\n$$\\therefore\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}f_w(s_t,a_t)$$\n\n<br><br>\n\n# 4. Application to Deriving Algorithms and Advantages\n\n<br>\n## 4.1 Application to Deriving Algorithms\nfeature의 linear combination에서 Gibbs distribution (softmax)를 하나의 policy로 생각해볼 수 있습니다.\n$$\\pi(s,a)=\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_b e^{\\theta^{T}\\phi_{sb} }}$$\n- $\\phi_{sa}$: state-action pair $s$, $a$를 나타내는 $l$-dimensional feature vector\n\ncompatible condition을 적용하면 다음과 같습니다. (4.2 Proof of application of compatible condition 참고)\n$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\frac{1}{\\pi(s,a)}=\\phi_{sa}-\\sum_b \\pi(s,b)\\phi_{sb}$$ \n\n이어서 $f_w$을 적분한 natural parameterization은 다음과 같습니다.\n$$f_w(s,a)=w^T\\big[ \\phi_{sa}-\\sum_b \\pi(s,b)\\phi_{sb}\\big]$$\n\n- 위의 수식에 대한 추가 설명\n    - $f_w$는 policy로서 같은 feature들에 대해 linear합니다.\n    - $f_w$는 각각의 state에 대해서 평균이 0입니다. ($\\sum_a\\pi(s,a)f_w(s,a)=0$)\n    - advantage function $A^\\pi(s,a)=Q^\\pi(s,a)-V^\\pi(s)$의 하나의 근사치로서 $f_w$를 생각해도 좋습니다.\n\n<br>\n## 4.2 Proof of application of compatible condition\n증명을 하기 전에 다음을 가정합니다.\n1. $(\\frac{f(x)}{g(x)})'=f'(x)\\cdot \\frac{1}{g(x)}-f(x)\\cdot \\frac{g'(x)}{g(x)^{2} }=\\frac{f'(x)\\cdot g(x)-f(x)\\cdot g'(x)}{g(x)^{2} }$\n2. $f(\\theta)=e^{\\theta^{T}\\phi_{sa} }$, $g(\\theta)=\\sum_be^{\\theta^{T}\\phi_{sb} }$\n\n$$\n\\begin{align}\n\\frac{\\partial f_w(s,a)}{\\partial w}&=\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\frac{1}{\\pi(s,a)}\n\\\\\\\\\n&=\\big(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }}\\big)'\\frac{1}{\\pi(s,a)}\n\\\\\\\\\n&=\\big[ \\phi_{sa}(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }})-(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }})\\frac{\\sum_b\\phi_{sb}e^{ {\\theta^T}\\phi_{sb} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }}\\big]\\frac{1}{\\pi(s,a)}\\\\\\\\\n&=\\big[ \\phi_{sa}\\pi(s,a)-\\pi(s,a)\\sum_b\\phi_{sb}\\pi(s,b)\\big]\\frac{1}{\\pi(s,a)}\n\\\\\\\\\n&=\\pi(s,a)\\big[ \\phi_{sa}-\\sum_b\\pi(s,b)\\phi_{sb}\\big]\\frac{1}{\\pi(s,a)}\n\\\\\\\\\n&\\therefore\\phi_{sa}-\\sum_b\\pi(s,b)\\phi_{sb}\n\\end{align}\n$$\n\n<br>\n## 4.3 Application to Advantages\nPolicy Gradient with Function Approximation Theorem (Theorem 2)는 advantage function으로 확장될 수 있습니다.\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_sd^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ f_w(s,a)+v(s) \\big]$$\n- 위의 수식에 대한 추가설명\n    - $v$ ($v$는 $S\\rightarrow\\Re$) 는 arbitrary function입니다.\n    - 이 수식은 $\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}=0$이기 때문에 가능해집니다.\n    - 즉, 이 수식은 $\\pi(s,a)$의 gradient에만 dependent하기 때문에 advantage 역할을 하는 함수들을 넣어도 아무런 상관이 없습니다.\n    - $v$의 선택은 Theorem들에 영향을 미치지 못하지만, 실질적으로 gradient estimator의 variance에 영향을 미칩니다.\n    - 이러한 문제는 전체적으로 이전의 연구에 reinforcement baseline의 사용에 있어서 유사합니다.\n    - (comment) 위의 수식에서 $f_w(s,a)+v(s)$와 Application to Deriving Algorithms의 $f_w(s,a)$와는 다른 것입니다. Application to Deriving Algorithms의 $f_w(s,a)$은 softmax에 의해 스스로 advantage function의 역할을 할 수 있습니다. 하지만 보통의 경우에는 그러지 못할 수도 있기 때문에 위의 수식처럼 $f_w(s,a)+v(s)$을 추가하여 zero mean만들어서 variance를 줄일 수 있습니다.\n\n<br><br>\n\n# 5. Convergence of Policy Iteration with Function Approximation\n\n<br>\n## 5.1 Theorem 3: Convergence of Policy Iteration with Function Approximation\n\npolicy iteration with function approximation은 locally optimal policy에 수렴합니다. $\\pi$와 $f_w$를 \n\n1. compatibility condition을 만족하는 policy와 value function에 대한\n2. 그리고 $\\max_{\\theta, s, a, i, j}\\big| \\frac{\\partial^2\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}\\big|<B<\\infty$에 대한\n\n어떠한 미분가능한 function approximator라고 합시다.\n\n- (comment) $\\max_{\\theta, s, a, i, j}\\big| \\frac{\\partial^2\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}\\big|<B<\\infty$에서 $\\pi(s,a)$라는 function은 이계미분값이 존재하고, 임의의 상수인 (Bound) B에 bound되어 있기 때문에 function의 그래프는 smooth하다고 볼 수 있습니다. (아래의 그림 중 빨간색 그래프 참고)\n\n<center> <img src=\"https://www.dropbox.com/s/xx02ejfg5ao19ps/Screen%20Shot%202018-07-10%20at%203.41.06%20PM.png?dl=1\" width=\"200\"> </center>\n\n이어서 ${\\alpha_k}$는 $\\lim_{k\\rightarrow\\infty}\\alpha_k=0$이며 $\\sum_k\\alpha_k=\\infty$를 만족하는 step-size sequence라고 합시다.\n\n그 때, bounded reward를 가진 MDP에 대해\n1. 어떠한 $\\theta_0, \\pi_k=\\pi(\\cdot,\\cdot,\\theta_k)$\n2. 그리고 $\\sum_sd^{\\pi_{k} }(s)\\sum_a\\pi_k(s,a)\\big[ Q^{\\pi_{k} }(s,a)-f_w(s,a)\\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$로 인하여 $w_k=w$, $\\theta_{k+1}=\\theta_k+\\alpha_k\\sum_sd^{\\pi_{k} }(s)\\sum_a\\frac{\\partial\\pi_k(s,a)}{\\partial\\theta}f_{w_{k} }(s,a)$\n\n으로 정의된 sequence $\\rho(\\pi_k)$는 $\\lim_{k\\rightarrow\\infty}\\frac{\\partial\\rho(\\pi_k)}{\\partial\\theta}=0$이기 때문에 수렴합니다.\n- sequence $\\rho(\\pi_k)_{k=0}^\\infty$에 대한 추가 설명\n    - $\\theta_{k+1}=\\theta_k+\\alpha_k\\sum_sd^{\\pi_{k} }(s)\\sum_a\\frac{\\partial\\pi_k(s,a)}{\\partial\\theta}f_{w_{k} }(s,a)$ 에 따라 $\\theta$가 1, 2, ..., $\\infty$로 갈텐데, 거기에 따른 objective function or performance의 sequence입니다.\n    - (comment) 굳이 sequence라는 표현이 없어도 될 것 같습니다. 어짜피 k가 $\\infty$로 가면 $\\rho(\\pi_k)$가 수렴한다는 의미이기 때문에 불필요해보입니다.\n\n<br>\n## 5.2 Proof of Theorem 3\n- Theorem 2는 $\\theta_k$ update가 gradient의 error를 최소화한다는 것을 증명했습니다.\n- $\\frac{\\partial^{2}\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}$와 MDP의 reward에서의 bound는, $\\frac{\\partial^{2}\\rho}{\\partial\\theta_i\\partial\\theta_j}$ 또한 bound된다는 것을 증명합니다.\n- step-size 필요조건 때문에 이러한 bound된 것들은 Proposition 3.5 from page 96 of Bertsekas and Tsitsiklis (1996)에 적용하기 위해 필요한 조건입니다.\n- Proposition 3.5 from page 96 of Bertsekas and Tsitsiklis (1996)은 local optimum으로 수렴한다는 것을 증명했습니다.\n- Proposition 3.5를 자세하게 설명하지는 않겠습니다. 일반적인 gradient method의 수렴성을 증명한 것이며, 이 논문의 policy gradient method도 gradient method의 일종이므로 특정 조건을 만족할 때 수렴한다는 것입니다. 수렴하는 원리는 소위 말하는 Lipschitz continuity condition을 만족하기 때문인데 다음과 같습니다. $L>0$인 임의의 상수입니다.\n$$\n\\parallel \\nabla f(r) - \\nabla f(\\bar{r}) \\parallel \\leq L \\parallel r - \\bar{r} \\parallel\n$$\n즉, 업데이트가 진행되어 나감에 따라서 gradient의 차이가 점점 줄어들게 되므로 언젠가는 0으로 수렴하게 됩니다.\n\n\n<br><br>\n\n# 6. Summary \n\n논문에서 설명한 policy gradient 기법을 요약하면 다음과 같습니다.\n\n* Original maximization problem: $\\max\\rho_{\\theta}=E\\left[R\\right]$\n* gradient ascent method를 이용한 parameter update: $\\theta_{t+1} = \\theta_{t} + \\alpha\\left.\\frac{\\partial E[R]}{\\partial\\theta}\\right|_{\\theta_t}$\n    - 그러나 우리는 gradient를 모르고, 추정하기도 어렵습니다. 왜냐하면 expectation이 안에 있기 때문입니다.\n    - 그렇다면 이것을 추정 가능한 형태로 바꿔야합니다. 다시 말해 expectation이 밖에 있는 형태로 바꾸는 것입니다.\n    - Expectation이 밖에 있으면 왜 추정이 유리할까요? 바로 Sample mean을 취하면 되기 때문입니다.\n* Approximate gradient\n    - 방법1: 논문의 Theorem 1을 이용하면 gradient와 expectation의 위치를 바꿀 수 있습니다.\n    $$\\frac{\\partial E[R]}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)$$\n    - 방법2: Log derivative trick을 이용하면 gradient와 expectation의 위치를 바꿀 수 있습니다. (Theorem 2)\n    $$\\theta_{t+1} = \\theta_{t} + \\alpha E\\left[ \\left. R\\frac{\\partial}{\\partial\\theta}\\log p_{\\theta} \\right\\vert \\theta_t\\right]$$\n    - 그리고 특정 trajectory를 따라가면서 return값을 구하고 이것을 여러 번 수행하여 sample mean을 취하면 gradient를 추정하는 것이 가능합니다.\n    - 이러한 기법을 제시한 기존의 연구가 REINFORCE입니다.\n    - 여러 Trajectory를 이용하므로 variance가 높을 수 밖에 없습니다.\n    - Advantage를 활용하거나 하는 방식으로 이후 여러 연구가 진행되었습니다.\n\n","slug":"sutton-pg","published":1,"updated":"2018-07-24T06:47:30.552Z","_id":"cjiinnj6n0002j28aigqpmv52","comments":1,"layout":"post","photos":[],"link":"","content":"<center> <img src=\"https://www.dropbox.com/s/83i4atz733ali0w/Screen%20Shot%202018-07-18%20at%2012.10.52%20AM.png?dl=1\" width=\"600\"> </center>\n\n<p>논문 저자 : Richard S. Sutton, David McAllester, Satinder Singh, Yishay Mansour<br>논문 링크 : <a href=\"http://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf\" target=\"_blank\" rel=\"noopener\">NIPS</a><br>Proceeding : Advances in Neural Information Processing Systems (NIPS) 2000<br>정리 : 김동민, 이동민</p>\n<hr>\n<h1 id=\"1-Intro-to-Policy-Gradient\"><a href=\"#1-Intro-to-Policy-Gradient\" class=\"headerlink\" title=\"1. Intro to Policy Gradient\"></a>1. Intro to Policy Gradient</h1><p>이 논문은 policy gradient (PG) 기법의 효시와도 같으며 향후 많은 파생연구를 낳은 중요한 논문입니다. 7페이지의 짧은 논문이지만 읽기에 만만한 논문은 아닙니다. 이 논문을 이해하기 위해 필요한 배경지식을 먼저 설명하고 논문을 차근차근 살펴보도록 하겠습니다.</p>\n<p><br></p>\n<h2 id=\"1-1-Value-Function-Approach\"><a href=\"#1-1-Value-Function-Approach\" class=\"headerlink\" title=\"1.1 Value Function Approach\"></a>1.1 Value Function Approach</h2><p>전통적으로 강화학습 기법은 value function을 기반으로 동작하였습니다. 특정 state에서 vaue function 또는 value function을 근사하는 함수(function approximation)를 최대화하는 action을 찾는 greedy action-selection policy가  대표적입니다.</p>\n<p>논문에서는 이러한 방법은 deterministic한 policy를 찾는 쪽으로 나아가게 되지만 종종 최적의 policy는 stochastic한 성질을 가지기 때문에 이 방법으로는 최적의 policy를 찾을 수 없다고 언급하고 있습니다. (그러나 이 논문이 나오고 나서 한참 후 David Silver를 필두로한 DeepMind의 연구진들은 high-dimensional action space를 가지는 application에서 보다 빠르게 동작하는 <a href=\"../../../06/16/dpg/\">deterministic policy gradient</a>을 개발하였습니다.) 아마도 이러한 부분은 $\\epsilon$-greedy action-selection policy로 개선될 수 있을 것입니다. 또 하나의 문제는 value function의 작은 변화로 인해서 action이 크게 변할 수도 있다는 것입니다. 이것은 알고리듬의 수렴성에 문제를 야기할 수 있습니다. 이러한 문제점을 해결하기 위하여 policy search라는 새로운 기법이 고안됩니다.</p>\n<p><br></p>\n<h2 id=\"1-2-Policy-Search\"><a href=\"#1-2-Policy-Search\" class=\"headerlink\" title=\"1.2 Policy Search\"></a>1.2 Policy Search</h2><p>policy search는 최적의 policy $\\pi^*$를 reward로부터 직접 찾습니다. policy search 방법은 크게 두 가지로 분류할 수 있습니다. 첫 번째는 gradient-based optimization으로 policy gradient method가 이에 속합니다. 두 번째는 gradient-free optimization으로 진화(evolutionary) 연산을 이용하는 것이 이에 속합니다. 이 논문에서는 제목에서 알 수 있듯이 gradient-based optimization을 다룹니다. gradient는 변화량을 의미합니다. 특정 policy x와 또 다른 policy y가 있을 때 policy가 얼마나 많이 변화했는지 어떻게 모델링할 수 있을까요? policy의 변화를 제어하는 어떤 파라미터가 있다면 이 파라미터를 조정하여 policy를 변화시킬 수 있습니다. policy x에 해당하는 파라미터값과 policy y에 해당하는 파라미터값의 차이가 policy의 변화량이라고 할 수 있습니다. 이와 같은 방식으로 policy의 변화를 모델링하기 위하여 파라미터 $\\theta$를 이용하여 policy를 $\\pi_{\\theta}$로 표현할 수 있습니다. 최적의 policy를 찾기 위하여 expected return $E\\left[R|\\theta\\right]$이 최대화되도록 parameter를 조정합니다. 이를 간단한 수식으로 정리하면 다음과 같습니다.</p>\n<p>$$<br>\\pi^* = \\arg \\max\\limits_\\pi  E\\left[ {\\left. R \\right|\\pi } \\right] \\to {\\text{original problem} }<br>\\\\  {\\Downarrow} \\\\<br>{\\text{policy parameterization by } } \\pi_{\\theta}: \\Theta  \\to \\Pi \\\\ {\\Downarrow} $$</p>\n<p>$$\\pi^* = \\arg \\max\\limits_{\\theta}  E\\left[ {\\left. R \\right|\\theta } \\right] \\to {\\text{policy search problem} }<br>$$</p>\n<p><br></p>\n<h2 id=\"1-3-How-to-Obtain-the-Expected-Return\"><a href=\"#1-3-How-to-Obtain-the-Expected-Return\" class=\"headerlink\" title=\"1.3 How to Obtain the Expected Return\"></a>1.3 How to Obtain the Expected Return</h2><p>expected return을 최대화하는 방향으로 policy를 update한다고 하였습니다. 그렇다면 expected return은 어떻게 구할 수 있을까요? 여기에도 크게 두 가지 방법이 있습니다. 첫 번째는 deterministic approximation으로 Markov decision process의 dynamics를 모델링한 후 수식을 통해 구하는 것입니다. 두 번째 방법은 monte carlo estimation으로 dynamics에 대한 모델을 하지 않고 많은 sample들을 얻은 후 empirical하게 expected return을 계산하는 방법입니다. 어느 방법이 더 좋을지는 풀고자 하는 문제의 특성에 따라 다를 것입니다. dynamics에 대한 모델이 어렵거나 변화가 큰 경우에는 두 번째 방법이 좀 더 현실적이지만 gradient를 구하는 것은 더 어렵습니다. 결국 gradient를 esimate하는방법을 고안해야 합니다. 가장 유명한 gradient estimation 방법이 1992년 R. J. Williams에 의해서 제안된 <a href=\"https://link.springer.com/content/pdf/10.1007/BF00992696.pdf\" target=\"_blank\" rel=\"noopener\">REINFORCE</a> 기법입니다. REINFORCE는 Monte Carlo estimate 또는 likelihood-ratio estimate라고 부르는 방법을 이용합니다. 이 방법에 대해서 좀 더 알아볼까요?</p>\n<h3 id=\"1-3-1-Monte-Carlo-Gradient-Estimation\"><a href=\"#1-3-1-Monte-Carlo-Gradient-Estimation\" class=\"headerlink\" title=\"1.3.1 Monte Carlo Gradient Estimation\"></a>1.3.1 Monte Carlo Gradient Estimation</h3><p>다음과 같은 parameter $\\theta$를 가지는 random variable $X$가 있습니다: $X:\\Omega\\mapsto\\mathcal{X}$. 그리고 이 $x$에 대한 함수 $f$가 있습니다: $f:\\mathcal{X}\\mapsto\\mathbb{R}$. expected return처럼 $E[f(x)]$를 최대화하고자 합니다. 이를 위해서는 $\\nabla_{\\theta}E[f(x)]$를 구해야 합니다. 이 때 log derivate trick을 이용하여 다음과 같이 수식을 변형시킬 수 있습니다.</p>\n<p>$$<br>\\begin{align}<br>\\nabla_{\\theta} E_{p(x;\\theta)}\\left[ {f(x)} \\right]<br>&amp;= \\nabla_\\theta\\int{f\\left( x \\right)p\\left( {x;\\theta} \\right) dx}<br>\\\\<br>&amp;= \\int { {\\nabla_\\theta }p\\left( {x;\\theta } \\right)f\\left( x \\right)dx}<br>\\\\<br>&amp;= \\int {\\frac{ {p\\left( {x;\\theta } \\right)} }{ {p\\left( {x;\\theta } \\right)} }{\\nabla_\\theta }p\\left( {x;\\theta } \\right)f\\left( x \\right)dx}<br>\\\\<br>&amp;= \\int {p\\left( {x;\\theta } \\right){\\nabla_\\theta }\\log p\\left( {x;\\theta } \\right)f\\left( x \\right)dx}<br>\\\\<br>&amp;= E_{p(x;\\theta)}[f(x)\\nabla_\\theta \\log p({x;\\theta})]<br>\\end{align}<br>$$</p>\n<p>이제 expectation은 많은 샘플들을 모아서 평균을 취하는 Monte Carlo 기법으로 근사화할 수 있습니다.</p>\n<p>$${\\nabla_{\\theta} }{E_{p\\left( {x;\\theta } \\right)} }\\left[ {f\\left( x \\right)} \\right] = \\frac{1}{N}\\sum_{n=1}^N f\\left(X_n\\right)\\nabla_\\theta\\log p\\left(X_n;\\theta\\right)$$</p>\n<p>이 방법을 사용하기 위해서 필요한 조건은 $\\log p\\left(X_n;\\theta\\right)$가 미분가능해야 한다는 것 뿐입니다. 그러나 이 방법은 얻은 샘플들에 의존하기 때문에 경우에 따라서 큰 variance를 가질 수 있습니다.</p>\n<p><br><br></p>\n<h1 id=\"2-Policy-Gradient-Methods\"><a href=\"#2-Policy-Gradient-Methods\" class=\"headerlink\" title=\"2. Policy Gradient Methods\"></a>2. Policy Gradient Methods</h1><p>이제 본격적으로 policy gradient 기법에 대해서 알아보겠습니다.</p>\n<p><br></p>\n<h2 id=\"2-1-System-Model\"><a href=\"#2-1-System-Model\" class=\"headerlink\" title=\"2.1 System Model\"></a>2.1 System Model</h2><p>논문에서 사용하는 수학기호(notation)와 가정들을 설명하는 시스템모델은 다음과 같습니다.</p>\n<ul>\n<li>Markov decision process (MDP) at each time $t\\in{0,1,2,…}$</li>\n<li>$S_t\\in\\mathcal{S}$: state</li>\n<li>$A_t\\in\\mathcal{A}$: action</li>\n<li>$r_t\\in \\mathcal{R}$: reward</li>\n<li>$\\theta\\in\\mathbb{R}^l$: vector of policy parameters, for $l\\ll\\left|\\mathcal{S}\\right|$</li>\n<li>$\\mathcal{P} _{s s’}^a = \\Pr[S _{t+1}=s’ \\vert S_t=s,A_t=a]$: state transition probabilities</li>\n<li>$\\mathcal{R} _{s s’}^a = E[R _{t+1}\\vert S_t=s,A_t=a]$: expected reward</li>\n<li>$\\pi(s,a,\\theta)=\\Pr[A_t=a|S_t=s,\\theta]$: policy, shortened as $\\pi(s,a)$ or $\\pi(a|s)$</li>\n<li>$\\theta$: policy parameter들의 vector  </li>\n<li>$\\rho$: 해당 policy들의 성능을 나타내는 척도 (예. average reward per step)  </li>\n<li>$\\alpha$: positive-definite한 step size </li>\n</ul>\n<p><br></p>\n<h2 id=\"2-2-Policy-Gradent-Approach\"><a href=\"#2-2-Policy-Gradent-Approach\" class=\"headerlink\" title=\"2.2 Policy Gradent Approach\"></a>2.2 Policy Gradent Approach</h2><p>policy gradient는 stochastic policy를 자체적인 파리미터를 가진 function approximator를 이용해서 근사화시킵니다.</p>\n<p>policy parameter는 gradient에 비례하여 업데이트됩니다.</p>\n<p>$$\\Delta\\theta \\approx \\alpha\\frac{\\partial\\rho}{\\partial\\theta}$$</p>\n<p>$\\rho$는 $\\theta$에 대하여 미분가능해야겠죠? 위와 같이 업데이트하면 local optimal policy로 수렴합니다. 논문의 가장 중요한 contribution은 특정 조건을 만족하는 function approximator를 이용하여 경험(experience, sample)을 축적하고 이것들을 이용하여 위의 gradient를 unbiased estimate할 수 있음을 증명한 것입니다.</p>\n<p>먼저 $\\rho$, 즉, reward를 표현하는 두 가지 방법에 대해서 알아보겠습니다.</p>\n<p><br></p>\n<h2 id=\"2-3-Average-Reward-Formulation\"><a href=\"#2-3-Average-Reward-Formulation\" class=\"headerlink\" title=\"2.3 Average Reward Formulation\"></a>2.3 Average Reward Formulation</h2><p>Average reward formulation은 시간의 흐름에 따른 reward를 표현한다기보다는 모든 시간의 reward를 평균을 내서 표현하는 방법입니다.</p>\n<ul>\n<li><p>Long‐term expected reward per step<br>$$\\rho(\\pi)=\\lim_{n\\to\\infty}\\frac{1}{n}E[r_1+r_2+\\cdots+r_n|\\pi] = \\sum_s d^\\pi(s)\\sum_a\\pi(s,a)\\mathcal{R}_s^a$$</p>\n<ul>\n<li>$d^\\pi(s): \\lim_{t\\to\\infty}\\Pr[S_t=s|s_0,\\pi]$: stationary distribution of states under $\\pi$</li>\n<li>$d^\\pi(s)$가 존재한다고 가정합니다. 그리고 모든 policy에 대해서 $d^\\pi(s)$는 $s_0$와 independent합니다.</li>\n<li>위의 (limit수식과 summation수식 사이의) 등식은 왜 성립할까요? Time average와 Ensemble average가 같다는 뜻으로 <a href=\"https://en.wikipedia.org/wiki/Ergodicity\" target=\"_blank\" rel=\"noopener\">ergodic</a>한 system에서 성립합니다.</li>\n</ul>\n</li>\n<li><p>Value of a state-action pair given a policy<br>$$Q^{\\pi(s,a)} = \\sum_{t=1}^\\infty E[r_t - \\rho(\\pi)|S_0=s, A_0=a, \\pi]$$</p>\n<ul>\n<li>위의 Q-function의 표현식이 정의인지 유도된 것인지는 이 포스트를 작성하는 저희도 아직 확실히 모르겠습니다. 다만, $r_t$들을 더하면 무한대로 발산할 가능성이 매우 높은데 $\\rho$를 빼줌으로해서 어찌보면 평균 reward로부터의 차이를 더하는 것이라고 할 수 있고 이것은 bound된 값일 가능성이 더 높기 때문에 Q-function을 이렇게 표현하는 것이 더 안전하다고 할 수 있습니다. 이 부분에 대해서 좋은 의견이 있으시면 피드백을 주시면 감사하겠습니다!</li>\n</ul>\n</li>\n<li><p>State-value function<br>$$V^\\pi(s) = \\sum_{a} \\pi(s,a)Q^\\pi(s,a)$$</p>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-4-Start-State-Formulation\"><a href=\"#2-4-Start-State-Formulation\" class=\"headerlink\" title=\"2.4 Start-State Formulation\"></a>2.4 Start-State Formulation</h2><p>start-state formulation은 시간의 흐름에 따라 감소하는 reward들을 표현합니다.</p>\n<ul>\n<li><p>Long-term expected reward per step with a designated start state $S_0$<br>$$\\rho(\\pi) = E\\left[\\left.\\sum_{t=1}^\\infty \\gamma^{t-1}r_t\\right|S_0,\\pi\\right]$$</p>\n<ul>\n<li>$\\gamma\\in\\left[0,1\\right]$: discount rate</li>\n</ul>\n</li>\n<li><p>Value of a state‐action pair given a policy<br>$$Q^\\pi(s,a) = E\\left[\\left.\\sum_{k=1}^\\infty \\gamma^{k-1} r_{t+k}\\right|S_t=s, A_t=a, \\pi\\right]$$</p>\n</li>\n<li><p>Alternative form of $Q^\\pi(s,a)$<br>$$<br>Q^\\pi(s,a) = R_s^a + \\gamma\\sum _{s’} \\mathcal{P} _{s s’}^a V^{\\pi}(s’)<br>$$</p>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-5-Policy-Gradient-Theorem\"><a href=\"#2-5-Policy-Gradient-Theorem\" class=\"headerlink\" title=\"2.5 Policy Gradient Theorem\"></a>2.5 Policy Gradient Theorem</h2><p>논문의 중요한 결과인 Theorem 1은 다음과 같습니다.</p>\n<p><strong>Theorem 1 (Policy Gradient)</strong> <em>For any MDP, in either the average‐reward or start‐state formulations,</em><br>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)$$</p>\n<p>놀라운 부분은 expected return의 gradient를 취할 때 $\\frac{\\partial d^\\pi(s)}{\\partial\\theta}$를 구하지 않아도 된다는 점입니다. 즉, policy의 변화가 state distribution에 영향을 주지 않는다는 것입니다. 이것은 다시 말하면 아래 수식과 같이 우리는 $\\frac{\\partial\\rho}{\\partial\\theta}$에 대한 unbiased estimator를 구할 수 있다는 뜻입니다.</p>\n<p>$$E_s\\left[\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)\\right] = \\frac{\\partial\\rho}{\\partial\\theta}$$</p>\n<p>이로인해서 우리는 gradient를 sampling을 통해서 추정할 수 있습니다. 그렇지만 이것은 sample이 아주 많을 때만 성립합니다!</p>\n<p>또 한 가지 문제는 우리는 $Q^\\pi(s,a)$의 정확한 값을 알 수 없다는 것 입니다. 이것을 estimate하기 위해서 현재의 return값 $R_t$를 이용할 수 있습니다.</p>\n<p><br></p>\n<h2 id=\"2-6-Proof-of-Policy-Gradient-Theorem\"><a href=\"#2-6-Proof-of-Policy-Gradient-Theorem\" class=\"headerlink\" title=\"2.6 Proof of Policy Gradient Theorem\"></a>2.6 Proof of Policy Gradient Theorem</h2><p>증명을 살펴보겠습니다.</p>\n<ul>\n<li>average-reward formulation을 이용할 때의 증명은 아래와 같습니다.</li>\n</ul>\n<p>$$<br>\\begin{align}<br>\\frac{\\partial V^\\pi(s)}{\\partial\\theta} &amp;\\overset{\\underset{\\mathrm{def} }{} }{=}\\frac{\\partial}{\\partial\\theta}\\sum_a \\pi(s,a)Q^\\pi(s,a)<br>\\\\<br>&amp;=\\sum_a \\frac{\\partial}{\\partial\\theta}\\left[\\pi(s,a)Q^\\pi(s,a)\\right]\\\\<br>&amp;=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\frac{\\partial Q^\\pi(s,a)}{\\partial\\theta}\\right]\\\\<br>&amp;=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\frac{\\partial}{\\partial\\theta}\\left[R_s^a -\\rho(\\pi) + \\sum_{s’}\\mathcal{P}_{s s’}^a V^{\\pi}(s’)\\right]\\right]\\\\<br>&amp;=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\left[-\\frac{\\partial\\rho}{\\partial\\theta} + \\sum_{s’}\\mathcal{P}_{s s’}^a \\frac{\\partial V^{\\pi}(s’)}{\\partial\\theta}\\right]\\right]\\\\<br>\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\sum_{s’}\\mathcal{P}_{s s’}^a \\frac{\\partial V^{\\pi}(s’)}{\\partial\\theta}\\right] - \\frac{\\partial V^\\pi(s)}{\\partial\\theta}<br>\\end{align}<br>$$</p>\n<p>양변에 stationary distribution에 대한 평균을 취합니다.<br>$$<br>\\begin{align}<br>\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+ \\sum_s d^\\pi(s)\\sum_a\\pi(s,a)\\sum_{s’}\\mathcal{P}_{s s’}^a \\frac{\\partial V^{\\pi}(s’)}{\\partial\\theta}<br>\\\\<br>&amp;- \\sum_s d^\\pi(s)\\frac{\\partial V^\\pi(s)}{\\partial\\theta}<br>\\end{align}<br>$$</p>\n<p>모든 state $s$에서 모든 state $s’$으로 이동하는 것은 state $s’$에 존재할 확률이므로 다음과 같이 표현할 수 있습니다.</p>\n<p>$$<br>\\begin{align}<br>\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+ \\sum_s d^\\pi(s’) \\frac{\\partial V^{\\pi}(s’)}{\\partial\\theta} - \\sum_s d^\\pi(s)\\frac{\\partial V^\\pi(s)}{\\partial\\theta}<br>\\end{align}<br>$$</p>\n<p>뒤의 두 항은 같은 식이므로 0이 됩니다. 따라서 다음과 같이 표현됩니다.<br>$$<br>\\begin{align}<br>\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)<br>\\end{align}<br>$$</p>\n<p>$\\frac{\\partial\\rho}{\\partial\\theta}$는 reward의 파라미터에 대한 미분값입니다. reward 자체가 이미 모든 state에 대한 평균값이므로 $s$의 함수가 아닙니다.  $s$에 대한 함수가 아니므로 $\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}=\\frac{\\partial\\rho}{\\partial\\theta}\\sum_s d^\\pi(s)=\\frac{\\partial\\rho}{\\partial\\theta}$입니다. 여기서 $d^\\pi(s)$는 state에 대한 확률값이죠. 모든 state에 대해서 이 값을 다 더하면 1이 됩니다.<br>이를 이용해서 위의 식을 다시 표현하면 아래와 같고 이를 통해 증명이 완성됩니다.</p>\n<p>$$<br>\\begin{align}<br>\\therefore\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)<br>\\end{align}<br>$$</p>\n<ul>\n<li>다음으로 start-state formulation에 대한 증명입니다.</li>\n</ul>\n<p>start-state formulation은 이 논문에서도 나오고 서튼책에서도 나오는데, 서튼책의 증명이 더 설명이 자세합니다. 그래서 논문에 있는 증명이 아닌 서튼책에 있는 증명으로 설명드리겠습니다. 다만, notation이 약간 다른데, 큰 문제 없이 이해하실 수 있으므로 별도의 설명은 하지 않겠습니다. (서튼책 <a href=\"https://drive.google.com/file/d/1xeUDVGWGUUv1-ccUMAZHJLej2C7aAFWY/view\" target=\"_blank\" rel=\"noopener\">Link</a>)</p>\n<p>$$\\nabla{v_\\pi}(s)=\\nabla\\big[\\sum_a\\pi(a|s)q_\\pi(s,a)\\big]$$ </p>\n<p>다음으로 $(f\\cdot g)’=f’\\cdot g+f\\cdot g’$ (product rule of calculus)에 의해 다음과 같이 나타낼 수 있습니다.<br>$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\nabla q_\\pi(s,a)\\big]$$  </p>\n<p>이어서 $p(s’,r|s,a) := Pr[S_t=s’, R_t=r|S_{t-1}=s,A_{t-1}=a ]$ (서튼책 3.2 공식) 에 의해 다음과 같이 나타낼 수 있습니다.</p>\n<p>$$=\\sum_a\\big[\\nabla\\pi(a|s)q_{\\pi}(s,a)+\\pi(a|s)\\nabla\\big[\\sum_{s’, r}p(s’,r|s,a)(r+v_{\\pi}(s’))\\big]\\big]$$  </p>\n<p>계속해서 $p(s’|s,a)=\\sum_{r\\in R}p(s’,r|s,a)$ (서튼책 3.4 공식) 에 의해 다음과 같이 나타낼 수 있습니다.<br>$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\sum_{s’}p(s’|s,a)\\nabla v_\\pi(s’) \\big]$$ </p>\n<p>여기서 $\\nabla v_\\pi(s’)$ 부분을 unrolling을 하면 다음과 같이 나타낼 수 있습니다.<br>$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\sum_{s’}p(s’|s,a)\\sum_{a’}\\big[\\nabla\\pi(a’|s’)q_\\pi(s’,a’)\\\\+\\pi(a’|s’)\\sum_{s’’}p(s’’|s’,a’)\\nabla v_\\pi(s’’))\\big] \\big]$$ </p>\n<p>$\\nabla v_\\pi(s’’)$에 대해서 unrolling을 하고 또 나오는 다른 항에 대해서도 계속 반복하다보면 다음과 같이 나타낼 수 있습니다.<br>$$=\\sum_{x\\in S}\\sum_{k=0}^{\\infty}\\Pr(s\\rightarrow x,k,\\pi)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$</p>\n<p>여기에서 $\\Pr(s\\rightarrow x,k,\\pi)$ 는 policy $\\pi$ 에 대해 state $s$에서 state $x$까지 $k$ step만큼 움직일 때의 변환 확률입니다. 따라서 위의 수식을 아래처럼 다시 나타낼 수 있습니다.<br> <!-- \\begin{align}  --><br>$$\\nabla J(\\theta)=\\nabla v_\\pi (s_0)$$<br>$$=\\sum_s(\\sum_{k=0}^{\\infty}\\Pr(s_0\\rightarrow s,k,\\pi))\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$<br>$$=\\sum_s \\eta(s) \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$<br><!-- \\end{align} --></p>\n<ul>\n<li>서튼책에서 $\\eta(s)$는 다음과 같이 표현할 수 있습니다.<ul>\n<li>$\\eta(s)$ = 평균적으로 하나의 episode마다 state에서 머무른 time step의 수 (page 199)</li>\n</ul>\n</li>\n<li>논문에서는 $\\eta(s)$를 다음과 같이 표현합니다.<ul>\n<li>$d^\\pi(s)=\\sum_{t=0}^{\\infty}\\gamma^t Pr(s_t=s|s_0,\\pi)$<br>($\\gamma=1$ is allowed only in episodic tasks = discounted weighting of states)</li>\n</ul>\n</li>\n</ul>\n<p>여기서부터가 논문에는 없는 내용입니다. 위의 최종 수식을 다시 한 번 적으면 아래와 같습니다.<br>$$\\sum_s \\eta(s) \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$</p>\n<p>위의 수식을 아래와 같이 바꿀 수 있습니다.<br>$$\\sum_{s’} \\eta(s’) \\cdot\\sum_s \\frac{\\eta(s)}{\\sum_{s’}\\eta(s’)} \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$</p>\n<p>그러면 이것을 새로운 기호로 다시 나타낼 수 있습니다.<br>$$\\sum_{s’} \\eta(s’) \\cdot\\sum_s \\mu(s)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$ </p>\n<ul>\n<li>여기서 $\\mu(s)$ 는 state distribution이라고 하며, weight를 state distribution으로 바꿔주는 과정이라고 생각할 수 있습니다. 쉽게 말해 어떠한 state에 agent가 머무르는 확률입니다.</li>\n</ul>\n<p>위의 수식은 아래의 수식과 비례합니다. 따라서 최종 형태는 다음과 같습니다.<br>$$\\propto \\sum_s \\mu(s)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$</p>\n<p><br><br></p>\n<h1 id=\"3-Policy-Gradient-with-Approximation\"><a href=\"#3-Policy-Gradient-with-Approximation\" class=\"headerlink\" title=\"3. Policy Gradient with Approximation\"></a>3. Policy Gradient with Approximation</h1><p>이 장에서는 앞서 다뤘던 Theorem 1 중에 $Q^\\pi$에 대해서 중점적으로 다룹니다. 어떠한 $Q^\\pi$가 학습된 function approximator로 근사된다고 합시다. </p>\n<p>$f_w$ ($f_w$는 $S × A \\rightarrow \\Re$)는 parameter $w$를 가지는 어떠한 $Q^\\pi$ 에 대한 근사치입니다. 학습된 function approximator에 대해서 생각하기 때문에, 학습된 $f_w$는 error를 최소화하는 방향으로 parameter $w$를 업데이트합니다. 그러면 다음과 같은 관계식을 생각할 수 있습니다.<br>$$\\Delta w_t \\propto \\frac{\\partial}{\\partial w}\\big[ {\\hat Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]^2 \\propto \\big[{\\hat Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s_t,a_t)}{\\partial w}$$<br>(여기서 ${\\hat Q}^\\pi(s_t,a_t)$ 는 $Q^\\pi(s_t,a_t)$의 unbiased estimator $R_t$입니다.)</p>\n<p>그러면 위와 같은 수식이 local optimum에 수렴을 했을 때, 다음과 같이 나타낼 수 있습니다.<br>$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$</p>\n<ul>\n<li>위의 수식에 대한 추가 설명<ul>\n<li>local optimum으로 수렴을 하기 때문에 당연히 위의 수식은 0이 됩니다.</li>\n<li>또한 stochastic policy이기 때문에 local optimum으로 수렴하려면 모든 state, action에 대해 expectation을 해야합니다. 따라서 $\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)$이 붙게 됩니다.</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-1-Theorem-2-Policy-Gradient-with-Function-Approximation\"><a href=\"#3-1-Theorem-2-Policy-Gradient-with-Function-Approximation\" class=\"headerlink\" title=\"3.1 Theorem 2: Policy Gradient with Function Approximation\"></a>3.1 Theorem 2: Policy Gradient with Function Approximation</h2><p>만약 $f_w$가 아래의 등식을 만족한다고 합시다.<br>$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$</p>\n<p>또한 $f_w$가 policy parameterization과 양립할 수 있다고 합시다. policy parameterization에 대한 여러가지 의미가 있는데 이 논문에서는 다음을 의미합니다.<br>$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}$$</p>\n<ul>\n<li>위의 수식에 대한 추가 설명<ul>\n<li>이 수식은 Monte-Calro Estimation을 이용하기 위한 조건입니다.</li>\n<li>Compatibility Condition이라고 부릅니다.</li>\n<li>우변(= $\\nabla\\log\\pi(s,a)$)으로 유도함으로써 아래의 수식과 같이 sampling을 이용한 추정이 가능해집니다.</li>\n</ul>\n</li>\n</ul>\n<p>따라서 위의 두 수식을 이용하여 아래와 같이 나타낼 수 있습니다.<br>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}f_w(s,a)$$</p>\n<p><br></p>\n<h2 id=\"3-2-Proof-of-Theorem-2\"><a href=\"#3-2-Proof-of-Theorem-2\" class=\"headerlink\" title=\"3.2 Proof of Theorem 2\"></a>3.2 Proof of Theorem 2</h2><p>$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$</p>\n<p>$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}$$</p>\n<p>위의 두 수식을 합치면 다음과 같이 나타낼 수 있습니다.<br>$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}=0$$</p>\n<p>분자와 분모에 있는 $\\pi(s,a)$를 지우면 아래와 같습니다.<br>$$\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]=0$$</p>\n<p>이어서 위의 수식이 0이기 때문에 policy gradient theorem(앞서 다뤘던 Theorem 1)에서 위의 수식을 뺄 수 있습니다.<br>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)-\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ Q^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]$$</p>\n<p>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ Q^\\pi(s_t,a_t)-Q^\\pi(s_t,a_t)+f_w(s_t,a_t) \\big]$$</p>\n<p>$$\\therefore\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}f_w(s_t,a_t)$$</p>\n<p><br><br></p>\n<h1 id=\"4-Application-to-Deriving-Algorithms-and-Advantages\"><a href=\"#4-Application-to-Deriving-Algorithms-and-Advantages\" class=\"headerlink\" title=\"4. Application to Deriving Algorithms and Advantages\"></a>4. Application to Deriving Algorithms and Advantages</h1><p><br></p>\n<h2 id=\"4-1-Application-to-Deriving-Algorithms\"><a href=\"#4-1-Application-to-Deriving-Algorithms\" class=\"headerlink\" title=\"4.1 Application to Deriving Algorithms\"></a>4.1 Application to Deriving Algorithms</h2><p>feature의 linear combination에서 Gibbs distribution (softmax)를 하나의 policy로 생각해볼 수 있습니다.<br>$$\\pi(s,a)=\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_b e^{\\theta^{T}\\phi_{sb} }}$$</p>\n<ul>\n<li>$\\phi_{sa}$: state-action pair $s$, $a$를 나타내는 $l$-dimensional feature vector</li>\n</ul>\n<p>compatible condition을 적용하면 다음과 같습니다. (4.2 Proof of application of compatible condition 참고)<br>$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\frac{1}{\\pi(s,a)}=\\phi_{sa}-\\sum_b \\pi(s,b)\\phi_{sb}$$ </p>\n<p>이어서 $f_w$을 적분한 natural parameterization은 다음과 같습니다.<br>$$f_w(s,a)=w^T\\big[ \\phi_{sa}-\\sum_b \\pi(s,b)\\phi_{sb}\\big]$$</p>\n<ul>\n<li>위의 수식에 대한 추가 설명<ul>\n<li>$f_w$는 policy로서 같은 feature들에 대해 linear합니다.</li>\n<li>$f_w$는 각각의 state에 대해서 평균이 0입니다. ($\\sum_a\\pi(s,a)f_w(s,a)=0$)</li>\n<li>advantage function $A^\\pi(s,a)=Q^\\pi(s,a)-V^\\pi(s)$의 하나의 근사치로서 $f_w$를 생각해도 좋습니다.</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"4-2-Proof-of-application-of-compatible-condition\"><a href=\"#4-2-Proof-of-application-of-compatible-condition\" class=\"headerlink\" title=\"4.2 Proof of application of compatible condition\"></a>4.2 Proof of application of compatible condition</h2><p>증명을 하기 전에 다음을 가정합니다.</p>\n<ol>\n<li>$(\\frac{f(x)}{g(x)})’=f’(x)\\cdot \\frac{1}{g(x)}-f(x)\\cdot \\frac{g’(x)}{g(x)^{2} }=\\frac{f’(x)\\cdot g(x)-f(x)\\cdot g’(x)}{g(x)^{2} }$</li>\n<li>$f(\\theta)=e^{\\theta^{T}\\phi_{sa} }$, $g(\\theta)=\\sum_be^{\\theta^{T}\\phi_{sb} }$</li>\n</ol>\n<p>$$<br>\\begin{align}<br>\\frac{\\partial f_w(s,a)}{\\partial w}&amp;=\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\frac{1}{\\pi(s,a)}<br>\\\\<br>&amp;=\\big(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }}\\big)’\\frac{1}{\\pi(s,a)}<br>\\\\<br>&amp;=\\big[ \\phi_{sa}(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }})-(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }})\\frac{\\sum_b\\phi_{sb}e^{ {\\theta^T}\\phi_{sb} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }}\\big]\\frac{1}{\\pi(s,a)}\\\\<br>&amp;=\\big[ \\phi_{sa}\\pi(s,a)-\\pi(s,a)\\sum_b\\phi_{sb}\\pi(s,b)\\big]\\frac{1}{\\pi(s,a)}<br>\\\\<br>&amp;=\\pi(s,a)\\big[ \\phi_{sa}-\\sum_b\\pi(s,b)\\phi_{sb}\\big]\\frac{1}{\\pi(s,a)}<br>\\\\<br>&amp;\\therefore\\phi_{sa}-\\sum_b\\pi(s,b)\\phi_{sb}<br>\\end{align}<br>$$</p>\n<p><br></p>\n<h2 id=\"4-3-Application-to-Advantages\"><a href=\"#4-3-Application-to-Advantages\" class=\"headerlink\" title=\"4.3 Application to Advantages\"></a>4.3 Application to Advantages</h2><p>Policy Gradient with Function Approximation Theorem (Theorem 2)는 advantage function으로 확장될 수 있습니다.<br>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_sd^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ f_w(s,a)+v(s) \\big]$$</p>\n<ul>\n<li>위의 수식에 대한 추가설명<ul>\n<li>$v$ ($v$는 $S\\rightarrow\\Re$) 는 arbitrary function입니다.</li>\n<li>이 수식은 $\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}=0$이기 때문에 가능해집니다.</li>\n<li>즉, 이 수식은 $\\pi(s,a)$의 gradient에만 dependent하기 때문에 advantage 역할을 하는 함수들을 넣어도 아무런 상관이 없습니다.</li>\n<li>$v$의 선택은 Theorem들에 영향을 미치지 못하지만, 실질적으로 gradient estimator의 variance에 영향을 미칩니다.</li>\n<li>이러한 문제는 전체적으로 이전의 연구에 reinforcement baseline의 사용에 있어서 유사합니다.</li>\n<li>(comment) 위의 수식에서 $f_w(s,a)+v(s)$와 Application to Deriving Algorithms의 $f_w(s,a)$와는 다른 것입니다. Application to Deriving Algorithms의 $f_w(s,a)$은 softmax에 의해 스스로 advantage function의 역할을 할 수 있습니다. 하지만 보통의 경우에는 그러지 못할 수도 있기 때문에 위의 수식처럼 $f_w(s,a)+v(s)$을 추가하여 zero mean만들어서 variance를 줄일 수 있습니다.</li>\n</ul>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"5-Convergence-of-Policy-Iteration-with-Function-Approximation\"><a href=\"#5-Convergence-of-Policy-Iteration-with-Function-Approximation\" class=\"headerlink\" title=\"5. Convergence of Policy Iteration with Function Approximation\"></a>5. Convergence of Policy Iteration with Function Approximation</h1><p><br></p>\n<h2 id=\"5-1-Theorem-3-Convergence-of-Policy-Iteration-with-Function-Approximation\"><a href=\"#5-1-Theorem-3-Convergence-of-Policy-Iteration-with-Function-Approximation\" class=\"headerlink\" title=\"5.1 Theorem 3: Convergence of Policy Iteration with Function Approximation\"></a>5.1 Theorem 3: Convergence of Policy Iteration with Function Approximation</h2><p>policy iteration with function approximation은 locally optimal policy에 수렴합니다. $\\pi$와 $f_w$를 </p>\n<ol>\n<li>compatibility condition을 만족하는 policy와 value function에 대한</li>\n<li>그리고 $\\max_{\\theta, s, a, i, j}\\big| \\frac{\\partial^2\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}\\big|&lt;B&lt;\\infty$에 대한</li>\n</ol>\n<p>어떠한 미분가능한 function approximator라고 합시다.</p>\n<ul>\n<li>(comment) $\\max_{\\theta, s, a, i, j}\\big| \\frac{\\partial^2\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}\\big|&lt;B&lt;\\infty$에서 $\\pi(s,a)$라는 function은 이계미분값이 존재하고, 임의의 상수인 (Bound) B에 bound되어 있기 때문에 function의 그래프는 smooth하다고 볼 수 있습니다. (아래의 그림 중 빨간색 그래프 참고)</li>\n</ul>\n<center> <img src=\"https://www.dropbox.com/s/xx02ejfg5ao19ps/Screen%20Shot%202018-07-10%20at%203.41.06%20PM.png?dl=1\" width=\"200\"> </center>\n\n<p>이어서 ${\\alpha_k}$는 $\\lim_{k\\rightarrow\\infty}\\alpha_k=0$이며 $\\sum_k\\alpha_k=\\infty$를 만족하는 step-size sequence라고 합시다.</p>\n<p>그 때, bounded reward를 가진 MDP에 대해</p>\n<ol>\n<li>어떠한 $\\theta_0, \\pi_k=\\pi(\\cdot,\\cdot,\\theta_k)$</li>\n<li>그리고 $\\sum_sd^{\\pi_{k} }(s)\\sum_a\\pi_k(s,a)\\big[ Q^{\\pi_{k} }(s,a)-f_w(s,a)\\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$로 인하여 $w_k=w$, $\\theta_{k+1}=\\theta_k+\\alpha_k\\sum_sd^{\\pi_{k} }(s)\\sum_a\\frac{\\partial\\pi_k(s,a)}{\\partial\\theta}f_{w_{k} }(s,a)$</li>\n</ol>\n<p>으로 정의된 sequence $\\rho(\\pi_k)$는 $\\lim_{k\\rightarrow\\infty}\\frac{\\partial\\rho(\\pi_k)}{\\partial\\theta}=0$이기 때문에 수렴합니다.</p>\n<ul>\n<li>sequence $\\rho(\\pi_k)_{k=0}^\\infty$에 대한 추가 설명<ul>\n<li>$\\theta_{k+1}=\\theta_k+\\alpha_k\\sum_sd^{\\pi_{k} }(s)\\sum_a\\frac{\\partial\\pi_k(s,a)}{\\partial\\theta}f_{w_{k} }(s,a)$ 에 따라 $\\theta$가 1, 2, …, $\\infty$로 갈텐데, 거기에 따른 objective function or performance의 sequence입니다.</li>\n<li>(comment) 굳이 sequence라는 표현이 없어도 될 것 같습니다. 어짜피 k가 $\\infty$로 가면 $\\rho(\\pi_k)$가 수렴한다는 의미이기 때문에 불필요해보입니다.</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"5-2-Proof-of-Theorem-3\"><a href=\"#5-2-Proof-of-Theorem-3\" class=\"headerlink\" title=\"5.2 Proof of Theorem 3\"></a>5.2 Proof of Theorem 3</h2><ul>\n<li>Theorem 2는 $\\theta_k$ update가 gradient의 error를 최소화한다는 것을 증명했습니다.</li>\n<li>$\\frac{\\partial^{2}\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}$와 MDP의 reward에서의 bound는, $\\frac{\\partial^{2}\\rho}{\\partial\\theta_i\\partial\\theta_j}$ 또한 bound된다는 것을 증명합니다.</li>\n<li>step-size 필요조건 때문에 이러한 bound된 것들은 Proposition 3.5 from page 96 of Bertsekas and Tsitsiklis (1996)에 적용하기 위해 필요한 조건입니다.</li>\n<li>Proposition 3.5 from page 96 of Bertsekas and Tsitsiklis (1996)은 local optimum으로 수렴한다는 것을 증명했습니다.</li>\n<li>Proposition 3.5를 자세하게 설명하지는 않겠습니다. 일반적인 gradient method의 수렴성을 증명한 것이며, 이 논문의 policy gradient method도 gradient method의 일종이므로 특정 조건을 만족할 때 수렴한다는 것입니다. 수렴하는 원리는 소위 말하는 Lipschitz continuity condition을 만족하기 때문인데 다음과 같습니다. $L&gt;0$인 임의의 상수입니다.<br>$$<br>\\parallel \\nabla f(r) - \\nabla f(\\bar{r}) \\parallel \\leq L \\parallel r - \\bar{r} \\parallel<br>$$<br>즉, 업데이트가 진행되어 나감에 따라서 gradient의 차이가 점점 줄어들게 되므로 언젠가는 0으로 수렴하게 됩니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"6-Summary\"><a href=\"#6-Summary\" class=\"headerlink\" title=\"6. Summary\"></a>6. Summary</h1><p>논문에서 설명한 policy gradient 기법을 요약하면 다음과 같습니다.</p>\n<ul>\n<li>Original maximization problem: $\\max\\rho_{\\theta}=E\\left[R\\right]$</li>\n<li>gradient ascent method를 이용한 parameter update: $\\theta_{t+1} = \\theta_{t} + \\alpha\\left.\\frac{\\partial E[R]}{\\partial\\theta}\\right|_{\\theta_t}$<ul>\n<li>그러나 우리는 gradient를 모르고, 추정하기도 어렵습니다. 왜냐하면 expectation이 안에 있기 때문입니다.</li>\n<li>그렇다면 이것을 추정 가능한 형태로 바꿔야합니다. 다시 말해 expectation이 밖에 있는 형태로 바꾸는 것입니다.</li>\n<li>Expectation이 밖에 있으면 왜 추정이 유리할까요? 바로 Sample mean을 취하면 되기 때문입니다.</li>\n</ul>\n</li>\n<li>Approximate gradient<ul>\n<li>방법1: 논문의 Theorem 1을 이용하면 gradient와 expectation의 위치를 바꿀 수 있습니다.<br>$$\\frac{\\partial E[R]}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)$$</li>\n<li>방법2: Log derivative trick을 이용하면 gradient와 expectation의 위치를 바꿀 수 있습니다. (Theorem 2)<br>$$\\theta_{t+1} = \\theta_{t} + \\alpha E\\left[ \\left. R\\frac{\\partial}{\\partial\\theta}\\log p_{\\theta} \\right\\vert \\theta_t\\right]$$</li>\n<li>그리고 특정 trajectory를 따라가면서 return값을 구하고 이것을 여러 번 수행하여 sample mean을 취하면 gradient를 추정하는 것이 가능합니다.</li>\n<li>이러한 기법을 제시한 기존의 연구가 REINFORCE입니다.</li>\n<li>여러 Trajectory를 이용하므로 variance가 높을 수 밖에 없습니다.</li>\n<li>Advantage를 활용하거나 하는 방식으로 이후 여러 연구가 진행되었습니다.</li>\n</ul>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<center> <img src=\"https://www.dropbox.com/s/83i4atz733ali0w/Screen%20Shot%202018-07-18%20at%2012.10.52%20AM.png?dl=1\" width=\"600\"> </center>\n\n<p>논문 저자 : Richard S. Sutton, David McAllester, Satinder Singh, Yishay Mansour<br>논문 링크 : <a href=\"http://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf\" target=\"_blank\" rel=\"noopener\">NIPS</a><br>Proceeding : Advances in Neural Information Processing Systems (NIPS) 2000<br>정리 : 김동민, 이동민</p>\n<hr>\n<h1 id=\"1-Intro-to-Policy-Gradient\"><a href=\"#1-Intro-to-Policy-Gradient\" class=\"headerlink\" title=\"1. Intro to Policy Gradient\"></a>1. Intro to Policy Gradient</h1><p>이 논문은 policy gradient (PG) 기법의 효시와도 같으며 향후 많은 파생연구를 낳은 중요한 논문입니다. 7페이지의 짧은 논문이지만 읽기에 만만한 논문은 아닙니다. 이 논문을 이해하기 위해 필요한 배경지식을 먼저 설명하고 논문을 차근차근 살펴보도록 하겠습니다.</p>\n<p><br></p>\n<h2 id=\"1-1-Value-Function-Approach\"><a href=\"#1-1-Value-Function-Approach\" class=\"headerlink\" title=\"1.1 Value Function Approach\"></a>1.1 Value Function Approach</h2><p>전통적으로 강화학습 기법은 value function을 기반으로 동작하였습니다. 특정 state에서 vaue function 또는 value function을 근사하는 함수(function approximation)를 최대화하는 action을 찾는 greedy action-selection policy가  대표적입니다.</p>\n<p>논문에서는 이러한 방법은 deterministic한 policy를 찾는 쪽으로 나아가게 되지만 종종 최적의 policy는 stochastic한 성질을 가지기 때문에 이 방법으로는 최적의 policy를 찾을 수 없다고 언급하고 있습니다. (그러나 이 논문이 나오고 나서 한참 후 David Silver를 필두로한 DeepMind의 연구진들은 high-dimensional action space를 가지는 application에서 보다 빠르게 동작하는 <a href=\"../../../06/16/dpg/\">deterministic policy gradient</a>을 개발하였습니다.) 아마도 이러한 부분은 $\\epsilon$-greedy action-selection policy로 개선될 수 있을 것입니다. 또 하나의 문제는 value function의 작은 변화로 인해서 action이 크게 변할 수도 있다는 것입니다. 이것은 알고리듬의 수렴성에 문제를 야기할 수 있습니다. 이러한 문제점을 해결하기 위하여 policy search라는 새로운 기법이 고안됩니다.</p>\n<p><br></p>\n<h2 id=\"1-2-Policy-Search\"><a href=\"#1-2-Policy-Search\" class=\"headerlink\" title=\"1.2 Policy Search\"></a>1.2 Policy Search</h2><p>policy search는 최적의 policy $\\pi^*$를 reward로부터 직접 찾습니다. policy search 방법은 크게 두 가지로 분류할 수 있습니다. 첫 번째는 gradient-based optimization으로 policy gradient method가 이에 속합니다. 두 번째는 gradient-free optimization으로 진화(evolutionary) 연산을 이용하는 것이 이에 속합니다. 이 논문에서는 제목에서 알 수 있듯이 gradient-based optimization을 다룹니다. gradient는 변화량을 의미합니다. 특정 policy x와 또 다른 policy y가 있을 때 policy가 얼마나 많이 변화했는지 어떻게 모델링할 수 있을까요? policy의 변화를 제어하는 어떤 파라미터가 있다면 이 파라미터를 조정하여 policy를 변화시킬 수 있습니다. policy x에 해당하는 파라미터값과 policy y에 해당하는 파라미터값의 차이가 policy의 변화량이라고 할 수 있습니다. 이와 같은 방식으로 policy의 변화를 모델링하기 위하여 파라미터 $\\theta$를 이용하여 policy를 $\\pi_{\\theta}$로 표현할 수 있습니다. 최적의 policy를 찾기 위하여 expected return $E\\left[R|\\theta\\right]$이 최대화되도록 parameter를 조정합니다. 이를 간단한 수식으로 정리하면 다음과 같습니다.</p>\n<p>$$<br>\\pi^* = \\arg \\max\\limits_\\pi  E\\left[ {\\left. R \\right|\\pi } \\right] \\to {\\text{original problem} }<br>\\\\  {\\Downarrow} \\\\<br>{\\text{policy parameterization by } } \\pi_{\\theta}: \\Theta  \\to \\Pi \\\\ {\\Downarrow} $$</p>\n<p>$$\\pi^* = \\arg \\max\\limits_{\\theta}  E\\left[ {\\left. R \\right|\\theta } \\right] \\to {\\text{policy search problem} }<br>$$</p>\n<p><br></p>\n<h2 id=\"1-3-How-to-Obtain-the-Expected-Return\"><a href=\"#1-3-How-to-Obtain-the-Expected-Return\" class=\"headerlink\" title=\"1.3 How to Obtain the Expected Return\"></a>1.3 How to Obtain the Expected Return</h2><p>expected return을 최대화하는 방향으로 policy를 update한다고 하였습니다. 그렇다면 expected return은 어떻게 구할 수 있을까요? 여기에도 크게 두 가지 방법이 있습니다. 첫 번째는 deterministic approximation으로 Markov decision process의 dynamics를 모델링한 후 수식을 통해 구하는 것입니다. 두 번째 방법은 monte carlo estimation으로 dynamics에 대한 모델을 하지 않고 많은 sample들을 얻은 후 empirical하게 expected return을 계산하는 방법입니다. 어느 방법이 더 좋을지는 풀고자 하는 문제의 특성에 따라 다를 것입니다. dynamics에 대한 모델이 어렵거나 변화가 큰 경우에는 두 번째 방법이 좀 더 현실적이지만 gradient를 구하는 것은 더 어렵습니다. 결국 gradient를 esimate하는방법을 고안해야 합니다. 가장 유명한 gradient estimation 방법이 1992년 R. J. Williams에 의해서 제안된 <a href=\"https://link.springer.com/content/pdf/10.1007/BF00992696.pdf\" target=\"_blank\" rel=\"noopener\">REINFORCE</a> 기법입니다. REINFORCE는 Monte Carlo estimate 또는 likelihood-ratio estimate라고 부르는 방법을 이용합니다. 이 방법에 대해서 좀 더 알아볼까요?</p>\n<h3 id=\"1-3-1-Monte-Carlo-Gradient-Estimation\"><a href=\"#1-3-1-Monte-Carlo-Gradient-Estimation\" class=\"headerlink\" title=\"1.3.1 Monte Carlo Gradient Estimation\"></a>1.3.1 Monte Carlo Gradient Estimation</h3><p>다음과 같은 parameter $\\theta$를 가지는 random variable $X$가 있습니다: $X:\\Omega\\mapsto\\mathcal{X}$. 그리고 이 $x$에 대한 함수 $f$가 있습니다: $f:\\mathcal{X}\\mapsto\\mathbb{R}$. expected return처럼 $E[f(x)]$를 최대화하고자 합니다. 이를 위해서는 $\\nabla_{\\theta}E[f(x)]$를 구해야 합니다. 이 때 log derivate trick을 이용하여 다음과 같이 수식을 변형시킬 수 있습니다.</p>\n<p>$$<br>\\begin{align}<br>\\nabla_{\\theta} E_{p(x;\\theta)}\\left[ {f(x)} \\right]<br>&amp;= \\nabla_\\theta\\int{f\\left( x \\right)p\\left( {x;\\theta} \\right) dx}<br>\\\\<br>&amp;= \\int { {\\nabla_\\theta }p\\left( {x;\\theta } \\right)f\\left( x \\right)dx}<br>\\\\<br>&amp;= \\int {\\frac{ {p\\left( {x;\\theta } \\right)} }{ {p\\left( {x;\\theta } \\right)} }{\\nabla_\\theta }p\\left( {x;\\theta } \\right)f\\left( x \\right)dx}<br>\\\\<br>&amp;= \\int {p\\left( {x;\\theta } \\right){\\nabla_\\theta }\\log p\\left( {x;\\theta } \\right)f\\left( x \\right)dx}<br>\\\\<br>&amp;= E_{p(x;\\theta)}[f(x)\\nabla_\\theta \\log p({x;\\theta})]<br>\\end{align}<br>$$</p>\n<p>이제 expectation은 많은 샘플들을 모아서 평균을 취하는 Monte Carlo 기법으로 근사화할 수 있습니다.</p>\n<p>$${\\nabla_{\\theta} }{E_{p\\left( {x;\\theta } \\right)} }\\left[ {f\\left( x \\right)} \\right] = \\frac{1}{N}\\sum_{n=1}^N f\\left(X_n\\right)\\nabla_\\theta\\log p\\left(X_n;\\theta\\right)$$</p>\n<p>이 방법을 사용하기 위해서 필요한 조건은 $\\log p\\left(X_n;\\theta\\right)$가 미분가능해야 한다는 것 뿐입니다. 그러나 이 방법은 얻은 샘플들에 의존하기 때문에 경우에 따라서 큰 variance를 가질 수 있습니다.</p>\n<p><br><br></p>\n<h1 id=\"2-Policy-Gradient-Methods\"><a href=\"#2-Policy-Gradient-Methods\" class=\"headerlink\" title=\"2. Policy Gradient Methods\"></a>2. Policy Gradient Methods</h1><p>이제 본격적으로 policy gradient 기법에 대해서 알아보겠습니다.</p>\n<p><br></p>\n<h2 id=\"2-1-System-Model\"><a href=\"#2-1-System-Model\" class=\"headerlink\" title=\"2.1 System Model\"></a>2.1 System Model</h2><p>논문에서 사용하는 수학기호(notation)와 가정들을 설명하는 시스템모델은 다음과 같습니다.</p>\n<ul>\n<li>Markov decision process (MDP) at each time $t\\in{0,1,2,…}$</li>\n<li>$S_t\\in\\mathcal{S}$: state</li>\n<li>$A_t\\in\\mathcal{A}$: action</li>\n<li>$r_t\\in \\mathcal{R}$: reward</li>\n<li>$\\theta\\in\\mathbb{R}^l$: vector of policy parameters, for $l\\ll\\left|\\mathcal{S}\\right|$</li>\n<li>$\\mathcal{P} _{s s’}^a = \\Pr[S _{t+1}=s’ \\vert S_t=s,A_t=a]$: state transition probabilities</li>\n<li>$\\mathcal{R} _{s s’}^a = E[R _{t+1}\\vert S_t=s,A_t=a]$: expected reward</li>\n<li>$\\pi(s,a,\\theta)=\\Pr[A_t=a|S_t=s,\\theta]$: policy, shortened as $\\pi(s,a)$ or $\\pi(a|s)$</li>\n<li>$\\theta$: policy parameter들의 vector  </li>\n<li>$\\rho$: 해당 policy들의 성능을 나타내는 척도 (예. average reward per step)  </li>\n<li>$\\alpha$: positive-definite한 step size </li>\n</ul>\n<p><br></p>\n<h2 id=\"2-2-Policy-Gradent-Approach\"><a href=\"#2-2-Policy-Gradent-Approach\" class=\"headerlink\" title=\"2.2 Policy Gradent Approach\"></a>2.2 Policy Gradent Approach</h2><p>policy gradient는 stochastic policy를 자체적인 파리미터를 가진 function approximator를 이용해서 근사화시킵니다.</p>\n<p>policy parameter는 gradient에 비례하여 업데이트됩니다.</p>\n<p>$$\\Delta\\theta \\approx \\alpha\\frac{\\partial\\rho}{\\partial\\theta}$$</p>\n<p>$\\rho$는 $\\theta$에 대하여 미분가능해야겠죠? 위와 같이 업데이트하면 local optimal policy로 수렴합니다. 논문의 가장 중요한 contribution은 특정 조건을 만족하는 function approximator를 이용하여 경험(experience, sample)을 축적하고 이것들을 이용하여 위의 gradient를 unbiased estimate할 수 있음을 증명한 것입니다.</p>\n<p>먼저 $\\rho$, 즉, reward를 표현하는 두 가지 방법에 대해서 알아보겠습니다.</p>\n<p><br></p>\n<h2 id=\"2-3-Average-Reward-Formulation\"><a href=\"#2-3-Average-Reward-Formulation\" class=\"headerlink\" title=\"2.3 Average Reward Formulation\"></a>2.3 Average Reward Formulation</h2><p>Average reward formulation은 시간의 흐름에 따른 reward를 표현한다기보다는 모든 시간의 reward를 평균을 내서 표현하는 방법입니다.</p>\n<ul>\n<li><p>Long‐term expected reward per step<br>$$\\rho(\\pi)=\\lim_{n\\to\\infty}\\frac{1}{n}E[r_1+r_2+\\cdots+r_n|\\pi] = \\sum_s d^\\pi(s)\\sum_a\\pi(s,a)\\mathcal{R}_s^a$$</p>\n<ul>\n<li>$d^\\pi(s): \\lim_{t\\to\\infty}\\Pr[S_t=s|s_0,\\pi]$: stationary distribution of states under $\\pi$</li>\n<li>$d^\\pi(s)$가 존재한다고 가정합니다. 그리고 모든 policy에 대해서 $d^\\pi(s)$는 $s_0$와 independent합니다.</li>\n<li>위의 (limit수식과 summation수식 사이의) 등식은 왜 성립할까요? Time average와 Ensemble average가 같다는 뜻으로 <a href=\"https://en.wikipedia.org/wiki/Ergodicity\" target=\"_blank\" rel=\"noopener\">ergodic</a>한 system에서 성립합니다.</li>\n</ul>\n</li>\n<li><p>Value of a state-action pair given a policy<br>$$Q^{\\pi(s,a)} = \\sum_{t=1}^\\infty E[r_t - \\rho(\\pi)|S_0=s, A_0=a, \\pi]$$</p>\n<ul>\n<li>위의 Q-function의 표현식이 정의인지 유도된 것인지는 이 포스트를 작성하는 저희도 아직 확실히 모르겠습니다. 다만, $r_t$들을 더하면 무한대로 발산할 가능성이 매우 높은데 $\\rho$를 빼줌으로해서 어찌보면 평균 reward로부터의 차이를 더하는 것이라고 할 수 있고 이것은 bound된 값일 가능성이 더 높기 때문에 Q-function을 이렇게 표현하는 것이 더 안전하다고 할 수 있습니다. 이 부분에 대해서 좋은 의견이 있으시면 피드백을 주시면 감사하겠습니다!</li>\n</ul>\n</li>\n<li><p>State-value function<br>$$V^\\pi(s) = \\sum_{a} \\pi(s,a)Q^\\pi(s,a)$$</p>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-4-Start-State-Formulation\"><a href=\"#2-4-Start-State-Formulation\" class=\"headerlink\" title=\"2.4 Start-State Formulation\"></a>2.4 Start-State Formulation</h2><p>start-state formulation은 시간의 흐름에 따라 감소하는 reward들을 표현합니다.</p>\n<ul>\n<li><p>Long-term expected reward per step with a designated start state $S_0$<br>$$\\rho(\\pi) = E\\left[\\left.\\sum_{t=1}^\\infty \\gamma^{t-1}r_t\\right|S_0,\\pi\\right]$$</p>\n<ul>\n<li>$\\gamma\\in\\left[0,1\\right]$: discount rate</li>\n</ul>\n</li>\n<li><p>Value of a state‐action pair given a policy<br>$$Q^\\pi(s,a) = E\\left[\\left.\\sum_{k=1}^\\infty \\gamma^{k-1} r_{t+k}\\right|S_t=s, A_t=a, \\pi\\right]$$</p>\n</li>\n<li><p>Alternative form of $Q^\\pi(s,a)$<br>$$<br>Q^\\pi(s,a) = R_s^a + \\gamma\\sum _{s’} \\mathcal{P} _{s s’}^a V^{\\pi}(s’)<br>$$</p>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-5-Policy-Gradient-Theorem\"><a href=\"#2-5-Policy-Gradient-Theorem\" class=\"headerlink\" title=\"2.5 Policy Gradient Theorem\"></a>2.5 Policy Gradient Theorem</h2><p>논문의 중요한 결과인 Theorem 1은 다음과 같습니다.</p>\n<p><strong>Theorem 1 (Policy Gradient)</strong> <em>For any MDP, in either the average‐reward or start‐state formulations,</em><br>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)$$</p>\n<p>놀라운 부분은 expected return의 gradient를 취할 때 $\\frac{\\partial d^\\pi(s)}{\\partial\\theta}$를 구하지 않아도 된다는 점입니다. 즉, policy의 변화가 state distribution에 영향을 주지 않는다는 것입니다. 이것은 다시 말하면 아래 수식과 같이 우리는 $\\frac{\\partial\\rho}{\\partial\\theta}$에 대한 unbiased estimator를 구할 수 있다는 뜻입니다.</p>\n<p>$$E_s\\left[\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)\\right] = \\frac{\\partial\\rho}{\\partial\\theta}$$</p>\n<p>이로인해서 우리는 gradient를 sampling을 통해서 추정할 수 있습니다. 그렇지만 이것은 sample이 아주 많을 때만 성립합니다!</p>\n<p>또 한 가지 문제는 우리는 $Q^\\pi(s,a)$의 정확한 값을 알 수 없다는 것 입니다. 이것을 estimate하기 위해서 현재의 return값 $R_t$를 이용할 수 있습니다.</p>\n<p><br></p>\n<h2 id=\"2-6-Proof-of-Policy-Gradient-Theorem\"><a href=\"#2-6-Proof-of-Policy-Gradient-Theorem\" class=\"headerlink\" title=\"2.6 Proof of Policy Gradient Theorem\"></a>2.6 Proof of Policy Gradient Theorem</h2><p>증명을 살펴보겠습니다.</p>\n<ul>\n<li>average-reward formulation을 이용할 때의 증명은 아래와 같습니다.</li>\n</ul>\n<p>$$<br>\\begin{align}<br>\\frac{\\partial V^\\pi(s)}{\\partial\\theta} &amp;\\overset{\\underset{\\mathrm{def} }{} }{=}\\frac{\\partial}{\\partial\\theta}\\sum_a \\pi(s,a)Q^\\pi(s,a)<br>\\\\<br>&amp;=\\sum_a \\frac{\\partial}{\\partial\\theta}\\left[\\pi(s,a)Q^\\pi(s,a)\\right]\\\\<br>&amp;=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\frac{\\partial Q^\\pi(s,a)}{\\partial\\theta}\\right]\\\\<br>&amp;=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\frac{\\partial}{\\partial\\theta}\\left[R_s^a -\\rho(\\pi) + \\sum_{s’}\\mathcal{P}_{s s’}^a V^{\\pi}(s’)\\right]\\right]\\\\<br>&amp;=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\left[-\\frac{\\partial\\rho}{\\partial\\theta} + \\sum_{s’}\\mathcal{P}_{s s’}^a \\frac{\\partial V^{\\pi}(s’)}{\\partial\\theta}\\right]\\right]\\\\<br>\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\sum_{s’}\\mathcal{P}_{s s’}^a \\frac{\\partial V^{\\pi}(s’)}{\\partial\\theta}\\right] - \\frac{\\partial V^\\pi(s)}{\\partial\\theta}<br>\\end{align}<br>$$</p>\n<p>양변에 stationary distribution에 대한 평균을 취합니다.<br>$$<br>\\begin{align}<br>\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+ \\sum_s d^\\pi(s)\\sum_a\\pi(s,a)\\sum_{s’}\\mathcal{P}_{s s’}^a \\frac{\\partial V^{\\pi}(s’)}{\\partial\\theta}<br>\\\\<br>&amp;- \\sum_s d^\\pi(s)\\frac{\\partial V^\\pi(s)}{\\partial\\theta}<br>\\end{align}<br>$$</p>\n<p>모든 state $s$에서 모든 state $s’$으로 이동하는 것은 state $s’$에 존재할 확률이므로 다음과 같이 표현할 수 있습니다.</p>\n<p>$$<br>\\begin{align}<br>\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+ \\sum_s d^\\pi(s’) \\frac{\\partial V^{\\pi}(s’)}{\\partial\\theta} - \\sum_s d^\\pi(s)\\frac{\\partial V^\\pi(s)}{\\partial\\theta}<br>\\end{align}<br>$$</p>\n<p>뒤의 두 항은 같은 식이므로 0이 됩니다. 따라서 다음과 같이 표현됩니다.<br>$$<br>\\begin{align}<br>\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)<br>\\end{align}<br>$$</p>\n<p>$\\frac{\\partial\\rho}{\\partial\\theta}$는 reward의 파라미터에 대한 미분값입니다. reward 자체가 이미 모든 state에 대한 평균값이므로 $s$의 함수가 아닙니다.  $s$에 대한 함수가 아니므로 $\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}=\\frac{\\partial\\rho}{\\partial\\theta}\\sum_s d^\\pi(s)=\\frac{\\partial\\rho}{\\partial\\theta}$입니다. 여기서 $d^\\pi(s)$는 state에 대한 확률값이죠. 모든 state에 대해서 이 값을 다 더하면 1이 됩니다.<br>이를 이용해서 위의 식을 다시 표현하면 아래와 같고 이를 통해 증명이 완성됩니다.</p>\n<p>$$<br>\\begin{align}<br>\\therefore\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)<br>\\end{align}<br>$$</p>\n<ul>\n<li>다음으로 start-state formulation에 대한 증명입니다.</li>\n</ul>\n<p>start-state formulation은 이 논문에서도 나오고 서튼책에서도 나오는데, 서튼책의 증명이 더 설명이 자세합니다. 그래서 논문에 있는 증명이 아닌 서튼책에 있는 증명으로 설명드리겠습니다. 다만, notation이 약간 다른데, 큰 문제 없이 이해하실 수 있으므로 별도의 설명은 하지 않겠습니다. (서튼책 <a href=\"https://drive.google.com/file/d/1xeUDVGWGUUv1-ccUMAZHJLej2C7aAFWY/view\" target=\"_blank\" rel=\"noopener\">Link</a>)</p>\n<p>$$\\nabla{v_\\pi}(s)=\\nabla\\big[\\sum_a\\pi(a|s)q_\\pi(s,a)\\big]$$ </p>\n<p>다음으로 $(f\\cdot g)’=f’\\cdot g+f\\cdot g’$ (product rule of calculus)에 의해 다음과 같이 나타낼 수 있습니다.<br>$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\nabla q_\\pi(s,a)\\big]$$  </p>\n<p>이어서 $p(s’,r|s,a) := Pr[S_t=s’, R_t=r|S_{t-1}=s,A_{t-1}=a ]$ (서튼책 3.2 공식) 에 의해 다음과 같이 나타낼 수 있습니다.</p>\n<p>$$=\\sum_a\\big[\\nabla\\pi(a|s)q_{\\pi}(s,a)+\\pi(a|s)\\nabla\\big[\\sum_{s’, r}p(s’,r|s,a)(r+v_{\\pi}(s’))\\big]\\big]$$  </p>\n<p>계속해서 $p(s’|s,a)=\\sum_{r\\in R}p(s’,r|s,a)$ (서튼책 3.4 공식) 에 의해 다음과 같이 나타낼 수 있습니다.<br>$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\sum_{s’}p(s’|s,a)\\nabla v_\\pi(s’) \\big]$$ </p>\n<p>여기서 $\\nabla v_\\pi(s’)$ 부분을 unrolling을 하면 다음과 같이 나타낼 수 있습니다.<br>$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\sum_{s’}p(s’|s,a)\\sum_{a’}\\big[\\nabla\\pi(a’|s’)q_\\pi(s’,a’)\\\\+\\pi(a’|s’)\\sum_{s’’}p(s’’|s’,a’)\\nabla v_\\pi(s’’))\\big] \\big]$$ </p>\n<p>$\\nabla v_\\pi(s’’)$에 대해서 unrolling을 하고 또 나오는 다른 항에 대해서도 계속 반복하다보면 다음과 같이 나타낼 수 있습니다.<br>$$=\\sum_{x\\in S}\\sum_{k=0}^{\\infty}\\Pr(s\\rightarrow x,k,\\pi)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$</p>\n<p>여기에서 $\\Pr(s\\rightarrow x,k,\\pi)$ 는 policy $\\pi$ 에 대해 state $s$에서 state $x$까지 $k$ step만큼 움직일 때의 변환 확률입니다. 따라서 위의 수식을 아래처럼 다시 나타낼 수 있습니다.<br> <!-- \\begin{align}  --><br>$$\\nabla J(\\theta)=\\nabla v_\\pi (s_0)$$<br>$$=\\sum_s(\\sum_{k=0}^{\\infty}\\Pr(s_0\\rightarrow s,k,\\pi))\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$<br>$$=\\sum_s \\eta(s) \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$<br><!-- \\end{align} --></p>\n<ul>\n<li>서튼책에서 $\\eta(s)$는 다음과 같이 표현할 수 있습니다.<ul>\n<li>$\\eta(s)$ = 평균적으로 하나의 episode마다 state에서 머무른 time step의 수 (page 199)</li>\n</ul>\n</li>\n<li>논문에서는 $\\eta(s)$를 다음과 같이 표현합니다.<ul>\n<li>$d^\\pi(s)=\\sum_{t=0}^{\\infty}\\gamma^t Pr(s_t=s|s_0,\\pi)$<br>($\\gamma=1$ is allowed only in episodic tasks = discounted weighting of states)</li>\n</ul>\n</li>\n</ul>\n<p>여기서부터가 논문에는 없는 내용입니다. 위의 최종 수식을 다시 한 번 적으면 아래와 같습니다.<br>$$\\sum_s \\eta(s) \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$</p>\n<p>위의 수식을 아래와 같이 바꿀 수 있습니다.<br>$$\\sum_{s’} \\eta(s’) \\cdot\\sum_s \\frac{\\eta(s)}{\\sum_{s’}\\eta(s’)} \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$</p>\n<p>그러면 이것을 새로운 기호로 다시 나타낼 수 있습니다.<br>$$\\sum_{s’} \\eta(s’) \\cdot\\sum_s \\mu(s)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$ </p>\n<ul>\n<li>여기서 $\\mu(s)$ 는 state distribution이라고 하며, weight를 state distribution으로 바꿔주는 과정이라고 생각할 수 있습니다. 쉽게 말해 어떠한 state에 agent가 머무르는 확률입니다.</li>\n</ul>\n<p>위의 수식은 아래의 수식과 비례합니다. 따라서 최종 형태는 다음과 같습니다.<br>$$\\propto \\sum_s \\mu(s)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$</p>\n<p><br><br></p>\n<h1 id=\"3-Policy-Gradient-with-Approximation\"><a href=\"#3-Policy-Gradient-with-Approximation\" class=\"headerlink\" title=\"3. Policy Gradient with Approximation\"></a>3. Policy Gradient with Approximation</h1><p>이 장에서는 앞서 다뤘던 Theorem 1 중에 $Q^\\pi$에 대해서 중점적으로 다룹니다. 어떠한 $Q^\\pi$가 학습된 function approximator로 근사된다고 합시다. </p>\n<p>$f_w$ ($f_w$는 $S × A \\rightarrow \\Re$)는 parameter $w$를 가지는 어떠한 $Q^\\pi$ 에 대한 근사치입니다. 학습된 function approximator에 대해서 생각하기 때문에, 학습된 $f_w$는 error를 최소화하는 방향으로 parameter $w$를 업데이트합니다. 그러면 다음과 같은 관계식을 생각할 수 있습니다.<br>$$\\Delta w_t \\propto \\frac{\\partial}{\\partial w}\\big[ {\\hat Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]^2 \\propto \\big[{\\hat Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s_t,a_t)}{\\partial w}$$<br>(여기서 ${\\hat Q}^\\pi(s_t,a_t)$ 는 $Q^\\pi(s_t,a_t)$의 unbiased estimator $R_t$입니다.)</p>\n<p>그러면 위와 같은 수식이 local optimum에 수렴을 했을 때, 다음과 같이 나타낼 수 있습니다.<br>$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$</p>\n<ul>\n<li>위의 수식에 대한 추가 설명<ul>\n<li>local optimum으로 수렴을 하기 때문에 당연히 위의 수식은 0이 됩니다.</li>\n<li>또한 stochastic policy이기 때문에 local optimum으로 수렴하려면 모든 state, action에 대해 expectation을 해야합니다. 따라서 $\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)$이 붙게 됩니다.</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-1-Theorem-2-Policy-Gradient-with-Function-Approximation\"><a href=\"#3-1-Theorem-2-Policy-Gradient-with-Function-Approximation\" class=\"headerlink\" title=\"3.1 Theorem 2: Policy Gradient with Function Approximation\"></a>3.1 Theorem 2: Policy Gradient with Function Approximation</h2><p>만약 $f_w$가 아래의 등식을 만족한다고 합시다.<br>$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$</p>\n<p>또한 $f_w$가 policy parameterization과 양립할 수 있다고 합시다. policy parameterization에 대한 여러가지 의미가 있는데 이 논문에서는 다음을 의미합니다.<br>$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}$$</p>\n<ul>\n<li>위의 수식에 대한 추가 설명<ul>\n<li>이 수식은 Monte-Calro Estimation을 이용하기 위한 조건입니다.</li>\n<li>Compatibility Condition이라고 부릅니다.</li>\n<li>우변(= $\\nabla\\log\\pi(s,a)$)으로 유도함으로써 아래의 수식과 같이 sampling을 이용한 추정이 가능해집니다.</li>\n</ul>\n</li>\n</ul>\n<p>따라서 위의 두 수식을 이용하여 아래와 같이 나타낼 수 있습니다.<br>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}f_w(s,a)$$</p>\n<p><br></p>\n<h2 id=\"3-2-Proof-of-Theorem-2\"><a href=\"#3-2-Proof-of-Theorem-2\" class=\"headerlink\" title=\"3.2 Proof of Theorem 2\"></a>3.2 Proof of Theorem 2</h2><p>$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$</p>\n<p>$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}$$</p>\n<p>위의 두 수식을 합치면 다음과 같이 나타낼 수 있습니다.<br>$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}=0$$</p>\n<p>분자와 분모에 있는 $\\pi(s,a)$를 지우면 아래와 같습니다.<br>$$\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]=0$$</p>\n<p>이어서 위의 수식이 0이기 때문에 policy gradient theorem(앞서 다뤘던 Theorem 1)에서 위의 수식을 뺄 수 있습니다.<br>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)-\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ Q^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]$$</p>\n<p>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ Q^\\pi(s_t,a_t)-Q^\\pi(s_t,a_t)+f_w(s_t,a_t) \\big]$$</p>\n<p>$$\\therefore\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}f_w(s_t,a_t)$$</p>\n<p><br><br></p>\n<h1 id=\"4-Application-to-Deriving-Algorithms-and-Advantages\"><a href=\"#4-Application-to-Deriving-Algorithms-and-Advantages\" class=\"headerlink\" title=\"4. Application to Deriving Algorithms and Advantages\"></a>4. Application to Deriving Algorithms and Advantages</h1><p><br></p>\n<h2 id=\"4-1-Application-to-Deriving-Algorithms\"><a href=\"#4-1-Application-to-Deriving-Algorithms\" class=\"headerlink\" title=\"4.1 Application to Deriving Algorithms\"></a>4.1 Application to Deriving Algorithms</h2><p>feature의 linear combination에서 Gibbs distribution (softmax)를 하나의 policy로 생각해볼 수 있습니다.<br>$$\\pi(s,a)=\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_b e^{\\theta^{T}\\phi_{sb} }}$$</p>\n<ul>\n<li>$\\phi_{sa}$: state-action pair $s$, $a$를 나타내는 $l$-dimensional feature vector</li>\n</ul>\n<p>compatible condition을 적용하면 다음과 같습니다. (4.2 Proof of application of compatible condition 참고)<br>$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\frac{1}{\\pi(s,a)}=\\phi_{sa}-\\sum_b \\pi(s,b)\\phi_{sb}$$ </p>\n<p>이어서 $f_w$을 적분한 natural parameterization은 다음과 같습니다.<br>$$f_w(s,a)=w^T\\big[ \\phi_{sa}-\\sum_b \\pi(s,b)\\phi_{sb}\\big]$$</p>\n<ul>\n<li>위의 수식에 대한 추가 설명<ul>\n<li>$f_w$는 policy로서 같은 feature들에 대해 linear합니다.</li>\n<li>$f_w$는 각각의 state에 대해서 평균이 0입니다. ($\\sum_a\\pi(s,a)f_w(s,a)=0$)</li>\n<li>advantage function $A^\\pi(s,a)=Q^\\pi(s,a)-V^\\pi(s)$의 하나의 근사치로서 $f_w$를 생각해도 좋습니다.</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"4-2-Proof-of-application-of-compatible-condition\"><a href=\"#4-2-Proof-of-application-of-compatible-condition\" class=\"headerlink\" title=\"4.2 Proof of application of compatible condition\"></a>4.2 Proof of application of compatible condition</h2><p>증명을 하기 전에 다음을 가정합니다.</p>\n<ol>\n<li>$(\\frac{f(x)}{g(x)})’=f’(x)\\cdot \\frac{1}{g(x)}-f(x)\\cdot \\frac{g’(x)}{g(x)^{2} }=\\frac{f’(x)\\cdot g(x)-f(x)\\cdot g’(x)}{g(x)^{2} }$</li>\n<li>$f(\\theta)=e^{\\theta^{T}\\phi_{sa} }$, $g(\\theta)=\\sum_be^{\\theta^{T}\\phi_{sb} }$</li>\n</ol>\n<p>$$<br>\\begin{align}<br>\\frac{\\partial f_w(s,a)}{\\partial w}&amp;=\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\frac{1}{\\pi(s,a)}<br>\\\\<br>&amp;=\\big(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }}\\big)’\\frac{1}{\\pi(s,a)}<br>\\\\<br>&amp;=\\big[ \\phi_{sa}(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }})-(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }})\\frac{\\sum_b\\phi_{sb}e^{ {\\theta^T}\\phi_{sb} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }}\\big]\\frac{1}{\\pi(s,a)}\\\\<br>&amp;=\\big[ \\phi_{sa}\\pi(s,a)-\\pi(s,a)\\sum_b\\phi_{sb}\\pi(s,b)\\big]\\frac{1}{\\pi(s,a)}<br>\\\\<br>&amp;=\\pi(s,a)\\big[ \\phi_{sa}-\\sum_b\\pi(s,b)\\phi_{sb}\\big]\\frac{1}{\\pi(s,a)}<br>\\\\<br>&amp;\\therefore\\phi_{sa}-\\sum_b\\pi(s,b)\\phi_{sb}<br>\\end{align}<br>$$</p>\n<p><br></p>\n<h2 id=\"4-3-Application-to-Advantages\"><a href=\"#4-3-Application-to-Advantages\" class=\"headerlink\" title=\"4.3 Application to Advantages\"></a>4.3 Application to Advantages</h2><p>Policy Gradient with Function Approximation Theorem (Theorem 2)는 advantage function으로 확장될 수 있습니다.<br>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_sd^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ f_w(s,a)+v(s) \\big]$$</p>\n<ul>\n<li>위의 수식에 대한 추가설명<ul>\n<li>$v$ ($v$는 $S\\rightarrow\\Re$) 는 arbitrary function입니다.</li>\n<li>이 수식은 $\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}=0$이기 때문에 가능해집니다.</li>\n<li>즉, 이 수식은 $\\pi(s,a)$의 gradient에만 dependent하기 때문에 advantage 역할을 하는 함수들을 넣어도 아무런 상관이 없습니다.</li>\n<li>$v$의 선택은 Theorem들에 영향을 미치지 못하지만, 실질적으로 gradient estimator의 variance에 영향을 미칩니다.</li>\n<li>이러한 문제는 전체적으로 이전의 연구에 reinforcement baseline의 사용에 있어서 유사합니다.</li>\n<li>(comment) 위의 수식에서 $f_w(s,a)+v(s)$와 Application to Deriving Algorithms의 $f_w(s,a)$와는 다른 것입니다. Application to Deriving Algorithms의 $f_w(s,a)$은 softmax에 의해 스스로 advantage function의 역할을 할 수 있습니다. 하지만 보통의 경우에는 그러지 못할 수도 있기 때문에 위의 수식처럼 $f_w(s,a)+v(s)$을 추가하여 zero mean만들어서 variance를 줄일 수 있습니다.</li>\n</ul>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"5-Convergence-of-Policy-Iteration-with-Function-Approximation\"><a href=\"#5-Convergence-of-Policy-Iteration-with-Function-Approximation\" class=\"headerlink\" title=\"5. Convergence of Policy Iteration with Function Approximation\"></a>5. Convergence of Policy Iteration with Function Approximation</h1><p><br></p>\n<h2 id=\"5-1-Theorem-3-Convergence-of-Policy-Iteration-with-Function-Approximation\"><a href=\"#5-1-Theorem-3-Convergence-of-Policy-Iteration-with-Function-Approximation\" class=\"headerlink\" title=\"5.1 Theorem 3: Convergence of Policy Iteration with Function Approximation\"></a>5.1 Theorem 3: Convergence of Policy Iteration with Function Approximation</h2><p>policy iteration with function approximation은 locally optimal policy에 수렴합니다. $\\pi$와 $f_w$를 </p>\n<ol>\n<li>compatibility condition을 만족하는 policy와 value function에 대한</li>\n<li>그리고 $\\max_{\\theta, s, a, i, j}\\big| \\frac{\\partial^2\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}\\big|&lt;B&lt;\\infty$에 대한</li>\n</ol>\n<p>어떠한 미분가능한 function approximator라고 합시다.</p>\n<ul>\n<li>(comment) $\\max_{\\theta, s, a, i, j}\\big| \\frac{\\partial^2\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}\\big|&lt;B&lt;\\infty$에서 $\\pi(s,a)$라는 function은 이계미분값이 존재하고, 임의의 상수인 (Bound) B에 bound되어 있기 때문에 function의 그래프는 smooth하다고 볼 수 있습니다. (아래의 그림 중 빨간색 그래프 참고)</li>\n</ul>\n<center> <img src=\"https://www.dropbox.com/s/xx02ejfg5ao19ps/Screen%20Shot%202018-07-10%20at%203.41.06%20PM.png?dl=1\" width=\"200\"> </center>\n\n<p>이어서 ${\\alpha_k}$는 $\\lim_{k\\rightarrow\\infty}\\alpha_k=0$이며 $\\sum_k\\alpha_k=\\infty$를 만족하는 step-size sequence라고 합시다.</p>\n<p>그 때, bounded reward를 가진 MDP에 대해</p>\n<ol>\n<li>어떠한 $\\theta_0, \\pi_k=\\pi(\\cdot,\\cdot,\\theta_k)$</li>\n<li>그리고 $\\sum_sd^{\\pi_{k} }(s)\\sum_a\\pi_k(s,a)\\big[ Q^{\\pi_{k} }(s,a)-f_w(s,a)\\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$로 인하여 $w_k=w$, $\\theta_{k+1}=\\theta_k+\\alpha_k\\sum_sd^{\\pi_{k} }(s)\\sum_a\\frac{\\partial\\pi_k(s,a)}{\\partial\\theta}f_{w_{k} }(s,a)$</li>\n</ol>\n<p>으로 정의된 sequence $\\rho(\\pi_k)$는 $\\lim_{k\\rightarrow\\infty}\\frac{\\partial\\rho(\\pi_k)}{\\partial\\theta}=0$이기 때문에 수렴합니다.</p>\n<ul>\n<li>sequence $\\rho(\\pi_k)_{k=0}^\\infty$에 대한 추가 설명<ul>\n<li>$\\theta_{k+1}=\\theta_k+\\alpha_k\\sum_sd^{\\pi_{k} }(s)\\sum_a\\frac{\\partial\\pi_k(s,a)}{\\partial\\theta}f_{w_{k} }(s,a)$ 에 따라 $\\theta$가 1, 2, …, $\\infty$로 갈텐데, 거기에 따른 objective function or performance의 sequence입니다.</li>\n<li>(comment) 굳이 sequence라는 표현이 없어도 될 것 같습니다. 어짜피 k가 $\\infty$로 가면 $\\rho(\\pi_k)$가 수렴한다는 의미이기 때문에 불필요해보입니다.</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"5-2-Proof-of-Theorem-3\"><a href=\"#5-2-Proof-of-Theorem-3\" class=\"headerlink\" title=\"5.2 Proof of Theorem 3\"></a>5.2 Proof of Theorem 3</h2><ul>\n<li>Theorem 2는 $\\theta_k$ update가 gradient의 error를 최소화한다는 것을 증명했습니다.</li>\n<li>$\\frac{\\partial^{2}\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}$와 MDP의 reward에서의 bound는, $\\frac{\\partial^{2}\\rho}{\\partial\\theta_i\\partial\\theta_j}$ 또한 bound된다는 것을 증명합니다.</li>\n<li>step-size 필요조건 때문에 이러한 bound된 것들은 Proposition 3.5 from page 96 of Bertsekas and Tsitsiklis (1996)에 적용하기 위해 필요한 조건입니다.</li>\n<li>Proposition 3.5 from page 96 of Bertsekas and Tsitsiklis (1996)은 local optimum으로 수렴한다는 것을 증명했습니다.</li>\n<li>Proposition 3.5를 자세하게 설명하지는 않겠습니다. 일반적인 gradient method의 수렴성을 증명한 것이며, 이 논문의 policy gradient method도 gradient method의 일종이므로 특정 조건을 만족할 때 수렴한다는 것입니다. 수렴하는 원리는 소위 말하는 Lipschitz continuity condition을 만족하기 때문인데 다음과 같습니다. $L&gt;0$인 임의의 상수입니다.<br>$$<br>\\parallel \\nabla f(r) - \\nabla f(\\bar{r}) \\parallel \\leq L \\parallel r - \\bar{r} \\parallel<br>$$<br>즉, 업데이트가 진행되어 나감에 따라서 gradient의 차이가 점점 줄어들게 되므로 언젠가는 0으로 수렴하게 됩니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"6-Summary\"><a href=\"#6-Summary\" class=\"headerlink\" title=\"6. Summary\"></a>6. Summary</h1><p>논문에서 설명한 policy gradient 기법을 요약하면 다음과 같습니다.</p>\n<ul>\n<li>Original maximization problem: $\\max\\rho_{\\theta}=E\\left[R\\right]$</li>\n<li>gradient ascent method를 이용한 parameter update: $\\theta_{t+1} = \\theta_{t} + \\alpha\\left.\\frac{\\partial E[R]}{\\partial\\theta}\\right|_{\\theta_t}$<ul>\n<li>그러나 우리는 gradient를 모르고, 추정하기도 어렵습니다. 왜냐하면 expectation이 안에 있기 때문입니다.</li>\n<li>그렇다면 이것을 추정 가능한 형태로 바꿔야합니다. 다시 말해 expectation이 밖에 있는 형태로 바꾸는 것입니다.</li>\n<li>Expectation이 밖에 있으면 왜 추정이 유리할까요? 바로 Sample mean을 취하면 되기 때문입니다.</li>\n</ul>\n</li>\n<li>Approximate gradient<ul>\n<li>방법1: 논문의 Theorem 1을 이용하면 gradient와 expectation의 위치를 바꿀 수 있습니다.<br>$$\\frac{\\partial E[R]}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)$$</li>\n<li>방법2: Log derivative trick을 이용하면 gradient와 expectation의 위치를 바꿀 수 있습니다. (Theorem 2)<br>$$\\theta_{t+1} = \\theta_{t} + \\alpha E\\left[ \\left. R\\frac{\\partial}{\\partial\\theta}\\log p_{\\theta} \\right\\vert \\theta_t\\right]$$</li>\n<li>그리고 특정 trajectory를 따라가면서 return값을 구하고 이것을 여러 번 수행하여 sample mean을 취하면 gradient를 추정하는 것이 가능합니다.</li>\n<li>이러한 기법을 제시한 기존의 연구가 REINFORCE입니다.</li>\n<li>여러 Trajectory를 이용하므로 variance가 높을 수 밖에 없습니다.</li>\n<li>Advantage를 활용하거나 하는 방식으로 이후 여러 연구가 진행되었습니다.</li>\n</ul>\n</li>\n</ul>\n"},{"title":"Deterministic Policy Gradient Algorithms","date":"2018-06-27T08:21:48.000Z","author":"장수영, 공민서","subtitle":"피지여행 2번째 논문","_content":"\n<center> <img src=\"https://www.dropbox.com/s/etwa3hn5c0pegdv/Screen%20Shot%202018-07-18%20at%2012.51.35%20AM.png?dl=1\" width=\"800\"> </center>\n\n논문 저자 : David Silver, Guy Lever, Nicloas Heess, Thomas Degris, Daan Wierstra, Martin Riedmiller\n논문 링크 : http://proceedings.mlr.press/v32/silver14.pdf\nProceeding : International Conference on Machine Learning (ICML) 2014\n정리 : 장수영, 공민서\n\n---\n\n## 1. Summary\n- Deterministic Policy Gradient (DPG) Theorem 제안함 [[Theorem 1](#deterministic-policy-gradient-theorem)]\n    1) DPG는 존재하며,\n    2) DPG는 Expected gradient of the action-value function의 형태를 띈다.\n- Policy variance가 0에 수렴할 경우, DPG는 Stochastic Policy Gradient(SPG)와 동일해짐을 보임 [[Theorem 2](#dpg-는-spg-의-limiting-case-임)]\n    - Theorem 2 로 인해 기존 Policy Gradient (PG) 와 관련된 기법들을 DPG 에 적용할 수 있게 됨\n        - 예. Sutton PG, natural gradients, actor-critic, episodic/batch methods\n- 적절한 exploration 을 위해 model-free, off-policy Actor-Critic algorithm 을 제안함\n    - Action-value function approximator 사용으로 인해 policy gradient가 bias되는 것을 방지하기 위해 compatibility conditions을 제공 [[Theorem 3](##deterministic-actor-critic-algorithms)]\n- DPG 는 SPG 보다 성능이 좋음\n    - 특히 high dimensional action spaces 을 가지는 tasks에서의 성능 향상이 큼\n        - SPG의 policy gradient는 state와 action spaces 모두에 대해서, DPG의 policy gradient는  state spaces에 대해서만 평균을 취함\n        - 결과적으로, action spaces의 dimension이 커질수록 data efficiency가 높은 DPG의 학습이 더 잘 이뤄지게 됨\n        - 무한정 학습을 시키게 되면, SPG도 최적으로 수렴할 것으로 예상되기에 위 성능 비교는 일정 iteration 내로 한정함\n    - 기존 기법들에 비해 computation 양이 많지 않음\n        - Computation 은 action dimensionality 와 policy parameters 수에 비례함\n<br>\n\n---\n## 2. Background\n### 2.1 Performance objective function\n\n$$\n\\begin{align}\nJ(\\pi_{\\theta}) &= \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\pi_{\\theta}(s,a)r(s,a)dads = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[r(s,a)]\n\\end{align}\n$$\n\n### 2.2 SPG Theorem\n- State distribution $ \\rho^{\\pi}(s) $ 은 policy parameters에 영향을 받지만, policy gradient 를 계산할 때는 state distribution 의 gradient 를 고려할 필요가 없다.\n- $$\\begin{eqnarray}\\nabla_{\\theta}J(\\pi_{\\theta}) &=& \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\\\ &=& E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]\n\\end{eqnarray}$$\n\n### 2.3 Stochastic Actor-Critic Algorithms\n- Actor 와 Critic 이 번갈아가면서 동작하며 stochastic policy 를 최적화하는 기법\n- Actor: $ Q^{\\pi}(s,a) $ 를 근사한 $ Q^w(s,a) $ 를 이용해 stochastic policy gradient 를 ascent 하는 방향으로 policy parameter $ \\theta $ 를 업데이트함으로써 stochastic policy 를 발전시킴\n    - $ \\nabla_{\\theta}J(\\pi_{\\theta}) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}log\\pi_{\\theta}(a|s)Q^{w}(s,a)] $\n- Critic: SARSA 나 Q-learning 같은 Temporal-difference (TD) learning 을 이용해 action-value function의 parameter, $ w $ 를 업데이트함으로써 $ Q^w(s,a) $ 가 $ Q^{\\pi}(s,a) $ 과 유사해지도록 함\n- 실제 값인 $ Q^{\\pi}(s,a) $ 대신 이를 근사한 $ Q^w(s,a) $ 를 사용하게 되면, 일반적으로 bias 가 발생하게 된다. 하지만, compatible condition 에 부합하는 $ Q^w(s,a) $ 를 사용하게 되면, bias 가 발생하지 않는다.\n\n### 2.4 Off-policy Actor-Critic\n- Distinct behavior policy $ \\beta(a|s) ( \\neq \\pi_{\\theta}(a|s) ) $ 로부터 샘플링된 trajectories 를 이용한 Actor-Critic\n- Performance objective function\n    - $\\begin{eqnarray}\n        J_{\\beta}(\\pi_{\\theta}) \n        &=& \\int_{S}\\rho^{\\beta}(s)V^{\\pi}(s)ds \\nonumber \\\\\\\\\n        &=& \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads\n        \\end{eqnarray} $\n- off-policy policy gradient\n    - $ \\begin{eqnarray}\n        \\nabla_{\\theta}J_{\\beta}(\\pi_{\\theta}) &\\approx& \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\end{eqnarray} $\n      $=E_{s \\sim \\rho^{\\beta}, a \\sim \\beta}[\\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)}\\nabla_{\\theta}log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]$\n    - off-policy policy gradient 식에서의 물결 표시는 [Degris, 2012b] 논문에 근거함\n        - [Degris, 2012b] \"Linear off-policy actor-critic,\" ICML 2012\n        - Exact off-policy policy gradient 와 이를 approximate 한 policy gradient 는 아래와 같음. (빨간색 상자에 있는 항목을 삭제함으로써 근사함)\n            - <img src=\"https://www.dropbox.com/s/xzpv3okc139c1fs/Screenshot%202018-06-16%2017.48.51.png?dl=1\" width=500px>\n        - [Degris, 2012b] Theorem 1 에 의해 policy parameter 가 approximated policy gradient ( $\\nabla_{u}𝑄^{\\pi,\\gamma}(𝑠,𝑎)$ term 제거)에 따라 업데이트되어도 policy 는 improve 가 됨이 보장되기에 exact off-policy policy gradient 대신 approximated off-policy policy gradient 를 사용해도 괜찮음.\n            - <img src=\"https://www.dropbox.com/s/mk13931r4scjngo/Screenshot%202018-06-16%2017.49.24.png?dl=1\" width=500px>\n    - off-policy policy gradient 식에서 $ \\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)} $ 는 importance sampling ratio 임\n        - off-policy actor-critic 에서는 $ \\beta $ 에 의해 샘플링된 trajectory 를 이용해서 stochastic policy $ \\pi $ 를 예측하는 것이기 때문에 imnportance sampling 이 필요함\n<br>\n\n---\n## 3. Gradient of Deterministic Policies\n### 3.1 Regulariy Conditions\n- 어떠한 이론이 성립하기 위한 전제 조건\n- Regularity conditions A.1\n    - $ p(s'|s,a), \\nabla_{a}p(s'|s,a), \\mu_{\\theta}(s), \\nabla_{\\theta}\\mu_{\\theta}(s), r(s,a), \\nabla_{a}r(s,a), p_{1}(s) $ are continuous in all parameters and variables $ s, a, s' $ and $ x $.\n- regularity conditions A.2\n    - There exists a $ b $ and $ L $ such that $ \\sup_{s}p_{1}(s) < b $, $ \\sup_{a,s,s'}p(s'|s,a) < b $, $ \\sup_{a,s}r(s,a) < b $, $ \\sup_{a,s,s'}\\|\\nabla_{a}p(s'|s,a)\\| < L $, and $ \\sup_{a,s}\\|\\nabla_{a}r(s,a)\\| < L $.\n\n### 3.2 Deterministic Policy Gradient Theorem\n- Deterministic policy\n    - $ \\mu_{\\theta} : S \\to A $ with parameter vector $ \\theta \\in \\rm I \\! R^n $\n- Probability distribution\n    - $ p(s \\to s', t, \\mu) $\n- Discounted state distribution\n    - $ \\rho^{\\mu}(s) $\n- Performance objective\n\n$$\nJ(\\mu_{\\theta}) = E[r^{\\gamma}_{1} | \\mu] \n$$\n\n$$\n= \\int_{S}\\rho^{\\mu}(s)r(s,\\mu_{\\theta}(s))ds \n= E_{s \\sim \\rho^{\\mu}}[r(s,\\mu_{\\theta}(s))]\n$$\n\n- DPG Theorem\n    - MDP 가 A.1 만족한다면, 아래 식(9)이 성립함\n    $\\nabla_{\\theta}J(\\mu_{\\theta}) = \\int_{S}\\rho^{\\mu}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}ds \\nonumber$\n    $= E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]   \\nonumber $ (9)\n    \n\t- DPG는 State space 에 대해서만 평균을 취하면 되기에, State와 Action space 모두에 대해 평균을 취해야 하는 [SPG](#spg-theorem)에 비해 data efficiency가 좋다. 즉, 더 적은 양의 데이터로도 학습이 잘 이뤄지게 된다.\n\n    \n### 3.3 DPG 형태에 대한 informal intuition\n- Generalized policy iteration\n    - 정책 평가와 정책 발전을 한 번 씩 번갈아 가면서 실행하는 정책 iteration\n        - 위와 같이 해도 정책 평가에서 예측한 가치함수가 최적 가치함수에 수렴함\n- 정책 평가\n    - action-value function $ Q^{\\pi}(s,a) $ or $ Q^{\\mu}(s,a) $ 을 estimate 하는 것\n- 정책 발전\n    - 위 estimated action-value function 에 따라 정책을 update 하는 것\n    - 주로 action-value function 에 대한 greedy maximisation 사용함\n        - $ \\mu^{k+1}(s) = \\arg\\max\\limits_{a}Q^{\\mu^{k}}(s,a) $\n        - greedy 정책 발전은 매 단계마다 global maximization을 해야하는데, 이는 continuous action spaces 에서 계산량이 급격히 늘어나게 됨.\n    - 그렇기에 policy gradient 방법이 나옴\n        - policy 를 $ \\theta $ 에 대해서 parameterize 함\n        - 매 단계마다 global maximisation 수행하는 대신, 방문하는 state $ s $ 마다 policy parameter 를 action-value function $ Q $ 의 $ \\theta $ 에 대한 gradient $ \\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s)) $ 방향으로 proportional 하게 update 함\n        - 하지만 각 state 는 다른 방향을 제시할 수 있기에, state distribution $ \\rho^{\\mu}(s) $ 에 대한 기대값을 취해 policy parameter 를 update 할 수도 있음\n            - $ \\theta^{k+1} = \\theta^{k} + \\alpha \\rm I\\!E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s))] $\n        - 이는 chain-rule 에 따라 아래와 같이 분리될 수 있음\n            - $ \\theta^{k+1} = \\theta^{k} + \\alpha \\rm I\\!E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu^{k}}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $ (7)\n            - chain rule: $ \\frac{\\partial Q}{\\partial \\theta} = \\frac{\\partial a}{\\partial \\theta} \\frac{\\partial Q}{\\partial a} $\n        - 하지만 state distribution $ \\rho^{\\mu} $ 은 정책에 dependent 함\n            - 정책이 바꾸게 되면, 바뀐 정책에 따라 방문하게 되는 state 가 변하게 되기 때문에 state distribution이 변하게 됨\n        - 그렇기에 정책 update 시 state distribution에 대한 gradient 를 고려하지 않는데 정책 발전이 이뤄진다는 것은 직관적으로 와닿지 않을 수 있음\n        - deterministic policy gradient theorem 은 state distribution에 대한 gradient 계산없이 위 식(7) 대로만 update 해도 performance objective 의 gradient 를 정확하게 따름을 의미한다.\n\n\n### 3.4 DPG 는 SPG 의 limiting case 임\n- stochastic policy parameterization\n    - $ \\pi_{\\mu_{\\theta},\\sigma} $ by a deterministic policy $ \\mu_{\\theta} : S \\to A $ and a variance parameter $ \\sigma $\n    - $ \\sigma = 0 $ 이면, $ \\pi_{\\mu_{\\theta},\\sigma} \\equiv \\mu_{\\theta} $\n- Theorem 2. Policy 의 variance 가 0 에 수렴하면, 즉, $ \\sigma \\to 0 $, stochastic policy gradient 와 deterministic policy gradient 는 동일해짐\n    - 조건 : stochastic policy $ \\pi_{\\mu_{\\theta},\\sigma} = \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $\n        - $ \\sigma $ 는 variance\n        - $ \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $ 는 conditions B.1 만족\n        - MDP 는 conditions A.1 및 A.2 만족\n    - 결과 :\n        - $ \\lim\\limits_{\\sigma\\downarrow0}\\nabla_{\\theta}J(\\pi_{\\mu_{\\theta},\\sigma}) = \\nabla_{\\theta}J(\\mu_{\\theta})  $\n            - 좌변은 standard stochastic gradient 이며, 우변은 deterministic gradient.\n    - 의미 :\n        - deterministic policy gradient 는 stochastic policy gradient 의 특수 case 임\n        - 기존 유명한 policy gradients 기법들에 deterministic policy gradients 를 적용할 수 있음\n            - 기존 기법들 예: compatible function approximation (Sutton, 1999), natural gradients (Kakade, 2001), actor-critic (Bhatnagar, 2007) or episodic/batch methods (Peters, 2005)\n<br>\n \n---\n## 4. Deterministic Actor-Critic Algorithms\n1. 살사 critic 을 이용한 on-policy actor-critic\n    - 단점\n        - deterministic policy 에 의해 행동하면 exploration 이 잘 되지 않기에, sub-optimal 에 빠지기 쉬움\n    - 목적\n        - 교훈/정보제공\n        - 환경에서 충분한 noise 를 제공하여 exploration을 시킬 수 있다면, deterministic policy 를 사용한다고 하여도 좋은 학습 결과를 얻을 수도 있음$\n            - 예. 바람이 agent의 행동에 영향(noise)을 줌\n    - Remind: 살사(SARSA) update rule\n        - $ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma Q(s_{t+1},a_{t+1}) - Q(s_{t},a_{t})) $\n    - Algorithm\n        - Critic 은 MSE 를 $ \\bf minimize $ 하는 방향, 즉, action-value function 을 stochastic gradient $ \\bf descent $ 방법으로 update 함\n            - $ MSE = [Q^{\\mu}(s,a) - Q^{w}(s,a)]^2 $\n                - critic 은 실제 $ Q^{\\mu}(s,a) $ 대신 미분 가능한 $ Q^{w}(s,a) $ 로 대체하여 action-value function 을 estimate 하며, 이 둘 간 Mean Square Error 를 최소화하는 것이 목표\n            - $ \\nabla_{w}MSE \\approx -2 * [r + \\gamma Q^{w}(s',a') - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $\n                - $ \\nabla_{w}MSE = -2 * [Q^{\\mu}(s,a) - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $\n                - $ Q^{\\mu}(s,a) $ 를 $ r + \\gamma Q^{w}(s',a') $ 로 대체\n                    - $ Q^{\\mu}(s,a) = r + \\gamma Q^{\\mu}(s',a') $\n            - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $\n                - $w_{t+1} = w_{t} - \\alpha_{w}\\nabla_{w}MSE  \\nonumber$\n                $ \\approx w_{t} - \\alpha_{w} * (-2 * [r + \\gamma Q^{w}(s',a') - Q^{w}(s,a)] \\nabla_{w}Q^{w}(s,a)$\n                - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},a_{t+1}) - Q^{w}(s_{t},a_{t}) $\n        - Actor 는 식(9)에 따라 보상이 $ \\bf maximize $ 되는 방향, 즉, deterministic policy 를 stochastic gradient $ \\bf ascent $ 방법으로 update함\n            - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $\n2. Q-learning 을 이용한 off-policy actor-critic\n    - stochastic behavior policy $ \\beta(a|s) $ 에 의해 생성된 trajectories 로부터 deterministic target policy $ \\mu_{\\theta}(s) $ 를 학습하는 off-policy actor-critic\n    - performance objective\n        - $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)V^{\\mu}(s)ds \\nonumber \\\\\\\\$\n          $= \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds \\nonumber \\\\\\\\$\n          $= E_{s \\sim \\rho^{\\beta}}[Q^{\\mu}(s,\\mu_{\\theta}(s))]$\n    - off-policy deterministic policy gradient\n        - $ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $\n            - 논문에는 아래와 같이 나와있는데, 물결 표시 부분은 오류로 판단됨.\n            - $ \\begin{eqnarray}\n                \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) &\\approx& \\int_{S}\\rho^{\\beta}(s)\\nabla_{\\theta}\\mu_{\\theta}(a|s)Q^{\\mu}(s,a)ds \\nonumber \\\\\n                &=& E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]\n                \\end{eqnarray} $\n            - 근거: Action이 deterministic 하기에 stochastic 경우와는 다르게 performance objective 에서 action 에 대해 평균을 구할 필요가 없음. 그렇기에, 곱의 미분이 있을 필요가 없고, [Degris, 2012b] 에서처럼 곱의 미분을 통해 생기는 action-value function 에 대한 gradient term 를 생략할 필요가 사라짐. [참고](#off-policy-actor-critic)\n    - Remind: 큐러닝(Q-learning) update rule\n        - $ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma \\max\\limits_{a}Q(s_{t+1},a) - Q(s_{t},a_{t})) $\n    - algorithm: OPDAC (Off-Policy Deterministic Actor-Critic)\n        - 살사를 이용한 on-policy deterministic actor-critic 과 아래 부분을 제외하고는 같음\n            - target policy 는 $ \\beta(a|s) $ 에 의해 생성된 trajectories 를 통해 학습함\n            - 업데이트 목표 부분에 실제 행동 값 $ a_{t+1} $ 이 아니라 정책으로부터 나온 행동 값 $ \\mu_{\\theta}(s_{t+1}) $ 사용함\n                - $ \\mu_{\\theta}(s_{t+1}) $ 는 가장 높은 Q 값을 가지는 행동. 즉, Q-learning.\n        - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $\n        - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $\n        - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $\n    - Stochastic off-policy actor-critic 은 대개 actor 와 critic 모두 importance sampling을 필요로 하지만, deterministic policy gradient 에선 importance sampling이 필요없음\n        - Actor 는 deterministic 이기에 sampling 자체가 필요없음\n            - Stochastic policy 인 경우, Actor 에서 importance sampling이 필요한 이유는 상태 $ s $ 에서의 가치 함수 값 $ V^{\\pi}(s) $ 을 estimate 하기 위해 $ \\pi $ 가 아니라 $ \\beta $ 에 따라 sampling을 한 후, 평균을 내기 때문임.\n            - 하지만 Deterministic policy 인 경우, 상태 $ s $ 에서의 가치 함수 값 $ V^{\\pi}(s) = Q^{\\pi}(s,\\mu_{\\theta}) $ 즉, action 이 상태 s 에 대해 deterministic 이기에 sampling 을 통해 estimate 할 필요가 없고, 따라서 importance sampling도 필요없어짐.\n            - stochastic vs. deterministic performance objective\n                - stochastic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads $\n                - deterministic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds $\n        - Critic 이 사용하는 Q-learning 은 importance sampling이 필요없는 off policy 알고리즘임.\n            - Q-learning 도 업데이트 목표를 특정 분포에서 샘플링을 통해 estimate 하는 것이 아니라 Q 함수를 최대화하는 action을 선택하는 것이기에 위 actor 에서의 deterministic 경우와 비슷하게 볼 수 있음\n3. compatible function approximation 및 gradient temporal-difference learning 을 이용한 actor-critic\n    - 위 살사/Q-learning 기반 on/off-policy 는 아래와 같은 문제가 존재\n        - function approximator 에 의한 bias\n            - 일반적으로 $ Q^{\\mu}(s,a) $ 를 $ Q^{w}(s,a) $ 로 대체하여 deterministic policy gradient 를 구하면, 그 gradient 는 ascent 하는 방향이 아닐 수도 있음\n        - off-policy learning 에 의한 instabilities\n    - 그래서 stochastic 처럼 $ \\nabla_{a}Q^{\\mu}(s,a) $ 를 $ \\nabla_{a}Q^{w}(s,a) $ 로 대체해도 deterministic policy gradient 에 영향을 미치지 않을 compatible function approximator $ Q^{w}(s,a) $ 를 찾아야 함\n    - Theorem 3. 아래 두 조건을 만족하면, $ Q^{w}(s,a) $ 는 deterministic policy $ \\mu_{\\theta}(s) $ 와 compatible 함. 즉, $ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $\n        - $ \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $\n        - $ w $ 는 $ MSE(\\theta, w) = E[\\epsilon(s;\\theta,w)^{\\top}\\epsilon(s;\\theta,w)] $ 를 최소화함\n            - $ \\epsilon(s;\\theta,w) = \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} - \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}  $\n    - Theorem 3 은 on-policy 뿐만 아니라 off-policy 에도 적용 가능함\n    - $ Q^{w}(s,a) = (a-\\mu_{\\theta}(s))^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top} w + V^{v}(s) $\n        - 어떠한 deterministic policy 에 대해서도 위 형태와 같은 compatible function approximator 가 존재함.\n        - 앞의 term 은 advantage 를, 뒤의 term 은 value 로 볼 수 있음\n    - $ Q^{w}(s,a) = \\phi(s,a)^{\\top} w + v^{\\top}\\phi(s) $\n        - 정의 : $ \\phi(s,a) \\overset{\\underset{\\mathrm{def}}{}}{=} \\nabla_{\\theta}\\mu_{\\theta}(s)(a-\\mu_{\\theta}(s)) $\n        - 일례 : $ V^{v}(s) = v^{\\top}\\phi(s) $\n        - Theorem 3 만족 여부\n            - 첫 번째 조건 만족.\n            - 두 번째 조건은 대강 만족.\n                - $ \\nabla_{a}Q^{\\mu}(s,a) $ 에 대한 unbiased sample 을 획득하기는 매우 어렵기에, 일반적인 정책 평가 방법들로 $ w $ 학습.\n                - 이 정책 평가 방법들을 이용하면 $ Q^{w}(s,a) \\approx Q^{\\mu}(s,a) $ 인 reasonable solution을 찾을 수 있기에 대강 $ \\nabla_{a}Q^{w}(s,a) \\approx \\nabla_{a}Q^{\\mu}(s,a) $ 이 될 것.\n        - action-value function 에 대한 linear function approximator 는 큰 값을 가지는 actions 에 대해선 diverge할 수 있어 global 하게 action-values 예측하기에는 좋지 않지만, local critic 에 사용할 때는 매우 유용하다.\n            - 즉, 절대값이 아니라 작은 변화량을 다루는 gradient method 경우엔 $ A^{w}(s,\\mu_{\\theta}(s)+\\delta) = \\delta^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $ 로, diverge 하지 않고, 값을 얻을 수 있음\n    - COPDAC-Q algorithm (Compatible Off-Policy Deterministic Actor-Critic Q-learning critic)\n        - Critic: 실제 action-value function 에 대한 linear function approximator 인 $ Q^{w}(s,a) = \\phi(s,a)^{\\top}w $ 를 estimate\n            - $ \\phi(s,a) = a^{\\top}\\nabla_{\\theta}\\mu_{\\theta} $\n            - Behavior policy $ \\beta(a|s) $ 로부터 얻은 samples를 이용하여 Q-learning 이나 gradient Q-learning 과 같은 off-policy algorithm 으로 학습 가능함\n        - Actor: estimated action-value function 에 대한 gradient 를 $ \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $ 로 치환 후, 정책을 업데이트함\n        - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $\n        - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) $\n        - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $\n    - off-policy Q-learning은 linear function approximation을 이용하면 diverge 할 수도 있음\n        - $ \\mu_{\\theta}(s_{t+1}) $ 이 diverge 할 수도 있기 때문으로 판단됨.\n        - 그렇기에 simple Q-learning 대신 다른 기법이 필요함.\n    - 그렇기에 critic 에 gradient Q-learning 사용한 COPDAC-GQ (Gradient Q-learning critic) algorithm 제안\n        - gradient temporal-difference learning 에 기반한 기법들은 true gradient descent algorithm 이며, converge가 보장됨. (Sutton, 2009)\n            - 기본 아이디어는 stochastic gradient descent 로 Mean-squared projected Bellman error (MSPBE) 를 최소화하는 것\n            - critic 이 actor 보다 빠른 time-scale 로 update 되도록 step size 들을 잘 조절하면, critic 은 MSPBE 를 최소화하는 parameters 로 converge 하게 됨\n            - critic 에 gradient temporal-difference learning 의 일종인 gradient Q-learning 사용한 논문 (Maei, 2010)\n    - COPDAC-GQ algorithm\n        - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $\n        - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $\n        - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) - \\alpha_{w}\\gamma\\phi(s_{t+1}, \\mu_{\\theta}(s_{t+1}))(\\phi(s_{t},a_{t})^{\\top} u_{t}) $\n        - $ v_{t+1} = v_{t} + \\alpha_{v}\\delta_{t}\\phi(s_{t}) - \\alpha_{v}\\gamma\\phi(s_{t+1})(\\phi(s_{t},a_{t})^{\\top} u_{t}) $\n        - $ u_{t+1} = u_{t} + \\alpha_{u}(\\delta_{t}-\\phi(s_{t}, a_{t})^{\\top} u_{t})\\phi(s_{t}, a_{t}) $\n    - stochastic actor-critic 과 같이 매 time-step 마다 update 시 필요한 계산의 복잡도는 $ O(mn) $\n        - m 은 action dimensions, n 은 number of policy parameters\n    - Natural policy gradient 를 이용해 deterministic policies 를 찾을 수 있음\n        - $ M(\\theta)^{-1}\\nabla_{\\theta}J(\\mu_{\\theta}) $ 는 any metric $ M(\\theta) $ 에 대한 our performance objective (식(14)) 의 steepest ascent direction 임 (Toussaint, 2012)\n        - Natural gradient 는 Fisher information metric $ M_{\\pi}(\\theta) $ 에 대한 steepest ascent direction 임\n            -  $ M_{\\pi}(\\theta) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)^{\\top}] $\n            - Fisher information metric 은 policy reparameterization 에 대해 불변임 (Bagnell, 2003)\n        - deterministic policies 에 대해 metric 으로 $ M_{\\mu}(\\theta) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}] $ 을 사용.\n        \t- 이는 variance 가 0 인 policy 에 대한 Fisher information metric 으로 볼 수 있음\n        \t- $ \\frac{\\nabla_{\\theta}\\pi_{\\theta}(a\\vert s)}{\\pi_{\\theta}(a\\vert s)}$ 에서 policy variance 가 0 이면, 특정 s 에 대한 $ \\pi_{\\theta}(a|s)$ 만 1 이 되고, 나머지는 0 임\n        - deterministic policy gradient theorem 과 compatible function approximation 을 결합하면 $ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] $ 이 됨\n            - $ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $\n            - $ \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)} \\approx \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $\n        - 그렇기에 steepest ascent direction 은 $ M_{\\mu}(\\theta)^{-1}\\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = w $ 이 됨\n            - $ E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}]^{-1}E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] = w $\n        - 이 알고리즘은 COPDAC-Q 혹은 COPDAC-GQ 에서 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $ 식을 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta}w_{t} $ 로 바꿔주기만 하면 됨\n\n## Experiments\n### Continuous Bandit\n- Stochastic Actor-Critic (SAC)과 COPDAC 간 성능 비교 수행\n    - Action dimension이 커질수록 성능 차이가 심함\n    - 빠르게 수렴하는 것을 통해 DPG의 data efficiency가 SPG에 비해 좋다는 것을 확인할 수 있지만, 반면, time-step이 증가할수록 SAC와 COPDAC 간 성능 차이가 줄어드는 것을 통해 성능 차이가 심하다는 것은 일정 time step 내에서만 해당하는 것이라고 유추해볼 수 있음\n    - <img src=\"https://www.dropbox.com/s/hrkyq0s2f24z66r/Screenshot%202018-06-16%2017.47.38.png?dl=1\">\n\n### Continuous Reinforcement Learning\n- COPDAC-Q가 SAC와 off-policy stochastic actor-critic(OffPAC-TD) 간 성능 비교 수행\n    - COPDAC-Q의 성능이 약간 더 좋음\n    - COPDAC-Q의 학습이 더 빨리 이뤄짐\n    - <img src=\"https://www.dropbox.com/s/qdca4augapmzsxi/Screenshot%202018-06-16%2017.47.07.png?dl=1\">\n### Octopus Arm\n- 목표: 6 segments octopus arm (20 action dimensions & 50 state dimensions)을 control하여 target을 맞추는 것\n    - COPDAC-Q 사용 시, action space dimension이 큰 octopus arm을 잘 control하여 target을 맞춤\n    - <img src=\"https://www.dropbox.com/s/xrxb0a52wntekld/Screenshot%202018-06-16%2017.46.28.png?dl=1\" width=600px>\n    - 기존 기법들은 action spaces 혹은 action 과 state spaces 둘 다 작은 경우들에 대해서만 실험했다고 하며, 비교하고 있지 않음.\n        - 기존 기법들이 6 segments octopus arm에서 동작을 잘 안 했을 것 같긴한데, 그래도 실험해서 결과를 보여주지...왜 안 했을까?\n    - 8 segment arm 동영상이 저자 홈페이지에 있다고 하는데, 안 보임.\n- [참고] Octopus Arm 이란?\n    - [OctopusArm Youtube Link](https://www.youtube.com/watch?v=AxeeHif0euY)\n    - <img src=\"https://www.dropbox.com/s/950ycj06sudakjx/Screenshot%202018-06-16%2017.45.52.png?dl=1\">\n","source":"_posts/dpg.md","raw":"---\ntitle: Deterministic Policy Gradient Algorithms\ndate: 2018-06-27 17:21:48\ntags: [\"프로젝트\", \"피지여행\"]\ncategories: 프로젝트\nauthor: 장수영, 공민서\nsubtitle: 피지여행 2번째 논문\n---\n\n<center> <img src=\"https://www.dropbox.com/s/etwa3hn5c0pegdv/Screen%20Shot%202018-07-18%20at%2012.51.35%20AM.png?dl=1\" width=\"800\"> </center>\n\n논문 저자 : David Silver, Guy Lever, Nicloas Heess, Thomas Degris, Daan Wierstra, Martin Riedmiller\n논문 링크 : http://proceedings.mlr.press/v32/silver14.pdf\nProceeding : International Conference on Machine Learning (ICML) 2014\n정리 : 장수영, 공민서\n\n---\n\n## 1. Summary\n- Deterministic Policy Gradient (DPG) Theorem 제안함 [[Theorem 1](#deterministic-policy-gradient-theorem)]\n    1) DPG는 존재하며,\n    2) DPG는 Expected gradient of the action-value function의 형태를 띈다.\n- Policy variance가 0에 수렴할 경우, DPG는 Stochastic Policy Gradient(SPG)와 동일해짐을 보임 [[Theorem 2](#dpg-는-spg-의-limiting-case-임)]\n    - Theorem 2 로 인해 기존 Policy Gradient (PG) 와 관련된 기법들을 DPG 에 적용할 수 있게 됨\n        - 예. Sutton PG, natural gradients, actor-critic, episodic/batch methods\n- 적절한 exploration 을 위해 model-free, off-policy Actor-Critic algorithm 을 제안함\n    - Action-value function approximator 사용으로 인해 policy gradient가 bias되는 것을 방지하기 위해 compatibility conditions을 제공 [[Theorem 3](##deterministic-actor-critic-algorithms)]\n- DPG 는 SPG 보다 성능이 좋음\n    - 특히 high dimensional action spaces 을 가지는 tasks에서의 성능 향상이 큼\n        - SPG의 policy gradient는 state와 action spaces 모두에 대해서, DPG의 policy gradient는  state spaces에 대해서만 평균을 취함\n        - 결과적으로, action spaces의 dimension이 커질수록 data efficiency가 높은 DPG의 학습이 더 잘 이뤄지게 됨\n        - 무한정 학습을 시키게 되면, SPG도 최적으로 수렴할 것으로 예상되기에 위 성능 비교는 일정 iteration 내로 한정함\n    - 기존 기법들에 비해 computation 양이 많지 않음\n        - Computation 은 action dimensionality 와 policy parameters 수에 비례함\n<br>\n\n---\n## 2. Background\n### 2.1 Performance objective function\n\n$$\n\\begin{align}\nJ(\\pi_{\\theta}) &= \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\pi_{\\theta}(s,a)r(s,a)dads = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[r(s,a)]\n\\end{align}\n$$\n\n### 2.2 SPG Theorem\n- State distribution $ \\rho^{\\pi}(s) $ 은 policy parameters에 영향을 받지만, policy gradient 를 계산할 때는 state distribution 의 gradient 를 고려할 필요가 없다.\n- $$\\begin{eqnarray}\\nabla_{\\theta}J(\\pi_{\\theta}) &=& \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\\\ &=& E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]\n\\end{eqnarray}$$\n\n### 2.3 Stochastic Actor-Critic Algorithms\n- Actor 와 Critic 이 번갈아가면서 동작하며 stochastic policy 를 최적화하는 기법\n- Actor: $ Q^{\\pi}(s,a) $ 를 근사한 $ Q^w(s,a) $ 를 이용해 stochastic policy gradient 를 ascent 하는 방향으로 policy parameter $ \\theta $ 를 업데이트함으로써 stochastic policy 를 발전시킴\n    - $ \\nabla_{\\theta}J(\\pi_{\\theta}) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}log\\pi_{\\theta}(a|s)Q^{w}(s,a)] $\n- Critic: SARSA 나 Q-learning 같은 Temporal-difference (TD) learning 을 이용해 action-value function의 parameter, $ w $ 를 업데이트함으로써 $ Q^w(s,a) $ 가 $ Q^{\\pi}(s,a) $ 과 유사해지도록 함\n- 실제 값인 $ Q^{\\pi}(s,a) $ 대신 이를 근사한 $ Q^w(s,a) $ 를 사용하게 되면, 일반적으로 bias 가 발생하게 된다. 하지만, compatible condition 에 부합하는 $ Q^w(s,a) $ 를 사용하게 되면, bias 가 발생하지 않는다.\n\n### 2.4 Off-policy Actor-Critic\n- Distinct behavior policy $ \\beta(a|s) ( \\neq \\pi_{\\theta}(a|s) ) $ 로부터 샘플링된 trajectories 를 이용한 Actor-Critic\n- Performance objective function\n    - $\\begin{eqnarray}\n        J_{\\beta}(\\pi_{\\theta}) \n        &=& \\int_{S}\\rho^{\\beta}(s)V^{\\pi}(s)ds \\nonumber \\\\\\\\\n        &=& \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads\n        \\end{eqnarray} $\n- off-policy policy gradient\n    - $ \\begin{eqnarray}\n        \\nabla_{\\theta}J_{\\beta}(\\pi_{\\theta}) &\\approx& \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\end{eqnarray} $\n      $=E_{s \\sim \\rho^{\\beta}, a \\sim \\beta}[\\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)}\\nabla_{\\theta}log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]$\n    - off-policy policy gradient 식에서의 물결 표시는 [Degris, 2012b] 논문에 근거함\n        - [Degris, 2012b] \"Linear off-policy actor-critic,\" ICML 2012\n        - Exact off-policy policy gradient 와 이를 approximate 한 policy gradient 는 아래와 같음. (빨간색 상자에 있는 항목을 삭제함으로써 근사함)\n            - <img src=\"https://www.dropbox.com/s/xzpv3okc139c1fs/Screenshot%202018-06-16%2017.48.51.png?dl=1\" width=500px>\n        - [Degris, 2012b] Theorem 1 에 의해 policy parameter 가 approximated policy gradient ( $\\nabla_{u}𝑄^{\\pi,\\gamma}(𝑠,𝑎)$ term 제거)에 따라 업데이트되어도 policy 는 improve 가 됨이 보장되기에 exact off-policy policy gradient 대신 approximated off-policy policy gradient 를 사용해도 괜찮음.\n            - <img src=\"https://www.dropbox.com/s/mk13931r4scjngo/Screenshot%202018-06-16%2017.49.24.png?dl=1\" width=500px>\n    - off-policy policy gradient 식에서 $ \\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)} $ 는 importance sampling ratio 임\n        - off-policy actor-critic 에서는 $ \\beta $ 에 의해 샘플링된 trajectory 를 이용해서 stochastic policy $ \\pi $ 를 예측하는 것이기 때문에 imnportance sampling 이 필요함\n<br>\n\n---\n## 3. Gradient of Deterministic Policies\n### 3.1 Regulariy Conditions\n- 어떠한 이론이 성립하기 위한 전제 조건\n- Regularity conditions A.1\n    - $ p(s'|s,a), \\nabla_{a}p(s'|s,a), \\mu_{\\theta}(s), \\nabla_{\\theta}\\mu_{\\theta}(s), r(s,a), \\nabla_{a}r(s,a), p_{1}(s) $ are continuous in all parameters and variables $ s, a, s' $ and $ x $.\n- regularity conditions A.2\n    - There exists a $ b $ and $ L $ such that $ \\sup_{s}p_{1}(s) < b $, $ \\sup_{a,s,s'}p(s'|s,a) < b $, $ \\sup_{a,s}r(s,a) < b $, $ \\sup_{a,s,s'}\\|\\nabla_{a}p(s'|s,a)\\| < L $, and $ \\sup_{a,s}\\|\\nabla_{a}r(s,a)\\| < L $.\n\n### 3.2 Deterministic Policy Gradient Theorem\n- Deterministic policy\n    - $ \\mu_{\\theta} : S \\to A $ with parameter vector $ \\theta \\in \\rm I \\! R^n $\n- Probability distribution\n    - $ p(s \\to s', t, \\mu) $\n- Discounted state distribution\n    - $ \\rho^{\\mu}(s) $\n- Performance objective\n\n$$\nJ(\\mu_{\\theta}) = E[r^{\\gamma}_{1} | \\mu] \n$$\n\n$$\n= \\int_{S}\\rho^{\\mu}(s)r(s,\\mu_{\\theta}(s))ds \n= E_{s \\sim \\rho^{\\mu}}[r(s,\\mu_{\\theta}(s))]\n$$\n\n- DPG Theorem\n    - MDP 가 A.1 만족한다면, 아래 식(9)이 성립함\n    $\\nabla_{\\theta}J(\\mu_{\\theta}) = \\int_{S}\\rho^{\\mu}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}ds \\nonumber$\n    $= E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]   \\nonumber $ (9)\n    \n\t- DPG는 State space 에 대해서만 평균을 취하면 되기에, State와 Action space 모두에 대해 평균을 취해야 하는 [SPG](#spg-theorem)에 비해 data efficiency가 좋다. 즉, 더 적은 양의 데이터로도 학습이 잘 이뤄지게 된다.\n\n    \n### 3.3 DPG 형태에 대한 informal intuition\n- Generalized policy iteration\n    - 정책 평가와 정책 발전을 한 번 씩 번갈아 가면서 실행하는 정책 iteration\n        - 위와 같이 해도 정책 평가에서 예측한 가치함수가 최적 가치함수에 수렴함\n- 정책 평가\n    - action-value function $ Q^{\\pi}(s,a) $ or $ Q^{\\mu}(s,a) $ 을 estimate 하는 것\n- 정책 발전\n    - 위 estimated action-value function 에 따라 정책을 update 하는 것\n    - 주로 action-value function 에 대한 greedy maximisation 사용함\n        - $ \\mu^{k+1}(s) = \\arg\\max\\limits_{a}Q^{\\mu^{k}}(s,a) $\n        - greedy 정책 발전은 매 단계마다 global maximization을 해야하는데, 이는 continuous action spaces 에서 계산량이 급격히 늘어나게 됨.\n    - 그렇기에 policy gradient 방법이 나옴\n        - policy 를 $ \\theta $ 에 대해서 parameterize 함\n        - 매 단계마다 global maximisation 수행하는 대신, 방문하는 state $ s $ 마다 policy parameter 를 action-value function $ Q $ 의 $ \\theta $ 에 대한 gradient $ \\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s)) $ 방향으로 proportional 하게 update 함\n        - 하지만 각 state 는 다른 방향을 제시할 수 있기에, state distribution $ \\rho^{\\mu}(s) $ 에 대한 기대값을 취해 policy parameter 를 update 할 수도 있음\n            - $ \\theta^{k+1} = \\theta^{k} + \\alpha \\rm I\\!E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s))] $\n        - 이는 chain-rule 에 따라 아래와 같이 분리될 수 있음\n            - $ \\theta^{k+1} = \\theta^{k} + \\alpha \\rm I\\!E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu^{k}}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $ (7)\n            - chain rule: $ \\frac{\\partial Q}{\\partial \\theta} = \\frac{\\partial a}{\\partial \\theta} \\frac{\\partial Q}{\\partial a} $\n        - 하지만 state distribution $ \\rho^{\\mu} $ 은 정책에 dependent 함\n            - 정책이 바꾸게 되면, 바뀐 정책에 따라 방문하게 되는 state 가 변하게 되기 때문에 state distribution이 변하게 됨\n        - 그렇기에 정책 update 시 state distribution에 대한 gradient 를 고려하지 않는데 정책 발전이 이뤄진다는 것은 직관적으로 와닿지 않을 수 있음\n        - deterministic policy gradient theorem 은 state distribution에 대한 gradient 계산없이 위 식(7) 대로만 update 해도 performance objective 의 gradient 를 정확하게 따름을 의미한다.\n\n\n### 3.4 DPG 는 SPG 의 limiting case 임\n- stochastic policy parameterization\n    - $ \\pi_{\\mu_{\\theta},\\sigma} $ by a deterministic policy $ \\mu_{\\theta} : S \\to A $ and a variance parameter $ \\sigma $\n    - $ \\sigma = 0 $ 이면, $ \\pi_{\\mu_{\\theta},\\sigma} \\equiv \\mu_{\\theta} $\n- Theorem 2. Policy 의 variance 가 0 에 수렴하면, 즉, $ \\sigma \\to 0 $, stochastic policy gradient 와 deterministic policy gradient 는 동일해짐\n    - 조건 : stochastic policy $ \\pi_{\\mu_{\\theta},\\sigma} = \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $\n        - $ \\sigma $ 는 variance\n        - $ \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $ 는 conditions B.1 만족\n        - MDP 는 conditions A.1 및 A.2 만족\n    - 결과 :\n        - $ \\lim\\limits_{\\sigma\\downarrow0}\\nabla_{\\theta}J(\\pi_{\\mu_{\\theta},\\sigma}) = \\nabla_{\\theta}J(\\mu_{\\theta})  $\n            - 좌변은 standard stochastic gradient 이며, 우변은 deterministic gradient.\n    - 의미 :\n        - deterministic policy gradient 는 stochastic policy gradient 의 특수 case 임\n        - 기존 유명한 policy gradients 기법들에 deterministic policy gradients 를 적용할 수 있음\n            - 기존 기법들 예: compatible function approximation (Sutton, 1999), natural gradients (Kakade, 2001), actor-critic (Bhatnagar, 2007) or episodic/batch methods (Peters, 2005)\n<br>\n \n---\n## 4. Deterministic Actor-Critic Algorithms\n1. 살사 critic 을 이용한 on-policy actor-critic\n    - 단점\n        - deterministic policy 에 의해 행동하면 exploration 이 잘 되지 않기에, sub-optimal 에 빠지기 쉬움\n    - 목적\n        - 교훈/정보제공\n        - 환경에서 충분한 noise 를 제공하여 exploration을 시킬 수 있다면, deterministic policy 를 사용한다고 하여도 좋은 학습 결과를 얻을 수도 있음$\n            - 예. 바람이 agent의 행동에 영향(noise)을 줌\n    - Remind: 살사(SARSA) update rule\n        - $ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma Q(s_{t+1},a_{t+1}) - Q(s_{t},a_{t})) $\n    - Algorithm\n        - Critic 은 MSE 를 $ \\bf minimize $ 하는 방향, 즉, action-value function 을 stochastic gradient $ \\bf descent $ 방법으로 update 함\n            - $ MSE = [Q^{\\mu}(s,a) - Q^{w}(s,a)]^2 $\n                - critic 은 실제 $ Q^{\\mu}(s,a) $ 대신 미분 가능한 $ Q^{w}(s,a) $ 로 대체하여 action-value function 을 estimate 하며, 이 둘 간 Mean Square Error 를 최소화하는 것이 목표\n            - $ \\nabla_{w}MSE \\approx -2 * [r + \\gamma Q^{w}(s',a') - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $\n                - $ \\nabla_{w}MSE = -2 * [Q^{\\mu}(s,a) - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $\n                - $ Q^{\\mu}(s,a) $ 를 $ r + \\gamma Q^{w}(s',a') $ 로 대체\n                    - $ Q^{\\mu}(s,a) = r + \\gamma Q^{\\mu}(s',a') $\n            - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $\n                - $w_{t+1} = w_{t} - \\alpha_{w}\\nabla_{w}MSE  \\nonumber$\n                $ \\approx w_{t} - \\alpha_{w} * (-2 * [r + \\gamma Q^{w}(s',a') - Q^{w}(s,a)] \\nabla_{w}Q^{w}(s,a)$\n                - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},a_{t+1}) - Q^{w}(s_{t},a_{t}) $\n        - Actor 는 식(9)에 따라 보상이 $ \\bf maximize $ 되는 방향, 즉, deterministic policy 를 stochastic gradient $ \\bf ascent $ 방법으로 update함\n            - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $\n2. Q-learning 을 이용한 off-policy actor-critic\n    - stochastic behavior policy $ \\beta(a|s) $ 에 의해 생성된 trajectories 로부터 deterministic target policy $ \\mu_{\\theta}(s) $ 를 학습하는 off-policy actor-critic\n    - performance objective\n        - $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)V^{\\mu}(s)ds \\nonumber \\\\\\\\$\n          $= \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds \\nonumber \\\\\\\\$\n          $= E_{s \\sim \\rho^{\\beta}}[Q^{\\mu}(s,\\mu_{\\theta}(s))]$\n    - off-policy deterministic policy gradient\n        - $ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $\n            - 논문에는 아래와 같이 나와있는데, 물결 표시 부분은 오류로 판단됨.\n            - $ \\begin{eqnarray}\n                \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) &\\approx& \\int_{S}\\rho^{\\beta}(s)\\nabla_{\\theta}\\mu_{\\theta}(a|s)Q^{\\mu}(s,a)ds \\nonumber \\\\\n                &=& E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]\n                \\end{eqnarray} $\n            - 근거: Action이 deterministic 하기에 stochastic 경우와는 다르게 performance objective 에서 action 에 대해 평균을 구할 필요가 없음. 그렇기에, 곱의 미분이 있을 필요가 없고, [Degris, 2012b] 에서처럼 곱의 미분을 통해 생기는 action-value function 에 대한 gradient term 를 생략할 필요가 사라짐. [참고](#off-policy-actor-critic)\n    - Remind: 큐러닝(Q-learning) update rule\n        - $ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma \\max\\limits_{a}Q(s_{t+1},a) - Q(s_{t},a_{t})) $\n    - algorithm: OPDAC (Off-Policy Deterministic Actor-Critic)\n        - 살사를 이용한 on-policy deterministic actor-critic 과 아래 부분을 제외하고는 같음\n            - target policy 는 $ \\beta(a|s) $ 에 의해 생성된 trajectories 를 통해 학습함\n            - 업데이트 목표 부분에 실제 행동 값 $ a_{t+1} $ 이 아니라 정책으로부터 나온 행동 값 $ \\mu_{\\theta}(s_{t+1}) $ 사용함\n                - $ \\mu_{\\theta}(s_{t+1}) $ 는 가장 높은 Q 값을 가지는 행동. 즉, Q-learning.\n        - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $\n        - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $\n        - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $\n    - Stochastic off-policy actor-critic 은 대개 actor 와 critic 모두 importance sampling을 필요로 하지만, deterministic policy gradient 에선 importance sampling이 필요없음\n        - Actor 는 deterministic 이기에 sampling 자체가 필요없음\n            - Stochastic policy 인 경우, Actor 에서 importance sampling이 필요한 이유는 상태 $ s $ 에서의 가치 함수 값 $ V^{\\pi}(s) $ 을 estimate 하기 위해 $ \\pi $ 가 아니라 $ \\beta $ 에 따라 sampling을 한 후, 평균을 내기 때문임.\n            - 하지만 Deterministic policy 인 경우, 상태 $ s $ 에서의 가치 함수 값 $ V^{\\pi}(s) = Q^{\\pi}(s,\\mu_{\\theta}) $ 즉, action 이 상태 s 에 대해 deterministic 이기에 sampling 을 통해 estimate 할 필요가 없고, 따라서 importance sampling도 필요없어짐.\n            - stochastic vs. deterministic performance objective\n                - stochastic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads $\n                - deterministic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds $\n        - Critic 이 사용하는 Q-learning 은 importance sampling이 필요없는 off policy 알고리즘임.\n            - Q-learning 도 업데이트 목표를 특정 분포에서 샘플링을 통해 estimate 하는 것이 아니라 Q 함수를 최대화하는 action을 선택하는 것이기에 위 actor 에서의 deterministic 경우와 비슷하게 볼 수 있음\n3. compatible function approximation 및 gradient temporal-difference learning 을 이용한 actor-critic\n    - 위 살사/Q-learning 기반 on/off-policy 는 아래와 같은 문제가 존재\n        - function approximator 에 의한 bias\n            - 일반적으로 $ Q^{\\mu}(s,a) $ 를 $ Q^{w}(s,a) $ 로 대체하여 deterministic policy gradient 를 구하면, 그 gradient 는 ascent 하는 방향이 아닐 수도 있음\n        - off-policy learning 에 의한 instabilities\n    - 그래서 stochastic 처럼 $ \\nabla_{a}Q^{\\mu}(s,a) $ 를 $ \\nabla_{a}Q^{w}(s,a) $ 로 대체해도 deterministic policy gradient 에 영향을 미치지 않을 compatible function approximator $ Q^{w}(s,a) $ 를 찾아야 함\n    - Theorem 3. 아래 두 조건을 만족하면, $ Q^{w}(s,a) $ 는 deterministic policy $ \\mu_{\\theta}(s) $ 와 compatible 함. 즉, $ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $\n        - $ \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $\n        - $ w $ 는 $ MSE(\\theta, w) = E[\\epsilon(s;\\theta,w)^{\\top}\\epsilon(s;\\theta,w)] $ 를 최소화함\n            - $ \\epsilon(s;\\theta,w) = \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} - \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}  $\n    - Theorem 3 은 on-policy 뿐만 아니라 off-policy 에도 적용 가능함\n    - $ Q^{w}(s,a) = (a-\\mu_{\\theta}(s))^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top} w + V^{v}(s) $\n        - 어떠한 deterministic policy 에 대해서도 위 형태와 같은 compatible function approximator 가 존재함.\n        - 앞의 term 은 advantage 를, 뒤의 term 은 value 로 볼 수 있음\n    - $ Q^{w}(s,a) = \\phi(s,a)^{\\top} w + v^{\\top}\\phi(s) $\n        - 정의 : $ \\phi(s,a) \\overset{\\underset{\\mathrm{def}}{}}{=} \\nabla_{\\theta}\\mu_{\\theta}(s)(a-\\mu_{\\theta}(s)) $\n        - 일례 : $ V^{v}(s) = v^{\\top}\\phi(s) $\n        - Theorem 3 만족 여부\n            - 첫 번째 조건 만족.\n            - 두 번째 조건은 대강 만족.\n                - $ \\nabla_{a}Q^{\\mu}(s,a) $ 에 대한 unbiased sample 을 획득하기는 매우 어렵기에, 일반적인 정책 평가 방법들로 $ w $ 학습.\n                - 이 정책 평가 방법들을 이용하면 $ Q^{w}(s,a) \\approx Q^{\\mu}(s,a) $ 인 reasonable solution을 찾을 수 있기에 대강 $ \\nabla_{a}Q^{w}(s,a) \\approx \\nabla_{a}Q^{\\mu}(s,a) $ 이 될 것.\n        - action-value function 에 대한 linear function approximator 는 큰 값을 가지는 actions 에 대해선 diverge할 수 있어 global 하게 action-values 예측하기에는 좋지 않지만, local critic 에 사용할 때는 매우 유용하다.\n            - 즉, 절대값이 아니라 작은 변화량을 다루는 gradient method 경우엔 $ A^{w}(s,\\mu_{\\theta}(s)+\\delta) = \\delta^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $ 로, diverge 하지 않고, 값을 얻을 수 있음\n    - COPDAC-Q algorithm (Compatible Off-Policy Deterministic Actor-Critic Q-learning critic)\n        - Critic: 실제 action-value function 에 대한 linear function approximator 인 $ Q^{w}(s,a) = \\phi(s,a)^{\\top}w $ 를 estimate\n            - $ \\phi(s,a) = a^{\\top}\\nabla_{\\theta}\\mu_{\\theta} $\n            - Behavior policy $ \\beta(a|s) $ 로부터 얻은 samples를 이용하여 Q-learning 이나 gradient Q-learning 과 같은 off-policy algorithm 으로 학습 가능함\n        - Actor: estimated action-value function 에 대한 gradient 를 $ \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $ 로 치환 후, 정책을 업데이트함\n        - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $\n        - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) $\n        - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $\n    - off-policy Q-learning은 linear function approximation을 이용하면 diverge 할 수도 있음\n        - $ \\mu_{\\theta}(s_{t+1}) $ 이 diverge 할 수도 있기 때문으로 판단됨.\n        - 그렇기에 simple Q-learning 대신 다른 기법이 필요함.\n    - 그렇기에 critic 에 gradient Q-learning 사용한 COPDAC-GQ (Gradient Q-learning critic) algorithm 제안\n        - gradient temporal-difference learning 에 기반한 기법들은 true gradient descent algorithm 이며, converge가 보장됨. (Sutton, 2009)\n            - 기본 아이디어는 stochastic gradient descent 로 Mean-squared projected Bellman error (MSPBE) 를 최소화하는 것\n            - critic 이 actor 보다 빠른 time-scale 로 update 되도록 step size 들을 잘 조절하면, critic 은 MSPBE 를 최소화하는 parameters 로 converge 하게 됨\n            - critic 에 gradient temporal-difference learning 의 일종인 gradient Q-learning 사용한 논문 (Maei, 2010)\n    - COPDAC-GQ algorithm\n        - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $\n        - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $\n        - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) - \\alpha_{w}\\gamma\\phi(s_{t+1}, \\mu_{\\theta}(s_{t+1}))(\\phi(s_{t},a_{t})^{\\top} u_{t}) $\n        - $ v_{t+1} = v_{t} + \\alpha_{v}\\delta_{t}\\phi(s_{t}) - \\alpha_{v}\\gamma\\phi(s_{t+1})(\\phi(s_{t},a_{t})^{\\top} u_{t}) $\n        - $ u_{t+1} = u_{t} + \\alpha_{u}(\\delta_{t}-\\phi(s_{t}, a_{t})^{\\top} u_{t})\\phi(s_{t}, a_{t}) $\n    - stochastic actor-critic 과 같이 매 time-step 마다 update 시 필요한 계산의 복잡도는 $ O(mn) $\n        - m 은 action dimensions, n 은 number of policy parameters\n    - Natural policy gradient 를 이용해 deterministic policies 를 찾을 수 있음\n        - $ M(\\theta)^{-1}\\nabla_{\\theta}J(\\mu_{\\theta}) $ 는 any metric $ M(\\theta) $ 에 대한 our performance objective (식(14)) 의 steepest ascent direction 임 (Toussaint, 2012)\n        - Natural gradient 는 Fisher information metric $ M_{\\pi}(\\theta) $ 에 대한 steepest ascent direction 임\n            -  $ M_{\\pi}(\\theta) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)^{\\top}] $\n            - Fisher information metric 은 policy reparameterization 에 대해 불변임 (Bagnell, 2003)\n        - deterministic policies 에 대해 metric 으로 $ M_{\\mu}(\\theta) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}] $ 을 사용.\n        \t- 이는 variance 가 0 인 policy 에 대한 Fisher information metric 으로 볼 수 있음\n        \t- $ \\frac{\\nabla_{\\theta}\\pi_{\\theta}(a\\vert s)}{\\pi_{\\theta}(a\\vert s)}$ 에서 policy variance 가 0 이면, 특정 s 에 대한 $ \\pi_{\\theta}(a|s)$ 만 1 이 되고, 나머지는 0 임\n        - deterministic policy gradient theorem 과 compatible function approximation 을 결합하면 $ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] $ 이 됨\n            - $ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $\n            - $ \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)} \\approx \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $\n        - 그렇기에 steepest ascent direction 은 $ M_{\\mu}(\\theta)^{-1}\\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = w $ 이 됨\n            - $ E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}]^{-1}E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] = w $\n        - 이 알고리즘은 COPDAC-Q 혹은 COPDAC-GQ 에서 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $ 식을 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta}w_{t} $ 로 바꿔주기만 하면 됨\n\n## Experiments\n### Continuous Bandit\n- Stochastic Actor-Critic (SAC)과 COPDAC 간 성능 비교 수행\n    - Action dimension이 커질수록 성능 차이가 심함\n    - 빠르게 수렴하는 것을 통해 DPG의 data efficiency가 SPG에 비해 좋다는 것을 확인할 수 있지만, 반면, time-step이 증가할수록 SAC와 COPDAC 간 성능 차이가 줄어드는 것을 통해 성능 차이가 심하다는 것은 일정 time step 내에서만 해당하는 것이라고 유추해볼 수 있음\n    - <img src=\"https://www.dropbox.com/s/hrkyq0s2f24z66r/Screenshot%202018-06-16%2017.47.38.png?dl=1\">\n\n### Continuous Reinforcement Learning\n- COPDAC-Q가 SAC와 off-policy stochastic actor-critic(OffPAC-TD) 간 성능 비교 수행\n    - COPDAC-Q의 성능이 약간 더 좋음\n    - COPDAC-Q의 학습이 더 빨리 이뤄짐\n    - <img src=\"https://www.dropbox.com/s/qdca4augapmzsxi/Screenshot%202018-06-16%2017.47.07.png?dl=1\">\n### Octopus Arm\n- 목표: 6 segments octopus arm (20 action dimensions & 50 state dimensions)을 control하여 target을 맞추는 것\n    - COPDAC-Q 사용 시, action space dimension이 큰 octopus arm을 잘 control하여 target을 맞춤\n    - <img src=\"https://www.dropbox.com/s/xrxb0a52wntekld/Screenshot%202018-06-16%2017.46.28.png?dl=1\" width=600px>\n    - 기존 기법들은 action spaces 혹은 action 과 state spaces 둘 다 작은 경우들에 대해서만 실험했다고 하며, 비교하고 있지 않음.\n        - 기존 기법들이 6 segments octopus arm에서 동작을 잘 안 했을 것 같긴한데, 그래도 실험해서 결과를 보여주지...왜 안 했을까?\n    - 8 segment arm 동영상이 저자 홈페이지에 있다고 하는데, 안 보임.\n- [참고] Octopus Arm 이란?\n    - [OctopusArm Youtube Link](https://www.youtube.com/watch?v=AxeeHif0euY)\n    - <img src=\"https://www.dropbox.com/s/950ycj06sudakjx/Screenshot%202018-06-16%2017.45.52.png?dl=1\">\n","slug":"dpg","published":1,"updated":"2018-07-22T05:48:08.297Z","_id":"cjiinnj6q0005j28a4kpaxi6x","comments":1,"layout":"post","photos":[],"link":"","content":"<center> <img src=\"https://www.dropbox.com/s/etwa3hn5c0pegdv/Screen%20Shot%202018-07-18%20at%2012.51.35%20AM.png?dl=1\" width=\"800\"> </center>\n\n<p>논문 저자 : David Silver, Guy Lever, Nicloas Heess, Thomas Degris, Daan Wierstra, Martin Riedmiller<br>논문 링크 : <a href=\"http://proceedings.mlr.press/v32/silver14.pdf\" target=\"_blank\" rel=\"noopener\">http://proceedings.mlr.press/v32/silver14.pdf</a><br>Proceeding : International Conference on Machine Learning (ICML) 2014<br>정리 : 장수영, 공민서</p>\n<hr>\n<h2 id=\"1-Summary\"><a href=\"#1-Summary\" class=\"headerlink\" title=\"1. Summary\"></a>1. Summary</h2><ul>\n<li>Deterministic Policy Gradient (DPG) Theorem 제안함 [<a href=\"#deterministic-policy-gradient-theorem\">Theorem 1</a>]<br>  1) DPG는 존재하며,<br>  2) DPG는 Expected gradient of the action-value function의 형태를 띈다.</li>\n<li>Policy variance가 0에 수렴할 경우, DPG는 Stochastic Policy Gradient(SPG)와 동일해짐을 보임 [<a href=\"#dpg-는-spg-의-limiting-case-임\">Theorem 2</a>]<ul>\n<li>Theorem 2 로 인해 기존 Policy Gradient (PG) 와 관련된 기법들을 DPG 에 적용할 수 있게 됨<ul>\n<li>예. Sutton PG, natural gradients, actor-critic, episodic/batch methods</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>적절한 exploration 을 위해 model-free, off-policy Actor-Critic algorithm 을 제안함<ul>\n<li>Action-value function approximator 사용으로 인해 policy gradient가 bias되는 것을 방지하기 위해 compatibility conditions을 제공 [<a href=\"##deterministic-actor-critic-algorithms\">Theorem 3</a>]</li>\n</ul>\n</li>\n<li>DPG 는 SPG 보다 성능이 좋음<ul>\n<li>특히 high dimensional action spaces 을 가지는 tasks에서의 성능 향상이 큼<ul>\n<li>SPG의 policy gradient는 state와 action spaces 모두에 대해서, DPG의 policy gradient는  state spaces에 대해서만 평균을 취함</li>\n<li>결과적으로, action spaces의 dimension이 커질수록 data efficiency가 높은 DPG의 학습이 더 잘 이뤄지게 됨</li>\n<li>무한정 학습을 시키게 되면, SPG도 최적으로 수렴할 것으로 예상되기에 위 성능 비교는 일정 iteration 내로 한정함</li>\n</ul>\n</li>\n<li>기존 기법들에 비해 computation 양이 많지 않음<ul>\n<li>Computation 은 action dimensionality 와 policy parameters 수에 비례함<br><br></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2 id=\"2-Background\"><a href=\"#2-Background\" class=\"headerlink\" title=\"2. Background\"></a>2. Background</h2><h3 id=\"2-1-Performance-objective-function\"><a href=\"#2-1-Performance-objective-function\" class=\"headerlink\" title=\"2.1 Performance objective function\"></a>2.1 Performance objective function</h3><p>$$<br>\\begin{align}<br>J(\\pi_{\\theta}) &amp;= \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\pi_{\\theta}(s,a)r(s,a)dads = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[r(s,a)]<br>\\end{align}<br>$$</p>\n<h3 id=\"2-2-SPG-Theorem\"><a href=\"#2-2-SPG-Theorem\" class=\"headerlink\" title=\"2.2 SPG Theorem\"></a>2.2 SPG Theorem</h3><ul>\n<li>State distribution $ \\rho^{\\pi}(s) $ 은 policy parameters에 영향을 받지만, policy gradient 를 계산할 때는 state distribution 의 gradient 를 고려할 필요가 없다.</li>\n<li>$$\\begin{eqnarray}\\nabla_{\\theta}J(\\pi_{\\theta}) &amp;=&amp; \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\ &amp;=&amp; E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]<br>\\end{eqnarray}$$</li>\n</ul>\n<h3 id=\"2-3-Stochastic-Actor-Critic-Algorithms\"><a href=\"#2-3-Stochastic-Actor-Critic-Algorithms\" class=\"headerlink\" title=\"2.3 Stochastic Actor-Critic Algorithms\"></a>2.3 Stochastic Actor-Critic Algorithms</h3><ul>\n<li>Actor 와 Critic 이 번갈아가면서 동작하며 stochastic policy 를 최적화하는 기법</li>\n<li>Actor: $ Q^{\\pi}(s,a) $ 를 근사한 $ Q^w(s,a) $ 를 이용해 stochastic policy gradient 를 ascent 하는 방향으로 policy parameter $ \\theta $ 를 업데이트함으로써 stochastic policy 를 발전시킴<ul>\n<li>$ \\nabla_{\\theta}J(\\pi_{\\theta}) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}log\\pi_{\\theta}(a|s)Q^{w}(s,a)] $</li>\n</ul>\n</li>\n<li>Critic: SARSA 나 Q-learning 같은 Temporal-difference (TD) learning 을 이용해 action-value function의 parameter, $ w $ 를 업데이트함으로써 $ Q^w(s,a) $ 가 $ Q^{\\pi}(s,a) $ 과 유사해지도록 함</li>\n<li>실제 값인 $ Q^{\\pi}(s,a) $ 대신 이를 근사한 $ Q^w(s,a) $ 를 사용하게 되면, 일반적으로 bias 가 발생하게 된다. 하지만, compatible condition 에 부합하는 $ Q^w(s,a) $ 를 사용하게 되면, bias 가 발생하지 않는다.</li>\n</ul>\n<h3 id=\"2-4-Off-policy-Actor-Critic\"><a href=\"#2-4-Off-policy-Actor-Critic\" class=\"headerlink\" title=\"2.4 Off-policy Actor-Critic\"></a>2.4 Off-policy Actor-Critic</h3><ul>\n<li>Distinct behavior policy $ \\beta(a|s) ( \\neq \\pi_{\\theta}(a|s) ) $ 로부터 샘플링된 trajectories 를 이용한 Actor-Critic</li>\n<li>Performance objective function<ul>\n<li>$\\begin{eqnarray}<br>  J_{\\beta}(\\pi_{\\theta})<br>  &amp;=&amp; \\int_{S}\\rho^{\\beta}(s)V^{\\pi}(s)ds \\nonumber \\\\<br>  &amp;=&amp; \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads<br>  \\end{eqnarray} $</li>\n</ul>\n</li>\n<li>off-policy policy gradient<ul>\n<li>$ \\begin{eqnarray}<br>  \\nabla_{\\theta}J_{\\beta}(\\pi_{\\theta}) &amp;\\approx&amp; \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\end{eqnarray} $<br>$=E_{s \\sim \\rho^{\\beta}, a \\sim \\beta}[\\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)}\\nabla_{\\theta}log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]$</li>\n<li>off-policy policy gradient 식에서의 물결 표시는 [Degris, 2012b] 논문에 근거함<ul>\n<li>[Degris, 2012b] “Linear off-policy actor-critic,” ICML 2012</li>\n<li>Exact off-policy policy gradient 와 이를 approximate 한 policy gradient 는 아래와 같음. (빨간색 상자에 있는 항목을 삭제함으로써 근사함)<ul>\n<li><img src=\"https://www.dropbox.com/s/xzpv3okc139c1fs/Screenshot%202018-06-16%2017.48.51.png?dl=1\" width=\"500px\"></li>\n</ul>\n</li>\n<li>[Degris, 2012b] Theorem 1 에 의해 policy parameter 가 approximated policy gradient ( $\\nabla_{u}𝑄^{\\pi,\\gamma}(𝑠,𝑎)$ term 제거)에 따라 업데이트되어도 policy 는 improve 가 됨이 보장되기에 exact off-policy policy gradient 대신 approximated off-policy policy gradient 를 사용해도 괜찮음.<ul>\n<li><img src=\"https://www.dropbox.com/s/mk13931r4scjngo/Screenshot%202018-06-16%2017.49.24.png?dl=1\" width=\"500px\"></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>off-policy policy gradient 식에서 $ \\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)} $ 는 importance sampling ratio 임<ul>\n<li>off-policy actor-critic 에서는 $ \\beta $ 에 의해 샘플링된 trajectory 를 이용해서 stochastic policy $ \\pi $ 를 예측하는 것이기 때문에 imnportance sampling 이 필요함<br><br></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2 id=\"3-Gradient-of-Deterministic-Policies\"><a href=\"#3-Gradient-of-Deterministic-Policies\" class=\"headerlink\" title=\"3. Gradient of Deterministic Policies\"></a>3. Gradient of Deterministic Policies</h2><h3 id=\"3-1-Regulariy-Conditions\"><a href=\"#3-1-Regulariy-Conditions\" class=\"headerlink\" title=\"3.1 Regulariy Conditions\"></a>3.1 Regulariy Conditions</h3><ul>\n<li>어떠한 이론이 성립하기 위한 전제 조건</li>\n<li>Regularity conditions A.1<ul>\n<li>$ p(s’|s,a), \\nabla_{a}p(s’|s,a), \\mu_{\\theta}(s), \\nabla_{\\theta}\\mu_{\\theta}(s), r(s,a), \\nabla_{a}r(s,a), p_{1}(s) $ are continuous in all parameters and variables $ s, a, s’ $ and $ x $.</li>\n</ul>\n</li>\n<li>regularity conditions A.2<ul>\n<li>There exists a $ b $ and $ L $ such that $ \\sup_{s}p_{1}(s) &lt; b $, $ \\sup_{a,s,s’}p(s’|s,a) &lt; b $, $ \\sup_{a,s}r(s,a) &lt; b $, $ \\sup_{a,s,s’}|\\nabla_{a}p(s’|s,a)| &lt; L $, and $ \\sup_{a,s}|\\nabla_{a}r(s,a)| &lt; L $.</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"3-2-Deterministic-Policy-Gradient-Theorem\"><a href=\"#3-2-Deterministic-Policy-Gradient-Theorem\" class=\"headerlink\" title=\"3.2 Deterministic Policy Gradient Theorem\"></a>3.2 Deterministic Policy Gradient Theorem</h3><ul>\n<li>Deterministic policy<ul>\n<li>$ \\mu_{\\theta} : S \\to A $ with parameter vector $ \\theta \\in \\rm I ! R^n $</li>\n</ul>\n</li>\n<li>Probability distribution<ul>\n<li>$ p(s \\to s’, t, \\mu) $</li>\n</ul>\n</li>\n<li>Discounted state distribution<ul>\n<li>$ \\rho^{\\mu}(s) $</li>\n</ul>\n</li>\n<li>Performance objective</li>\n</ul>\n<p>$$<br>J(\\mu_{\\theta}) = E[r^{\\gamma}_{1} | \\mu]<br>$$</p>\n<p>$$<br>= \\int_{S}\\rho^{\\mu}(s)r(s,\\mu_{\\theta}(s))ds<br>= E_{s \\sim \\rho^{\\mu}}[r(s,\\mu_{\\theta}(s))]<br>$$</p>\n<ul>\n<li><p>DPG Theorem</p>\n<ul>\n<li><p>MDP 가 A.1 만족한다면, 아래 식(9)이 성립함<br>$\\nabla_{\\theta}J(\\mu_{\\theta}) = \\int_{S}\\rho^{\\mu}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}ds \\nonumber$<br>$= E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]   \\nonumber $ (9)</p>\n</li>\n<li><p>DPG는 State space 에 대해서만 평균을 취하면 되기에, State와 Action space 모두에 대해 평균을 취해야 하는 <a href=\"#spg-theorem\">SPG</a>에 비해 data efficiency가 좋다. 즉, 더 적은 양의 데이터로도 학습이 잘 이뤄지게 된다.</p>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"3-3-DPG-형태에-대한-informal-intuition\"><a href=\"#3-3-DPG-형태에-대한-informal-intuition\" class=\"headerlink\" title=\"3.3 DPG 형태에 대한 informal intuition\"></a>3.3 DPG 형태에 대한 informal intuition</h3><ul>\n<li>Generalized policy iteration<ul>\n<li>정책 평가와 정책 발전을 한 번 씩 번갈아 가면서 실행하는 정책 iteration<ul>\n<li>위와 같이 해도 정책 평가에서 예측한 가치함수가 최적 가치함수에 수렴함</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>정책 평가<ul>\n<li>action-value function $ Q^{\\pi}(s,a) $ or $ Q^{\\mu}(s,a) $ 을 estimate 하는 것</li>\n</ul>\n</li>\n<li>정책 발전<ul>\n<li>위 estimated action-value function 에 따라 정책을 update 하는 것</li>\n<li>주로 action-value function 에 대한 greedy maximisation 사용함<ul>\n<li>$ \\mu^{k+1}(s) = \\arg\\max\\limits_{a}Q^{\\mu^{k}}(s,a) $</li>\n<li>greedy 정책 발전은 매 단계마다 global maximization을 해야하는데, 이는 continuous action spaces 에서 계산량이 급격히 늘어나게 됨.</li>\n</ul>\n</li>\n<li>그렇기에 policy gradient 방법이 나옴<ul>\n<li>policy 를 $ \\theta $ 에 대해서 parameterize 함</li>\n<li>매 단계마다 global maximisation 수행하는 대신, 방문하는 state $ s $ 마다 policy parameter 를 action-value function $ Q $ 의 $ \\theta $ 에 대한 gradient $ \\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s)) $ 방향으로 proportional 하게 update 함</li>\n<li>하지만 각 state 는 다른 방향을 제시할 수 있기에, state distribution $ \\rho^{\\mu}(s) $ 에 대한 기대값을 취해 policy parameter 를 update 할 수도 있음<ul>\n<li>$ \\theta^{k+1} = \\theta^{k} + \\alpha \\rm I!E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s))] $</li>\n</ul>\n</li>\n<li>이는 chain-rule 에 따라 아래와 같이 분리될 수 있음<ul>\n<li>$ \\theta^{k+1} = \\theta^{k} + \\alpha \\rm I!E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu^{k}}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $ (7)</li>\n<li>chain rule: $ \\frac{\\partial Q}{\\partial \\theta} = \\frac{\\partial a}{\\partial \\theta} \\frac{\\partial Q}{\\partial a} $</li>\n</ul>\n</li>\n<li>하지만 state distribution $ \\rho^{\\mu} $ 은 정책에 dependent 함<ul>\n<li>정책이 바꾸게 되면, 바뀐 정책에 따라 방문하게 되는 state 가 변하게 되기 때문에 state distribution이 변하게 됨</li>\n</ul>\n</li>\n<li>그렇기에 정책 update 시 state distribution에 대한 gradient 를 고려하지 않는데 정책 발전이 이뤄진다는 것은 직관적으로 와닿지 않을 수 있음</li>\n<li>deterministic policy gradient theorem 은 state distribution에 대한 gradient 계산없이 위 식(7) 대로만 update 해도 performance objective 의 gradient 를 정확하게 따름을 의미한다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"3-4-DPG-는-SPG-의-limiting-case-임\"><a href=\"#3-4-DPG-는-SPG-의-limiting-case-임\" class=\"headerlink\" title=\"3.4 DPG 는 SPG 의 limiting case 임\"></a>3.4 DPG 는 SPG 의 limiting case 임</h3><ul>\n<li>stochastic policy parameterization<ul>\n<li>$ \\pi_{\\mu_{\\theta},\\sigma} $ by a deterministic policy $ \\mu_{\\theta} : S \\to A $ and a variance parameter $ \\sigma $</li>\n<li>$ \\sigma = 0 $ 이면, $ \\pi_{\\mu_{\\theta},\\sigma} \\equiv \\mu_{\\theta} $</li>\n</ul>\n</li>\n<li>Theorem 2. Policy 의 variance 가 0 에 수렴하면, 즉, $ \\sigma \\to 0 $, stochastic policy gradient 와 deterministic policy gradient 는 동일해짐<ul>\n<li>조건 : stochastic policy $ \\pi_{\\mu_{\\theta},\\sigma} = \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $<ul>\n<li>$ \\sigma $ 는 variance</li>\n<li>$ \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $ 는 conditions B.1 만족</li>\n<li>MDP 는 conditions A.1 및 A.2 만족</li>\n</ul>\n</li>\n<li>결과 :<ul>\n<li>$ \\lim\\limits_{\\sigma\\downarrow0}\\nabla_{\\theta}J(\\pi_{\\mu_{\\theta},\\sigma}) = \\nabla_{\\theta}J(\\mu_{\\theta})  $<ul>\n<li>좌변은 standard stochastic gradient 이며, 우변은 deterministic gradient.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>의미 :<ul>\n<li>deterministic policy gradient 는 stochastic policy gradient 의 특수 case 임</li>\n<li>기존 유명한 policy gradients 기법들에 deterministic policy gradients 를 적용할 수 있음<ul>\n<li>기존 기법들 예: compatible function approximation (Sutton, 1999), natural gradients (Kakade, 2001), actor-critic (Bhatnagar, 2007) or episodic/batch methods (Peters, 2005)<br><br></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2 id=\"4-Deterministic-Actor-Critic-Algorithms\"><a href=\"#4-Deterministic-Actor-Critic-Algorithms\" class=\"headerlink\" title=\"4. Deterministic Actor-Critic Algorithms\"></a>4. Deterministic Actor-Critic Algorithms</h2><ol>\n<li>살사 critic 을 이용한 on-policy actor-critic<ul>\n<li>단점<ul>\n<li>deterministic policy 에 의해 행동하면 exploration 이 잘 되지 않기에, sub-optimal 에 빠지기 쉬움</li>\n</ul>\n</li>\n<li>목적<ul>\n<li>교훈/정보제공</li>\n<li>환경에서 충분한 noise 를 제공하여 exploration을 시킬 수 있다면, deterministic policy 를 사용한다고 하여도 좋은 학습 결과를 얻을 수도 있음$<ul>\n<li>예. 바람이 agent의 행동에 영향(noise)을 줌</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Remind: 살사(SARSA) update rule<ul>\n<li>$ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma Q(s_{t+1},a_{t+1}) - Q(s_{t},a_{t})) $</li>\n</ul>\n</li>\n<li>Algorithm<ul>\n<li>Critic 은 MSE 를 $ \\bf minimize $ 하는 방향, 즉, action-value function 을 stochastic gradient $ \\bf descent $ 방법으로 update 함<ul>\n<li>$ MSE = [Q^{\\mu}(s,a) - Q^{w}(s,a)]^2 $<ul>\n<li>critic 은 실제 $ Q^{\\mu}(s,a) $ 대신 미분 가능한 $ Q^{w}(s,a) $ 로 대체하여 action-value function 을 estimate 하며, 이 둘 간 Mean Square Error 를 최소화하는 것이 목표</li>\n</ul>\n</li>\n<li>$ \\nabla_{w}MSE \\approx -2 * [r + \\gamma Q^{w}(s’,a’) - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $<ul>\n<li>$ \\nabla_{w}MSE = -2 * [Q^{\\mu}(s,a) - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $</li>\n<li>$ Q^{\\mu}(s,a) $ 를 $ r + \\gamma Q^{w}(s’,a’) $ 로 대체<ul>\n<li>$ Q^{\\mu}(s,a) = r + \\gamma Q^{\\mu}(s’,a’) $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $<ul>\n<li>$w_{t+1} = w_{t} - \\alpha_{w}\\nabla_{w}MSE  \\nonumber$<br>$ \\approx w_{t} - \\alpha_{w} <em> (-2 </em> [r + \\gamma Q^{w}(s’,a’) - Q^{w}(s,a)] \\nabla_{w}Q^{w}(s,a)$</li>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},a_{t+1}) - Q^{w}(s_{t},a_{t}) $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Actor 는 식(9)에 따라 보상이 $ \\bf maximize $ 되는 방향, 즉, deterministic policy 를 stochastic gradient $ \\bf ascent $ 방법으로 update함<ul>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Q-learning 을 이용한 off-policy actor-critic<ul>\n<li>stochastic behavior policy $ \\beta(a|s) $ 에 의해 생성된 trajectories 로부터 deterministic target policy $ \\mu_{\\theta}(s) $ 를 학습하는 off-policy actor-critic</li>\n<li>performance objective<ul>\n<li>$ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)V^{\\mu}(s)ds \\nonumber \\\\$<br>$= \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds \\nonumber \\\\$<br>$= E_{s \\sim \\rho^{\\beta}}[Q^{\\mu}(s,\\mu_{\\theta}(s))]$</li>\n</ul>\n</li>\n<li>off-policy deterministic policy gradient<ul>\n<li>$ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $<ul>\n<li>논문에는 아래와 같이 나와있는데, 물결 표시 부분은 오류로 판단됨.</li>\n<li>$ \\begin{eqnarray}<br>  \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) &amp;\\approx&amp; \\int_{S}\\rho^{\\beta}(s)\\nabla_{\\theta}\\mu_{\\theta}(a|s)Q^{\\mu}(s,a)ds \\nonumber \\<br>  &amp;=&amp; E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]<br>  \\end{eqnarray} $</li>\n<li>근거: Action이 deterministic 하기에 stochastic 경우와는 다르게 performance objective 에서 action 에 대해 평균을 구할 필요가 없음. 그렇기에, 곱의 미분이 있을 필요가 없고, [Degris, 2012b] 에서처럼 곱의 미분을 통해 생기는 action-value function 에 대한 gradient term 를 생략할 필요가 사라짐. <a href=\"#off-policy-actor-critic\">참고</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Remind: 큐러닝(Q-learning) update rule<ul>\n<li>$ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma \\max\\limits_{a}Q(s_{t+1},a) - Q(s_{t},a_{t})) $</li>\n</ul>\n</li>\n<li>algorithm: OPDAC (Off-Policy Deterministic Actor-Critic)<ul>\n<li>살사를 이용한 on-policy deterministic actor-critic 과 아래 부분을 제외하고는 같음<ul>\n<li>target policy 는 $ \\beta(a|s) $ 에 의해 생성된 trajectories 를 통해 학습함</li>\n<li>업데이트 목표 부분에 실제 행동 값 $ a_{t+1} $ 이 아니라 정책으로부터 나온 행동 값 $ \\mu_{\\theta}(s_{t+1}) $ 사용함<ul>\n<li>$ \\mu_{\\theta}(s_{t+1}) $ 는 가장 높은 Q 값을 가지는 행동. 즉, Q-learning.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $</li>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $</li>\n</ul>\n</li>\n<li>Stochastic off-policy actor-critic 은 대개 actor 와 critic 모두 importance sampling을 필요로 하지만, deterministic policy gradient 에선 importance sampling이 필요없음<ul>\n<li>Actor 는 deterministic 이기에 sampling 자체가 필요없음<ul>\n<li>Stochastic policy 인 경우, Actor 에서 importance sampling이 필요한 이유는 상태 $ s $ 에서의 가치 함수 값 $ V^{\\pi}(s) $ 을 estimate 하기 위해 $ \\pi $ 가 아니라 $ \\beta $ 에 따라 sampling을 한 후, 평균을 내기 때문임.</li>\n<li>하지만 Deterministic policy 인 경우, 상태 $ s $ 에서의 가치 함수 값 $ V^{\\pi}(s) = Q^{\\pi}(s,\\mu_{\\theta}) $ 즉, action 이 상태 s 에 대해 deterministic 이기에 sampling 을 통해 estimate 할 필요가 없고, 따라서 importance sampling도 필요없어짐.</li>\n<li>stochastic vs. deterministic performance objective<ul>\n<li>stochastic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads $</li>\n<li>deterministic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Critic 이 사용하는 Q-learning 은 importance sampling이 필요없는 off policy 알고리즘임.<ul>\n<li>Q-learning 도 업데이트 목표를 특정 분포에서 샘플링을 통해 estimate 하는 것이 아니라 Q 함수를 최대화하는 action을 선택하는 것이기에 위 actor 에서의 deterministic 경우와 비슷하게 볼 수 있음</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>compatible function approximation 및 gradient temporal-difference learning 을 이용한 actor-critic<ul>\n<li>위 살사/Q-learning 기반 on/off-policy 는 아래와 같은 문제가 존재<ul>\n<li>function approximator 에 의한 bias<ul>\n<li>일반적으로 $ Q^{\\mu}(s,a) $ 를 $ Q^{w}(s,a) $ 로 대체하여 deterministic policy gradient 를 구하면, 그 gradient 는 ascent 하는 방향이 아닐 수도 있음</li>\n</ul>\n</li>\n<li>off-policy learning 에 의한 instabilities</li>\n</ul>\n</li>\n<li>그래서 stochastic 처럼 $ \\nabla_{a}Q^{\\mu}(s,a) $ 를 $ \\nabla_{a}Q^{w}(s,a) $ 로 대체해도 deterministic policy gradient 에 영향을 미치지 않을 compatible function approximator $ Q^{w}(s,a) $ 를 찾아야 함</li>\n<li>Theorem 3. 아래 두 조건을 만족하면, $ Q^{w}(s,a) $ 는 deterministic policy $ \\mu_{\\theta}(s) $ 와 compatible 함. 즉, $ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $<ul>\n<li>$ \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $</li>\n<li>$ w $ 는 $ MSE(\\theta, w) = E[\\epsilon(s;\\theta,w)^{\\top}\\epsilon(s;\\theta,w)] $ 를 최소화함<ul>\n<li>$ \\epsilon(s;\\theta,w) = \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} - \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}  $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Theorem 3 은 on-policy 뿐만 아니라 off-policy 에도 적용 가능함</li>\n<li>$ Q^{w}(s,a) = (a-\\mu_{\\theta}(s))^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top} w + V^{v}(s) $<ul>\n<li>어떠한 deterministic policy 에 대해서도 위 형태와 같은 compatible function approximator 가 존재함.</li>\n<li>앞의 term 은 advantage 를, 뒤의 term 은 value 로 볼 수 있음</li>\n</ul>\n</li>\n<li>$ Q^{w}(s,a) = \\phi(s,a)^{\\top} w + v^{\\top}\\phi(s) $<ul>\n<li>정의 : $ \\phi(s,a) \\overset{\\underset{\\mathrm{def}}{}}{=} \\nabla_{\\theta}\\mu_{\\theta}(s)(a-\\mu_{\\theta}(s)) $</li>\n<li>일례 : $ V^{v}(s) = v^{\\top}\\phi(s) $</li>\n<li>Theorem 3 만족 여부<ul>\n<li>첫 번째 조건 만족.</li>\n<li>두 번째 조건은 대강 만족.<ul>\n<li>$ \\nabla_{a}Q^{\\mu}(s,a) $ 에 대한 unbiased sample 을 획득하기는 매우 어렵기에, 일반적인 정책 평가 방법들로 $ w $ 학습.</li>\n<li>이 정책 평가 방법들을 이용하면 $ Q^{w}(s,a) \\approx Q^{\\mu}(s,a) $ 인 reasonable solution을 찾을 수 있기에 대강 $ \\nabla_{a}Q^{w}(s,a) \\approx \\nabla_{a}Q^{\\mu}(s,a) $ 이 될 것.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>action-value function 에 대한 linear function approximator 는 큰 값을 가지는 actions 에 대해선 diverge할 수 있어 global 하게 action-values 예측하기에는 좋지 않지만, local critic 에 사용할 때는 매우 유용하다.<ul>\n<li>즉, 절대값이 아니라 작은 변화량을 다루는 gradient method 경우엔 $ A^{w}(s,\\mu_{\\theta}(s)+\\delta) = \\delta^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $ 로, diverge 하지 않고, 값을 얻을 수 있음</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>COPDAC-Q algorithm (Compatible Off-Policy Deterministic Actor-Critic Q-learning critic)<ul>\n<li>Critic: 실제 action-value function 에 대한 linear function approximator 인 $ Q^{w}(s,a) = \\phi(s,a)^{\\top}w $ 를 estimate<ul>\n<li>$ \\phi(s,a) = a^{\\top}\\nabla_{\\theta}\\mu_{\\theta} $</li>\n<li>Behavior policy $ \\beta(a|s) $ 로부터 얻은 samples를 이용하여 Q-learning 이나 gradient Q-learning 과 같은 off-policy algorithm 으로 학습 가능함</li>\n</ul>\n</li>\n<li>Actor: estimated action-value function 에 대한 gradient 를 $ \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $ 로 치환 후, 정책을 업데이트함</li>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) $</li>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $</li>\n</ul>\n</li>\n<li>off-policy Q-learning은 linear function approximation을 이용하면 diverge 할 수도 있음<ul>\n<li>$ \\mu_{\\theta}(s_{t+1}) $ 이 diverge 할 수도 있기 때문으로 판단됨.</li>\n<li>그렇기에 simple Q-learning 대신 다른 기법이 필요함.</li>\n</ul>\n</li>\n<li>그렇기에 critic 에 gradient Q-learning 사용한 COPDAC-GQ (Gradient Q-learning critic) algorithm 제안<ul>\n<li>gradient temporal-difference learning 에 기반한 기법들은 true gradient descent algorithm 이며, converge가 보장됨. (Sutton, 2009)<ul>\n<li>기본 아이디어는 stochastic gradient descent 로 Mean-squared projected Bellman error (MSPBE) 를 최소화하는 것</li>\n<li>critic 이 actor 보다 빠른 time-scale 로 update 되도록 step size 들을 잘 조절하면, critic 은 MSPBE 를 최소화하는 parameters 로 converge 하게 됨</li>\n<li>critic 에 gradient temporal-difference learning 의 일종인 gradient Q-learning 사용한 논문 (Maei, 2010)</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>COPDAC-GQ algorithm<ul>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $</li>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) - \\alpha_{w}\\gamma\\phi(s_{t+1}, \\mu_{\\theta}(s_{t+1}))(\\phi(s_{t},a_{t})^{\\top} u_{t}) $</li>\n<li>$ v_{t+1} = v_{t} + \\alpha_{v}\\delta_{t}\\phi(s_{t}) - \\alpha_{v}\\gamma\\phi(s_{t+1})(\\phi(s_{t},a_{t})^{\\top} u_{t}) $</li>\n<li>$ u_{t+1} = u_{t} + \\alpha_{u}(\\delta_{t}-\\phi(s_{t}, a_{t})^{\\top} u_{t})\\phi(s_{t}, a_{t}) $</li>\n</ul>\n</li>\n<li>stochastic actor-critic 과 같이 매 time-step 마다 update 시 필요한 계산의 복잡도는 $ O(mn) $<ul>\n<li>m 은 action dimensions, n 은 number of policy parameters</li>\n</ul>\n</li>\n<li>Natural policy gradient 를 이용해 deterministic policies 를 찾을 수 있음<ul>\n<li>$ M(\\theta)^{-1}\\nabla_{\\theta}J(\\mu_{\\theta}) $ 는 any metric $ M(\\theta) $ 에 대한 our performance objective (식(14)) 의 steepest ascent direction 임 (Toussaint, 2012)</li>\n<li>Natural gradient 는 Fisher information metric $ M_{\\pi}(\\theta) $ 에 대한 steepest ascent direction 임<ul>\n<li>$ M_{\\pi}(\\theta) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)^{\\top}] $</li>\n<li>Fisher information metric 은 policy reparameterization 에 대해 불변임 (Bagnell, 2003)</li>\n</ul>\n</li>\n<li>deterministic policies 에 대해 metric 으로 $ M_{\\mu}(\\theta) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}] $ 을 사용.<ul>\n<li>이는 variance 가 0 인 policy 에 대한 Fisher information metric 으로 볼 수 있음</li>\n<li>$ \\frac{\\nabla_{\\theta}\\pi_{\\theta}(a\\vert s)}{\\pi_{\\theta}(a\\vert s)}$ 에서 policy variance 가 0 이면, 특정 s 에 대한 $ \\pi_{\\theta}(a|s)$ 만 1 이 되고, 나머지는 0 임</li>\n</ul>\n</li>\n<li>deterministic policy gradient theorem 과 compatible function approximation 을 결합하면 $ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] $ 이 됨<ul>\n<li>$ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $</li>\n<li>$ \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)} \\approx \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $</li>\n</ul>\n</li>\n<li>그렇기에 steepest ascent direction 은 $ M_{\\mu}(\\theta)^{-1}\\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = w $ 이 됨<ul>\n<li>$ E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}]^{-1}E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] = w $</li>\n</ul>\n</li>\n<li>이 알고리즘은 COPDAC-Q 혹은 COPDAC-GQ 에서 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $ 식을 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta}w_{t} $ 로 바꿔주기만 하면 됨</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"Experiments\"><a href=\"#Experiments\" class=\"headerlink\" title=\"Experiments\"></a>Experiments</h2><h3 id=\"Continuous-Bandit\"><a href=\"#Continuous-Bandit\" class=\"headerlink\" title=\"Continuous Bandit\"></a>Continuous Bandit</h3><ul>\n<li>Stochastic Actor-Critic (SAC)과 COPDAC 간 성능 비교 수행<ul>\n<li>Action dimension이 커질수록 성능 차이가 심함</li>\n<li>빠르게 수렴하는 것을 통해 DPG의 data efficiency가 SPG에 비해 좋다는 것을 확인할 수 있지만, 반면, time-step이 증가할수록 SAC와 COPDAC 간 성능 차이가 줄어드는 것을 통해 성능 차이가 심하다는 것은 일정 time step 내에서만 해당하는 것이라고 유추해볼 수 있음</li>\n<li><img src=\"https://www.dropbox.com/s/hrkyq0s2f24z66r/Screenshot%202018-06-16%2017.47.38.png?dl=1\"></li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Continuous-Reinforcement-Learning\"><a href=\"#Continuous-Reinforcement-Learning\" class=\"headerlink\" title=\"Continuous Reinforcement Learning\"></a>Continuous Reinforcement Learning</h3><ul>\n<li>COPDAC-Q가 SAC와 off-policy stochastic actor-critic(OffPAC-TD) 간 성능 비교 수행<ul>\n<li>COPDAC-Q의 성능이 약간 더 좋음</li>\n<li>COPDAC-Q의 학습이 더 빨리 이뤄짐</li>\n<li><img src=\"https://www.dropbox.com/s/qdca4augapmzsxi/Screenshot%202018-06-16%2017.47.07.png?dl=1\"><h3 id=\"Octopus-Arm\"><a href=\"#Octopus-Arm\" class=\"headerlink\" title=\"Octopus Arm\"></a>Octopus Arm</h3></li>\n</ul>\n</li>\n<li>목표: 6 segments octopus arm (20 action dimensions &amp; 50 state dimensions)을 control하여 target을 맞추는 것<ul>\n<li>COPDAC-Q 사용 시, action space dimension이 큰 octopus arm을 잘 control하여 target을 맞춤</li>\n<li><img src=\"https://www.dropbox.com/s/xrxb0a52wntekld/Screenshot%202018-06-16%2017.46.28.png?dl=1\" width=\"600px\"></li>\n<li>기존 기법들은 action spaces 혹은 action 과 state spaces 둘 다 작은 경우들에 대해서만 실험했다고 하며, 비교하고 있지 않음.<ul>\n<li>기존 기법들이 6 segments octopus arm에서 동작을 잘 안 했을 것 같긴한데, 그래도 실험해서 결과를 보여주지…왜 안 했을까?</li>\n</ul>\n</li>\n<li>8 segment arm 동영상이 저자 홈페이지에 있다고 하는데, 안 보임.</li>\n</ul>\n</li>\n<li>[참고] Octopus Arm 이란?<ul>\n<li><a href=\"https://www.youtube.com/watch?v=AxeeHif0euY\" target=\"_blank\" rel=\"noopener\">OctopusArm Youtube Link</a></li>\n<li><img src=\"https://www.dropbox.com/s/950ycj06sudakjx/Screenshot%202018-06-16%2017.45.52.png?dl=1\"></li>\n</ul>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<center> <img src=\"https://www.dropbox.com/s/etwa3hn5c0pegdv/Screen%20Shot%202018-07-18%20at%2012.51.35%20AM.png?dl=1\" width=\"800\"> </center>\n\n<p>논문 저자 : David Silver, Guy Lever, Nicloas Heess, Thomas Degris, Daan Wierstra, Martin Riedmiller<br>논문 링크 : <a href=\"http://proceedings.mlr.press/v32/silver14.pdf\" target=\"_blank\" rel=\"noopener\">http://proceedings.mlr.press/v32/silver14.pdf</a><br>Proceeding : International Conference on Machine Learning (ICML) 2014<br>정리 : 장수영, 공민서</p>\n<hr>\n<h2 id=\"1-Summary\"><a href=\"#1-Summary\" class=\"headerlink\" title=\"1. Summary\"></a>1. Summary</h2><ul>\n<li>Deterministic Policy Gradient (DPG) Theorem 제안함 [<a href=\"#deterministic-policy-gradient-theorem\">Theorem 1</a>]<br>  1) DPG는 존재하며,<br>  2) DPG는 Expected gradient of the action-value function의 형태를 띈다.</li>\n<li>Policy variance가 0에 수렴할 경우, DPG는 Stochastic Policy Gradient(SPG)와 동일해짐을 보임 [<a href=\"#dpg-는-spg-의-limiting-case-임\">Theorem 2</a>]<ul>\n<li>Theorem 2 로 인해 기존 Policy Gradient (PG) 와 관련된 기법들을 DPG 에 적용할 수 있게 됨<ul>\n<li>예. Sutton PG, natural gradients, actor-critic, episodic/batch methods</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>적절한 exploration 을 위해 model-free, off-policy Actor-Critic algorithm 을 제안함<ul>\n<li>Action-value function approximator 사용으로 인해 policy gradient가 bias되는 것을 방지하기 위해 compatibility conditions을 제공 [<a href=\"##deterministic-actor-critic-algorithms\">Theorem 3</a>]</li>\n</ul>\n</li>\n<li>DPG 는 SPG 보다 성능이 좋음<ul>\n<li>특히 high dimensional action spaces 을 가지는 tasks에서의 성능 향상이 큼<ul>\n<li>SPG의 policy gradient는 state와 action spaces 모두에 대해서, DPG의 policy gradient는  state spaces에 대해서만 평균을 취함</li>\n<li>결과적으로, action spaces의 dimension이 커질수록 data efficiency가 높은 DPG의 학습이 더 잘 이뤄지게 됨</li>\n<li>무한정 학습을 시키게 되면, SPG도 최적으로 수렴할 것으로 예상되기에 위 성능 비교는 일정 iteration 내로 한정함</li>\n</ul>\n</li>\n<li>기존 기법들에 비해 computation 양이 많지 않음<ul>\n<li>Computation 은 action dimensionality 와 policy parameters 수에 비례함<br><br></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2 id=\"2-Background\"><a href=\"#2-Background\" class=\"headerlink\" title=\"2. Background\"></a>2. Background</h2><h3 id=\"2-1-Performance-objective-function\"><a href=\"#2-1-Performance-objective-function\" class=\"headerlink\" title=\"2.1 Performance objective function\"></a>2.1 Performance objective function</h3><p>$$<br>\\begin{align}<br>J(\\pi_{\\theta}) &amp;= \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\pi_{\\theta}(s,a)r(s,a)dads = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[r(s,a)]<br>\\end{align}<br>$$</p>\n<h3 id=\"2-2-SPG-Theorem\"><a href=\"#2-2-SPG-Theorem\" class=\"headerlink\" title=\"2.2 SPG Theorem\"></a>2.2 SPG Theorem</h3><ul>\n<li>State distribution $ \\rho^{\\pi}(s) $ 은 policy parameters에 영향을 받지만, policy gradient 를 계산할 때는 state distribution 의 gradient 를 고려할 필요가 없다.</li>\n<li>$$\\begin{eqnarray}\\nabla_{\\theta}J(\\pi_{\\theta}) &amp;=&amp; \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\ &amp;=&amp; E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]<br>\\end{eqnarray}$$</li>\n</ul>\n<h3 id=\"2-3-Stochastic-Actor-Critic-Algorithms\"><a href=\"#2-3-Stochastic-Actor-Critic-Algorithms\" class=\"headerlink\" title=\"2.3 Stochastic Actor-Critic Algorithms\"></a>2.3 Stochastic Actor-Critic Algorithms</h3><ul>\n<li>Actor 와 Critic 이 번갈아가면서 동작하며 stochastic policy 를 최적화하는 기법</li>\n<li>Actor: $ Q^{\\pi}(s,a) $ 를 근사한 $ Q^w(s,a) $ 를 이용해 stochastic policy gradient 를 ascent 하는 방향으로 policy parameter $ \\theta $ 를 업데이트함으로써 stochastic policy 를 발전시킴<ul>\n<li>$ \\nabla_{\\theta}J(\\pi_{\\theta}) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}log\\pi_{\\theta}(a|s)Q^{w}(s,a)] $</li>\n</ul>\n</li>\n<li>Critic: SARSA 나 Q-learning 같은 Temporal-difference (TD) learning 을 이용해 action-value function의 parameter, $ w $ 를 업데이트함으로써 $ Q^w(s,a) $ 가 $ Q^{\\pi}(s,a) $ 과 유사해지도록 함</li>\n<li>실제 값인 $ Q^{\\pi}(s,a) $ 대신 이를 근사한 $ Q^w(s,a) $ 를 사용하게 되면, 일반적으로 bias 가 발생하게 된다. 하지만, compatible condition 에 부합하는 $ Q^w(s,a) $ 를 사용하게 되면, bias 가 발생하지 않는다.</li>\n</ul>\n<h3 id=\"2-4-Off-policy-Actor-Critic\"><a href=\"#2-4-Off-policy-Actor-Critic\" class=\"headerlink\" title=\"2.4 Off-policy Actor-Critic\"></a>2.4 Off-policy Actor-Critic</h3><ul>\n<li>Distinct behavior policy $ \\beta(a|s) ( \\neq \\pi_{\\theta}(a|s) ) $ 로부터 샘플링된 trajectories 를 이용한 Actor-Critic</li>\n<li>Performance objective function<ul>\n<li>$\\begin{eqnarray}<br>  J_{\\beta}(\\pi_{\\theta})<br>  &amp;=&amp; \\int_{S}\\rho^{\\beta}(s)V^{\\pi}(s)ds \\nonumber \\\\<br>  &amp;=&amp; \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads<br>  \\end{eqnarray} $</li>\n</ul>\n</li>\n<li>off-policy policy gradient<ul>\n<li>$ \\begin{eqnarray}<br>  \\nabla_{\\theta}J_{\\beta}(\\pi_{\\theta}) &amp;\\approx&amp; \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\end{eqnarray} $<br>$=E_{s \\sim \\rho^{\\beta}, a \\sim \\beta}[\\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)}\\nabla_{\\theta}log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]$</li>\n<li>off-policy policy gradient 식에서의 물결 표시는 [Degris, 2012b] 논문에 근거함<ul>\n<li>[Degris, 2012b] “Linear off-policy actor-critic,” ICML 2012</li>\n<li>Exact off-policy policy gradient 와 이를 approximate 한 policy gradient 는 아래와 같음. (빨간색 상자에 있는 항목을 삭제함으로써 근사함)<ul>\n<li><img src=\"https://www.dropbox.com/s/xzpv3okc139c1fs/Screenshot%202018-06-16%2017.48.51.png?dl=1\" width=\"500px\"></li>\n</ul>\n</li>\n<li>[Degris, 2012b] Theorem 1 에 의해 policy parameter 가 approximated policy gradient ( $\\nabla_{u}𝑄^{\\pi,\\gamma}(𝑠,𝑎)$ term 제거)에 따라 업데이트되어도 policy 는 improve 가 됨이 보장되기에 exact off-policy policy gradient 대신 approximated off-policy policy gradient 를 사용해도 괜찮음.<ul>\n<li><img src=\"https://www.dropbox.com/s/mk13931r4scjngo/Screenshot%202018-06-16%2017.49.24.png?dl=1\" width=\"500px\"></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>off-policy policy gradient 식에서 $ \\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)} $ 는 importance sampling ratio 임<ul>\n<li>off-policy actor-critic 에서는 $ \\beta $ 에 의해 샘플링된 trajectory 를 이용해서 stochastic policy $ \\pi $ 를 예측하는 것이기 때문에 imnportance sampling 이 필요함<br><br></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2 id=\"3-Gradient-of-Deterministic-Policies\"><a href=\"#3-Gradient-of-Deterministic-Policies\" class=\"headerlink\" title=\"3. Gradient of Deterministic Policies\"></a>3. Gradient of Deterministic Policies</h2><h3 id=\"3-1-Regulariy-Conditions\"><a href=\"#3-1-Regulariy-Conditions\" class=\"headerlink\" title=\"3.1 Regulariy Conditions\"></a>3.1 Regulariy Conditions</h3><ul>\n<li>어떠한 이론이 성립하기 위한 전제 조건</li>\n<li>Regularity conditions A.1<ul>\n<li>$ p(s’|s,a), \\nabla_{a}p(s’|s,a), \\mu_{\\theta}(s), \\nabla_{\\theta}\\mu_{\\theta}(s), r(s,a), \\nabla_{a}r(s,a), p_{1}(s) $ are continuous in all parameters and variables $ s, a, s’ $ and $ x $.</li>\n</ul>\n</li>\n<li>regularity conditions A.2<ul>\n<li>There exists a $ b $ and $ L $ such that $ \\sup_{s}p_{1}(s) &lt; b $, $ \\sup_{a,s,s’}p(s’|s,a) &lt; b $, $ \\sup_{a,s}r(s,a) &lt; b $, $ \\sup_{a,s,s’}|\\nabla_{a}p(s’|s,a)| &lt; L $, and $ \\sup_{a,s}|\\nabla_{a}r(s,a)| &lt; L $.</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"3-2-Deterministic-Policy-Gradient-Theorem\"><a href=\"#3-2-Deterministic-Policy-Gradient-Theorem\" class=\"headerlink\" title=\"3.2 Deterministic Policy Gradient Theorem\"></a>3.2 Deterministic Policy Gradient Theorem</h3><ul>\n<li>Deterministic policy<ul>\n<li>$ \\mu_{\\theta} : S \\to A $ with parameter vector $ \\theta \\in \\rm I ! R^n $</li>\n</ul>\n</li>\n<li>Probability distribution<ul>\n<li>$ p(s \\to s’, t, \\mu) $</li>\n</ul>\n</li>\n<li>Discounted state distribution<ul>\n<li>$ \\rho^{\\mu}(s) $</li>\n</ul>\n</li>\n<li>Performance objective</li>\n</ul>\n<p>$$<br>J(\\mu_{\\theta}) = E[r^{\\gamma}_{1} | \\mu]<br>$$</p>\n<p>$$<br>= \\int_{S}\\rho^{\\mu}(s)r(s,\\mu_{\\theta}(s))ds<br>= E_{s \\sim \\rho^{\\mu}}[r(s,\\mu_{\\theta}(s))]<br>$$</p>\n<ul>\n<li><p>DPG Theorem</p>\n<ul>\n<li><p>MDP 가 A.1 만족한다면, 아래 식(9)이 성립함<br>$\\nabla_{\\theta}J(\\mu_{\\theta}) = \\int_{S}\\rho^{\\mu}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}ds \\nonumber$<br>$= E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]   \\nonumber $ (9)</p>\n</li>\n<li><p>DPG는 State space 에 대해서만 평균을 취하면 되기에, State와 Action space 모두에 대해 평균을 취해야 하는 <a href=\"#spg-theorem\">SPG</a>에 비해 data efficiency가 좋다. 즉, 더 적은 양의 데이터로도 학습이 잘 이뤄지게 된다.</p>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"3-3-DPG-형태에-대한-informal-intuition\"><a href=\"#3-3-DPG-형태에-대한-informal-intuition\" class=\"headerlink\" title=\"3.3 DPG 형태에 대한 informal intuition\"></a>3.3 DPG 형태에 대한 informal intuition</h3><ul>\n<li>Generalized policy iteration<ul>\n<li>정책 평가와 정책 발전을 한 번 씩 번갈아 가면서 실행하는 정책 iteration<ul>\n<li>위와 같이 해도 정책 평가에서 예측한 가치함수가 최적 가치함수에 수렴함</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>정책 평가<ul>\n<li>action-value function $ Q^{\\pi}(s,a) $ or $ Q^{\\mu}(s,a) $ 을 estimate 하는 것</li>\n</ul>\n</li>\n<li>정책 발전<ul>\n<li>위 estimated action-value function 에 따라 정책을 update 하는 것</li>\n<li>주로 action-value function 에 대한 greedy maximisation 사용함<ul>\n<li>$ \\mu^{k+1}(s) = \\arg\\max\\limits_{a}Q^{\\mu^{k}}(s,a) $</li>\n<li>greedy 정책 발전은 매 단계마다 global maximization을 해야하는데, 이는 continuous action spaces 에서 계산량이 급격히 늘어나게 됨.</li>\n</ul>\n</li>\n<li>그렇기에 policy gradient 방법이 나옴<ul>\n<li>policy 를 $ \\theta $ 에 대해서 parameterize 함</li>\n<li>매 단계마다 global maximisation 수행하는 대신, 방문하는 state $ s $ 마다 policy parameter 를 action-value function $ Q $ 의 $ \\theta $ 에 대한 gradient $ \\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s)) $ 방향으로 proportional 하게 update 함</li>\n<li>하지만 각 state 는 다른 방향을 제시할 수 있기에, state distribution $ \\rho^{\\mu}(s) $ 에 대한 기대값을 취해 policy parameter 를 update 할 수도 있음<ul>\n<li>$ \\theta^{k+1} = \\theta^{k} + \\alpha \\rm I!E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s))] $</li>\n</ul>\n</li>\n<li>이는 chain-rule 에 따라 아래와 같이 분리될 수 있음<ul>\n<li>$ \\theta^{k+1} = \\theta^{k} + \\alpha \\rm I!E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu^{k}}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $ (7)</li>\n<li>chain rule: $ \\frac{\\partial Q}{\\partial \\theta} = \\frac{\\partial a}{\\partial \\theta} \\frac{\\partial Q}{\\partial a} $</li>\n</ul>\n</li>\n<li>하지만 state distribution $ \\rho^{\\mu} $ 은 정책에 dependent 함<ul>\n<li>정책이 바꾸게 되면, 바뀐 정책에 따라 방문하게 되는 state 가 변하게 되기 때문에 state distribution이 변하게 됨</li>\n</ul>\n</li>\n<li>그렇기에 정책 update 시 state distribution에 대한 gradient 를 고려하지 않는데 정책 발전이 이뤄진다는 것은 직관적으로 와닿지 않을 수 있음</li>\n<li>deterministic policy gradient theorem 은 state distribution에 대한 gradient 계산없이 위 식(7) 대로만 update 해도 performance objective 의 gradient 를 정확하게 따름을 의미한다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"3-4-DPG-는-SPG-의-limiting-case-임\"><a href=\"#3-4-DPG-는-SPG-의-limiting-case-임\" class=\"headerlink\" title=\"3.4 DPG 는 SPG 의 limiting case 임\"></a>3.4 DPG 는 SPG 의 limiting case 임</h3><ul>\n<li>stochastic policy parameterization<ul>\n<li>$ \\pi_{\\mu_{\\theta},\\sigma} $ by a deterministic policy $ \\mu_{\\theta} : S \\to A $ and a variance parameter $ \\sigma $</li>\n<li>$ \\sigma = 0 $ 이면, $ \\pi_{\\mu_{\\theta},\\sigma} \\equiv \\mu_{\\theta} $</li>\n</ul>\n</li>\n<li>Theorem 2. Policy 의 variance 가 0 에 수렴하면, 즉, $ \\sigma \\to 0 $, stochastic policy gradient 와 deterministic policy gradient 는 동일해짐<ul>\n<li>조건 : stochastic policy $ \\pi_{\\mu_{\\theta},\\sigma} = \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $<ul>\n<li>$ \\sigma $ 는 variance</li>\n<li>$ \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $ 는 conditions B.1 만족</li>\n<li>MDP 는 conditions A.1 및 A.2 만족</li>\n</ul>\n</li>\n<li>결과 :<ul>\n<li>$ \\lim\\limits_{\\sigma\\downarrow0}\\nabla_{\\theta}J(\\pi_{\\mu_{\\theta},\\sigma}) = \\nabla_{\\theta}J(\\mu_{\\theta})  $<ul>\n<li>좌변은 standard stochastic gradient 이며, 우변은 deterministic gradient.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>의미 :<ul>\n<li>deterministic policy gradient 는 stochastic policy gradient 의 특수 case 임</li>\n<li>기존 유명한 policy gradients 기법들에 deterministic policy gradients 를 적용할 수 있음<ul>\n<li>기존 기법들 예: compatible function approximation (Sutton, 1999), natural gradients (Kakade, 2001), actor-critic (Bhatnagar, 2007) or episodic/batch methods (Peters, 2005)<br><br></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2 id=\"4-Deterministic-Actor-Critic-Algorithms\"><a href=\"#4-Deterministic-Actor-Critic-Algorithms\" class=\"headerlink\" title=\"4. Deterministic Actor-Critic Algorithms\"></a>4. Deterministic Actor-Critic Algorithms</h2><ol>\n<li>살사 critic 을 이용한 on-policy actor-critic<ul>\n<li>단점<ul>\n<li>deterministic policy 에 의해 행동하면 exploration 이 잘 되지 않기에, sub-optimal 에 빠지기 쉬움</li>\n</ul>\n</li>\n<li>목적<ul>\n<li>교훈/정보제공</li>\n<li>환경에서 충분한 noise 를 제공하여 exploration을 시킬 수 있다면, deterministic policy 를 사용한다고 하여도 좋은 학습 결과를 얻을 수도 있음$<ul>\n<li>예. 바람이 agent의 행동에 영향(noise)을 줌</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Remind: 살사(SARSA) update rule<ul>\n<li>$ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma Q(s_{t+1},a_{t+1}) - Q(s_{t},a_{t})) $</li>\n</ul>\n</li>\n<li>Algorithm<ul>\n<li>Critic 은 MSE 를 $ \\bf minimize $ 하는 방향, 즉, action-value function 을 stochastic gradient $ \\bf descent $ 방법으로 update 함<ul>\n<li>$ MSE = [Q^{\\mu}(s,a) - Q^{w}(s,a)]^2 $<ul>\n<li>critic 은 실제 $ Q^{\\mu}(s,a) $ 대신 미분 가능한 $ Q^{w}(s,a) $ 로 대체하여 action-value function 을 estimate 하며, 이 둘 간 Mean Square Error 를 최소화하는 것이 목표</li>\n</ul>\n</li>\n<li>$ \\nabla_{w}MSE \\approx -2 * [r + \\gamma Q^{w}(s’,a’) - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $<ul>\n<li>$ \\nabla_{w}MSE = -2 * [Q^{\\mu}(s,a) - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $</li>\n<li>$ Q^{\\mu}(s,a) $ 를 $ r + \\gamma Q^{w}(s’,a’) $ 로 대체<ul>\n<li>$ Q^{\\mu}(s,a) = r + \\gamma Q^{\\mu}(s’,a’) $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $<ul>\n<li>$w_{t+1} = w_{t} - \\alpha_{w}\\nabla_{w}MSE  \\nonumber$<br>$ \\approx w_{t} - \\alpha_{w} <em> (-2 </em> [r + \\gamma Q^{w}(s’,a’) - Q^{w}(s,a)] \\nabla_{w}Q^{w}(s,a)$</li>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},a_{t+1}) - Q^{w}(s_{t},a_{t}) $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Actor 는 식(9)에 따라 보상이 $ \\bf maximize $ 되는 방향, 즉, deterministic policy 를 stochastic gradient $ \\bf ascent $ 방법으로 update함<ul>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Q-learning 을 이용한 off-policy actor-critic<ul>\n<li>stochastic behavior policy $ \\beta(a|s) $ 에 의해 생성된 trajectories 로부터 deterministic target policy $ \\mu_{\\theta}(s) $ 를 학습하는 off-policy actor-critic</li>\n<li>performance objective<ul>\n<li>$ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)V^{\\mu}(s)ds \\nonumber \\\\$<br>$= \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds \\nonumber \\\\$<br>$= E_{s \\sim \\rho^{\\beta}}[Q^{\\mu}(s,\\mu_{\\theta}(s))]$</li>\n</ul>\n</li>\n<li>off-policy deterministic policy gradient<ul>\n<li>$ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $<ul>\n<li>논문에는 아래와 같이 나와있는데, 물결 표시 부분은 오류로 판단됨.</li>\n<li>$ \\begin{eqnarray}<br>  \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) &amp;\\approx&amp; \\int_{S}\\rho^{\\beta}(s)\\nabla_{\\theta}\\mu_{\\theta}(a|s)Q^{\\mu}(s,a)ds \\nonumber \\<br>  &amp;=&amp; E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]<br>  \\end{eqnarray} $</li>\n<li>근거: Action이 deterministic 하기에 stochastic 경우와는 다르게 performance objective 에서 action 에 대해 평균을 구할 필요가 없음. 그렇기에, 곱의 미분이 있을 필요가 없고, [Degris, 2012b] 에서처럼 곱의 미분을 통해 생기는 action-value function 에 대한 gradient term 를 생략할 필요가 사라짐. <a href=\"#off-policy-actor-critic\">참고</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Remind: 큐러닝(Q-learning) update rule<ul>\n<li>$ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma \\max\\limits_{a}Q(s_{t+1},a) - Q(s_{t},a_{t})) $</li>\n</ul>\n</li>\n<li>algorithm: OPDAC (Off-Policy Deterministic Actor-Critic)<ul>\n<li>살사를 이용한 on-policy deterministic actor-critic 과 아래 부분을 제외하고는 같음<ul>\n<li>target policy 는 $ \\beta(a|s) $ 에 의해 생성된 trajectories 를 통해 학습함</li>\n<li>업데이트 목표 부분에 실제 행동 값 $ a_{t+1} $ 이 아니라 정책으로부터 나온 행동 값 $ \\mu_{\\theta}(s_{t+1}) $ 사용함<ul>\n<li>$ \\mu_{\\theta}(s_{t+1}) $ 는 가장 높은 Q 값을 가지는 행동. 즉, Q-learning.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $</li>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $</li>\n</ul>\n</li>\n<li>Stochastic off-policy actor-critic 은 대개 actor 와 critic 모두 importance sampling을 필요로 하지만, deterministic policy gradient 에선 importance sampling이 필요없음<ul>\n<li>Actor 는 deterministic 이기에 sampling 자체가 필요없음<ul>\n<li>Stochastic policy 인 경우, Actor 에서 importance sampling이 필요한 이유는 상태 $ s $ 에서의 가치 함수 값 $ V^{\\pi}(s) $ 을 estimate 하기 위해 $ \\pi $ 가 아니라 $ \\beta $ 에 따라 sampling을 한 후, 평균을 내기 때문임.</li>\n<li>하지만 Deterministic policy 인 경우, 상태 $ s $ 에서의 가치 함수 값 $ V^{\\pi}(s) = Q^{\\pi}(s,\\mu_{\\theta}) $ 즉, action 이 상태 s 에 대해 deterministic 이기에 sampling 을 통해 estimate 할 필요가 없고, 따라서 importance sampling도 필요없어짐.</li>\n<li>stochastic vs. deterministic performance objective<ul>\n<li>stochastic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads $</li>\n<li>deterministic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Critic 이 사용하는 Q-learning 은 importance sampling이 필요없는 off policy 알고리즘임.<ul>\n<li>Q-learning 도 업데이트 목표를 특정 분포에서 샘플링을 통해 estimate 하는 것이 아니라 Q 함수를 최대화하는 action을 선택하는 것이기에 위 actor 에서의 deterministic 경우와 비슷하게 볼 수 있음</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>compatible function approximation 및 gradient temporal-difference learning 을 이용한 actor-critic<ul>\n<li>위 살사/Q-learning 기반 on/off-policy 는 아래와 같은 문제가 존재<ul>\n<li>function approximator 에 의한 bias<ul>\n<li>일반적으로 $ Q^{\\mu}(s,a) $ 를 $ Q^{w}(s,a) $ 로 대체하여 deterministic policy gradient 를 구하면, 그 gradient 는 ascent 하는 방향이 아닐 수도 있음</li>\n</ul>\n</li>\n<li>off-policy learning 에 의한 instabilities</li>\n</ul>\n</li>\n<li>그래서 stochastic 처럼 $ \\nabla_{a}Q^{\\mu}(s,a) $ 를 $ \\nabla_{a}Q^{w}(s,a) $ 로 대체해도 deterministic policy gradient 에 영향을 미치지 않을 compatible function approximator $ Q^{w}(s,a) $ 를 찾아야 함</li>\n<li>Theorem 3. 아래 두 조건을 만족하면, $ Q^{w}(s,a) $ 는 deterministic policy $ \\mu_{\\theta}(s) $ 와 compatible 함. 즉, $ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $<ul>\n<li>$ \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $</li>\n<li>$ w $ 는 $ MSE(\\theta, w) = E[\\epsilon(s;\\theta,w)^{\\top}\\epsilon(s;\\theta,w)] $ 를 최소화함<ul>\n<li>$ \\epsilon(s;\\theta,w) = \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} - \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}  $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Theorem 3 은 on-policy 뿐만 아니라 off-policy 에도 적용 가능함</li>\n<li>$ Q^{w}(s,a) = (a-\\mu_{\\theta}(s))^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top} w + V^{v}(s) $<ul>\n<li>어떠한 deterministic policy 에 대해서도 위 형태와 같은 compatible function approximator 가 존재함.</li>\n<li>앞의 term 은 advantage 를, 뒤의 term 은 value 로 볼 수 있음</li>\n</ul>\n</li>\n<li>$ Q^{w}(s,a) = \\phi(s,a)^{\\top} w + v^{\\top}\\phi(s) $<ul>\n<li>정의 : $ \\phi(s,a) \\overset{\\underset{\\mathrm{def}}{}}{=} \\nabla_{\\theta}\\mu_{\\theta}(s)(a-\\mu_{\\theta}(s)) $</li>\n<li>일례 : $ V^{v}(s) = v^{\\top}\\phi(s) $</li>\n<li>Theorem 3 만족 여부<ul>\n<li>첫 번째 조건 만족.</li>\n<li>두 번째 조건은 대강 만족.<ul>\n<li>$ \\nabla_{a}Q^{\\mu}(s,a) $ 에 대한 unbiased sample 을 획득하기는 매우 어렵기에, 일반적인 정책 평가 방법들로 $ w $ 학습.</li>\n<li>이 정책 평가 방법들을 이용하면 $ Q^{w}(s,a) \\approx Q^{\\mu}(s,a) $ 인 reasonable solution을 찾을 수 있기에 대강 $ \\nabla_{a}Q^{w}(s,a) \\approx \\nabla_{a}Q^{\\mu}(s,a) $ 이 될 것.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>action-value function 에 대한 linear function approximator 는 큰 값을 가지는 actions 에 대해선 diverge할 수 있어 global 하게 action-values 예측하기에는 좋지 않지만, local critic 에 사용할 때는 매우 유용하다.<ul>\n<li>즉, 절대값이 아니라 작은 변화량을 다루는 gradient method 경우엔 $ A^{w}(s,\\mu_{\\theta}(s)+\\delta) = \\delta^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $ 로, diverge 하지 않고, 값을 얻을 수 있음</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>COPDAC-Q algorithm (Compatible Off-Policy Deterministic Actor-Critic Q-learning critic)<ul>\n<li>Critic: 실제 action-value function 에 대한 linear function approximator 인 $ Q^{w}(s,a) = \\phi(s,a)^{\\top}w $ 를 estimate<ul>\n<li>$ \\phi(s,a) = a^{\\top}\\nabla_{\\theta}\\mu_{\\theta} $</li>\n<li>Behavior policy $ \\beta(a|s) $ 로부터 얻은 samples를 이용하여 Q-learning 이나 gradient Q-learning 과 같은 off-policy algorithm 으로 학습 가능함</li>\n</ul>\n</li>\n<li>Actor: estimated action-value function 에 대한 gradient 를 $ \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $ 로 치환 후, 정책을 업데이트함</li>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) $</li>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $</li>\n</ul>\n</li>\n<li>off-policy Q-learning은 linear function approximation을 이용하면 diverge 할 수도 있음<ul>\n<li>$ \\mu_{\\theta}(s_{t+1}) $ 이 diverge 할 수도 있기 때문으로 판단됨.</li>\n<li>그렇기에 simple Q-learning 대신 다른 기법이 필요함.</li>\n</ul>\n</li>\n<li>그렇기에 critic 에 gradient Q-learning 사용한 COPDAC-GQ (Gradient Q-learning critic) algorithm 제안<ul>\n<li>gradient temporal-difference learning 에 기반한 기법들은 true gradient descent algorithm 이며, converge가 보장됨. (Sutton, 2009)<ul>\n<li>기본 아이디어는 stochastic gradient descent 로 Mean-squared projected Bellman error (MSPBE) 를 최소화하는 것</li>\n<li>critic 이 actor 보다 빠른 time-scale 로 update 되도록 step size 들을 잘 조절하면, critic 은 MSPBE 를 최소화하는 parameters 로 converge 하게 됨</li>\n<li>critic 에 gradient temporal-difference learning 의 일종인 gradient Q-learning 사용한 논문 (Maei, 2010)</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>COPDAC-GQ algorithm<ul>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $</li>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) - \\alpha_{w}\\gamma\\phi(s_{t+1}, \\mu_{\\theta}(s_{t+1}))(\\phi(s_{t},a_{t})^{\\top} u_{t}) $</li>\n<li>$ v_{t+1} = v_{t} + \\alpha_{v}\\delta_{t}\\phi(s_{t}) - \\alpha_{v}\\gamma\\phi(s_{t+1})(\\phi(s_{t},a_{t})^{\\top} u_{t}) $</li>\n<li>$ u_{t+1} = u_{t} + \\alpha_{u}(\\delta_{t}-\\phi(s_{t}, a_{t})^{\\top} u_{t})\\phi(s_{t}, a_{t}) $</li>\n</ul>\n</li>\n<li>stochastic actor-critic 과 같이 매 time-step 마다 update 시 필요한 계산의 복잡도는 $ O(mn) $<ul>\n<li>m 은 action dimensions, n 은 number of policy parameters</li>\n</ul>\n</li>\n<li>Natural policy gradient 를 이용해 deterministic policies 를 찾을 수 있음<ul>\n<li>$ M(\\theta)^{-1}\\nabla_{\\theta}J(\\mu_{\\theta}) $ 는 any metric $ M(\\theta) $ 에 대한 our performance objective (식(14)) 의 steepest ascent direction 임 (Toussaint, 2012)</li>\n<li>Natural gradient 는 Fisher information metric $ M_{\\pi}(\\theta) $ 에 대한 steepest ascent direction 임<ul>\n<li>$ M_{\\pi}(\\theta) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)^{\\top}] $</li>\n<li>Fisher information metric 은 policy reparameterization 에 대해 불변임 (Bagnell, 2003)</li>\n</ul>\n</li>\n<li>deterministic policies 에 대해 metric 으로 $ M_{\\mu}(\\theta) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}] $ 을 사용.<ul>\n<li>이는 variance 가 0 인 policy 에 대한 Fisher information metric 으로 볼 수 있음</li>\n<li>$ \\frac{\\nabla_{\\theta}\\pi_{\\theta}(a\\vert s)}{\\pi_{\\theta}(a\\vert s)}$ 에서 policy variance 가 0 이면, 특정 s 에 대한 $ \\pi_{\\theta}(a|s)$ 만 1 이 되고, 나머지는 0 임</li>\n</ul>\n</li>\n<li>deterministic policy gradient theorem 과 compatible function approximation 을 결합하면 $ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] $ 이 됨<ul>\n<li>$ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $</li>\n<li>$ \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)} \\approx \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $</li>\n</ul>\n</li>\n<li>그렇기에 steepest ascent direction 은 $ M_{\\mu}(\\theta)^{-1}\\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = w $ 이 됨<ul>\n<li>$ E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}]^{-1}E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] = w $</li>\n</ul>\n</li>\n<li>이 알고리즘은 COPDAC-Q 혹은 COPDAC-GQ 에서 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $ 식을 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta}w_{t} $ 로 바꿔주기만 하면 됨</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"Experiments\"><a href=\"#Experiments\" class=\"headerlink\" title=\"Experiments\"></a>Experiments</h2><h3 id=\"Continuous-Bandit\"><a href=\"#Continuous-Bandit\" class=\"headerlink\" title=\"Continuous Bandit\"></a>Continuous Bandit</h3><ul>\n<li>Stochastic Actor-Critic (SAC)과 COPDAC 간 성능 비교 수행<ul>\n<li>Action dimension이 커질수록 성능 차이가 심함</li>\n<li>빠르게 수렴하는 것을 통해 DPG의 data efficiency가 SPG에 비해 좋다는 것을 확인할 수 있지만, 반면, time-step이 증가할수록 SAC와 COPDAC 간 성능 차이가 줄어드는 것을 통해 성능 차이가 심하다는 것은 일정 time step 내에서만 해당하는 것이라고 유추해볼 수 있음</li>\n<li><img src=\"https://www.dropbox.com/s/hrkyq0s2f24z66r/Screenshot%202018-06-16%2017.47.38.png?dl=1\"></li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Continuous-Reinforcement-Learning\"><a href=\"#Continuous-Reinforcement-Learning\" class=\"headerlink\" title=\"Continuous Reinforcement Learning\"></a>Continuous Reinforcement Learning</h3><ul>\n<li>COPDAC-Q가 SAC와 off-policy stochastic actor-critic(OffPAC-TD) 간 성능 비교 수행<ul>\n<li>COPDAC-Q의 성능이 약간 더 좋음</li>\n<li>COPDAC-Q의 학습이 더 빨리 이뤄짐</li>\n<li><img src=\"https://www.dropbox.com/s/qdca4augapmzsxi/Screenshot%202018-06-16%2017.47.07.png?dl=1\"><h3 id=\"Octopus-Arm\"><a href=\"#Octopus-Arm\" class=\"headerlink\" title=\"Octopus Arm\"></a>Octopus Arm</h3></li>\n</ul>\n</li>\n<li>목표: 6 segments octopus arm (20 action dimensions &amp; 50 state dimensions)을 control하여 target을 맞추는 것<ul>\n<li>COPDAC-Q 사용 시, action space dimension이 큰 octopus arm을 잘 control하여 target을 맞춤</li>\n<li><img src=\"https://www.dropbox.com/s/xrxb0a52wntekld/Screenshot%202018-06-16%2017.46.28.png?dl=1\" width=\"600px\"></li>\n<li>기존 기법들은 action spaces 혹은 action 과 state spaces 둘 다 작은 경우들에 대해서만 실험했다고 하며, 비교하고 있지 않음.<ul>\n<li>기존 기법들이 6 segments octopus arm에서 동작을 잘 안 했을 것 같긴한데, 그래도 실험해서 결과를 보여주지…왜 안 했을까?</li>\n</ul>\n</li>\n<li>8 segment arm 동영상이 저자 홈페이지에 있다고 하는데, 안 보임.</li>\n</ul>\n</li>\n<li>[참고] Octopus Arm 이란?<ul>\n<li><a href=\"https://www.youtube.com/watch?v=AxeeHif0euY\" target=\"_blank\" rel=\"noopener\">OctopusArm Youtube Link</a></li>\n<li><img src=\"https://www.dropbox.com/s/950ycj06sudakjx/Screenshot%202018-06-16%2017.45.52.png?dl=1\"></li>\n</ul>\n</li>\n</ul>\n"},{"title":"High-Dimensional Continuous Control using Generalized Advantage Estimation","date":"2018-06-12T10:18:45.000Z","author":"양혁렬, 이동민","subtitle":"피지여행 7번째 논문","_content":"\n<center> <img src=\"https://www.dropbox.com/s/p8gfpyo6xf9wm5w/Screen%20Shot%202018-07-18%20at%201.25.53%20AM.png?dl=1\" width=\"700\"> </center>\n\n논문 저자 : John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan and Pieter Abbeel\n논문 링크 : https://arxiv.org/pdf/1506.02438.pdf\nProceeding : ??\n정리 : 양혁렬, 이동민\n\n---\n\n# 1. 들어가며...\n\n현존하는 Policy Gradient Method들의 목적은 누적되는 reward들을 optimization하는 것입니다. 하지만 학습할 때에 많은 양의 sample이 필요로 하고, 들어오는 data가 nonstationarity임에도 불구하고 stable and steady improvement가 어렵습니다.\n\n그래서 이 논문에서는 다음과 같은 방법을 제시합니다.\n\n- TD($\\lambda$)와 유사한 advantage function의 exponentially-weighted estimator를 사용하여 policy gradient estimate의 variance를 줄이는 것 \n- policy와 value function에 대한 Trust Region Optimization 사용하는 것\n\n3D locomotion tasks에 대한 empirical results는 다음과 같습니다.\n\n- bipedal and quadrupedal simulated robots의 달리는 자세를 학습\n- bipedal 사람이 땅에 누워있다가 일어서는 것을 학습\n\n※ TD($\\lambda$)를 혹시 모르실 경우도 있을 것 같아 간략하게 정리하였습니다. http://dongminlee.tistory.com/10 를 먼저 보시고 아래의 내용을 봐주시기 바랍니다!\n\n<br><br>\n\n# 2. Introduction\n\n기본적으로 \"parameterized stochastic policy\"를 가정합니다. 이 때 expected total returns의 gradient에 대한 unbiased estimate를 얻을 수 있는데 이것을 REINFORCE라고 부릅니다. 하지만 하나의 action의 결과가 과거와 미래의 action의 결과로 혼동되기 때문에 gradient estimator의 high variance는 시간에 따라 scaling됩니다.\n\n또 다른 방법은 Actor-Critic이 있습니다. 이 방법은 empirical returns보다 하나의 value function을 사용합니다. 또한 bias하고 lower variance를 가진 estimator입니다. 구체적으로 말하자면, high variance하다면 더 sampling을 하면 되는 반면에 bias는 매우 치명적입니다. 다시 말해 bias는 algorithm이 converge하는 데에 실패하거나 또는 local optimum이 아닌 poor solution에 converge하도록 만듭니다.\n\n따라서 이 논문에서는 $\\gamma\\in [0,1]$ and $\\lambda\\in [0,1]$에 의해 parameterized estimation scheme인 Generalized Advantage Estimator(GAE)를 다룹니다.\n\n이 논문을 대략적으로 요약하자면 다음과 같습니다.\n\n- 효과적인 variance reduction scheme인 GAE를 다룹니다. 또한 실험할 때 batch trust region algorithm에 더하여 다양한 algorithm들에 적용됩니다.\n- value function에 대해 trust region optimization method를 사용합니다. 이렇게 함으로서 더 robust하고 efficient한 방법이 됩니다.\n- A와 B를 합쳐서, 실험적으로 control task에 neural network policies를 learning하는 데에 있어서 효과적인 algorithm을 얻습니다. 이러한 결과는 high-demensional continuous control에 RL을 사용함으로서 state of the art로 확장되었습니다.\n\n<br><br>\n\n# 3. Preliminaries\n\n먼저 policy optimization의 \"undiscounted formulation\"을 가정합니다. (undiscounted formulation에 주목합시다.)\n\n- initial state $s_0$는 distribution $\\rho_0$으로부터 sampling된 것입니다.\n- 하나의 trajectory ($s_0, a_0, s_1, a_1, ...$)는 terminal state에 도달될 때 까지 policy $a_t$ ~ $\\pi(a_t | s_t)$에 따라서 action을 sampling하고, dynamics $s_{t+1}$ ~ $P(s_{t+1} | s_t, a_t)$에 따라서 state를 sampling함으로써 생성됩니다.\n- reward $r_t = r(s_t, a_t, s_{t+1})$은 매 time step마다 받아집니다.\n- 목표는 모든 policies에 대해 finite하다고 가정됨으로서 expected total reward $\\sum_{t=0}^{\\infty} r_t$를 maximize하는 것입니다.\n\n여기서 중요한 점은 $\\gamma$를 discount의 parameter로 사용하는 것이 아니라 \"bias-variance tradeoff를 조절하는 parameter로 사용한다\" 는 것입니다.\n\npolicy gradient method는 gradient $g := \\nabla_\\theta \\mathbb{E} [\\sum_{t=0}^\\infty r_t]$를 반복적으로 estimate함으로써 expected total reward를 maximize하는 것인데, policy gradient에는 여러 다른 표현들이 있습니다.\n$$g = \\mathbb{E} [\\sum_{t=0}^\\infty \\Phi_t \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$\n\n여기서 $\\Phi_t$는 아래 중의 하나일 수 있습니다.\n\n1. $\\sum_{t=0}^\\infty r_t$: trajectory의 total reward\n2. $\\sum_{t'=t}^\\infty r_t'$: action $a_t$ 후의 reward\n3. $\\sum_{t'=t}^\\infty r_t' - b(s_t)$: 2의 baselined version\n4. $Q^\\pi (s_t, a_t)$: state-action value function\n5. $A^\\pi (s_t, a_t)$: advantage function\n6. $r_t + V^\\pi (s_{t+1}) - V^\\pi (s_t)$: TD residual\n\n위의 번호 중 4, 5, 6의 수식들은 다음의 정의를 사용합니다.\n\n- $V^\\pi (s_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_t:\\infty} [\\sum_{l=0}^\\infty r_{t+1}]$\n- $Q^\\pi (s_t, a_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty} [\\sum_{l=0}^\\infty r_{t+l}]$\n- $A^\\pi (s_t, a_t) := Q^\\pi (s_t, a_t) - V^\\pi (s_t)$, (Advantage function)\n\n추가적으로 colon notation $a : b$는 포괄적인 범위 $(a, a+1, ... , b)$입니다. (잘 기억해둡시다. 뒤에 계속해서 colon notation이 나옵니다.)\n\n여기서부터 parameter $\\gamma$에 대해 좀 더 자세히 알아봅시다. parameter $\\gamma$는 bias하면서 동시에 reward를 downweighting함으로써 variance를 줄입니다. 다시 말해 MDP의 discounted formulation에서 사용된 discounted factor와 일치하지만, 이 논문에서는 \"$\\gamma$를 undiscounted MDP에서 variance reduction parameter\"로 다룹니다. -> 결과는 같지만, 의미와 의도가 다릅니다.\n\ndiscounted value function들은 다음과 같습니다.\n\n- $V^{\\pi, \\gamma} (s_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_t:\\infty} [\\sum_{l=0}^\\infty \\gamma^l r_{t+l}]$ \n- $Q^\\pi (s_t, a_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty} [\\sum_{l=0}^\\infty \\gamma^l r_{t+l}]$\n- $A^{\\pi, \\gamma} (s_t, a_t) := Q^{\\pi, \\gamma} (s_t, a_t) - V^{\\pi, \\gamma} (s_t)$\n\n따라서 policy gradient에서의 discounted approximation은 다음과 같이 나타낼 수 있습니다.\n$$g^\\gamma := \\mathbb{E}_{s_{0:\\infty} a_{0:\\infty}} [\\sum_{t=0}^\\infty A^{\\pi, \\gamma} (s_t, a_t) \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$\n뒤이어 나오는 section에서는 위의 수식에서 $A^{\\pi, \\gamma}$에 대해 biased (but not too biased) estimator를 얻는 방법에 대해서 나옵니다.\n\n다음으로 advantage function에서 새롭게 접하는 \"$\\gamma$-just estimator\"의 notation에 대해 알아봅시다.\n\n- 먼저 $\\gamma$-just estimator는 $g^\\gamma$ ${}^1$를 estimate하기 위해 위의 수식에서 $A^{\\pi, \\gamma}$를 $\\gamma$-just estimator로 사용할 때, bias하지 않은 estimator라고 합시다. 그리고 이 $\\gamma$-just advantage estimator를 $\\hat{A}_t (s_{0:\\infty} , a_{0:\\infty})$라고 하고, 전체의 trajectory에 대한 하나의 function이라고 합시다.\n- ${}^1$에서 이 논문의 저자가 하고 싶은 말이 있는 것 같습니다. 개인적으로 $\\gamma$-just estimator를 이해하는 데에 있어서 중요한 주석이라 정확히 해석하고자 합니다.\n    - $A^\\pi$를 $A^{\\pi, \\gamma}$로 사용함으로써 이미 bias하다라고 말했지만, \"이 논문에서 하고자 하는 것은 $g^\\gamma$에 대해 unbiased estimate를 얻고 싶은 것입니다.\" 하지만 undiscounted MDP의 policy gradient에 대해서는 당연히 $\\gamma$를 사용하기 때문에 biased estimate를 얻습니다. 개인적으로 이것은 일단 무시하고 $\\gamma$를 사용할 때 어떻게 unbiased estimate를 얻을 지에 대해 좀 더 포커스를 맞추고 있는 것 같습니다.\n    - 그러니까 저 $A^{\\pi, \\gamma}$를 $\\gamma$-just estimator로 바꿔줌으로써 unbiased estimate를 하고 싶다는 것이 뒤이어 나오는 정의와 명제의 핵심이라고 할 수 있습니다.\n\nDefinition 1.\n먼저 가정을 합니다. (가정을 바탕으로 이루어지는 정의라는 것을 주목합시다.) 만약\n$$\\mathbb{E}_{s_{0:\\infty} a_{0:\\infty}} [\\hat{A}_t (s_{0:\\infty}, a_{0:\\infty}) \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)] = \\mathbb{E}_{s_{0:\\infty} a_{0:\\infty}} [A^{\\pi, \\gamma} (s_t, a_t) \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$\n두 수식이 같다면, estimator $\\hat{A}_t$는 $\\gamma$-just입니다.\n\n그리고 만약 모든 t에 대해서 $\\hat{A}_t$이 $\\gamma$-just이라면, 다음과 같이 표현할 수 있습니다.\n$$\\mathbb{E}_{s_{0:\\infty} a_{0:\\infty}} [\\hat{A}_t (s_{0:\\infty}, a_{0:\\infty}) \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)] = g^\\gamma$$\n위의 수식이 바로 unbiased estimate입니다.\n\n$\\gamma$-just인 $\\hat{A}_t$에 대한 한가지 조건은 $\\hat{A}_t$이 두 가지 function $Q_t$ and $b_t$로 나뉠 수 있다는 것입니다.\n\n- $Q_t$는 $\\gamma$-discounted Q-function의 unbiased estimator입니다.\n- $b_t$는 action $a_t$전에 sampling된 states and actions의 arbitrary function이다. \n\nProposition 1.\n모든 $(s_t, a_t)$에 대해,\n$$\\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty | s_t, a_t} [Q_t (s_{t:\\infty}, a_{t:\\infty})] = Q^{\\pi, \\gamma} (s_t, a_t)$$\n로 인하여 $\\hat{A}_t$이 \n$$\\hat{A}_{s_{0:\\infty}, a_{0:\\infty}} = Q_t (s_{0:\\infty}, a_{0:\\infty}) - b_t (s_{0:t}, a_{0:t-1})$$\n형태라고 가정합시다. (가정을 바탕으로 이루어지는 명제라는 점을 주목합시다.)\n\n그 때, $\\hat{A}_t$은 $\\gamma$-just입니다.\n\n이 명제에 대한 증명은 Appendix B에 있습니. 그리고 $\\hat{A}_t$에 대한 $\\gamma$-just advantage estimator들은 다음과 같습니다.\n\n- $\\sum_{l=0}^\\infty \\gamma^l r_{t+1}$\n- $A^{\\pi, \\gamma} (s_t, a_t)$\n- $Q^{\\pi, \\gamma} (s_t, a_t)$\n- $r_t + \\gamma V^{\\pi, \\gamma} (s_{t+1}) - V^{\\pi, \\gamma} (s_t)$\n\n증명을 보기 전에 먼저 Advantage function에 대해서 먼저 살펴봅시다. 아래의 내용은 Sutton이 쓴 논문인 Policy Gradient Methods for Reinforcement Learning with Function Approximation(2000)에서 나온 내용을 리뷰한 것입니다. \n\n<center> <img src=\"https://www.dropbox.com/s/x737yq97ut6gp1a/Screen%20Shot%202018-07-15%20at%201.16.09%20PM.png?dl=1\" width=\"600\"> </center>\n\n여기서는 \"이 수식은 $\\sum_a \\frac{\\partial \\pi (s,a)}{\\partial \\theta} = 0$이기 때문에 가능해진다.\", \"즉, 이 수식은 $\\pi(s,a)$의 gradient에만 dependent하기 때문에 advantage 역할을 하는 함수들을 넣어도 아무런 상관이 없다\"라는 두 문장을 기억해둡시다.\n\n이제 증명을 보면 됩니다.\n\n<center> <img src=\"https://www.dropbox.com/s/e7sj2fwcm1f7hof/figure2.jpg?dl=1\" width=\"600\"> </center>\n<center> <img src=\"https://www.dropbox.com/s/kyk4jes1202az4m/figure3.jpg?dl=1\" width=\"600\"> </center>\n\n위의 증명 수식에서 Q와 b의 각각 세번째 수식과 마지막 수식을 자세히 봅시다.\n\n1. $Q$에 대한 증명\n    - 빨간색 부분\n        - $\\mathbb{E}_{s_{t+1:\\infty}, a_{t+1:\\infty}} [Q_t (s_{0:\\infty}, a_{0:\\infty})]$를 쉽게 말하자면 \"모든(과거, 현재, 미래) state와 action에 대해서 expectation을 $t+1$부터 $\\infty$까지 하겠다.\"라는 말입니다. 이렇게 함으로써 원래는 $Q^\\pi (s_t, a_t)$가 나와야 맞는 건데, 앞에서 살펴봤다시피 4번째 수식 밑줄 친 자리에는 advantage 역할을 하는 함수들을 넣어도 결과값에 아무런 영향을 끼치지 않기 때문에 의도적으로 $A^\\pi (s_t, a_t)$로 바꾼 것입니다. 또한 이 증명을 바탕으로 $\\hat{A}$이 $\\gamma$-just라는 것을 표현하고 싶기 때문이라고 봐도 됩니다.\n\n    - 파란색 부분\n        - 또한 $a_{0:t}$에서 $a_{0:t-1}$로 변한 것은 $A^\\pi (s_t, a_t)$에 baseline이 들어가기 때문에 바꿔준 것입니다.\n\n2. $b$에 대한 증명\n    - 빨간색 부분\n        - $\\mathbb{E}_{s_{t+1:\\infty}, a_{t+1:\\infty}} [\\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$가 0으로 바뀌는 것은 위에서 설명했듯이 $\\nabla_\\theta$자체가 $\\log \\pi_\\theta$만 gradient하기 때문에 이것을 expectation을 취하면 0이 됩니다.\n\n<br><br>\n\n# 4. Advantage Function Estimation\n\n이번 section에는 discounted advantage function $A^{\\pi, \\gamma} (s_t, a_t)$의 accurate estimate $\\hat{A}_t$에 대해서 살펴봅시다. 이에 따른 수식은 다음과 같습니다.\n\n$$\\hat{g} = \\frac{1}{N} \\sum_{n=1}^N \\sum_{t=0}^\\infty \\hat{A}_t^n \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t^n | s_t^n)$$\n\n여기서 n은 a batch of episodes에 대하여 index한 것입니다.\n\n$V$를 approximate value function이라고 합시다. 그리고 $\\delta_t^V = r_t + \\gamma V(s_{t+1}) - V(s_t)$이라고 합시다.\n\n만약 (이전과 마찬가지로 가정부터 합니다.) correct value function $V = V^{\\pi, \\gamma}$가 있다고 한다면, 이것은 $\\gamma$-just advantage estimator입니다. 실제로, $A^{\\pi, \\gamma}$의 unbiased estimator는 다음과 같습니다.\n$$\\mathbb{E}_{s_{t+1}} [\\delta_t^{V^{\\pi, \\gamma}}] = \\mathbb{E}_{s_{t+1}} [r_t + \\gamma V^{\\pi, \\gamma} (s_{t+1}) - V^{\\pi, \\gamma} (s_t)]$$\n$$= \\mathbb{E}_{s_{t+1}} [Q^{\\pi, \\gamma} (s_t, a_t) - V^{\\pi, \\gamma} (s_t)] = A^{\\pi, \\gamma} (s_t, a_t)$$\n그러나, \"이 estimator는 유일하게 $V = V^{\\pi, \\gamma}$에 대한 $\\gamma$-just입니다.\" 다른 경우라면 이것은 biased policy gradient estimate일 것입니다. (우리가 하고 싶은 것은 $V$에 대해서만 unbiased estimator가 아니라 advantage function에 대해서 일반화된 unbiased estimator를 얻고 싶은 것입니다. 그래서 아래에서도 나오겠지만, $\\gamma$와 함께 $\\lambda$를 이용한 estimator가 나옵니다.ㅏ)\n\n그래서 $\\delta$에 대해 $k$의 sum으로 생각해봅시다. 이것을 $\\hat{A}_t^{(k)}$라고 하자. 그러면 아래와 같이 표현할 수 있습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/ra7hxksveg2hz45/figure4.jpg?dl=1\" width=\"600\"> </center>\n\n$\\hat{A}_t^{(k)}$은 returns의 $k$-step estimate와 연관지을 수 있고, $\\delta_t^V = \\hat{A}_t^{(1)}$의 case와 유사하게도 $\\hat{A}_t^{(k)}$를 advantage function의 estimator로 생각할 수 있습니다.\n\n여기서 $k \\rightarrow \\infty$로 생각해보면 bias가 일반적으로 점점 더 작아집니다. 왜냐하면 $\\gamma^k V(s_{t+k})$가 점점 많이 discounted되서 $-V(s_t)$가 bias에 영향을 미치지 못하기 때문입니다. $k \\rightarrow \\infty$를 취하면 다음과 같은 수식이 나옵니다.\n\n$$\\hat{A}_t^{(\\infty)} = \\sum_{l=0}^\\infty \\gamma^l \\delta_{t+l}^V = -V(s_t) + \\sum_{l=0}^\\infty \\gamma^l r_{t+l}$$\n우변의 수식과 같이 empirical returns에서 value function baseline을 뺀 것으로 나타낼 수 있습니다.\n\nGeneralized Advantage Estimator GAE($\\gamma, \\lambda$)는 $k$-step estimators의 exponentially-weighted average로 나타낼 수 있습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/yg1ybmfkep3towu/figure5.jpg?dl=1\" width=\"600\"> </center>\n\n(TD($\\lambda$)를 떠올려주세요..!)\n\n위의 마지막 수식에서도 알 수 있듯이, advantage estimator는 Bellman residual terms의 discounted sum과 관련있는 간단한 수식입니다. 다음 section에서 modified reward function을 가진 MDP에 returns로서의 관점에서 위의 마지막 수식을 더 자세히 살펴봅시다. 위의 수식은 TD($\\lambda$)와 많이 유사합니다. 그러나 TD($\\lambda$)는 value function를 estimator하고, 여기서는 advantage function을 estimator합니다.\n\n위의 수식에서 $\\lambda = 0$ and $\\lambda = 1$에 대해서는 특별한 case가 존재한다. 수식으로 표현하면 다음과 같습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/ufglxzanhcnbi36/figure6.jpg?dl=1\" width=\"600\"> </center>\n\n- $GAE(\\gamma, 1)$은 $V$의 정확도와 관계없이 $\\gamma$-just입니다. 그러나 returns의 sum때문에 high variance합니다.\n- $GAE(\\gamma, 0)$은 $V = V^{\\pi, \\gamma}$에 대해 $\\gamma$-just입니다. 그리고 bias하지만 일반적으로 훨씬 lower variance를 가집니다. \n- $0 < \\lambda < 1$에 대해 GAE는 parameter $\\lambda$를 control하여 bias와 variance사이에 compromise를 만듭니다.\n\n두 가지 별개의 parameter $\\gamma$ and $\\lambda$를 가진 advantage estimator는 bias-variance tradeoff에 도움을 줍니다. 그러나 이 두 가지 parameter는 각각 다른 목적을 가지고 작동합니다.\n\n- $\\gamma$는 가장 중요하게 value function $V^{\\pi, \\gamma}$ 의 scale을 결정합니다. 또한 $\\gamma$는 $\\lambda$에 의존하지 않습니다. $\\gamma < 1$로 정하는 것은 policy gradient estimate에서 bias합니다.\n- 반면에, $\\lambda < 1$는 유일하게 value function이 부정확할 때 bias합니다. 그리고 경험상, $\\lambda$의 best value는 $\\gamma$의 best value보다 훨씬 더 낮습니다. 왜냐하면 $\\lambda$가 정확한 value function에 대해 $\\gamma$보다 훨씬 덜 bias하기 때문입니다.\n\nGAE를 사용할 때, $g^\\gamma$의 biased estimator를 구성할 수 있습니다. 수식은 다음과 같습니다.\n$$g^\\gamma \\approx \\mathbb{E} [\\sum_{t=0}^\\infty] \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) \\hat{A}_t^{GAE(\\gamma, \\lambda)}] = \\mathbb{E} [\\sum_{t=0}^{\\infty} \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) \\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+1}^V]$$\n여기서 $\\lambda = 1$일 때 동일해집니다.\n\n<br><br>\n\n# 5. Interpretation as Reward Shaping\n\n이번 section에서는 앞서 다뤘던 수식 $\\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^V$를 modified reward function의 MDP의 관점으로 생각해봅시다. 조금 더 구체적으로 말하자면, MDP에서 reward shaping transformation을 실행한 후에 적용된 extra discounted factor로서 $\\lambda$를 어떻게 볼 것인지에 대해서 다룹니다.\n\n(개인적인 comment) 한 가지 먼저 언급하자면, 본래의 목적은 reward shaping이 아니라 variance reduction입니다. 이번 section은 그저 이전에 이러한 개념이 있었고, gae를 다른 관점에서 생각해보자라는 뜻에서 나온 section인 것 같습니다. '아~ 이러한 개념이 있구나~!' 정도로만 알면 될 것 같습니다. 'gae가 reward shaping의 효과까지 있어서 이러한 section을 넣은 것일까?' 라는 생각도 해봤지만 아직 잘 모르겠습니다. 실험부분에도 딱히 하지 않은 걸로 봐서는.. 아닌 것 같기도 하고.. 아직까지는 그저 'reward shaping의 관점에서 봤을 때에도 gae로 만들어줄 수 있다.' 정도만 생각하려고 합니다.\n\nReward Shaping이란 개념 자체는 일반적인 알고리즘 분야(최단경로 문제 등)에도 있습니다. 하지만 이 개념이 머신러닝에 적용된 건 Andrew Y. Ng이 쓴 Policy Invariance under Reward Transformations Theory and Application to Reward Shaping(1999) 논문입니다. 논문에서 Reward Shaping을 설명하는 부분은 간략하게 다음과 같습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/0w9cuxjkhz2mbcy/figure7.jpg?dl=1\" width=\"400\"> </center>\n<center> <img src=\"https://www.dropbox.com/s/xcp9eqvcbv3268g/figure8.jpg?dl=1\" width=\"400\"> </center>\n\n위의 글만 보고 이해가 잘 안됩니다. 그래서 다른 자료들을 찾던 중에 다음의 그림을 찾게 되었습니다. (아래의 그림은 Youtube에 있는 [Udacity 영상](https://www.youtube.com/watch?v=xManAGjbx2k&t=95s)에서 있는 그림입니다.)\n\n<center> <img src=\"https://www.dropbox.com/s/87kacngez9cl2e6/figure9.jpg?dl=1\" width=\"400\"> </center>\n\n이 그림을 통해서 Reward Shaping을 이해해봅시다.\n\nReward Shaping(보상 형성)을 통해서 뭘 하고 싶은지 목적부터 봅시다. reward space가 sparse 한 경우에 reward가 너무 드문드문 나옵니다. 따라서 이것을 꾸준히 reward를 받을 수 있도록 바꿉니다. potential-based shaping function인 $\\Phi$를 만들어서 더하고 빼줍니다. 저 $\\Phi$ 자리에는 대표적으로 state value function이 많이 들어간다고 합니다. (아직 목적이 이해가 안될 수 있습니다. Reward Shaping에 대해서 다 보고 나서 다시 한 번 읽어봅시다.)\n\n위의 그림은 돌고래가 점프하여 불구멍을 통과해야하는 환경입니다. 하지만 저 불구멍을 통과하기 위해서는 (1) 점프도 해야하고, (2) 불도 피해야하고, (3) 알맞게 착지까지 완료해야합니다. 이렇게 해야 reward를 +1 얻습니다. \n\n생각해봅시다. 어느 세월에 점프도 해야하고.. 불도 피해야하고.. 알맞게 착지까지해서 reward를 +1 얻겠습니까? 따라서 그 전에도 잘하고 있는 지, 못하고 있는 지를 판단하기 위해, 다시 말해 reward를 꾸준히 받도록 다음과 같이 transformed reward function $\\tilde{r}$을 정의합니다.\n\n$$\\tilde{r} (s, a, s') = r(s, a, s') + \\gamma \\Phi (s') - \\Phi (s)$$\n\n- 여기서 $\\Phi : S \\rightarrow \\mathbb{R}$를 state space에서의 arbitrary scalar-valued function을 나타냅니다. 그리고 $\\Phi$자리에는 대표적으로 state value function이 들어간다고 생각합시다.\n- 형태가 TD residual term의 결과와 비슷하지만 의미와 의도가 다릅니다. reward shaping은 sparse reward 때문이고, 이전 section에서 봤던 gae는 variance reduction때문에 나온 것입니다.\n\n<center> <img src=\"https://www.dropbox.com/s/tkfpqniaazwfbld/figure10.jpg?dl=1\" width=\"600\"> </center>\n\n이 transformation은 discounted advantage function $A^{\\pi, \\gamma}$으로도 둘 수 있습니다. state $s_t$를 시작하여 하나의 trajectory의 rewards의 discounted sum을 표현하면 다음과 같습니다.\n$$\\sum_{l=0}^\\infty \\gamma^l \\tilde{r} (s_{t+l}, a_t, s_{t+l+1}) = \\sum_{l=0}^\\infty \\gamma^l r(s_{t+l}, a_{t+l}, s_{t+l+1}) - \\Phi(s_t)$$\n\n$\\tilde{Q}^{\\pi, \\gamma}, \\tilde{V}^{\\pi, \\gamma}, \\tilde{A}^{\\pi, \\gamma}$를  transformed MDP의 value function과 advantage function이라고 하면, 다음과 같은 수식이 나옵니다.\n$$\\tilde{Q}^{\\pi, \\gamma} (s, a) = Q^{\\pi, \\gamma} (s, a) - \\Phi (s)$$\n$$\\tilde{V}^{\\pi, \\gamma} (s, a) = V^{\\pi, \\gamma} (s, a) - \\Phi (s)$$\n$$\\tilde{A}^{\\pi, \\gamma} (s, a) = (Q^{\\pi, \\gamma} (s, a) - \\Phi (s)) - (V^\\pi (s) - \\Phi (s)) = A^{\\pi, \\gamma} (s, a)$$\n\n이제 reward shaping의 idea를 가지고 어떻게 policy gradient estimate를 얻을 수 있는 지에 대해서 알아봅시다.\n\n- $0 \\le \\lambda \\le 1$의 범위에 있는 steeper discount $\\gamma \\lambda$를 사용합니다.\n- shaped reward $\\tilde{r}$는 Bellman residual term $\\delta^V$와 동일합니다.\n- 그리고 $\\Phi = V$와 같다고 보면 다음과 같은 수식이 나옵니다.\n\n$$\\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\tilde{r} (s_{t+l}, a_t, s_{t+l+1}) = \\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^V = \\hat{A}_t^{GAE(\\gamma, \\lambda)}$$\n이렇게 shaped rewards의 $\\gamma \\lambda$-discounted sum을 고려함으로써 GAE를 얻을 수 있습니다. 더 정확하게 $\\lambda = 1$은 $g^\\gamma$의 unbiased estimate이고, 반면에 $\\lambda < 1$은 biased estimate입니다.\n\n좀 더 나아가서, shaping transformation과 parameters $\\gamma$ and $\\lambda$의 결과를 보기 위해, response function $\\chi$를 이용하면 다음과 같은 수식이 나옵니다.\n$$\\chi (l; s_t, a_t) = \\mathbb{E} [r_{t+l} | s_t, a_t] - \\mathbb{E} [r_{t+l} | s_t]$$\n추가적으로 $A^{\\pi, \\gamma} (s, a) = \\sum_{l=0}^\\infty \\gamma^l \\chi (l; s, a)$입니다.\n그래서 discounted policy gradient estimator는 다음과 같이 쓸 수 있다.\n$$\\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) A^{\\pi, \\gamma} (s_t, a_t) = \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) \\sum_{l=0}^\\infty \\gamma^l \\chi (l; s, a)$$\n\n<br><br>\n\n# 6. Value Fuction Estimation\n\n이번 section에서는 Trust Region Optimization Scheme에 따라 Value function을 Estimation합니다.\n\n<br>\n## 6.1 Simplest approach\n\n$$minimize_{\\phi} \\sum_{n=1}^{N} \\vert\\vert V_{\\phi}(s_n) - \\hat{V_n} \\vert\\vert^{2}$$\n\n- 위는 가장 간단하게 non-linear approximation으로 푸는 방법입니다.\n- $\\hat{V_t} = \\sum_{l=0}^{\\infty}\\gamma^l r_{t+l}$은 reward 에 대한 discounted sum을 의미합니다.\n\n<br>\n## 6.2 Trust region method to optimize the value function\n\n- Value function을 최적화 하기 위해 trust region method를 사용합니다.\n- Trust region은 최근 데이터에 대해 overfitting되는 것을 막아줍니다.\n\nTrust region문제를 풀기 위해서는 다음 스텝을 따릅니다.\n\n- $\\sigma^2 = \\frac{1}{N} \\sum_{n=1}^{N} \\vert\\vert V_{\\phi old}(s_n) - \\hat{V_n} \\vert\\vert^{2}$을 계산합니다.\n- 그 후에 다음과 같은 constrained opimization문제를 풉니다.\n\n$$minimize_{\\phi} \\, \\sum_{n=1}^N \\parallel V_\\phi (s_n) - \\hat{V}_n \\parallel^2$$\n$$subject \\, \\, to \\, \\frac{1}{N} \\sum_{n=1}^N \\frac{\\parallel V_\\phi (s_n) - V_{\\phi old} (s_n) \\parallel^2}{2 \\sigma^2} \\le \\epsilon$$\n\n- 위의 수식은 사실 old Value function과 새로운 Value function KL distance가 $\\epsilon$ 다 작아야한다는 수식과 같습니다. Value function이 평균은 $V_{\\phi}(s)$이고 분산이 $\\sigma^2$인 conditional Gaussian distribution으로 parameterize되었을 뿐입니다.\n\n<img width =\"400px\" src=\"https://www.dropbox.com/s/uw0v05feu8chqmc/Screenshot%202018-07-08%2010.04.38.png?dl=1\"> \n\n이 trust region문제의 답을 conjudate gradient algorithm을 이용하여 근사값을 구할 수 있습니다. 특히, 다음과 같은  quadratic program을 풀게됩니다.\n\n$$minimize_{\\phi} \\, g^T (\\phi - \\phi_{old})$$\n$$subject \\, \\, to \\, \\frac{1}{N} \\sum_{n=1}^N (\\phi - \\phi_{old})^T H(\\phi - \\phi_{old}) \\le \\epsilon$$\n\n- 여기서 $g$는 objective 의 gradient입니다.\n- $j_n = \\nabla_{\\phi} V_{\\phi}(s_n)$일때, $H = \\frac{1}{N} \\sum_{n} j_n j^T_n$이며, $H$는 objective의 hessian에 대해서 gaussian newton method로 근사한 값입니다. 따라서, value function을 conditional probability로 해석한다면 Fisher information matrix가 됩니다.\n- 구현할때의 방법은 TRPO에서 사용한 방법과 모두 같습니다.\n\n<br><br>\n\n# 7.Experiments\n\n실험은 다음 두 가지 물음에 대해서 디자인 되었습니다.\n\n- GAE에 따라서 episodic total reward를 최적화 할때, $\\lambda$와 $\\gamma$가 변함에 따라서 어떤 경험적인 효과를 볼 수 있는지?\n- GAE와 trust region alogorithm을 policy와 value function 모두에 함께 사용했을 때 어려운 문제에 적용되는 큰 뉴럴넷을 최적화 할 수 있을지?\n\n<br>\n## 7.1 Policy Optimization Algorithm\n\nPolicy update는 TRPO를 사용합니다. TRPO에 대한 설명은 여기서는 생략하겠습니다. TRPO 포스트를 보고 돌아와주세요!\n\n- 이전 TRPO에서 이미 TRPO와 다른 많은 알고리즘들을 비교하였기 때문에, 여기서는 똑같은 짓을 반복하지 않고 $\\lambda$, $\\gamma$가 변함에 따라 어떤 영향이 있는 지에 대한 실험에 집중하겠다고 합니다. (귀찮았던거죠..)\n\n- TRPO를 적용한 GAE의 최종 알고리즘은 다음과 같습니다.\n\n<center> <img width = \"600px\" src=\"https://www.dropbox.com/s/b1klz11f2frrvg4/Screenshot%202018-07-08%2010.33.54.png?dl=1\"> </center>\n<center> <img width = \"300px\" src=\"https://www.dropbox.com/s/35fxte05pqtiel7/Screenshot%202018-07-08%2010.35.22.png?dl=1\"> </center>\n<center> <img width = \"300px\" src=\"https://www.dropbox.com/s/u35pjow3w50bvz1/Screenshot%202018-07-08%2010.36.03.png?dl=1\"> </center>\n\n- 여기서 주의할 점은 Policy update($\\theta_i \\rightarrow \\theta_{i+1}$)에서 $V_{\\phi_i}$를 사용했다는 점입니다.\n- 만약 Value function을 먼저 update하게 된다면 추가적인 bias가 발생합니다.\n- 극단적으로 생각해보아서, 우리가 Value function을 완벽하게 overfit을 해낸다면 Bellman residual($r_t + \\gamma V(s_{t+1}) - V(S_t)$)은 0이 됩니다. 그럼 Policy gradient의 estimation도 거의 0이 될 것입니다.\n\n<br>\n## 7.2 Expermint details\n\n### 7.2.1 Environment \n실험에서 사용된 환경은 다음 네 가지 입니다.\n\n1. classic cart-pole (x 3D)\n2. bipedal locomotion\n3. quadrupedal locomotion\n4. dynamically standing up for the biped\n\n### 7.2.2 Architecture\n\n- 3D robot task에 대해서는 같은 모델을 사용하였습니다.\n    - layers  = [100, 50, 25] 각각 tanh 사용. (Policy와 Value 네트워크 모두)\n    - Final output layer은 linear\n- Cartpole에 대해서는 1개의 layer 안에 20개의 hidden unit만 있는 linear policy를 사용했다고 합니다.\n\n### 7.2.3 Task\n\n- Cartpole\n    - 한 배치당 20 개의 trajectory를 모았고, maximum length는 1000입니다.\n- 3D biped locomotion\n    - 33 dim state , 10 dim action\n    - 50000 time step per batch\n- 3D quadruped locomotion\n    - 29 dim state, 8 dim action\n    - 200000 time step per batch\n- 3D biped locomotion Standing\n    - 33 dim state , 10 dim action\n    - 200000 time step per batch\n\n\n### 7.2.3 results\ncost의 관점에서 결과를 나타내었다고 합니다. Cost는 negative reward와 이것이 최소화 되었는가로 정의되었습니다.\n\n#### 7.2.3.1 Cartpole\n\n<center> <img width = \"500px\" src=\"https://www.dropbox.com/s/x9pbms1wvg38lda/Screenshot%202018-07-08%2011.08.22.png?dl=1\"> </center>\n\n- 왼쪽 그림은 $\\gamma$를 0.99로 고정시켜놓은 상태에서 $\\lambda$를 변화시킴에 따라서 cost를 측정한 것입니다. \n- 오른쪽은 $\\gamma$와 $\\lambda$를 둘 다 변화 시키면서 성능을 그림으로 나타낸 표입니다. 흰색에 가까울 수록 좋은 성능입니다. \n\n#### 7.2.3.2 3D BIPEDAL LOCOMOTINO\n\n<center> <img width = \"500px\" src=\"https://www.dropbox.com/s/i9wj4p6ijojsy82/Screenshot%202018-07-08%2011.25.08.png?dl=1\"> </center>\n\n\n- 다른 random seed로 부터 9번 씩 시도한 결과를 mean을 취해서 사용합니다.\n- Best performance는   $\\gamma \\in [0.99, 0.995]$ 그리고  $\\lambda \\in [0.96, 0.99]$일때. \n- 1000 iteration 후에 빠르고 부드럽고 안정적인 걸음거이가 나옵니다.\n- 실제로 걸린 시간은 0.01(타입스텝당 시간) * 50000(배치당 타임스텝) * 1000(배치) * 3600(초->시간) * 24 = 5.8일 정도가 걸렸습니다.\n\n#### 7.2.3.3 다른 ROBOT TASKS\n\n<center> <img width = \"500px\" src=\"https://www.dropbox.com/s/fuuat65we52quht/Screenshot%202018-07-08%2011.33.11.png?dl=1\"> </center>\n\n- 다른 로봇 TASK에 대해서는 아주 제한적인 실험만 진행합니다.(시간이 부족했던듯 하네요..)\n- Quadruped에 대해서는 $\\gamma = 0.995$로 fix, $\\lambda \\in {0, 0.96}$\n- Standingup에 대해서는 $\\gamma = 0.99$로 fix, $\\lambda \\in {0, 0.96}$\n\n<br><br>\n\n# 8. Discussion\n\n<br>\n## 8.1 Main discussion\n\n지금까지 복잡하고 어려운 control problem에서 Reinforcement Learning(RL)은 high sample complexity 때문에 제한이 되어왔습니다. 따라서 이 논문에서 그 제한을 풀고자 advantage function의 good estimate를 얻는 \"variance reduction\"에 대해 연구하였습니다.\n\n\"Generalized Advantage Estimator(GAE)\"라는 것을 제안했고, 이것은 bias-variance tradeoff를 조절하는 두 개의 parameter $\\gamma,\\lambda$를 가집니다.\n또한 어떻게 Trust Region Policy Optimization과 value function을 optimize하는 Trust Region Algorithm의 idea를 합치는 지를 보였습니다.\n\n이렇게 함으로써 보다 더 복잡하고 어려운 control task들을 해결할 수 있었습니다.\n\nGAE의 실험적인 입증으로는 robotic locomotion을 simulation하는 domain입니다. 실험에서도 보여준 것처럼 [0.9, 0.99]의 범위에서 $\\lambda$의 적절한 중간의 값을 통해 best performance를 얻습니다. 좀 더 나아가 연구되어야할 점은 adaptive or automatic하도록 estimator parameter $\\gamma,\\lambda$를 조절하는 방법입니다.\n\n<br>\n## 8.2 Future work\n\nValue function estimation error와 Policy gradient estimation error사이의 관계를 알아낸다면, 우리는 Value function fitting에 더 잘 맞는 error metric(policy gradient estimation 의 정확성과 더 잘 맞는 value function)을 사용할 수 있습니다. 여기서 Policy와 Value function의 파라미터를 공유하는 모델을 만드는 것은 아주 흥미롭고 이점이 많습니다. 하지만 수렴을 보장하도록 적절한 numerical optimization을 제시해야 할 것입니다.\n\n추가적으로 DDPG는 별로라고 합니다. 그리고 TD(0)는 bias가 너무 크고, poor performance로 이끈다고 합니다. 특히나 이 논문에서는 low-dimention의 쉬운 문제들만 해결했습니다.\n\n<center> <img width = \"500px\" src=\"https://www.dropbox.com/s/nhc7t9psul5lr3x/Screenshot%202018-07-08%2011.45.15.png?dl=1\"> </center>\n\n<br>\n## 8.3 FAQ\n\n- Compatible features와는 무슨 관계?\n     - Compatible features는 value function을 이용하는 policy gradient 알고리즘들과 함께 자주 언급됩니다.\n     - Actor Critic의 저자는 policy의 제한된 representation power때문에, policy gradient는 단지 advantage function space의 subspace에만 의존하게 됩니다.\n     - 이 subspace는 compatible features에 의해 span됩니다.\n     - 이 이론은 현재 문제 구조를 어떻게 이용해야 advantage function에 대해 더 나은 estimation을 할 수 있는 지에 대한 지침을 주지 않습니다. GAE 논문의 idea와 orthogonal합니다.\n- 왜 Q function을 사용하지 않는가?\n     - 먼저 state-value function이 더 낮은 차원의 input을 가진다. 그래서 Q function보다 더 학습하기가 쉽습니다.\n     - 두 번째로 이 논문에서 제안하는 방법으로는 high bias estimator에서 low bias estimator로 $\\lambda$를 통해서 부드럽게 interpolate를 할 수 있습니다.\n     - 반면에 Q를 사용하면 단지 high-bias estimator 밖에 사용할 수 없습니다.\n     - 특히나 return에 대한 one-step estimation은 엄두를 못낼 정도로 bias가 큽니다.\n","source":"_posts/gae.md","raw":"---\ntitle: High-Dimensional Continuous Control using Generalized Advantage Estimation\ndate: 2018-06-12 19:18:45\ntags: [\"프로젝트\", \"피지여행\"]\ncategories: 프로젝트\nauthor: 양혁렬, 이동민\nsubtitle: 피지여행 7번째 논문\n---\n\n<center> <img src=\"https://www.dropbox.com/s/p8gfpyo6xf9wm5w/Screen%20Shot%202018-07-18%20at%201.25.53%20AM.png?dl=1\" width=\"700\"> </center>\n\n논문 저자 : John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan and Pieter Abbeel\n논문 링크 : https://arxiv.org/pdf/1506.02438.pdf\nProceeding : ??\n정리 : 양혁렬, 이동민\n\n---\n\n# 1. 들어가며...\n\n현존하는 Policy Gradient Method들의 목적은 누적되는 reward들을 optimization하는 것입니다. 하지만 학습할 때에 많은 양의 sample이 필요로 하고, 들어오는 data가 nonstationarity임에도 불구하고 stable and steady improvement가 어렵습니다.\n\n그래서 이 논문에서는 다음과 같은 방법을 제시합니다.\n\n- TD($\\lambda$)와 유사한 advantage function의 exponentially-weighted estimator를 사용하여 policy gradient estimate의 variance를 줄이는 것 \n- policy와 value function에 대한 Trust Region Optimization 사용하는 것\n\n3D locomotion tasks에 대한 empirical results는 다음과 같습니다.\n\n- bipedal and quadrupedal simulated robots의 달리는 자세를 학습\n- bipedal 사람이 땅에 누워있다가 일어서는 것을 학습\n\n※ TD($\\lambda$)를 혹시 모르실 경우도 있을 것 같아 간략하게 정리하였습니다. http://dongminlee.tistory.com/10 를 먼저 보시고 아래의 내용을 봐주시기 바랍니다!\n\n<br><br>\n\n# 2. Introduction\n\n기본적으로 \"parameterized stochastic policy\"를 가정합니다. 이 때 expected total returns의 gradient에 대한 unbiased estimate를 얻을 수 있는데 이것을 REINFORCE라고 부릅니다. 하지만 하나의 action의 결과가 과거와 미래의 action의 결과로 혼동되기 때문에 gradient estimator의 high variance는 시간에 따라 scaling됩니다.\n\n또 다른 방법은 Actor-Critic이 있습니다. 이 방법은 empirical returns보다 하나의 value function을 사용합니다. 또한 bias하고 lower variance를 가진 estimator입니다. 구체적으로 말하자면, high variance하다면 더 sampling을 하면 되는 반면에 bias는 매우 치명적입니다. 다시 말해 bias는 algorithm이 converge하는 데에 실패하거나 또는 local optimum이 아닌 poor solution에 converge하도록 만듭니다.\n\n따라서 이 논문에서는 $\\gamma\\in [0,1]$ and $\\lambda\\in [0,1]$에 의해 parameterized estimation scheme인 Generalized Advantage Estimator(GAE)를 다룹니다.\n\n이 논문을 대략적으로 요약하자면 다음과 같습니다.\n\n- 효과적인 variance reduction scheme인 GAE를 다룹니다. 또한 실험할 때 batch trust region algorithm에 더하여 다양한 algorithm들에 적용됩니다.\n- value function에 대해 trust region optimization method를 사용합니다. 이렇게 함으로서 더 robust하고 efficient한 방법이 됩니다.\n- A와 B를 합쳐서, 실험적으로 control task에 neural network policies를 learning하는 데에 있어서 효과적인 algorithm을 얻습니다. 이러한 결과는 high-demensional continuous control에 RL을 사용함으로서 state of the art로 확장되었습니다.\n\n<br><br>\n\n# 3. Preliminaries\n\n먼저 policy optimization의 \"undiscounted formulation\"을 가정합니다. (undiscounted formulation에 주목합시다.)\n\n- initial state $s_0$는 distribution $\\rho_0$으로부터 sampling된 것입니다.\n- 하나의 trajectory ($s_0, a_0, s_1, a_1, ...$)는 terminal state에 도달될 때 까지 policy $a_t$ ~ $\\pi(a_t | s_t)$에 따라서 action을 sampling하고, dynamics $s_{t+1}$ ~ $P(s_{t+1} | s_t, a_t)$에 따라서 state를 sampling함으로써 생성됩니다.\n- reward $r_t = r(s_t, a_t, s_{t+1})$은 매 time step마다 받아집니다.\n- 목표는 모든 policies에 대해 finite하다고 가정됨으로서 expected total reward $\\sum_{t=0}^{\\infty} r_t$를 maximize하는 것입니다.\n\n여기서 중요한 점은 $\\gamma$를 discount의 parameter로 사용하는 것이 아니라 \"bias-variance tradeoff를 조절하는 parameter로 사용한다\" 는 것입니다.\n\npolicy gradient method는 gradient $g := \\nabla_\\theta \\mathbb{E} [\\sum_{t=0}^\\infty r_t]$를 반복적으로 estimate함으로써 expected total reward를 maximize하는 것인데, policy gradient에는 여러 다른 표현들이 있습니다.\n$$g = \\mathbb{E} [\\sum_{t=0}^\\infty \\Phi_t \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$\n\n여기서 $\\Phi_t$는 아래 중의 하나일 수 있습니다.\n\n1. $\\sum_{t=0}^\\infty r_t$: trajectory의 total reward\n2. $\\sum_{t'=t}^\\infty r_t'$: action $a_t$ 후의 reward\n3. $\\sum_{t'=t}^\\infty r_t' - b(s_t)$: 2의 baselined version\n4. $Q^\\pi (s_t, a_t)$: state-action value function\n5. $A^\\pi (s_t, a_t)$: advantage function\n6. $r_t + V^\\pi (s_{t+1}) - V^\\pi (s_t)$: TD residual\n\n위의 번호 중 4, 5, 6의 수식들은 다음의 정의를 사용합니다.\n\n- $V^\\pi (s_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_t:\\infty} [\\sum_{l=0}^\\infty r_{t+1}]$\n- $Q^\\pi (s_t, a_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty} [\\sum_{l=0}^\\infty r_{t+l}]$\n- $A^\\pi (s_t, a_t) := Q^\\pi (s_t, a_t) - V^\\pi (s_t)$, (Advantage function)\n\n추가적으로 colon notation $a : b$는 포괄적인 범위 $(a, a+1, ... , b)$입니다. (잘 기억해둡시다. 뒤에 계속해서 colon notation이 나옵니다.)\n\n여기서부터 parameter $\\gamma$에 대해 좀 더 자세히 알아봅시다. parameter $\\gamma$는 bias하면서 동시에 reward를 downweighting함으로써 variance를 줄입니다. 다시 말해 MDP의 discounted formulation에서 사용된 discounted factor와 일치하지만, 이 논문에서는 \"$\\gamma$를 undiscounted MDP에서 variance reduction parameter\"로 다룹니다. -> 결과는 같지만, 의미와 의도가 다릅니다.\n\ndiscounted value function들은 다음과 같습니다.\n\n- $V^{\\pi, \\gamma} (s_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_t:\\infty} [\\sum_{l=0}^\\infty \\gamma^l r_{t+l}]$ \n- $Q^\\pi (s_t, a_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty} [\\sum_{l=0}^\\infty \\gamma^l r_{t+l}]$\n- $A^{\\pi, \\gamma} (s_t, a_t) := Q^{\\pi, \\gamma} (s_t, a_t) - V^{\\pi, \\gamma} (s_t)$\n\n따라서 policy gradient에서의 discounted approximation은 다음과 같이 나타낼 수 있습니다.\n$$g^\\gamma := \\mathbb{E}_{s_{0:\\infty} a_{0:\\infty}} [\\sum_{t=0}^\\infty A^{\\pi, \\gamma} (s_t, a_t) \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$\n뒤이어 나오는 section에서는 위의 수식에서 $A^{\\pi, \\gamma}$에 대해 biased (but not too biased) estimator를 얻는 방법에 대해서 나옵니다.\n\n다음으로 advantage function에서 새롭게 접하는 \"$\\gamma$-just estimator\"의 notation에 대해 알아봅시다.\n\n- 먼저 $\\gamma$-just estimator는 $g^\\gamma$ ${}^1$를 estimate하기 위해 위의 수식에서 $A^{\\pi, \\gamma}$를 $\\gamma$-just estimator로 사용할 때, bias하지 않은 estimator라고 합시다. 그리고 이 $\\gamma$-just advantage estimator를 $\\hat{A}_t (s_{0:\\infty} , a_{0:\\infty})$라고 하고, 전체의 trajectory에 대한 하나의 function이라고 합시다.\n- ${}^1$에서 이 논문의 저자가 하고 싶은 말이 있는 것 같습니다. 개인적으로 $\\gamma$-just estimator를 이해하는 데에 있어서 중요한 주석이라 정확히 해석하고자 합니다.\n    - $A^\\pi$를 $A^{\\pi, \\gamma}$로 사용함으로써 이미 bias하다라고 말했지만, \"이 논문에서 하고자 하는 것은 $g^\\gamma$에 대해 unbiased estimate를 얻고 싶은 것입니다.\" 하지만 undiscounted MDP의 policy gradient에 대해서는 당연히 $\\gamma$를 사용하기 때문에 biased estimate를 얻습니다. 개인적으로 이것은 일단 무시하고 $\\gamma$를 사용할 때 어떻게 unbiased estimate를 얻을 지에 대해 좀 더 포커스를 맞추고 있는 것 같습니다.\n    - 그러니까 저 $A^{\\pi, \\gamma}$를 $\\gamma$-just estimator로 바꿔줌으로써 unbiased estimate를 하고 싶다는 것이 뒤이어 나오는 정의와 명제의 핵심이라고 할 수 있습니다.\n\nDefinition 1.\n먼저 가정을 합니다. (가정을 바탕으로 이루어지는 정의라는 것을 주목합시다.) 만약\n$$\\mathbb{E}_{s_{0:\\infty} a_{0:\\infty}} [\\hat{A}_t (s_{0:\\infty}, a_{0:\\infty}) \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)] = \\mathbb{E}_{s_{0:\\infty} a_{0:\\infty}} [A^{\\pi, \\gamma} (s_t, a_t) \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$\n두 수식이 같다면, estimator $\\hat{A}_t$는 $\\gamma$-just입니다.\n\n그리고 만약 모든 t에 대해서 $\\hat{A}_t$이 $\\gamma$-just이라면, 다음과 같이 표현할 수 있습니다.\n$$\\mathbb{E}_{s_{0:\\infty} a_{0:\\infty}} [\\hat{A}_t (s_{0:\\infty}, a_{0:\\infty}) \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)] = g^\\gamma$$\n위의 수식이 바로 unbiased estimate입니다.\n\n$\\gamma$-just인 $\\hat{A}_t$에 대한 한가지 조건은 $\\hat{A}_t$이 두 가지 function $Q_t$ and $b_t$로 나뉠 수 있다는 것입니다.\n\n- $Q_t$는 $\\gamma$-discounted Q-function의 unbiased estimator입니다.\n- $b_t$는 action $a_t$전에 sampling된 states and actions의 arbitrary function이다. \n\nProposition 1.\n모든 $(s_t, a_t)$에 대해,\n$$\\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty | s_t, a_t} [Q_t (s_{t:\\infty}, a_{t:\\infty})] = Q^{\\pi, \\gamma} (s_t, a_t)$$\n로 인하여 $\\hat{A}_t$이 \n$$\\hat{A}_{s_{0:\\infty}, a_{0:\\infty}} = Q_t (s_{0:\\infty}, a_{0:\\infty}) - b_t (s_{0:t}, a_{0:t-1})$$\n형태라고 가정합시다. (가정을 바탕으로 이루어지는 명제라는 점을 주목합시다.)\n\n그 때, $\\hat{A}_t$은 $\\gamma$-just입니다.\n\n이 명제에 대한 증명은 Appendix B에 있습니. 그리고 $\\hat{A}_t$에 대한 $\\gamma$-just advantage estimator들은 다음과 같습니다.\n\n- $\\sum_{l=0}^\\infty \\gamma^l r_{t+1}$\n- $A^{\\pi, \\gamma} (s_t, a_t)$\n- $Q^{\\pi, \\gamma} (s_t, a_t)$\n- $r_t + \\gamma V^{\\pi, \\gamma} (s_{t+1}) - V^{\\pi, \\gamma} (s_t)$\n\n증명을 보기 전에 먼저 Advantage function에 대해서 먼저 살펴봅시다. 아래의 내용은 Sutton이 쓴 논문인 Policy Gradient Methods for Reinforcement Learning with Function Approximation(2000)에서 나온 내용을 리뷰한 것입니다. \n\n<center> <img src=\"https://www.dropbox.com/s/x737yq97ut6gp1a/Screen%20Shot%202018-07-15%20at%201.16.09%20PM.png?dl=1\" width=\"600\"> </center>\n\n여기서는 \"이 수식은 $\\sum_a \\frac{\\partial \\pi (s,a)}{\\partial \\theta} = 0$이기 때문에 가능해진다.\", \"즉, 이 수식은 $\\pi(s,a)$의 gradient에만 dependent하기 때문에 advantage 역할을 하는 함수들을 넣어도 아무런 상관이 없다\"라는 두 문장을 기억해둡시다.\n\n이제 증명을 보면 됩니다.\n\n<center> <img src=\"https://www.dropbox.com/s/e7sj2fwcm1f7hof/figure2.jpg?dl=1\" width=\"600\"> </center>\n<center> <img src=\"https://www.dropbox.com/s/kyk4jes1202az4m/figure3.jpg?dl=1\" width=\"600\"> </center>\n\n위의 증명 수식에서 Q와 b의 각각 세번째 수식과 마지막 수식을 자세히 봅시다.\n\n1. $Q$에 대한 증명\n    - 빨간색 부분\n        - $\\mathbb{E}_{s_{t+1:\\infty}, a_{t+1:\\infty}} [Q_t (s_{0:\\infty}, a_{0:\\infty})]$를 쉽게 말하자면 \"모든(과거, 현재, 미래) state와 action에 대해서 expectation을 $t+1$부터 $\\infty$까지 하겠다.\"라는 말입니다. 이렇게 함으로써 원래는 $Q^\\pi (s_t, a_t)$가 나와야 맞는 건데, 앞에서 살펴봤다시피 4번째 수식 밑줄 친 자리에는 advantage 역할을 하는 함수들을 넣어도 결과값에 아무런 영향을 끼치지 않기 때문에 의도적으로 $A^\\pi (s_t, a_t)$로 바꾼 것입니다. 또한 이 증명을 바탕으로 $\\hat{A}$이 $\\gamma$-just라는 것을 표현하고 싶기 때문이라고 봐도 됩니다.\n\n    - 파란색 부분\n        - 또한 $a_{0:t}$에서 $a_{0:t-1}$로 변한 것은 $A^\\pi (s_t, a_t)$에 baseline이 들어가기 때문에 바꿔준 것입니다.\n\n2. $b$에 대한 증명\n    - 빨간색 부분\n        - $\\mathbb{E}_{s_{t+1:\\infty}, a_{t+1:\\infty}} [\\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$가 0으로 바뀌는 것은 위에서 설명했듯이 $\\nabla_\\theta$자체가 $\\log \\pi_\\theta$만 gradient하기 때문에 이것을 expectation을 취하면 0이 됩니다.\n\n<br><br>\n\n# 4. Advantage Function Estimation\n\n이번 section에는 discounted advantage function $A^{\\pi, \\gamma} (s_t, a_t)$의 accurate estimate $\\hat{A}_t$에 대해서 살펴봅시다. 이에 따른 수식은 다음과 같습니다.\n\n$$\\hat{g} = \\frac{1}{N} \\sum_{n=1}^N \\sum_{t=0}^\\infty \\hat{A}_t^n \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t^n | s_t^n)$$\n\n여기서 n은 a batch of episodes에 대하여 index한 것입니다.\n\n$V$를 approximate value function이라고 합시다. 그리고 $\\delta_t^V = r_t + \\gamma V(s_{t+1}) - V(s_t)$이라고 합시다.\n\n만약 (이전과 마찬가지로 가정부터 합니다.) correct value function $V = V^{\\pi, \\gamma}$가 있다고 한다면, 이것은 $\\gamma$-just advantage estimator입니다. 실제로, $A^{\\pi, \\gamma}$의 unbiased estimator는 다음과 같습니다.\n$$\\mathbb{E}_{s_{t+1}} [\\delta_t^{V^{\\pi, \\gamma}}] = \\mathbb{E}_{s_{t+1}} [r_t + \\gamma V^{\\pi, \\gamma} (s_{t+1}) - V^{\\pi, \\gamma} (s_t)]$$\n$$= \\mathbb{E}_{s_{t+1}} [Q^{\\pi, \\gamma} (s_t, a_t) - V^{\\pi, \\gamma} (s_t)] = A^{\\pi, \\gamma} (s_t, a_t)$$\n그러나, \"이 estimator는 유일하게 $V = V^{\\pi, \\gamma}$에 대한 $\\gamma$-just입니다.\" 다른 경우라면 이것은 biased policy gradient estimate일 것입니다. (우리가 하고 싶은 것은 $V$에 대해서만 unbiased estimator가 아니라 advantage function에 대해서 일반화된 unbiased estimator를 얻고 싶은 것입니다. 그래서 아래에서도 나오겠지만, $\\gamma$와 함께 $\\lambda$를 이용한 estimator가 나옵니다.ㅏ)\n\n그래서 $\\delta$에 대해 $k$의 sum으로 생각해봅시다. 이것을 $\\hat{A}_t^{(k)}$라고 하자. 그러면 아래와 같이 표현할 수 있습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/ra7hxksveg2hz45/figure4.jpg?dl=1\" width=\"600\"> </center>\n\n$\\hat{A}_t^{(k)}$은 returns의 $k$-step estimate와 연관지을 수 있고, $\\delta_t^V = \\hat{A}_t^{(1)}$의 case와 유사하게도 $\\hat{A}_t^{(k)}$를 advantage function의 estimator로 생각할 수 있습니다.\n\n여기서 $k \\rightarrow \\infty$로 생각해보면 bias가 일반적으로 점점 더 작아집니다. 왜냐하면 $\\gamma^k V(s_{t+k})$가 점점 많이 discounted되서 $-V(s_t)$가 bias에 영향을 미치지 못하기 때문입니다. $k \\rightarrow \\infty$를 취하면 다음과 같은 수식이 나옵니다.\n\n$$\\hat{A}_t^{(\\infty)} = \\sum_{l=0}^\\infty \\gamma^l \\delta_{t+l}^V = -V(s_t) + \\sum_{l=0}^\\infty \\gamma^l r_{t+l}$$\n우변의 수식과 같이 empirical returns에서 value function baseline을 뺀 것으로 나타낼 수 있습니다.\n\nGeneralized Advantage Estimator GAE($\\gamma, \\lambda$)는 $k$-step estimators의 exponentially-weighted average로 나타낼 수 있습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/yg1ybmfkep3towu/figure5.jpg?dl=1\" width=\"600\"> </center>\n\n(TD($\\lambda$)를 떠올려주세요..!)\n\n위의 마지막 수식에서도 알 수 있듯이, advantage estimator는 Bellman residual terms의 discounted sum과 관련있는 간단한 수식입니다. 다음 section에서 modified reward function을 가진 MDP에 returns로서의 관점에서 위의 마지막 수식을 더 자세히 살펴봅시다. 위의 수식은 TD($\\lambda$)와 많이 유사합니다. 그러나 TD($\\lambda$)는 value function를 estimator하고, 여기서는 advantage function을 estimator합니다.\n\n위의 수식에서 $\\lambda = 0$ and $\\lambda = 1$에 대해서는 특별한 case가 존재한다. 수식으로 표현하면 다음과 같습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/ufglxzanhcnbi36/figure6.jpg?dl=1\" width=\"600\"> </center>\n\n- $GAE(\\gamma, 1)$은 $V$의 정확도와 관계없이 $\\gamma$-just입니다. 그러나 returns의 sum때문에 high variance합니다.\n- $GAE(\\gamma, 0)$은 $V = V^{\\pi, \\gamma}$에 대해 $\\gamma$-just입니다. 그리고 bias하지만 일반적으로 훨씬 lower variance를 가집니다. \n- $0 < \\lambda < 1$에 대해 GAE는 parameter $\\lambda$를 control하여 bias와 variance사이에 compromise를 만듭니다.\n\n두 가지 별개의 parameter $\\gamma$ and $\\lambda$를 가진 advantage estimator는 bias-variance tradeoff에 도움을 줍니다. 그러나 이 두 가지 parameter는 각각 다른 목적을 가지고 작동합니다.\n\n- $\\gamma$는 가장 중요하게 value function $V^{\\pi, \\gamma}$ 의 scale을 결정합니다. 또한 $\\gamma$는 $\\lambda$에 의존하지 않습니다. $\\gamma < 1$로 정하는 것은 policy gradient estimate에서 bias합니다.\n- 반면에, $\\lambda < 1$는 유일하게 value function이 부정확할 때 bias합니다. 그리고 경험상, $\\lambda$의 best value는 $\\gamma$의 best value보다 훨씬 더 낮습니다. 왜냐하면 $\\lambda$가 정확한 value function에 대해 $\\gamma$보다 훨씬 덜 bias하기 때문입니다.\n\nGAE를 사용할 때, $g^\\gamma$의 biased estimator를 구성할 수 있습니다. 수식은 다음과 같습니다.\n$$g^\\gamma \\approx \\mathbb{E} [\\sum_{t=0}^\\infty] \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) \\hat{A}_t^{GAE(\\gamma, \\lambda)}] = \\mathbb{E} [\\sum_{t=0}^{\\infty} \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) \\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+1}^V]$$\n여기서 $\\lambda = 1$일 때 동일해집니다.\n\n<br><br>\n\n# 5. Interpretation as Reward Shaping\n\n이번 section에서는 앞서 다뤘던 수식 $\\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^V$를 modified reward function의 MDP의 관점으로 생각해봅시다. 조금 더 구체적으로 말하자면, MDP에서 reward shaping transformation을 실행한 후에 적용된 extra discounted factor로서 $\\lambda$를 어떻게 볼 것인지에 대해서 다룹니다.\n\n(개인적인 comment) 한 가지 먼저 언급하자면, 본래의 목적은 reward shaping이 아니라 variance reduction입니다. 이번 section은 그저 이전에 이러한 개념이 있었고, gae를 다른 관점에서 생각해보자라는 뜻에서 나온 section인 것 같습니다. '아~ 이러한 개념이 있구나~!' 정도로만 알면 될 것 같습니다. 'gae가 reward shaping의 효과까지 있어서 이러한 section을 넣은 것일까?' 라는 생각도 해봤지만 아직 잘 모르겠습니다. 실험부분에도 딱히 하지 않은 걸로 봐서는.. 아닌 것 같기도 하고.. 아직까지는 그저 'reward shaping의 관점에서 봤을 때에도 gae로 만들어줄 수 있다.' 정도만 생각하려고 합니다.\n\nReward Shaping이란 개념 자체는 일반적인 알고리즘 분야(최단경로 문제 등)에도 있습니다. 하지만 이 개념이 머신러닝에 적용된 건 Andrew Y. Ng이 쓴 Policy Invariance under Reward Transformations Theory and Application to Reward Shaping(1999) 논문입니다. 논문에서 Reward Shaping을 설명하는 부분은 간략하게 다음과 같습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/0w9cuxjkhz2mbcy/figure7.jpg?dl=1\" width=\"400\"> </center>\n<center> <img src=\"https://www.dropbox.com/s/xcp9eqvcbv3268g/figure8.jpg?dl=1\" width=\"400\"> </center>\n\n위의 글만 보고 이해가 잘 안됩니다. 그래서 다른 자료들을 찾던 중에 다음의 그림을 찾게 되었습니다. (아래의 그림은 Youtube에 있는 [Udacity 영상](https://www.youtube.com/watch?v=xManAGjbx2k&t=95s)에서 있는 그림입니다.)\n\n<center> <img src=\"https://www.dropbox.com/s/87kacngez9cl2e6/figure9.jpg?dl=1\" width=\"400\"> </center>\n\n이 그림을 통해서 Reward Shaping을 이해해봅시다.\n\nReward Shaping(보상 형성)을 통해서 뭘 하고 싶은지 목적부터 봅시다. reward space가 sparse 한 경우에 reward가 너무 드문드문 나옵니다. 따라서 이것을 꾸준히 reward를 받을 수 있도록 바꿉니다. potential-based shaping function인 $\\Phi$를 만들어서 더하고 빼줍니다. 저 $\\Phi$ 자리에는 대표적으로 state value function이 많이 들어간다고 합니다. (아직 목적이 이해가 안될 수 있습니다. Reward Shaping에 대해서 다 보고 나서 다시 한 번 읽어봅시다.)\n\n위의 그림은 돌고래가 점프하여 불구멍을 통과해야하는 환경입니다. 하지만 저 불구멍을 통과하기 위해서는 (1) 점프도 해야하고, (2) 불도 피해야하고, (3) 알맞게 착지까지 완료해야합니다. 이렇게 해야 reward를 +1 얻습니다. \n\n생각해봅시다. 어느 세월에 점프도 해야하고.. 불도 피해야하고.. 알맞게 착지까지해서 reward를 +1 얻겠습니까? 따라서 그 전에도 잘하고 있는 지, 못하고 있는 지를 판단하기 위해, 다시 말해 reward를 꾸준히 받도록 다음과 같이 transformed reward function $\\tilde{r}$을 정의합니다.\n\n$$\\tilde{r} (s, a, s') = r(s, a, s') + \\gamma \\Phi (s') - \\Phi (s)$$\n\n- 여기서 $\\Phi : S \\rightarrow \\mathbb{R}$를 state space에서의 arbitrary scalar-valued function을 나타냅니다. 그리고 $\\Phi$자리에는 대표적으로 state value function이 들어간다고 생각합시다.\n- 형태가 TD residual term의 결과와 비슷하지만 의미와 의도가 다릅니다. reward shaping은 sparse reward 때문이고, 이전 section에서 봤던 gae는 variance reduction때문에 나온 것입니다.\n\n<center> <img src=\"https://www.dropbox.com/s/tkfpqniaazwfbld/figure10.jpg?dl=1\" width=\"600\"> </center>\n\n이 transformation은 discounted advantage function $A^{\\pi, \\gamma}$으로도 둘 수 있습니다. state $s_t$를 시작하여 하나의 trajectory의 rewards의 discounted sum을 표현하면 다음과 같습니다.\n$$\\sum_{l=0}^\\infty \\gamma^l \\tilde{r} (s_{t+l}, a_t, s_{t+l+1}) = \\sum_{l=0}^\\infty \\gamma^l r(s_{t+l}, a_{t+l}, s_{t+l+1}) - \\Phi(s_t)$$\n\n$\\tilde{Q}^{\\pi, \\gamma}, \\tilde{V}^{\\pi, \\gamma}, \\tilde{A}^{\\pi, \\gamma}$를  transformed MDP의 value function과 advantage function이라고 하면, 다음과 같은 수식이 나옵니다.\n$$\\tilde{Q}^{\\pi, \\gamma} (s, a) = Q^{\\pi, \\gamma} (s, a) - \\Phi (s)$$\n$$\\tilde{V}^{\\pi, \\gamma} (s, a) = V^{\\pi, \\gamma} (s, a) - \\Phi (s)$$\n$$\\tilde{A}^{\\pi, \\gamma} (s, a) = (Q^{\\pi, \\gamma} (s, a) - \\Phi (s)) - (V^\\pi (s) - \\Phi (s)) = A^{\\pi, \\gamma} (s, a)$$\n\n이제 reward shaping의 idea를 가지고 어떻게 policy gradient estimate를 얻을 수 있는 지에 대해서 알아봅시다.\n\n- $0 \\le \\lambda \\le 1$의 범위에 있는 steeper discount $\\gamma \\lambda$를 사용합니다.\n- shaped reward $\\tilde{r}$는 Bellman residual term $\\delta^V$와 동일합니다.\n- 그리고 $\\Phi = V$와 같다고 보면 다음과 같은 수식이 나옵니다.\n\n$$\\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\tilde{r} (s_{t+l}, a_t, s_{t+l+1}) = \\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^V = \\hat{A}_t^{GAE(\\gamma, \\lambda)}$$\n이렇게 shaped rewards의 $\\gamma \\lambda$-discounted sum을 고려함으로써 GAE를 얻을 수 있습니다. 더 정확하게 $\\lambda = 1$은 $g^\\gamma$의 unbiased estimate이고, 반면에 $\\lambda < 1$은 biased estimate입니다.\n\n좀 더 나아가서, shaping transformation과 parameters $\\gamma$ and $\\lambda$의 결과를 보기 위해, response function $\\chi$를 이용하면 다음과 같은 수식이 나옵니다.\n$$\\chi (l; s_t, a_t) = \\mathbb{E} [r_{t+l} | s_t, a_t] - \\mathbb{E} [r_{t+l} | s_t]$$\n추가적으로 $A^{\\pi, \\gamma} (s, a) = \\sum_{l=0}^\\infty \\gamma^l \\chi (l; s, a)$입니다.\n그래서 discounted policy gradient estimator는 다음과 같이 쓸 수 있다.\n$$\\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) A^{\\pi, \\gamma} (s_t, a_t) = \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) \\sum_{l=0}^\\infty \\gamma^l \\chi (l; s, a)$$\n\n<br><br>\n\n# 6. Value Fuction Estimation\n\n이번 section에서는 Trust Region Optimization Scheme에 따라 Value function을 Estimation합니다.\n\n<br>\n## 6.1 Simplest approach\n\n$$minimize_{\\phi} \\sum_{n=1}^{N} \\vert\\vert V_{\\phi}(s_n) - \\hat{V_n} \\vert\\vert^{2}$$\n\n- 위는 가장 간단하게 non-linear approximation으로 푸는 방법입니다.\n- $\\hat{V_t} = \\sum_{l=0}^{\\infty}\\gamma^l r_{t+l}$은 reward 에 대한 discounted sum을 의미합니다.\n\n<br>\n## 6.2 Trust region method to optimize the value function\n\n- Value function을 최적화 하기 위해 trust region method를 사용합니다.\n- Trust region은 최근 데이터에 대해 overfitting되는 것을 막아줍니다.\n\nTrust region문제를 풀기 위해서는 다음 스텝을 따릅니다.\n\n- $\\sigma^2 = \\frac{1}{N} \\sum_{n=1}^{N} \\vert\\vert V_{\\phi old}(s_n) - \\hat{V_n} \\vert\\vert^{2}$을 계산합니다.\n- 그 후에 다음과 같은 constrained opimization문제를 풉니다.\n\n$$minimize_{\\phi} \\, \\sum_{n=1}^N \\parallel V_\\phi (s_n) - \\hat{V}_n \\parallel^2$$\n$$subject \\, \\, to \\, \\frac{1}{N} \\sum_{n=1}^N \\frac{\\parallel V_\\phi (s_n) - V_{\\phi old} (s_n) \\parallel^2}{2 \\sigma^2} \\le \\epsilon$$\n\n- 위의 수식은 사실 old Value function과 새로운 Value function KL distance가 $\\epsilon$ 다 작아야한다는 수식과 같습니다. Value function이 평균은 $V_{\\phi}(s)$이고 분산이 $\\sigma^2$인 conditional Gaussian distribution으로 parameterize되었을 뿐입니다.\n\n<img width =\"400px\" src=\"https://www.dropbox.com/s/uw0v05feu8chqmc/Screenshot%202018-07-08%2010.04.38.png?dl=1\"> \n\n이 trust region문제의 답을 conjudate gradient algorithm을 이용하여 근사값을 구할 수 있습니다. 특히, 다음과 같은  quadratic program을 풀게됩니다.\n\n$$minimize_{\\phi} \\, g^T (\\phi - \\phi_{old})$$\n$$subject \\, \\, to \\, \\frac{1}{N} \\sum_{n=1}^N (\\phi - \\phi_{old})^T H(\\phi - \\phi_{old}) \\le \\epsilon$$\n\n- 여기서 $g$는 objective 의 gradient입니다.\n- $j_n = \\nabla_{\\phi} V_{\\phi}(s_n)$일때, $H = \\frac{1}{N} \\sum_{n} j_n j^T_n$이며, $H$는 objective의 hessian에 대해서 gaussian newton method로 근사한 값입니다. 따라서, value function을 conditional probability로 해석한다면 Fisher information matrix가 됩니다.\n- 구현할때의 방법은 TRPO에서 사용한 방법과 모두 같습니다.\n\n<br><br>\n\n# 7.Experiments\n\n실험은 다음 두 가지 물음에 대해서 디자인 되었습니다.\n\n- GAE에 따라서 episodic total reward를 최적화 할때, $\\lambda$와 $\\gamma$가 변함에 따라서 어떤 경험적인 효과를 볼 수 있는지?\n- GAE와 trust region alogorithm을 policy와 value function 모두에 함께 사용했을 때 어려운 문제에 적용되는 큰 뉴럴넷을 최적화 할 수 있을지?\n\n<br>\n## 7.1 Policy Optimization Algorithm\n\nPolicy update는 TRPO를 사용합니다. TRPO에 대한 설명은 여기서는 생략하겠습니다. TRPO 포스트를 보고 돌아와주세요!\n\n- 이전 TRPO에서 이미 TRPO와 다른 많은 알고리즘들을 비교하였기 때문에, 여기서는 똑같은 짓을 반복하지 않고 $\\lambda$, $\\gamma$가 변함에 따라 어떤 영향이 있는 지에 대한 실험에 집중하겠다고 합니다. (귀찮았던거죠..)\n\n- TRPO를 적용한 GAE의 최종 알고리즘은 다음과 같습니다.\n\n<center> <img width = \"600px\" src=\"https://www.dropbox.com/s/b1klz11f2frrvg4/Screenshot%202018-07-08%2010.33.54.png?dl=1\"> </center>\n<center> <img width = \"300px\" src=\"https://www.dropbox.com/s/35fxte05pqtiel7/Screenshot%202018-07-08%2010.35.22.png?dl=1\"> </center>\n<center> <img width = \"300px\" src=\"https://www.dropbox.com/s/u35pjow3w50bvz1/Screenshot%202018-07-08%2010.36.03.png?dl=1\"> </center>\n\n- 여기서 주의할 점은 Policy update($\\theta_i \\rightarrow \\theta_{i+1}$)에서 $V_{\\phi_i}$를 사용했다는 점입니다.\n- 만약 Value function을 먼저 update하게 된다면 추가적인 bias가 발생합니다.\n- 극단적으로 생각해보아서, 우리가 Value function을 완벽하게 overfit을 해낸다면 Bellman residual($r_t + \\gamma V(s_{t+1}) - V(S_t)$)은 0이 됩니다. 그럼 Policy gradient의 estimation도 거의 0이 될 것입니다.\n\n<br>\n## 7.2 Expermint details\n\n### 7.2.1 Environment \n실험에서 사용된 환경은 다음 네 가지 입니다.\n\n1. classic cart-pole (x 3D)\n2. bipedal locomotion\n3. quadrupedal locomotion\n4. dynamically standing up for the biped\n\n### 7.2.2 Architecture\n\n- 3D robot task에 대해서는 같은 모델을 사용하였습니다.\n    - layers  = [100, 50, 25] 각각 tanh 사용. (Policy와 Value 네트워크 모두)\n    - Final output layer은 linear\n- Cartpole에 대해서는 1개의 layer 안에 20개의 hidden unit만 있는 linear policy를 사용했다고 합니다.\n\n### 7.2.3 Task\n\n- Cartpole\n    - 한 배치당 20 개의 trajectory를 모았고, maximum length는 1000입니다.\n- 3D biped locomotion\n    - 33 dim state , 10 dim action\n    - 50000 time step per batch\n- 3D quadruped locomotion\n    - 29 dim state, 8 dim action\n    - 200000 time step per batch\n- 3D biped locomotion Standing\n    - 33 dim state , 10 dim action\n    - 200000 time step per batch\n\n\n### 7.2.3 results\ncost의 관점에서 결과를 나타내었다고 합니다. Cost는 negative reward와 이것이 최소화 되었는가로 정의되었습니다.\n\n#### 7.2.3.1 Cartpole\n\n<center> <img width = \"500px\" src=\"https://www.dropbox.com/s/x9pbms1wvg38lda/Screenshot%202018-07-08%2011.08.22.png?dl=1\"> </center>\n\n- 왼쪽 그림은 $\\gamma$를 0.99로 고정시켜놓은 상태에서 $\\lambda$를 변화시킴에 따라서 cost를 측정한 것입니다. \n- 오른쪽은 $\\gamma$와 $\\lambda$를 둘 다 변화 시키면서 성능을 그림으로 나타낸 표입니다. 흰색에 가까울 수록 좋은 성능입니다. \n\n#### 7.2.3.2 3D BIPEDAL LOCOMOTINO\n\n<center> <img width = \"500px\" src=\"https://www.dropbox.com/s/i9wj4p6ijojsy82/Screenshot%202018-07-08%2011.25.08.png?dl=1\"> </center>\n\n\n- 다른 random seed로 부터 9번 씩 시도한 결과를 mean을 취해서 사용합니다.\n- Best performance는   $\\gamma \\in [0.99, 0.995]$ 그리고  $\\lambda \\in [0.96, 0.99]$일때. \n- 1000 iteration 후에 빠르고 부드럽고 안정적인 걸음거이가 나옵니다.\n- 실제로 걸린 시간은 0.01(타입스텝당 시간) * 50000(배치당 타임스텝) * 1000(배치) * 3600(초->시간) * 24 = 5.8일 정도가 걸렸습니다.\n\n#### 7.2.3.3 다른 ROBOT TASKS\n\n<center> <img width = \"500px\" src=\"https://www.dropbox.com/s/fuuat65we52quht/Screenshot%202018-07-08%2011.33.11.png?dl=1\"> </center>\n\n- 다른 로봇 TASK에 대해서는 아주 제한적인 실험만 진행합니다.(시간이 부족했던듯 하네요..)\n- Quadruped에 대해서는 $\\gamma = 0.995$로 fix, $\\lambda \\in {0, 0.96}$\n- Standingup에 대해서는 $\\gamma = 0.99$로 fix, $\\lambda \\in {0, 0.96}$\n\n<br><br>\n\n# 8. Discussion\n\n<br>\n## 8.1 Main discussion\n\n지금까지 복잡하고 어려운 control problem에서 Reinforcement Learning(RL)은 high sample complexity 때문에 제한이 되어왔습니다. 따라서 이 논문에서 그 제한을 풀고자 advantage function의 good estimate를 얻는 \"variance reduction\"에 대해 연구하였습니다.\n\n\"Generalized Advantage Estimator(GAE)\"라는 것을 제안했고, 이것은 bias-variance tradeoff를 조절하는 두 개의 parameter $\\gamma,\\lambda$를 가집니다.\n또한 어떻게 Trust Region Policy Optimization과 value function을 optimize하는 Trust Region Algorithm의 idea를 합치는 지를 보였습니다.\n\n이렇게 함으로써 보다 더 복잡하고 어려운 control task들을 해결할 수 있었습니다.\n\nGAE의 실험적인 입증으로는 robotic locomotion을 simulation하는 domain입니다. 실험에서도 보여준 것처럼 [0.9, 0.99]의 범위에서 $\\lambda$의 적절한 중간의 값을 통해 best performance를 얻습니다. 좀 더 나아가 연구되어야할 점은 adaptive or automatic하도록 estimator parameter $\\gamma,\\lambda$를 조절하는 방법입니다.\n\n<br>\n## 8.2 Future work\n\nValue function estimation error와 Policy gradient estimation error사이의 관계를 알아낸다면, 우리는 Value function fitting에 더 잘 맞는 error metric(policy gradient estimation 의 정확성과 더 잘 맞는 value function)을 사용할 수 있습니다. 여기서 Policy와 Value function의 파라미터를 공유하는 모델을 만드는 것은 아주 흥미롭고 이점이 많습니다. 하지만 수렴을 보장하도록 적절한 numerical optimization을 제시해야 할 것입니다.\n\n추가적으로 DDPG는 별로라고 합니다. 그리고 TD(0)는 bias가 너무 크고, poor performance로 이끈다고 합니다. 특히나 이 논문에서는 low-dimention의 쉬운 문제들만 해결했습니다.\n\n<center> <img width = \"500px\" src=\"https://www.dropbox.com/s/nhc7t9psul5lr3x/Screenshot%202018-07-08%2011.45.15.png?dl=1\"> </center>\n\n<br>\n## 8.3 FAQ\n\n- Compatible features와는 무슨 관계?\n     - Compatible features는 value function을 이용하는 policy gradient 알고리즘들과 함께 자주 언급됩니다.\n     - Actor Critic의 저자는 policy의 제한된 representation power때문에, policy gradient는 단지 advantage function space의 subspace에만 의존하게 됩니다.\n     - 이 subspace는 compatible features에 의해 span됩니다.\n     - 이 이론은 현재 문제 구조를 어떻게 이용해야 advantage function에 대해 더 나은 estimation을 할 수 있는 지에 대한 지침을 주지 않습니다. GAE 논문의 idea와 orthogonal합니다.\n- 왜 Q function을 사용하지 않는가?\n     - 먼저 state-value function이 더 낮은 차원의 input을 가진다. 그래서 Q function보다 더 학습하기가 쉽습니다.\n     - 두 번째로 이 논문에서 제안하는 방법으로는 high bias estimator에서 low bias estimator로 $\\lambda$를 통해서 부드럽게 interpolate를 할 수 있습니다.\n     - 반면에 Q를 사용하면 단지 high-bias estimator 밖에 사용할 수 없습니다.\n     - 특히나 return에 대한 one-step estimation은 엄두를 못낼 정도로 bias가 큽니다.\n","slug":"gae","published":1,"updated":"2018-07-24T06:43:48.808Z","_id":"cjjgzyg440000xo155e3g8dvy","comments":1,"layout":"post","photos":[],"link":"","content":"<center> <img src=\"https://www.dropbox.com/s/p8gfpyo6xf9wm5w/Screen%20Shot%202018-07-18%20at%201.25.53%20AM.png?dl=1\" width=\"700\"> </center>\n\n<p>논문 저자 : John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan and Pieter Abbeel<br>논문 링크 : <a href=\"https://arxiv.org/pdf/1506.02438.pdf\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/pdf/1506.02438.pdf</a><br>Proceeding : ??<br>정리 : 양혁렬, 이동민</p>\n<hr>\n<h1 id=\"1-들어가며…\"><a href=\"#1-들어가며…\" class=\"headerlink\" title=\"1. 들어가며…\"></a>1. 들어가며…</h1><p>현존하는 Policy Gradient Method들의 목적은 누적되는 reward들을 optimization하는 것입니다. 하지만 학습할 때에 많은 양의 sample이 필요로 하고, 들어오는 data가 nonstationarity임에도 불구하고 stable and steady improvement가 어렵습니다.</p>\n<p>그래서 이 논문에서는 다음과 같은 방법을 제시합니다.</p>\n<ul>\n<li>TD($\\lambda$)와 유사한 advantage function의 exponentially-weighted estimator를 사용하여 policy gradient estimate의 variance를 줄이는 것 </li>\n<li>policy와 value function에 대한 Trust Region Optimization 사용하는 것</li>\n</ul>\n<p>3D locomotion tasks에 대한 empirical results는 다음과 같습니다.</p>\n<ul>\n<li>bipedal and quadrupedal simulated robots의 달리는 자세를 학습</li>\n<li>bipedal 사람이 땅에 누워있다가 일어서는 것을 학습</li>\n</ul>\n<p>※ TD($\\lambda$)를 혹시 모르실 경우도 있을 것 같아 간략하게 정리하였습니다. <a href=\"http://dongminlee.tistory.com/10\" target=\"_blank\" rel=\"noopener\">http://dongminlee.tistory.com/10</a> 를 먼저 보시고 아래의 내용을 봐주시기 바랍니다!</p>\n<p><br><br></p>\n<h1 id=\"2-Introduction\"><a href=\"#2-Introduction\" class=\"headerlink\" title=\"2. Introduction\"></a>2. Introduction</h1><p>기본적으로 “parameterized stochastic policy”를 가정합니다. 이 때 expected total returns의 gradient에 대한 unbiased estimate를 얻을 수 있는데 이것을 REINFORCE라고 부릅니다. 하지만 하나의 action의 결과가 과거와 미래의 action의 결과로 혼동되기 때문에 gradient estimator의 high variance는 시간에 따라 scaling됩니다.</p>\n<p>또 다른 방법은 Actor-Critic이 있습니다. 이 방법은 empirical returns보다 하나의 value function을 사용합니다. 또한 bias하고 lower variance를 가진 estimator입니다. 구체적으로 말하자면, high variance하다면 더 sampling을 하면 되는 반면에 bias는 매우 치명적입니다. 다시 말해 bias는 algorithm이 converge하는 데에 실패하거나 또는 local optimum이 아닌 poor solution에 converge하도록 만듭니다.</p>\n<p>따라서 이 논문에서는 $\\gamma\\in [0,1]$ and $\\lambda\\in [0,1]$에 의해 parameterized estimation scheme인 Generalized Advantage Estimator(GAE)를 다룹니다.</p>\n<p>이 논문을 대략적으로 요약하자면 다음과 같습니다.</p>\n<ul>\n<li>효과적인 variance reduction scheme인 GAE를 다룹니다. 또한 실험할 때 batch trust region algorithm에 더하여 다양한 algorithm들에 적용됩니다.</li>\n<li>value function에 대해 trust region optimization method를 사용합니다. 이렇게 함으로서 더 robust하고 efficient한 방법이 됩니다.</li>\n<li>A와 B를 합쳐서, 실험적으로 control task에 neural network policies를 learning하는 데에 있어서 효과적인 algorithm을 얻습니다. 이러한 결과는 high-demensional continuous control에 RL을 사용함으로서 state of the art로 확장되었습니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"3-Preliminaries\"><a href=\"#3-Preliminaries\" class=\"headerlink\" title=\"3. Preliminaries\"></a>3. Preliminaries</h1><p>먼저 policy optimization의 “undiscounted formulation”을 가정합니다. (undiscounted formulation에 주목합시다.)</p>\n<ul>\n<li>initial state $s_0$는 distribution $\\rho_0$으로부터 sampling된 것입니다.</li>\n<li>하나의 trajectory ($s_0, a_0, s_1, a_1, …$)는 terminal state에 도달될 때 까지 policy $a_t$ ~ $\\pi(a_t | s_t)$에 따라서 action을 sampling하고, dynamics $s_{t+1}$ ~ $P(s_{t+1} | s_t, a_t)$에 따라서 state를 sampling함으로써 생성됩니다.</li>\n<li>reward $r_t = r(s_t, a_t, s_{t+1})$은 매 time step마다 받아집니다.</li>\n<li>목표는 모든 policies에 대해 finite하다고 가정됨으로서 expected total reward $\\sum_{t=0}^{\\infty} r_t$를 maximize하는 것입니다.</li>\n</ul>\n<p>여기서 중요한 점은 $\\gamma$를 discount의 parameter로 사용하는 것이 아니라 “bias-variance tradeoff를 조절하는 parameter로 사용한다” 는 것입니다.</p>\n<p>policy gradient method는 gradient $g := \\nabla_\\theta \\mathbb{E} [\\sum_{t=0}^\\infty r_t]$를 반복적으로 estimate함으로써 expected total reward를 maximize하는 것인데, policy gradient에는 여러 다른 표현들이 있습니다.<br>$$g = \\mathbb{E} [\\sum_{t=0}^\\infty \\Phi_t \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$</p>\n<p>여기서 $\\Phi_t$는 아래 중의 하나일 수 있습니다.</p>\n<ol>\n<li>$\\sum_{t=0}^\\infty r_t$: trajectory의 total reward</li>\n<li>$\\sum_{t’=t}^\\infty r_t’$: action $a_t$ 후의 reward</li>\n<li>$\\sum_{t’=t}^\\infty r_t’ - b(s_t)$: 2의 baselined version</li>\n<li>$Q^\\pi (s_t, a_t)$: state-action value function</li>\n<li>$A^\\pi (s_t, a_t)$: advantage function</li>\n<li>$r_t + V^\\pi (s_{t+1}) - V^\\pi (s_t)$: TD residual</li>\n</ol>\n<p>위의 번호 중 4, 5, 6의 수식들은 다음의 정의를 사용합니다.</p>\n<ul>\n<li>$V^\\pi (s_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_t:\\infty} [\\sum_{l=0}^\\infty r_{t+1}]$</li>\n<li>$Q^\\pi (s_t, a_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty} [\\sum_{l=0}^\\infty r_{t+l}]$</li>\n<li>$A^\\pi (s_t, a_t) := Q^\\pi (s_t, a_t) - V^\\pi (s_t)$, (Advantage function)</li>\n</ul>\n<p>추가적으로 colon notation $a : b$는 포괄적인 범위 $(a, a+1, … , b)$입니다. (잘 기억해둡시다. 뒤에 계속해서 colon notation이 나옵니다.)</p>\n<p>여기서부터 parameter $\\gamma$에 대해 좀 더 자세히 알아봅시다. parameter $\\gamma$는 bias하면서 동시에 reward를 downweighting함으로써 variance를 줄입니다. 다시 말해 MDP의 discounted formulation에서 사용된 discounted factor와 일치하지만, 이 논문에서는 “$\\gamma$를 undiscounted MDP에서 variance reduction parameter”로 다룹니다. -&gt; 결과는 같지만, 의미와 의도가 다릅니다.</p>\n<p>discounted value function들은 다음과 같습니다.</p>\n<ul>\n<li>$V^{\\pi, \\gamma} (s_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_t:\\infty} [\\sum_{l=0}^\\infty \\gamma^l r_{t+l}]$ </li>\n<li>$Q^\\pi (s_t, a_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty} [\\sum_{l=0}^\\infty \\gamma^l r_{t+l}]$</li>\n<li>$A^{\\pi, \\gamma} (s_t, a_t) := Q^{\\pi, \\gamma} (s_t, a_t) - V^{\\pi, \\gamma} (s_t)$</li>\n</ul>\n<p>따라서 policy gradient에서의 discounted approximation은 다음과 같이 나타낼 수 있습니다.<br>$$g^\\gamma := \\mathbb{E}_{s_{0:\\infty} a_{0:\\infty}} [\\sum_{t=0}^\\infty A^{\\pi, \\gamma} (s_t, a_t) \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$<br>뒤이어 나오는 section에서는 위의 수식에서 $A^{\\pi, \\gamma}$에 대해 biased (but not too biased) estimator를 얻는 방법에 대해서 나옵니다.</p>\n<p>다음으로 advantage function에서 새롭게 접하는 “$\\gamma$-just estimator”의 notation에 대해 알아봅시다.</p>\n<ul>\n<li>먼저 $\\gamma$-just estimator는 $g^\\gamma$ ${}^1$를 estimate하기 위해 위의 수식에서 $A^{\\pi, \\gamma}$를 $\\gamma$-just estimator로 사용할 때, bias하지 않은 estimator라고 합시다. 그리고 이 $\\gamma$-just advantage estimator를 $\\hat{A}<em>t (s</em>{0:\\infty} , a_{0:\\infty})$라고 하고, 전체의 trajectory에 대한 하나의 function이라고 합시다.</li>\n<li>${}^1$에서 이 논문의 저자가 하고 싶은 말이 있는 것 같습니다. 개인적으로 $\\gamma$-just estimator를 이해하는 데에 있어서 중요한 주석이라 정확히 해석하고자 합니다.<ul>\n<li>$A^\\pi$를 $A^{\\pi, \\gamma}$로 사용함으로써 이미 bias하다라고 말했지만, “이 논문에서 하고자 하는 것은 $g^\\gamma$에 대해 unbiased estimate를 얻고 싶은 것입니다.” 하지만 undiscounted MDP의 policy gradient에 대해서는 당연히 $\\gamma$를 사용하기 때문에 biased estimate를 얻습니다. 개인적으로 이것은 일단 무시하고 $\\gamma$를 사용할 때 어떻게 unbiased estimate를 얻을 지에 대해 좀 더 포커스를 맞추고 있는 것 같습니다.</li>\n<li>그러니까 저 $A^{\\pi, \\gamma}$를 $\\gamma$-just estimator로 바꿔줌으로써 unbiased estimate를 하고 싶다는 것이 뒤이어 나오는 정의와 명제의 핵심이라고 할 수 있습니다.</li>\n</ul>\n</li>\n</ul>\n<p>Definition 1.<br>먼저 가정을 합니다. (가정을 바탕으로 이루어지는 정의라는 것을 주목합시다.) 만약<br>$$\\mathbb{E}_{s_{0:\\infty} a_{0:\\infty}} [\\hat{A}<em>t (s</em>{0:\\infty}, a_{0:\\infty}) \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)] = \\mathbb{E}_{s_{0:\\infty} a_{0:\\infty}} [A^{\\pi, \\gamma} (s_t, a_t) \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$<br>두 수식이 같다면, estimator $\\hat{A}_t$는 $\\gamma$-just입니다.</p>\n<p>그리고 만약 모든 t에 대해서 $\\hat{A}<em>t$이 $\\gamma$-just이라면, 다음과 같이 표현할 수 있습니다.<br>$$\\mathbb{E}</em>{s_{0:\\infty} a_{0:\\infty}} [\\hat{A}<em>t (s</em>{0:\\infty}, a_{0:\\infty}) \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)] = g^\\gamma$$<br>위의 수식이 바로 unbiased estimate입니다.</p>\n<p>$\\gamma$-just인 $\\hat{A}_t$에 대한 한가지 조건은 $\\hat{A}_t$이 두 가지 function $Q_t$ and $b_t$로 나뉠 수 있다는 것입니다.</p>\n<ul>\n<li>$Q_t$는 $\\gamma$-discounted Q-function의 unbiased estimator입니다.</li>\n<li>$b_t$는 action $a_t$전에 sampling된 states and actions의 arbitrary function이다. </li>\n</ul>\n<p>Proposition 1.<br>모든 $(s_t, a_t)$에 대해,<br>$$\\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty | s_t, a_t} [Q_t (s_{t:\\infty}, a_{t:\\infty})] = Q^{\\pi, \\gamma} (s_t, a_t)$$<br>로 인하여 $\\hat{A}<em>t$이<br>$$\\hat{A}</em>{s_{0:\\infty}, a_{0:\\infty}} = Q_t (s_{0:\\infty}, a_{0:\\infty}) - b_t (s_{0:t}, a_{0:t-1})$$<br>형태라고 가정합시다. (가정을 바탕으로 이루어지는 명제라는 점을 주목합시다.)</p>\n<p>그 때, $\\hat{A}_t$은 $\\gamma$-just입니다.</p>\n<p>이 명제에 대한 증명은 Appendix B에 있습니. 그리고 $\\hat{A}_t$에 대한 $\\gamma$-just advantage estimator들은 다음과 같습니다.</p>\n<ul>\n<li>$\\sum_{l=0}^\\infty \\gamma^l r_{t+1}$</li>\n<li>$A^{\\pi, \\gamma} (s_t, a_t)$</li>\n<li>$Q^{\\pi, \\gamma} (s_t, a_t)$</li>\n<li>$r_t + \\gamma V^{\\pi, \\gamma} (s_{t+1}) - V^{\\pi, \\gamma} (s_t)$</li>\n</ul>\n<p>증명을 보기 전에 먼저 Advantage function에 대해서 먼저 살펴봅시다. 아래의 내용은 Sutton이 쓴 논문인 Policy Gradient Methods for Reinforcement Learning with Function Approximation(2000)에서 나온 내용을 리뷰한 것입니다. </p>\n<center> <img src=\"https://www.dropbox.com/s/x737yq97ut6gp1a/Screen%20Shot%202018-07-15%20at%201.16.09%20PM.png?dl=1\" width=\"600\"> </center>\n\n<p>여기서는 “이 수식은 $\\sum_a \\frac{\\partial \\pi (s,a)}{\\partial \\theta} = 0$이기 때문에 가능해진다.”, “즉, 이 수식은 $\\pi(s,a)$의 gradient에만 dependent하기 때문에 advantage 역할을 하는 함수들을 넣어도 아무런 상관이 없다”라는 두 문장을 기억해둡시다.</p>\n<p>이제 증명을 보면 됩니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/e7sj2fwcm1f7hof/figure2.jpg?dl=1\" width=\"600\"> </center><br><center> <img src=\"https://www.dropbox.com/s/kyk4jes1202az4m/figure3.jpg?dl=1\" width=\"600\"> </center>\n\n<p>위의 증명 수식에서 Q와 b의 각각 세번째 수식과 마지막 수식을 자세히 봅시다.</p>\n<ol>\n<li><p>$Q$에 대한 증명</p>\n<ul>\n<li><p>빨간색 부분</p>\n<ul>\n<li>$\\mathbb{E}_{s_{t+1:\\infty}, a_{t+1:\\infty}} [Q_t (s_{0:\\infty}, a_{0:\\infty})]$를 쉽게 말하자면 “모든(과거, 현재, 미래) state와 action에 대해서 expectation을 $t+1$부터 $\\infty$까지 하겠다.”라는 말입니다. 이렇게 함으로써 원래는 $Q^\\pi (s_t, a_t)$가 나와야 맞는 건데, 앞에서 살펴봤다시피 4번째 수식 밑줄 친 자리에는 advantage 역할을 하는 함수들을 넣어도 결과값에 아무런 영향을 끼치지 않기 때문에 의도적으로 $A^\\pi (s_t, a_t)$로 바꾼 것입니다. 또한 이 증명을 바탕으로 $\\hat{A}$이 $\\gamma$-just라는 것을 표현하고 싶기 때문이라고 봐도 됩니다.</li>\n</ul>\n</li>\n<li><p>파란색 부분</p>\n<ul>\n<li>또한 $a_{0:t}$에서 $a_{0:t-1}$로 변한 것은 $A^\\pi (s_t, a_t)$에 baseline이 들어가기 때문에 바꿔준 것입니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>$b$에 대한 증명</p>\n<ul>\n<li>빨간색 부분<ul>\n<li>$\\mathbb{E}_{s_{t+1:\\infty}, a_{t+1:\\infty}} [\\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$가 0으로 바뀌는 것은 위에서 설명했듯이 $\\nabla_\\theta$자체가 $\\log \\pi_\\theta$만 gradient하기 때문에 이것을 expectation을 취하면 0이 됩니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<p><br><br></p>\n<h1 id=\"4-Advantage-Function-Estimation\"><a href=\"#4-Advantage-Function-Estimation\" class=\"headerlink\" title=\"4. Advantage Function Estimation\"></a>4. Advantage Function Estimation</h1><p>이번 section에는 discounted advantage function $A^{\\pi, \\gamma} (s_t, a_t)$의 accurate estimate $\\hat{A}_t$에 대해서 살펴봅시다. 이에 따른 수식은 다음과 같습니다.</p>\n<p>$$\\hat{g} = \\frac{1}{N} \\sum_{n=1}^N \\sum_{t=0}^\\infty \\hat{A}<em>t^n \\nabla</em>{\\theta} \\log \\pi_{\\theta}(a_t^n | s_t^n)$$</p>\n<p>여기서 n은 a batch of episodes에 대하여 index한 것입니다.</p>\n<p>$V$를 approximate value function이라고 합시다. 그리고 $\\delta_t^V = r_t + \\gamma V(s_{t+1}) - V(s_t)$이라고 합시다.</p>\n<p>만약 (이전과 마찬가지로 가정부터 합니다.) correct value function $V = V^{\\pi, \\gamma}$가 있다고 한다면, 이것은 $\\gamma$-just advantage estimator입니다. 실제로, $A^{\\pi, \\gamma}$의 unbiased estimator는 다음과 같습니다.<br>$$\\mathbb{E}_{s_{t+1}} [\\delta_t^{V^{\\pi, \\gamma}}] = \\mathbb{E}_{s_{t+1}} [r_t + \\gamma V^{\\pi, \\gamma} (s_{t+1}) - V^{\\pi, \\gamma} (s_t)]$$<br>$$= \\mathbb{E}_{s_{t+1}} [Q^{\\pi, \\gamma} (s_t, a_t) - V^{\\pi, \\gamma} (s_t)] = A^{\\pi, \\gamma} (s_t, a_t)$$<br>그러나, “이 estimator는 유일하게 $V = V^{\\pi, \\gamma}$에 대한 $\\gamma$-just입니다.” 다른 경우라면 이것은 biased policy gradient estimate일 것입니다. (우리가 하고 싶은 것은 $V$에 대해서만 unbiased estimator가 아니라 advantage function에 대해서 일반화된 unbiased estimator를 얻고 싶은 것입니다. 그래서 아래에서도 나오겠지만, $\\gamma$와 함께 $\\lambda$를 이용한 estimator가 나옵니다.ㅏ)</p>\n<p>그래서 $\\delta$에 대해 $k$의 sum으로 생각해봅시다. 이것을 $\\hat{A}_t^{(k)}$라고 하자. 그러면 아래와 같이 표현할 수 있습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/ra7hxksveg2hz45/figure4.jpg?dl=1\" width=\"600\"> </center>\n\n<p>$\\hat{A}_t^{(k)}$은 returns의 $k$-step estimate와 연관지을 수 있고, $\\delta_t^V = \\hat{A}_t^{(1)}$의 case와 유사하게도 $\\hat{A}_t^{(k)}$를 advantage function의 estimator로 생각할 수 있습니다.</p>\n<p>여기서 $k \\rightarrow \\infty$로 생각해보면 bias가 일반적으로 점점 더 작아집니다. 왜냐하면 $\\gamma^k V(s_{t+k})$가 점점 많이 discounted되서 $-V(s_t)$가 bias에 영향을 미치지 못하기 때문입니다. $k \\rightarrow \\infty$를 취하면 다음과 같은 수식이 나옵니다.</p>\n<p>$$\\hat{A}<em>t^{(\\infty)} = \\sum</em>{l=0}^\\infty \\gamma^l \\delta_{t+l}^V = -V(s_t) + \\sum_{l=0}^\\infty \\gamma^l r_{t+l}$$<br>우변의 수식과 같이 empirical returns에서 value function baseline을 뺀 것으로 나타낼 수 있습니다.</p>\n<p>Generalized Advantage Estimator GAE($\\gamma, \\lambda$)는 $k$-step estimators의 exponentially-weighted average로 나타낼 수 있습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/yg1ybmfkep3towu/figure5.jpg?dl=1\" width=\"600\"> </center>\n\n<p>(TD($\\lambda$)를 떠올려주세요..!)</p>\n<p>위의 마지막 수식에서도 알 수 있듯이, advantage estimator는 Bellman residual terms의 discounted sum과 관련있는 간단한 수식입니다. 다음 section에서 modified reward function을 가진 MDP에 returns로서의 관점에서 위의 마지막 수식을 더 자세히 살펴봅시다. 위의 수식은 TD($\\lambda$)와 많이 유사합니다. 그러나 TD($\\lambda$)는 value function를 estimator하고, 여기서는 advantage function을 estimator합니다.</p>\n<p>위의 수식에서 $\\lambda = 0$ and $\\lambda = 1$에 대해서는 특별한 case가 존재한다. 수식으로 표현하면 다음과 같습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/ufglxzanhcnbi36/figure6.jpg?dl=1\" width=\"600\"> </center>\n\n<ul>\n<li>$GAE(\\gamma, 1)$은 $V$의 정확도와 관계없이 $\\gamma$-just입니다. 그러나 returns의 sum때문에 high variance합니다.</li>\n<li>$GAE(\\gamma, 0)$은 $V = V^{\\pi, \\gamma}$에 대해 $\\gamma$-just입니다. 그리고 bias하지만 일반적으로 훨씬 lower variance를 가집니다. </li>\n<li>$0 &lt; \\lambda &lt; 1$에 대해 GAE는 parameter $\\lambda$를 control하여 bias와 variance사이에 compromise를 만듭니다.</li>\n</ul>\n<p>두 가지 별개의 parameter $\\gamma$ and $\\lambda$를 가진 advantage estimator는 bias-variance tradeoff에 도움을 줍니다. 그러나 이 두 가지 parameter는 각각 다른 목적을 가지고 작동합니다.</p>\n<ul>\n<li>$\\gamma$는 가장 중요하게 value function $V^{\\pi, \\gamma}$ 의 scale을 결정합니다. 또한 $\\gamma$는 $\\lambda$에 의존하지 않습니다. $\\gamma &lt; 1$로 정하는 것은 policy gradient estimate에서 bias합니다.</li>\n<li>반면에, $\\lambda &lt; 1$는 유일하게 value function이 부정확할 때 bias합니다. 그리고 경험상, $\\lambda$의 best value는 $\\gamma$의 best value보다 훨씬 더 낮습니다. 왜냐하면 $\\lambda$가 정확한 value function에 대해 $\\gamma$보다 훨씬 덜 bias하기 때문입니다.</li>\n</ul>\n<p>GAE를 사용할 때, $g^\\gamma$의 biased estimator를 구성할 수 있습니다. 수식은 다음과 같습니다.<br>$$g^\\gamma \\approx \\mathbb{E} [\\sum_{t=0}^\\infty] \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) \\hat{A}<em>t^{GAE(\\gamma, \\lambda)}] = \\mathbb{E} [\\sum</em>{t=0}^{\\infty} \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) \\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+1}^V]$$<br>여기서 $\\lambda = 1$일 때 동일해집니다.</p>\n<p><br><br></p>\n<h1 id=\"5-Interpretation-as-Reward-Shaping\"><a href=\"#5-Interpretation-as-Reward-Shaping\" class=\"headerlink\" title=\"5. Interpretation as Reward Shaping\"></a>5. Interpretation as Reward Shaping</h1><p>이번 section에서는 앞서 다뤘던 수식 $\\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^V$를 modified reward function의 MDP의 관점으로 생각해봅시다. 조금 더 구체적으로 말하자면, MDP에서 reward shaping transformation을 실행한 후에 적용된 extra discounted factor로서 $\\lambda$를 어떻게 볼 것인지에 대해서 다룹니다.</p>\n<p>(개인적인 comment) 한 가지 먼저 언급하자면, 본래의 목적은 reward shaping이 아니라 variance reduction입니다. 이번 section은 그저 이전에 이러한 개념이 있었고, gae를 다른 관점에서 생각해보자라는 뜻에서 나온 section인 것 같습니다. ‘아~ 이러한 개념이 있구나~!’ 정도로만 알면 될 것 같습니다. ‘gae가 reward shaping의 효과까지 있어서 이러한 section을 넣은 것일까?’ 라는 생각도 해봤지만 아직 잘 모르겠습니다. 실험부분에도 딱히 하지 않은 걸로 봐서는.. 아닌 것 같기도 하고.. 아직까지는 그저 ‘reward shaping의 관점에서 봤을 때에도 gae로 만들어줄 수 있다.’ 정도만 생각하려고 합니다.</p>\n<p>Reward Shaping이란 개념 자체는 일반적인 알고리즘 분야(최단경로 문제 등)에도 있습니다. 하지만 이 개념이 머신러닝에 적용된 건 Andrew Y. Ng이 쓴 Policy Invariance under Reward Transformations Theory and Application to Reward Shaping(1999) 논문입니다. 논문에서 Reward Shaping을 설명하는 부분은 간략하게 다음과 같습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/0w9cuxjkhz2mbcy/figure7.jpg?dl=1\" width=\"400\"> </center><br><center> <img src=\"https://www.dropbox.com/s/xcp9eqvcbv3268g/figure8.jpg?dl=1\" width=\"400\"> </center>\n\n<p>위의 글만 보고 이해가 잘 안됩니다. 그래서 다른 자료들을 찾던 중에 다음의 그림을 찾게 되었습니다. (아래의 그림은 Youtube에 있는 <a href=\"https://www.youtube.com/watch?v=xManAGjbx2k&amp;t=95s\" target=\"_blank\" rel=\"noopener\">Udacity 영상</a>에서 있는 그림입니다.)</p>\n<center> <img src=\"https://www.dropbox.com/s/87kacngez9cl2e6/figure9.jpg?dl=1\" width=\"400\"> </center>\n\n<p>이 그림을 통해서 Reward Shaping을 이해해봅시다.</p>\n<p>Reward Shaping(보상 형성)을 통해서 뭘 하고 싶은지 목적부터 봅시다. reward space가 sparse 한 경우에 reward가 너무 드문드문 나옵니다. 따라서 이것을 꾸준히 reward를 받을 수 있도록 바꿉니다. potential-based shaping function인 $\\Phi$를 만들어서 더하고 빼줍니다. 저 $\\Phi$ 자리에는 대표적으로 state value function이 많이 들어간다고 합니다. (아직 목적이 이해가 안될 수 있습니다. Reward Shaping에 대해서 다 보고 나서 다시 한 번 읽어봅시다.)</p>\n<p>위의 그림은 돌고래가 점프하여 불구멍을 통과해야하는 환경입니다. 하지만 저 불구멍을 통과하기 위해서는 (1) 점프도 해야하고, (2) 불도 피해야하고, (3) 알맞게 착지까지 완료해야합니다. 이렇게 해야 reward를 +1 얻습니다. </p>\n<p>생각해봅시다. 어느 세월에 점프도 해야하고.. 불도 피해야하고.. 알맞게 착지까지해서 reward를 +1 얻겠습니까? 따라서 그 전에도 잘하고 있는 지, 못하고 있는 지를 판단하기 위해, 다시 말해 reward를 꾸준히 받도록 다음과 같이 transformed reward function $\\tilde{r}$을 정의합니다.</p>\n<p>$$\\tilde{r} (s, a, s’) = r(s, a, s’) + \\gamma \\Phi (s’) - \\Phi (s)$$</p>\n<ul>\n<li>여기서 $\\Phi : S \\rightarrow \\mathbb{R}$를 state space에서의 arbitrary scalar-valued function을 나타냅니다. 그리고 $\\Phi$자리에는 대표적으로 state value function이 들어간다고 생각합시다.</li>\n<li>형태가 TD residual term의 결과와 비슷하지만 의미와 의도가 다릅니다. reward shaping은 sparse reward 때문이고, 이전 section에서 봤던 gae는 variance reduction때문에 나온 것입니다.</li>\n</ul>\n<center> <img src=\"https://www.dropbox.com/s/tkfpqniaazwfbld/figure10.jpg?dl=1\" width=\"600\"> </center>\n\n<p>이 transformation은 discounted advantage function $A^{\\pi, \\gamma}$으로도 둘 수 있습니다. state $s_t$를 시작하여 하나의 trajectory의 rewards의 discounted sum을 표현하면 다음과 같습니다.<br>$$\\sum_{l=0}^\\infty \\gamma^l \\tilde{r} (s_{t+l}, a_t, s_{t+l+1}) = \\sum_{l=0}^\\infty \\gamma^l r(s_{t+l}, a_{t+l}, s_{t+l+1}) - \\Phi(s_t)$$</p>\n<p>$\\tilde{Q}^{\\pi, \\gamma}, \\tilde{V}^{\\pi, \\gamma}, \\tilde{A}^{\\pi, \\gamma}$를  transformed MDP의 value function과 advantage function이라고 하면, 다음과 같은 수식이 나옵니다.<br>$$\\tilde{Q}^{\\pi, \\gamma} (s, a) = Q^{\\pi, \\gamma} (s, a) - \\Phi (s)$$<br>$$\\tilde{V}^{\\pi, \\gamma} (s, a) = V^{\\pi, \\gamma} (s, a) - \\Phi (s)$$<br>$$\\tilde{A}^{\\pi, \\gamma} (s, a) = (Q^{\\pi, \\gamma} (s, a) - \\Phi (s)) - (V^\\pi (s) - \\Phi (s)) = A^{\\pi, \\gamma} (s, a)$$</p>\n<p>이제 reward shaping의 idea를 가지고 어떻게 policy gradient estimate를 얻을 수 있는 지에 대해서 알아봅시다.</p>\n<ul>\n<li>$0 \\le \\lambda \\le 1$의 범위에 있는 steeper discount $\\gamma \\lambda$를 사용합니다.</li>\n<li>shaped reward $\\tilde{r}$는 Bellman residual term $\\delta^V$와 동일합니다.</li>\n<li>그리고 $\\Phi = V$와 같다고 보면 다음과 같은 수식이 나옵니다.</li>\n</ul>\n<p>$$\\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\tilde{r} (s_{t+l}, a_t, s_{t+l+1}) = \\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^V = \\hat{A}_t^{GAE(\\gamma, \\lambda)}$$<br>이렇게 shaped rewards의 $\\gamma \\lambda$-discounted sum을 고려함으로써 GAE를 얻을 수 있습니다. 더 정확하게 $\\lambda = 1$은 $g^\\gamma$의 unbiased estimate이고, 반면에 $\\lambda &lt; 1$은 biased estimate입니다.</p>\n<p>좀 더 나아가서, shaping transformation과 parameters $\\gamma$ and $\\lambda$의 결과를 보기 위해, response function $\\chi$를 이용하면 다음과 같은 수식이 나옵니다.<br>$$\\chi (l; s_t, a_t) = \\mathbb{E} [r_{t+l} | s_t, a_t] - \\mathbb{E} [r_{t+l} | s_t]$$<br>추가적으로 $A^{\\pi, \\gamma} (s, a) = \\sum_{l=0}^\\infty \\gamma^l \\chi (l; s, a)$입니다.<br>그래서 discounted policy gradient estimator는 다음과 같이 쓸 수 있다.<br>$$\\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) A^{\\pi, \\gamma} (s_t, a_t) = \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) \\sum_{l=0}^\\infty \\gamma^l \\chi (l; s, a)$$</p>\n<p><br><br></p>\n<h1 id=\"6-Value-Fuction-Estimation\"><a href=\"#6-Value-Fuction-Estimation\" class=\"headerlink\" title=\"6. Value Fuction Estimation\"></a>6. Value Fuction Estimation</h1><p>이번 section에서는 Trust Region Optimization Scheme에 따라 Value function을 Estimation합니다.</p>\n<p><br></p>\n<h2 id=\"6-1-Simplest-approach\"><a href=\"#6-1-Simplest-approach\" class=\"headerlink\" title=\"6.1 Simplest approach\"></a>6.1 Simplest approach</h2><p>$$minimize_{\\phi} \\sum_{n=1}^{N} \\vert\\vert V_{\\phi}(s_n) - \\hat{V_n} \\vert\\vert^{2}$$</p>\n<ul>\n<li>위는 가장 간단하게 non-linear approximation으로 푸는 방법입니다.</li>\n<li>$\\hat{V_t} = \\sum_{l=0}^{\\infty}\\gamma^l r_{t+l}$은 reward 에 대한 discounted sum을 의미합니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"6-2-Trust-region-method-to-optimize-the-value-function\"><a href=\"#6-2-Trust-region-method-to-optimize-the-value-function\" class=\"headerlink\" title=\"6.2 Trust region method to optimize the value function\"></a>6.2 Trust region method to optimize the value function</h2><ul>\n<li>Value function을 최적화 하기 위해 trust region method를 사용합니다.</li>\n<li>Trust region은 최근 데이터에 대해 overfitting되는 것을 막아줍니다.</li>\n</ul>\n<p>Trust region문제를 풀기 위해서는 다음 스텝을 따릅니다.</p>\n<ul>\n<li>$\\sigma^2 = \\frac{1}{N} \\sum_{n=1}^{N} \\vert\\vert V_{\\phi old}(s_n) - \\hat{V_n} \\vert\\vert^{2}$을 계산합니다.</li>\n<li>그 후에 다음과 같은 constrained opimization문제를 풉니다.</li>\n</ul>\n<p>$$minimize_{\\phi} \\, \\sum_{n=1}^N \\parallel V_\\phi (s_n) - \\hat{V}<em>n \\parallel^2$$<br>$$subject \\, \\, to \\, \\frac{1}{N} \\sum</em>{n=1}^N \\frac{\\parallel V_\\phi (s_n) - V_{\\phi old} (s_n) \\parallel^2}{2 \\sigma^2} \\le \\epsilon$$</p>\n<ul>\n<li>위의 수식은 사실 old Value function과 새로운 Value function KL distance가 $\\epsilon$ 다 작아야한다는 수식과 같습니다. Value function이 평균은 $V_{\\phi}(s)$이고 분산이 $\\sigma^2$인 conditional Gaussian distribution으로 parameterize되었을 뿐입니다.</li>\n</ul>\n<p><img width=\"400px\" src=\"https://www.dropbox.com/s/uw0v05feu8chqmc/Screenshot%202018-07-08%2010.04.38.png?dl=1\"> </p>\n<p>이 trust region문제의 답을 conjudate gradient algorithm을 이용하여 근사값을 구할 수 있습니다. 특히, 다음과 같은  quadratic program을 풀게됩니다.</p>\n<p>$$minimize_{\\phi} \\, g^T (\\phi - \\phi_{old})$$<br>$$subject \\, \\, to \\, \\frac{1}{N} \\sum_{n=1}^N (\\phi - \\phi_{old})^T H(\\phi - \\phi_{old}) \\le \\epsilon$$</p>\n<ul>\n<li>여기서 $g$는 objective 의 gradient입니다.</li>\n<li>$j_n = \\nabla_{\\phi} V_{\\phi}(s_n)$일때, $H = \\frac{1}{N} \\sum_{n} j_n j^T_n$이며, $H$는 objective의 hessian에 대해서 gaussian newton method로 근사한 값입니다. 따라서, value function을 conditional probability로 해석한다면 Fisher information matrix가 됩니다.</li>\n<li>구현할때의 방법은 TRPO에서 사용한 방법과 모두 같습니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"7-Experiments\"><a href=\"#7-Experiments\" class=\"headerlink\" title=\"7.Experiments\"></a>7.Experiments</h1><p>실험은 다음 두 가지 물음에 대해서 디자인 되었습니다.</p>\n<ul>\n<li>GAE에 따라서 episodic total reward를 최적화 할때, $\\lambda$와 $\\gamma$가 변함에 따라서 어떤 경험적인 효과를 볼 수 있는지?</li>\n<li>GAE와 trust region alogorithm을 policy와 value function 모두에 함께 사용했을 때 어려운 문제에 적용되는 큰 뉴럴넷을 최적화 할 수 있을지?</li>\n</ul>\n<p><br></p>\n<h2 id=\"7-1-Policy-Optimization-Algorithm\"><a href=\"#7-1-Policy-Optimization-Algorithm\" class=\"headerlink\" title=\"7.1 Policy Optimization Algorithm\"></a>7.1 Policy Optimization Algorithm</h2><p>Policy update는 TRPO를 사용합니다. TRPO에 대한 설명은 여기서는 생략하겠습니다. TRPO 포스트를 보고 돌아와주세요!</p>\n<ul>\n<li><p>이전 TRPO에서 이미 TRPO와 다른 많은 알고리즘들을 비교하였기 때문에, 여기서는 똑같은 짓을 반복하지 않고 $\\lambda$, $\\gamma$가 변함에 따라 어떤 영향이 있는 지에 대한 실험에 집중하겠다고 합니다. (귀찮았던거죠..)</p>\n</li>\n<li><p>TRPO를 적용한 GAE의 최종 알고리즘은 다음과 같습니다.</p>\n</li>\n</ul>\n<center> <img width=\"600px\" src=\"https://www.dropbox.com/s/b1klz11f2frrvg4/Screenshot%202018-07-08%2010.33.54.png?dl=1\"> </center><br><center> <img width=\"300px\" src=\"https://www.dropbox.com/s/35fxte05pqtiel7/Screenshot%202018-07-08%2010.35.22.png?dl=1\"> </center><br><center> <img width=\"300px\" src=\"https://www.dropbox.com/s/u35pjow3w50bvz1/Screenshot%202018-07-08%2010.36.03.png?dl=1\"> </center>\n\n<ul>\n<li>여기서 주의할 점은 Policy update($\\theta_i \\rightarrow \\theta_{i+1}$)에서 $V_{\\phi_i}$를 사용했다는 점입니다.</li>\n<li>만약 Value function을 먼저 update하게 된다면 추가적인 bias가 발생합니다.</li>\n<li>극단적으로 생각해보아서, 우리가 Value function을 완벽하게 overfit을 해낸다면 Bellman residual($r_t + \\gamma V(s_{t+1}) - V(S_t)$)은 0이 됩니다. 그럼 Policy gradient의 estimation도 거의 0이 될 것입니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"7-2-Expermint-details\"><a href=\"#7-2-Expermint-details\" class=\"headerlink\" title=\"7.2 Expermint details\"></a>7.2 Expermint details</h2><h3 id=\"7-2-1-Environment\"><a href=\"#7-2-1-Environment\" class=\"headerlink\" title=\"7.2.1 Environment\"></a>7.2.1 Environment</h3><p>실험에서 사용된 환경은 다음 네 가지 입니다.</p>\n<ol>\n<li>classic cart-pole (x 3D)</li>\n<li>bipedal locomotion</li>\n<li>quadrupedal locomotion</li>\n<li>dynamically standing up for the biped</li>\n</ol>\n<h3 id=\"7-2-2-Architecture\"><a href=\"#7-2-2-Architecture\" class=\"headerlink\" title=\"7.2.2 Architecture\"></a>7.2.2 Architecture</h3><ul>\n<li>3D robot task에 대해서는 같은 모델을 사용하였습니다.<ul>\n<li>layers  = [100, 50, 25] 각각 tanh 사용. (Policy와 Value 네트워크 모두)</li>\n<li>Final output layer은 linear</li>\n</ul>\n</li>\n<li>Cartpole에 대해서는 1개의 layer 안에 20개의 hidden unit만 있는 linear policy를 사용했다고 합니다.</li>\n</ul>\n<h3 id=\"7-2-3-Task\"><a href=\"#7-2-3-Task\" class=\"headerlink\" title=\"7.2.3 Task\"></a>7.2.3 Task</h3><ul>\n<li>Cartpole<ul>\n<li>한 배치당 20 개의 trajectory를 모았고, maximum length는 1000입니다.</li>\n</ul>\n</li>\n<li>3D biped locomotion<ul>\n<li>33 dim state , 10 dim action</li>\n<li>50000 time step per batch</li>\n</ul>\n</li>\n<li>3D quadruped locomotion<ul>\n<li>29 dim state, 8 dim action</li>\n<li>200000 time step per batch</li>\n</ul>\n</li>\n<li>3D biped locomotion Standing<ul>\n<li>33 dim state , 10 dim action</li>\n<li>200000 time step per batch</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"7-2-3-results\"><a href=\"#7-2-3-results\" class=\"headerlink\" title=\"7.2.3 results\"></a>7.2.3 results</h3><p>cost의 관점에서 결과를 나타내었다고 합니다. Cost는 negative reward와 이것이 최소화 되었는가로 정의되었습니다.</p>\n<h4 id=\"7-2-3-1-Cartpole\"><a href=\"#7-2-3-1-Cartpole\" class=\"headerlink\" title=\"7.2.3.1 Cartpole\"></a>7.2.3.1 Cartpole</h4><center> <img width=\"500px\" src=\"https://www.dropbox.com/s/x9pbms1wvg38lda/Screenshot%202018-07-08%2011.08.22.png?dl=1\"> </center>\n\n<ul>\n<li>왼쪽 그림은 $\\gamma$를 0.99로 고정시켜놓은 상태에서 $\\lambda$를 변화시킴에 따라서 cost를 측정한 것입니다. </li>\n<li>오른쪽은 $\\gamma$와 $\\lambda$를 둘 다 변화 시키면서 성능을 그림으로 나타낸 표입니다. 흰색에 가까울 수록 좋은 성능입니다. </li>\n</ul>\n<h4 id=\"7-2-3-2-3D-BIPEDAL-LOCOMOTINO\"><a href=\"#7-2-3-2-3D-BIPEDAL-LOCOMOTINO\" class=\"headerlink\" title=\"7.2.3.2 3D BIPEDAL LOCOMOTINO\"></a>7.2.3.2 3D BIPEDAL LOCOMOTINO</h4><center> <img width=\"500px\" src=\"https://www.dropbox.com/s/i9wj4p6ijojsy82/Screenshot%202018-07-08%2011.25.08.png?dl=1\"> </center>\n\n\n<ul>\n<li>다른 random seed로 부터 9번 씩 시도한 결과를 mean을 취해서 사용합니다.</li>\n<li>Best performance는   $\\gamma \\in [0.99, 0.995]$ 그리고  $\\lambda \\in [0.96, 0.99]$일때. </li>\n<li>1000 iteration 후에 빠르고 부드럽고 안정적인 걸음거이가 나옵니다.</li>\n<li>실제로 걸린 시간은 0.01(타입스텝당 시간) <em> 50000(배치당 타임스텝) </em> 1000(배치) <em> 3600(초-&gt;시간) </em> 24 = 5.8일 정도가 걸렸습니다.</li>\n</ul>\n<h4 id=\"7-2-3-3-다른-ROBOT-TASKS\"><a href=\"#7-2-3-3-다른-ROBOT-TASKS\" class=\"headerlink\" title=\"7.2.3.3 다른 ROBOT TASKS\"></a>7.2.3.3 다른 ROBOT TASKS</h4><center> <img width=\"500px\" src=\"https://www.dropbox.com/s/fuuat65we52quht/Screenshot%202018-07-08%2011.33.11.png?dl=1\"> </center>\n\n<ul>\n<li>다른 로봇 TASK에 대해서는 아주 제한적인 실험만 진행합니다.(시간이 부족했던듯 하네요..)</li>\n<li>Quadruped에 대해서는 $\\gamma = 0.995$로 fix, $\\lambda \\in {0, 0.96}$</li>\n<li>Standingup에 대해서는 $\\gamma = 0.99$로 fix, $\\lambda \\in {0, 0.96}$</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"8-Discussion\"><a href=\"#8-Discussion\" class=\"headerlink\" title=\"8. Discussion\"></a>8. Discussion</h1><p><br></p>\n<h2 id=\"8-1-Main-discussion\"><a href=\"#8-1-Main-discussion\" class=\"headerlink\" title=\"8.1 Main discussion\"></a>8.1 Main discussion</h2><p>지금까지 복잡하고 어려운 control problem에서 Reinforcement Learning(RL)은 high sample complexity 때문에 제한이 되어왔습니다. 따라서 이 논문에서 그 제한을 풀고자 advantage function의 good estimate를 얻는 “variance reduction”에 대해 연구하였습니다.</p>\n<p>“Generalized Advantage Estimator(GAE)”라는 것을 제안했고, 이것은 bias-variance tradeoff를 조절하는 두 개의 parameter $\\gamma,\\lambda$를 가집니다.<br>또한 어떻게 Trust Region Policy Optimization과 value function을 optimize하는 Trust Region Algorithm의 idea를 합치는 지를 보였습니다.</p>\n<p>이렇게 함으로써 보다 더 복잡하고 어려운 control task들을 해결할 수 있었습니다.</p>\n<p>GAE의 실험적인 입증으로는 robotic locomotion을 simulation하는 domain입니다. 실험에서도 보여준 것처럼 [0.9, 0.99]의 범위에서 $\\lambda$의 적절한 중간의 값을 통해 best performance를 얻습니다. 좀 더 나아가 연구되어야할 점은 adaptive or automatic하도록 estimator parameter $\\gamma,\\lambda$를 조절하는 방법입니다.</p>\n<p><br></p>\n<h2 id=\"8-2-Future-work\"><a href=\"#8-2-Future-work\" class=\"headerlink\" title=\"8.2 Future work\"></a>8.2 Future work</h2><p>Value function estimation error와 Policy gradient estimation error사이의 관계를 알아낸다면, 우리는 Value function fitting에 더 잘 맞는 error metric(policy gradient estimation 의 정확성과 더 잘 맞는 value function)을 사용할 수 있습니다. 여기서 Policy와 Value function의 파라미터를 공유하는 모델을 만드는 것은 아주 흥미롭고 이점이 많습니다. 하지만 수렴을 보장하도록 적절한 numerical optimization을 제시해야 할 것입니다.</p>\n<p>추가적으로 DDPG는 별로라고 합니다. 그리고 TD(0)는 bias가 너무 크고, poor performance로 이끈다고 합니다. 특히나 이 논문에서는 low-dimention의 쉬운 문제들만 해결했습니다.</p>\n<center> <img width=\"500px\" src=\"https://www.dropbox.com/s/nhc7t9psul5lr3x/Screenshot%202018-07-08%2011.45.15.png?dl=1\"> </center>\n\n<p><br></p>\n<h2 id=\"8-3-FAQ\"><a href=\"#8-3-FAQ\" class=\"headerlink\" title=\"8.3 FAQ\"></a>8.3 FAQ</h2><ul>\n<li>Compatible features와는 무슨 관계?<ul>\n<li>Compatible features는 value function을 이용하는 policy gradient 알고리즘들과 함께 자주 언급됩니다.</li>\n<li>Actor Critic의 저자는 policy의 제한된 representation power때문에, policy gradient는 단지 advantage function space의 subspace에만 의존하게 됩니다.</li>\n<li>이 subspace는 compatible features에 의해 span됩니다.</li>\n<li>이 이론은 현재 문제 구조를 어떻게 이용해야 advantage function에 대해 더 나은 estimation을 할 수 있는 지에 대한 지침을 주지 않습니다. GAE 논문의 idea와 orthogonal합니다.</li>\n</ul>\n</li>\n<li>왜 Q function을 사용하지 않는가?<ul>\n<li>먼저 state-value function이 더 낮은 차원의 input을 가진다. 그래서 Q function보다 더 학습하기가 쉽습니다.</li>\n<li>두 번째로 이 논문에서 제안하는 방법으로는 high bias estimator에서 low bias estimator로 $\\lambda$를 통해서 부드럽게 interpolate를 할 수 있습니다.</li>\n<li>반면에 Q를 사용하면 단지 high-bias estimator 밖에 사용할 수 없습니다.</li>\n<li>특히나 return에 대한 one-step estimation은 엄두를 못낼 정도로 bias가 큽니다.</li>\n</ul>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<center> <img src=\"https://www.dropbox.com/s/p8gfpyo6xf9wm5w/Screen%20Shot%202018-07-18%20at%201.25.53%20AM.png?dl=1\" width=\"700\"> </center>\n\n<p>논문 저자 : John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan and Pieter Abbeel<br>논문 링크 : <a href=\"https://arxiv.org/pdf/1506.02438.pdf\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/pdf/1506.02438.pdf</a><br>Proceeding : ??<br>정리 : 양혁렬, 이동민</p>\n<hr>\n<h1 id=\"1-들어가며…\"><a href=\"#1-들어가며…\" class=\"headerlink\" title=\"1. 들어가며…\"></a>1. 들어가며…</h1><p>현존하는 Policy Gradient Method들의 목적은 누적되는 reward들을 optimization하는 것입니다. 하지만 학습할 때에 많은 양의 sample이 필요로 하고, 들어오는 data가 nonstationarity임에도 불구하고 stable and steady improvement가 어렵습니다.</p>\n<p>그래서 이 논문에서는 다음과 같은 방법을 제시합니다.</p>\n<ul>\n<li>TD($\\lambda$)와 유사한 advantage function의 exponentially-weighted estimator를 사용하여 policy gradient estimate의 variance를 줄이는 것 </li>\n<li>policy와 value function에 대한 Trust Region Optimization 사용하는 것</li>\n</ul>\n<p>3D locomotion tasks에 대한 empirical results는 다음과 같습니다.</p>\n<ul>\n<li>bipedal and quadrupedal simulated robots의 달리는 자세를 학습</li>\n<li>bipedal 사람이 땅에 누워있다가 일어서는 것을 학습</li>\n</ul>\n<p>※ TD($\\lambda$)를 혹시 모르실 경우도 있을 것 같아 간략하게 정리하였습니다. <a href=\"http://dongminlee.tistory.com/10\" target=\"_blank\" rel=\"noopener\">http://dongminlee.tistory.com/10</a> 를 먼저 보시고 아래의 내용을 봐주시기 바랍니다!</p>\n<p><br><br></p>\n<h1 id=\"2-Introduction\"><a href=\"#2-Introduction\" class=\"headerlink\" title=\"2. Introduction\"></a>2. Introduction</h1><p>기본적으로 “parameterized stochastic policy”를 가정합니다. 이 때 expected total returns의 gradient에 대한 unbiased estimate를 얻을 수 있는데 이것을 REINFORCE라고 부릅니다. 하지만 하나의 action의 결과가 과거와 미래의 action의 결과로 혼동되기 때문에 gradient estimator의 high variance는 시간에 따라 scaling됩니다.</p>\n<p>또 다른 방법은 Actor-Critic이 있습니다. 이 방법은 empirical returns보다 하나의 value function을 사용합니다. 또한 bias하고 lower variance를 가진 estimator입니다. 구체적으로 말하자면, high variance하다면 더 sampling을 하면 되는 반면에 bias는 매우 치명적입니다. 다시 말해 bias는 algorithm이 converge하는 데에 실패하거나 또는 local optimum이 아닌 poor solution에 converge하도록 만듭니다.</p>\n<p>따라서 이 논문에서는 $\\gamma\\in [0,1]$ and $\\lambda\\in [0,1]$에 의해 parameterized estimation scheme인 Generalized Advantage Estimator(GAE)를 다룹니다.</p>\n<p>이 논문을 대략적으로 요약하자면 다음과 같습니다.</p>\n<ul>\n<li>효과적인 variance reduction scheme인 GAE를 다룹니다. 또한 실험할 때 batch trust region algorithm에 더하여 다양한 algorithm들에 적용됩니다.</li>\n<li>value function에 대해 trust region optimization method를 사용합니다. 이렇게 함으로서 더 robust하고 efficient한 방법이 됩니다.</li>\n<li>A와 B를 합쳐서, 실험적으로 control task에 neural network policies를 learning하는 데에 있어서 효과적인 algorithm을 얻습니다. 이러한 결과는 high-demensional continuous control에 RL을 사용함으로서 state of the art로 확장되었습니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"3-Preliminaries\"><a href=\"#3-Preliminaries\" class=\"headerlink\" title=\"3. Preliminaries\"></a>3. Preliminaries</h1><p>먼저 policy optimization의 “undiscounted formulation”을 가정합니다. (undiscounted formulation에 주목합시다.)</p>\n<ul>\n<li>initial state $s_0$는 distribution $\\rho_0$으로부터 sampling된 것입니다.</li>\n<li>하나의 trajectory ($s_0, a_0, s_1, a_1, …$)는 terminal state에 도달될 때 까지 policy $a_t$ ~ $\\pi(a_t | s_t)$에 따라서 action을 sampling하고, dynamics $s_{t+1}$ ~ $P(s_{t+1} | s_t, a_t)$에 따라서 state를 sampling함으로써 생성됩니다.</li>\n<li>reward $r_t = r(s_t, a_t, s_{t+1})$은 매 time step마다 받아집니다.</li>\n<li>목표는 모든 policies에 대해 finite하다고 가정됨으로서 expected total reward $\\sum_{t=0}^{\\infty} r_t$를 maximize하는 것입니다.</li>\n</ul>\n<p>여기서 중요한 점은 $\\gamma$를 discount의 parameter로 사용하는 것이 아니라 “bias-variance tradeoff를 조절하는 parameter로 사용한다” 는 것입니다.</p>\n<p>policy gradient method는 gradient $g := \\nabla_\\theta \\mathbb{E} [\\sum_{t=0}^\\infty r_t]$를 반복적으로 estimate함으로써 expected total reward를 maximize하는 것인데, policy gradient에는 여러 다른 표현들이 있습니다.<br>$$g = \\mathbb{E} [\\sum_{t=0}^\\infty \\Phi_t \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$</p>\n<p>여기서 $\\Phi_t$는 아래 중의 하나일 수 있습니다.</p>\n<ol>\n<li>$\\sum_{t=0}^\\infty r_t$: trajectory의 total reward</li>\n<li>$\\sum_{t’=t}^\\infty r_t’$: action $a_t$ 후의 reward</li>\n<li>$\\sum_{t’=t}^\\infty r_t’ - b(s_t)$: 2의 baselined version</li>\n<li>$Q^\\pi (s_t, a_t)$: state-action value function</li>\n<li>$A^\\pi (s_t, a_t)$: advantage function</li>\n<li>$r_t + V^\\pi (s_{t+1}) - V^\\pi (s_t)$: TD residual</li>\n</ol>\n<p>위의 번호 중 4, 5, 6의 수식들은 다음의 정의를 사용합니다.</p>\n<ul>\n<li>$V^\\pi (s_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_t:\\infty} [\\sum_{l=0}^\\infty r_{t+1}]$</li>\n<li>$Q^\\pi (s_t, a_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty} [\\sum_{l=0}^\\infty r_{t+l}]$</li>\n<li>$A^\\pi (s_t, a_t) := Q^\\pi (s_t, a_t) - V^\\pi (s_t)$, (Advantage function)</li>\n</ul>\n<p>추가적으로 colon notation $a : b$는 포괄적인 범위 $(a, a+1, … , b)$입니다. (잘 기억해둡시다. 뒤에 계속해서 colon notation이 나옵니다.)</p>\n<p>여기서부터 parameter $\\gamma$에 대해 좀 더 자세히 알아봅시다. parameter $\\gamma$는 bias하면서 동시에 reward를 downweighting함으로써 variance를 줄입니다. 다시 말해 MDP의 discounted formulation에서 사용된 discounted factor와 일치하지만, 이 논문에서는 “$\\gamma$를 undiscounted MDP에서 variance reduction parameter”로 다룹니다. -&gt; 결과는 같지만, 의미와 의도가 다릅니다.</p>\n<p>discounted value function들은 다음과 같습니다.</p>\n<ul>\n<li>$V^{\\pi, \\gamma} (s_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_t:\\infty} [\\sum_{l=0}^\\infty \\gamma^l r_{t+l}]$ </li>\n<li>$Q^\\pi (s_t, a_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty} [\\sum_{l=0}^\\infty \\gamma^l r_{t+l}]$</li>\n<li>$A^{\\pi, \\gamma} (s_t, a_t) := Q^{\\pi, \\gamma} (s_t, a_t) - V^{\\pi, \\gamma} (s_t)$</li>\n</ul>\n<p>따라서 policy gradient에서의 discounted approximation은 다음과 같이 나타낼 수 있습니다.<br>$$g^\\gamma := \\mathbb{E}_{s_{0:\\infty} a_{0:\\infty}} [\\sum_{t=0}^\\infty A^{\\pi, \\gamma} (s_t, a_t) \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$<br>뒤이어 나오는 section에서는 위의 수식에서 $A^{\\pi, \\gamma}$에 대해 biased (but not too biased) estimator를 얻는 방법에 대해서 나옵니다.</p>\n<p>다음으로 advantage function에서 새롭게 접하는 “$\\gamma$-just estimator”의 notation에 대해 알아봅시다.</p>\n<ul>\n<li>먼저 $\\gamma$-just estimator는 $g^\\gamma$ ${}^1$를 estimate하기 위해 위의 수식에서 $A^{\\pi, \\gamma}$를 $\\gamma$-just estimator로 사용할 때, bias하지 않은 estimator라고 합시다. 그리고 이 $\\gamma$-just advantage estimator를 $\\hat{A}<em>t (s</em>{0:\\infty} , a_{0:\\infty})$라고 하고, 전체의 trajectory에 대한 하나의 function이라고 합시다.</li>\n<li>${}^1$에서 이 논문의 저자가 하고 싶은 말이 있는 것 같습니다. 개인적으로 $\\gamma$-just estimator를 이해하는 데에 있어서 중요한 주석이라 정확히 해석하고자 합니다.<ul>\n<li>$A^\\pi$를 $A^{\\pi, \\gamma}$로 사용함으로써 이미 bias하다라고 말했지만, “이 논문에서 하고자 하는 것은 $g^\\gamma$에 대해 unbiased estimate를 얻고 싶은 것입니다.” 하지만 undiscounted MDP의 policy gradient에 대해서는 당연히 $\\gamma$를 사용하기 때문에 biased estimate를 얻습니다. 개인적으로 이것은 일단 무시하고 $\\gamma$를 사용할 때 어떻게 unbiased estimate를 얻을 지에 대해 좀 더 포커스를 맞추고 있는 것 같습니다.</li>\n<li>그러니까 저 $A^{\\pi, \\gamma}$를 $\\gamma$-just estimator로 바꿔줌으로써 unbiased estimate를 하고 싶다는 것이 뒤이어 나오는 정의와 명제의 핵심이라고 할 수 있습니다.</li>\n</ul>\n</li>\n</ul>\n<p>Definition 1.<br>먼저 가정을 합니다. (가정을 바탕으로 이루어지는 정의라는 것을 주목합시다.) 만약<br>$$\\mathbb{E}_{s_{0:\\infty} a_{0:\\infty}} [\\hat{A}<em>t (s</em>{0:\\infty}, a_{0:\\infty}) \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)] = \\mathbb{E}_{s_{0:\\infty} a_{0:\\infty}} [A^{\\pi, \\gamma} (s_t, a_t) \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$<br>두 수식이 같다면, estimator $\\hat{A}_t$는 $\\gamma$-just입니다.</p>\n<p>그리고 만약 모든 t에 대해서 $\\hat{A}<em>t$이 $\\gamma$-just이라면, 다음과 같이 표현할 수 있습니다.<br>$$\\mathbb{E}</em>{s_{0:\\infty} a_{0:\\infty}} [\\hat{A}<em>t (s</em>{0:\\infty}, a_{0:\\infty}) \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)] = g^\\gamma$$<br>위의 수식이 바로 unbiased estimate입니다.</p>\n<p>$\\gamma$-just인 $\\hat{A}_t$에 대한 한가지 조건은 $\\hat{A}_t$이 두 가지 function $Q_t$ and $b_t$로 나뉠 수 있다는 것입니다.</p>\n<ul>\n<li>$Q_t$는 $\\gamma$-discounted Q-function의 unbiased estimator입니다.</li>\n<li>$b_t$는 action $a_t$전에 sampling된 states and actions의 arbitrary function이다. </li>\n</ul>\n<p>Proposition 1.<br>모든 $(s_t, a_t)$에 대해,<br>$$\\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty | s_t, a_t} [Q_t (s_{t:\\infty}, a_{t:\\infty})] = Q^{\\pi, \\gamma} (s_t, a_t)$$<br>로 인하여 $\\hat{A}<em>t$이<br>$$\\hat{A}</em>{s_{0:\\infty}, a_{0:\\infty}} = Q_t (s_{0:\\infty}, a_{0:\\infty}) - b_t (s_{0:t}, a_{0:t-1})$$<br>형태라고 가정합시다. (가정을 바탕으로 이루어지는 명제라는 점을 주목합시다.)</p>\n<p>그 때, $\\hat{A}_t$은 $\\gamma$-just입니다.</p>\n<p>이 명제에 대한 증명은 Appendix B에 있습니. 그리고 $\\hat{A}_t$에 대한 $\\gamma$-just advantage estimator들은 다음과 같습니다.</p>\n<ul>\n<li>$\\sum_{l=0}^\\infty \\gamma^l r_{t+1}$</li>\n<li>$A^{\\pi, \\gamma} (s_t, a_t)$</li>\n<li>$Q^{\\pi, \\gamma} (s_t, a_t)$</li>\n<li>$r_t + \\gamma V^{\\pi, \\gamma} (s_{t+1}) - V^{\\pi, \\gamma} (s_t)$</li>\n</ul>\n<p>증명을 보기 전에 먼저 Advantage function에 대해서 먼저 살펴봅시다. 아래의 내용은 Sutton이 쓴 논문인 Policy Gradient Methods for Reinforcement Learning with Function Approximation(2000)에서 나온 내용을 리뷰한 것입니다. </p>\n<center> <img src=\"https://www.dropbox.com/s/x737yq97ut6gp1a/Screen%20Shot%202018-07-15%20at%201.16.09%20PM.png?dl=1\" width=\"600\"> </center>\n\n<p>여기서는 “이 수식은 $\\sum_a \\frac{\\partial \\pi (s,a)}{\\partial \\theta} = 0$이기 때문에 가능해진다.”, “즉, 이 수식은 $\\pi(s,a)$의 gradient에만 dependent하기 때문에 advantage 역할을 하는 함수들을 넣어도 아무런 상관이 없다”라는 두 문장을 기억해둡시다.</p>\n<p>이제 증명을 보면 됩니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/e7sj2fwcm1f7hof/figure2.jpg?dl=1\" width=\"600\"> </center><br><center> <img src=\"https://www.dropbox.com/s/kyk4jes1202az4m/figure3.jpg?dl=1\" width=\"600\"> </center>\n\n<p>위의 증명 수식에서 Q와 b의 각각 세번째 수식과 마지막 수식을 자세히 봅시다.</p>\n<ol>\n<li><p>$Q$에 대한 증명</p>\n<ul>\n<li><p>빨간색 부분</p>\n<ul>\n<li>$\\mathbb{E}_{s_{t+1:\\infty}, a_{t+1:\\infty}} [Q_t (s_{0:\\infty}, a_{0:\\infty})]$를 쉽게 말하자면 “모든(과거, 현재, 미래) state와 action에 대해서 expectation을 $t+1$부터 $\\infty$까지 하겠다.”라는 말입니다. 이렇게 함으로써 원래는 $Q^\\pi (s_t, a_t)$가 나와야 맞는 건데, 앞에서 살펴봤다시피 4번째 수식 밑줄 친 자리에는 advantage 역할을 하는 함수들을 넣어도 결과값에 아무런 영향을 끼치지 않기 때문에 의도적으로 $A^\\pi (s_t, a_t)$로 바꾼 것입니다. 또한 이 증명을 바탕으로 $\\hat{A}$이 $\\gamma$-just라는 것을 표현하고 싶기 때문이라고 봐도 됩니다.</li>\n</ul>\n</li>\n<li><p>파란색 부분</p>\n<ul>\n<li>또한 $a_{0:t}$에서 $a_{0:t-1}$로 변한 것은 $A^\\pi (s_t, a_t)$에 baseline이 들어가기 때문에 바꿔준 것입니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>$b$에 대한 증명</p>\n<ul>\n<li>빨간색 부분<ul>\n<li>$\\mathbb{E}_{s_{t+1:\\infty}, a_{t+1:\\infty}} [\\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$가 0으로 바뀌는 것은 위에서 설명했듯이 $\\nabla_\\theta$자체가 $\\log \\pi_\\theta$만 gradient하기 때문에 이것을 expectation을 취하면 0이 됩니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<p><br><br></p>\n<h1 id=\"4-Advantage-Function-Estimation\"><a href=\"#4-Advantage-Function-Estimation\" class=\"headerlink\" title=\"4. Advantage Function Estimation\"></a>4. Advantage Function Estimation</h1><p>이번 section에는 discounted advantage function $A^{\\pi, \\gamma} (s_t, a_t)$의 accurate estimate $\\hat{A}_t$에 대해서 살펴봅시다. 이에 따른 수식은 다음과 같습니다.</p>\n<p>$$\\hat{g} = \\frac{1}{N} \\sum_{n=1}^N \\sum_{t=0}^\\infty \\hat{A}<em>t^n \\nabla</em>{\\theta} \\log \\pi_{\\theta}(a_t^n | s_t^n)$$</p>\n<p>여기서 n은 a batch of episodes에 대하여 index한 것입니다.</p>\n<p>$V$를 approximate value function이라고 합시다. 그리고 $\\delta_t^V = r_t + \\gamma V(s_{t+1}) - V(s_t)$이라고 합시다.</p>\n<p>만약 (이전과 마찬가지로 가정부터 합니다.) correct value function $V = V^{\\pi, \\gamma}$가 있다고 한다면, 이것은 $\\gamma$-just advantage estimator입니다. 실제로, $A^{\\pi, \\gamma}$의 unbiased estimator는 다음과 같습니다.<br>$$\\mathbb{E}_{s_{t+1}} [\\delta_t^{V^{\\pi, \\gamma}}] = \\mathbb{E}_{s_{t+1}} [r_t + \\gamma V^{\\pi, \\gamma} (s_{t+1}) - V^{\\pi, \\gamma} (s_t)]$$<br>$$= \\mathbb{E}_{s_{t+1}} [Q^{\\pi, \\gamma} (s_t, a_t) - V^{\\pi, \\gamma} (s_t)] = A^{\\pi, \\gamma} (s_t, a_t)$$<br>그러나, “이 estimator는 유일하게 $V = V^{\\pi, \\gamma}$에 대한 $\\gamma$-just입니다.” 다른 경우라면 이것은 biased policy gradient estimate일 것입니다. (우리가 하고 싶은 것은 $V$에 대해서만 unbiased estimator가 아니라 advantage function에 대해서 일반화된 unbiased estimator를 얻고 싶은 것입니다. 그래서 아래에서도 나오겠지만, $\\gamma$와 함께 $\\lambda$를 이용한 estimator가 나옵니다.ㅏ)</p>\n<p>그래서 $\\delta$에 대해 $k$의 sum으로 생각해봅시다. 이것을 $\\hat{A}_t^{(k)}$라고 하자. 그러면 아래와 같이 표현할 수 있습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/ra7hxksveg2hz45/figure4.jpg?dl=1\" width=\"600\"> </center>\n\n<p>$\\hat{A}_t^{(k)}$은 returns의 $k$-step estimate와 연관지을 수 있고, $\\delta_t^V = \\hat{A}_t^{(1)}$의 case와 유사하게도 $\\hat{A}_t^{(k)}$를 advantage function의 estimator로 생각할 수 있습니다.</p>\n<p>여기서 $k \\rightarrow \\infty$로 생각해보면 bias가 일반적으로 점점 더 작아집니다. 왜냐하면 $\\gamma^k V(s_{t+k})$가 점점 많이 discounted되서 $-V(s_t)$가 bias에 영향을 미치지 못하기 때문입니다. $k \\rightarrow \\infty$를 취하면 다음과 같은 수식이 나옵니다.</p>\n<p>$$\\hat{A}<em>t^{(\\infty)} = \\sum</em>{l=0}^\\infty \\gamma^l \\delta_{t+l}^V = -V(s_t) + \\sum_{l=0}^\\infty \\gamma^l r_{t+l}$$<br>우변의 수식과 같이 empirical returns에서 value function baseline을 뺀 것으로 나타낼 수 있습니다.</p>\n<p>Generalized Advantage Estimator GAE($\\gamma, \\lambda$)는 $k$-step estimators의 exponentially-weighted average로 나타낼 수 있습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/yg1ybmfkep3towu/figure5.jpg?dl=1\" width=\"600\"> </center>\n\n<p>(TD($\\lambda$)를 떠올려주세요..!)</p>\n<p>위의 마지막 수식에서도 알 수 있듯이, advantage estimator는 Bellman residual terms의 discounted sum과 관련있는 간단한 수식입니다. 다음 section에서 modified reward function을 가진 MDP에 returns로서의 관점에서 위의 마지막 수식을 더 자세히 살펴봅시다. 위의 수식은 TD($\\lambda$)와 많이 유사합니다. 그러나 TD($\\lambda$)는 value function를 estimator하고, 여기서는 advantage function을 estimator합니다.</p>\n<p>위의 수식에서 $\\lambda = 0$ and $\\lambda = 1$에 대해서는 특별한 case가 존재한다. 수식으로 표현하면 다음과 같습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/ufglxzanhcnbi36/figure6.jpg?dl=1\" width=\"600\"> </center>\n\n<ul>\n<li>$GAE(\\gamma, 1)$은 $V$의 정확도와 관계없이 $\\gamma$-just입니다. 그러나 returns의 sum때문에 high variance합니다.</li>\n<li>$GAE(\\gamma, 0)$은 $V = V^{\\pi, \\gamma}$에 대해 $\\gamma$-just입니다. 그리고 bias하지만 일반적으로 훨씬 lower variance를 가집니다. </li>\n<li>$0 &lt; \\lambda &lt; 1$에 대해 GAE는 parameter $\\lambda$를 control하여 bias와 variance사이에 compromise를 만듭니다.</li>\n</ul>\n<p>두 가지 별개의 parameter $\\gamma$ and $\\lambda$를 가진 advantage estimator는 bias-variance tradeoff에 도움을 줍니다. 그러나 이 두 가지 parameter는 각각 다른 목적을 가지고 작동합니다.</p>\n<ul>\n<li>$\\gamma$는 가장 중요하게 value function $V^{\\pi, \\gamma}$ 의 scale을 결정합니다. 또한 $\\gamma$는 $\\lambda$에 의존하지 않습니다. $\\gamma &lt; 1$로 정하는 것은 policy gradient estimate에서 bias합니다.</li>\n<li>반면에, $\\lambda &lt; 1$는 유일하게 value function이 부정확할 때 bias합니다. 그리고 경험상, $\\lambda$의 best value는 $\\gamma$의 best value보다 훨씬 더 낮습니다. 왜냐하면 $\\lambda$가 정확한 value function에 대해 $\\gamma$보다 훨씬 덜 bias하기 때문입니다.</li>\n</ul>\n<p>GAE를 사용할 때, $g^\\gamma$의 biased estimator를 구성할 수 있습니다. 수식은 다음과 같습니다.<br>$$g^\\gamma \\approx \\mathbb{E} [\\sum_{t=0}^\\infty] \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) \\hat{A}<em>t^{GAE(\\gamma, \\lambda)}] = \\mathbb{E} [\\sum</em>{t=0}^{\\infty} \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) \\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+1}^V]$$<br>여기서 $\\lambda = 1$일 때 동일해집니다.</p>\n<p><br><br></p>\n<h1 id=\"5-Interpretation-as-Reward-Shaping\"><a href=\"#5-Interpretation-as-Reward-Shaping\" class=\"headerlink\" title=\"5. Interpretation as Reward Shaping\"></a>5. Interpretation as Reward Shaping</h1><p>이번 section에서는 앞서 다뤘던 수식 $\\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^V$를 modified reward function의 MDP의 관점으로 생각해봅시다. 조금 더 구체적으로 말하자면, MDP에서 reward shaping transformation을 실행한 후에 적용된 extra discounted factor로서 $\\lambda$를 어떻게 볼 것인지에 대해서 다룹니다.</p>\n<p>(개인적인 comment) 한 가지 먼저 언급하자면, 본래의 목적은 reward shaping이 아니라 variance reduction입니다. 이번 section은 그저 이전에 이러한 개념이 있었고, gae를 다른 관점에서 생각해보자라는 뜻에서 나온 section인 것 같습니다. ‘아~ 이러한 개념이 있구나~!’ 정도로만 알면 될 것 같습니다. ‘gae가 reward shaping의 효과까지 있어서 이러한 section을 넣은 것일까?’ 라는 생각도 해봤지만 아직 잘 모르겠습니다. 실험부분에도 딱히 하지 않은 걸로 봐서는.. 아닌 것 같기도 하고.. 아직까지는 그저 ‘reward shaping의 관점에서 봤을 때에도 gae로 만들어줄 수 있다.’ 정도만 생각하려고 합니다.</p>\n<p>Reward Shaping이란 개념 자체는 일반적인 알고리즘 분야(최단경로 문제 등)에도 있습니다. 하지만 이 개념이 머신러닝에 적용된 건 Andrew Y. Ng이 쓴 Policy Invariance under Reward Transformations Theory and Application to Reward Shaping(1999) 논문입니다. 논문에서 Reward Shaping을 설명하는 부분은 간략하게 다음과 같습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/0w9cuxjkhz2mbcy/figure7.jpg?dl=1\" width=\"400\"> </center><br><center> <img src=\"https://www.dropbox.com/s/xcp9eqvcbv3268g/figure8.jpg?dl=1\" width=\"400\"> </center>\n\n<p>위의 글만 보고 이해가 잘 안됩니다. 그래서 다른 자료들을 찾던 중에 다음의 그림을 찾게 되었습니다. (아래의 그림은 Youtube에 있는 <a href=\"https://www.youtube.com/watch?v=xManAGjbx2k&amp;t=95s\" target=\"_blank\" rel=\"noopener\">Udacity 영상</a>에서 있는 그림입니다.)</p>\n<center> <img src=\"https://www.dropbox.com/s/87kacngez9cl2e6/figure9.jpg?dl=1\" width=\"400\"> </center>\n\n<p>이 그림을 통해서 Reward Shaping을 이해해봅시다.</p>\n<p>Reward Shaping(보상 형성)을 통해서 뭘 하고 싶은지 목적부터 봅시다. reward space가 sparse 한 경우에 reward가 너무 드문드문 나옵니다. 따라서 이것을 꾸준히 reward를 받을 수 있도록 바꿉니다. potential-based shaping function인 $\\Phi$를 만들어서 더하고 빼줍니다. 저 $\\Phi$ 자리에는 대표적으로 state value function이 많이 들어간다고 합니다. (아직 목적이 이해가 안될 수 있습니다. Reward Shaping에 대해서 다 보고 나서 다시 한 번 읽어봅시다.)</p>\n<p>위의 그림은 돌고래가 점프하여 불구멍을 통과해야하는 환경입니다. 하지만 저 불구멍을 통과하기 위해서는 (1) 점프도 해야하고, (2) 불도 피해야하고, (3) 알맞게 착지까지 완료해야합니다. 이렇게 해야 reward를 +1 얻습니다. </p>\n<p>생각해봅시다. 어느 세월에 점프도 해야하고.. 불도 피해야하고.. 알맞게 착지까지해서 reward를 +1 얻겠습니까? 따라서 그 전에도 잘하고 있는 지, 못하고 있는 지를 판단하기 위해, 다시 말해 reward를 꾸준히 받도록 다음과 같이 transformed reward function $\\tilde{r}$을 정의합니다.</p>\n<p>$$\\tilde{r} (s, a, s’) = r(s, a, s’) + \\gamma \\Phi (s’) - \\Phi (s)$$</p>\n<ul>\n<li>여기서 $\\Phi : S \\rightarrow \\mathbb{R}$를 state space에서의 arbitrary scalar-valued function을 나타냅니다. 그리고 $\\Phi$자리에는 대표적으로 state value function이 들어간다고 생각합시다.</li>\n<li>형태가 TD residual term의 결과와 비슷하지만 의미와 의도가 다릅니다. reward shaping은 sparse reward 때문이고, 이전 section에서 봤던 gae는 variance reduction때문에 나온 것입니다.</li>\n</ul>\n<center> <img src=\"https://www.dropbox.com/s/tkfpqniaazwfbld/figure10.jpg?dl=1\" width=\"600\"> </center>\n\n<p>이 transformation은 discounted advantage function $A^{\\pi, \\gamma}$으로도 둘 수 있습니다. state $s_t$를 시작하여 하나의 trajectory의 rewards의 discounted sum을 표현하면 다음과 같습니다.<br>$$\\sum_{l=0}^\\infty \\gamma^l \\tilde{r} (s_{t+l}, a_t, s_{t+l+1}) = \\sum_{l=0}^\\infty \\gamma^l r(s_{t+l}, a_{t+l}, s_{t+l+1}) - \\Phi(s_t)$$</p>\n<p>$\\tilde{Q}^{\\pi, \\gamma}, \\tilde{V}^{\\pi, \\gamma}, \\tilde{A}^{\\pi, \\gamma}$를  transformed MDP의 value function과 advantage function이라고 하면, 다음과 같은 수식이 나옵니다.<br>$$\\tilde{Q}^{\\pi, \\gamma} (s, a) = Q^{\\pi, \\gamma} (s, a) - \\Phi (s)$$<br>$$\\tilde{V}^{\\pi, \\gamma} (s, a) = V^{\\pi, \\gamma} (s, a) - \\Phi (s)$$<br>$$\\tilde{A}^{\\pi, \\gamma} (s, a) = (Q^{\\pi, \\gamma} (s, a) - \\Phi (s)) - (V^\\pi (s) - \\Phi (s)) = A^{\\pi, \\gamma} (s, a)$$</p>\n<p>이제 reward shaping의 idea를 가지고 어떻게 policy gradient estimate를 얻을 수 있는 지에 대해서 알아봅시다.</p>\n<ul>\n<li>$0 \\le \\lambda \\le 1$의 범위에 있는 steeper discount $\\gamma \\lambda$를 사용합니다.</li>\n<li>shaped reward $\\tilde{r}$는 Bellman residual term $\\delta^V$와 동일합니다.</li>\n<li>그리고 $\\Phi = V$와 같다고 보면 다음과 같은 수식이 나옵니다.</li>\n</ul>\n<p>$$\\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\tilde{r} (s_{t+l}, a_t, s_{t+l+1}) = \\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^V = \\hat{A}_t^{GAE(\\gamma, \\lambda)}$$<br>이렇게 shaped rewards의 $\\gamma \\lambda$-discounted sum을 고려함으로써 GAE를 얻을 수 있습니다. 더 정확하게 $\\lambda = 1$은 $g^\\gamma$의 unbiased estimate이고, 반면에 $\\lambda &lt; 1$은 biased estimate입니다.</p>\n<p>좀 더 나아가서, shaping transformation과 parameters $\\gamma$ and $\\lambda$의 결과를 보기 위해, response function $\\chi$를 이용하면 다음과 같은 수식이 나옵니다.<br>$$\\chi (l; s_t, a_t) = \\mathbb{E} [r_{t+l} | s_t, a_t] - \\mathbb{E} [r_{t+l} | s_t]$$<br>추가적으로 $A^{\\pi, \\gamma} (s, a) = \\sum_{l=0}^\\infty \\gamma^l \\chi (l; s, a)$입니다.<br>그래서 discounted policy gradient estimator는 다음과 같이 쓸 수 있다.<br>$$\\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) A^{\\pi, \\gamma} (s_t, a_t) = \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) \\sum_{l=0}^\\infty \\gamma^l \\chi (l; s, a)$$</p>\n<p><br><br></p>\n<h1 id=\"6-Value-Fuction-Estimation\"><a href=\"#6-Value-Fuction-Estimation\" class=\"headerlink\" title=\"6. Value Fuction Estimation\"></a>6. Value Fuction Estimation</h1><p>이번 section에서는 Trust Region Optimization Scheme에 따라 Value function을 Estimation합니다.</p>\n<p><br></p>\n<h2 id=\"6-1-Simplest-approach\"><a href=\"#6-1-Simplest-approach\" class=\"headerlink\" title=\"6.1 Simplest approach\"></a>6.1 Simplest approach</h2><p>$$minimize_{\\phi} \\sum_{n=1}^{N} \\vert\\vert V_{\\phi}(s_n) - \\hat{V_n} \\vert\\vert^{2}$$</p>\n<ul>\n<li>위는 가장 간단하게 non-linear approximation으로 푸는 방법입니다.</li>\n<li>$\\hat{V_t} = \\sum_{l=0}^{\\infty}\\gamma^l r_{t+l}$은 reward 에 대한 discounted sum을 의미합니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"6-2-Trust-region-method-to-optimize-the-value-function\"><a href=\"#6-2-Trust-region-method-to-optimize-the-value-function\" class=\"headerlink\" title=\"6.2 Trust region method to optimize the value function\"></a>6.2 Trust region method to optimize the value function</h2><ul>\n<li>Value function을 최적화 하기 위해 trust region method를 사용합니다.</li>\n<li>Trust region은 최근 데이터에 대해 overfitting되는 것을 막아줍니다.</li>\n</ul>\n<p>Trust region문제를 풀기 위해서는 다음 스텝을 따릅니다.</p>\n<ul>\n<li>$\\sigma^2 = \\frac{1}{N} \\sum_{n=1}^{N} \\vert\\vert V_{\\phi old}(s_n) - \\hat{V_n} \\vert\\vert^{2}$을 계산합니다.</li>\n<li>그 후에 다음과 같은 constrained opimization문제를 풉니다.</li>\n</ul>\n<p>$$minimize_{\\phi} \\, \\sum_{n=1}^N \\parallel V_\\phi (s_n) - \\hat{V}<em>n \\parallel^2$$<br>$$subject \\, \\, to \\, \\frac{1}{N} \\sum</em>{n=1}^N \\frac{\\parallel V_\\phi (s_n) - V_{\\phi old} (s_n) \\parallel^2}{2 \\sigma^2} \\le \\epsilon$$</p>\n<ul>\n<li>위의 수식은 사실 old Value function과 새로운 Value function KL distance가 $\\epsilon$ 다 작아야한다는 수식과 같습니다. Value function이 평균은 $V_{\\phi}(s)$이고 분산이 $\\sigma^2$인 conditional Gaussian distribution으로 parameterize되었을 뿐입니다.</li>\n</ul>\n<p><img width=\"400px\" src=\"https://www.dropbox.com/s/uw0v05feu8chqmc/Screenshot%202018-07-08%2010.04.38.png?dl=1\"> </p>\n<p>이 trust region문제의 답을 conjudate gradient algorithm을 이용하여 근사값을 구할 수 있습니다. 특히, 다음과 같은  quadratic program을 풀게됩니다.</p>\n<p>$$minimize_{\\phi} \\, g^T (\\phi - \\phi_{old})$$<br>$$subject \\, \\, to \\, \\frac{1}{N} \\sum_{n=1}^N (\\phi - \\phi_{old})^T H(\\phi - \\phi_{old}) \\le \\epsilon$$</p>\n<ul>\n<li>여기서 $g$는 objective 의 gradient입니다.</li>\n<li>$j_n = \\nabla_{\\phi} V_{\\phi}(s_n)$일때, $H = \\frac{1}{N} \\sum_{n} j_n j^T_n$이며, $H$는 objective의 hessian에 대해서 gaussian newton method로 근사한 값입니다. 따라서, value function을 conditional probability로 해석한다면 Fisher information matrix가 됩니다.</li>\n<li>구현할때의 방법은 TRPO에서 사용한 방법과 모두 같습니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"7-Experiments\"><a href=\"#7-Experiments\" class=\"headerlink\" title=\"7.Experiments\"></a>7.Experiments</h1><p>실험은 다음 두 가지 물음에 대해서 디자인 되었습니다.</p>\n<ul>\n<li>GAE에 따라서 episodic total reward를 최적화 할때, $\\lambda$와 $\\gamma$가 변함에 따라서 어떤 경험적인 효과를 볼 수 있는지?</li>\n<li>GAE와 trust region alogorithm을 policy와 value function 모두에 함께 사용했을 때 어려운 문제에 적용되는 큰 뉴럴넷을 최적화 할 수 있을지?</li>\n</ul>\n<p><br></p>\n<h2 id=\"7-1-Policy-Optimization-Algorithm\"><a href=\"#7-1-Policy-Optimization-Algorithm\" class=\"headerlink\" title=\"7.1 Policy Optimization Algorithm\"></a>7.1 Policy Optimization Algorithm</h2><p>Policy update는 TRPO를 사용합니다. TRPO에 대한 설명은 여기서는 생략하겠습니다. TRPO 포스트를 보고 돌아와주세요!</p>\n<ul>\n<li><p>이전 TRPO에서 이미 TRPO와 다른 많은 알고리즘들을 비교하였기 때문에, 여기서는 똑같은 짓을 반복하지 않고 $\\lambda$, $\\gamma$가 변함에 따라 어떤 영향이 있는 지에 대한 실험에 집중하겠다고 합니다. (귀찮았던거죠..)</p>\n</li>\n<li><p>TRPO를 적용한 GAE의 최종 알고리즘은 다음과 같습니다.</p>\n</li>\n</ul>\n<center> <img width=\"600px\" src=\"https://www.dropbox.com/s/b1klz11f2frrvg4/Screenshot%202018-07-08%2010.33.54.png?dl=1\"> </center><br><center> <img width=\"300px\" src=\"https://www.dropbox.com/s/35fxte05pqtiel7/Screenshot%202018-07-08%2010.35.22.png?dl=1\"> </center><br><center> <img width=\"300px\" src=\"https://www.dropbox.com/s/u35pjow3w50bvz1/Screenshot%202018-07-08%2010.36.03.png?dl=1\"> </center>\n\n<ul>\n<li>여기서 주의할 점은 Policy update($\\theta_i \\rightarrow \\theta_{i+1}$)에서 $V_{\\phi_i}$를 사용했다는 점입니다.</li>\n<li>만약 Value function을 먼저 update하게 된다면 추가적인 bias가 발생합니다.</li>\n<li>극단적으로 생각해보아서, 우리가 Value function을 완벽하게 overfit을 해낸다면 Bellman residual($r_t + \\gamma V(s_{t+1}) - V(S_t)$)은 0이 됩니다. 그럼 Policy gradient의 estimation도 거의 0이 될 것입니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"7-2-Expermint-details\"><a href=\"#7-2-Expermint-details\" class=\"headerlink\" title=\"7.2 Expermint details\"></a>7.2 Expermint details</h2><h3 id=\"7-2-1-Environment\"><a href=\"#7-2-1-Environment\" class=\"headerlink\" title=\"7.2.1 Environment\"></a>7.2.1 Environment</h3><p>실험에서 사용된 환경은 다음 네 가지 입니다.</p>\n<ol>\n<li>classic cart-pole (x 3D)</li>\n<li>bipedal locomotion</li>\n<li>quadrupedal locomotion</li>\n<li>dynamically standing up for the biped</li>\n</ol>\n<h3 id=\"7-2-2-Architecture\"><a href=\"#7-2-2-Architecture\" class=\"headerlink\" title=\"7.2.2 Architecture\"></a>7.2.2 Architecture</h3><ul>\n<li>3D robot task에 대해서는 같은 모델을 사용하였습니다.<ul>\n<li>layers  = [100, 50, 25] 각각 tanh 사용. (Policy와 Value 네트워크 모두)</li>\n<li>Final output layer은 linear</li>\n</ul>\n</li>\n<li>Cartpole에 대해서는 1개의 layer 안에 20개의 hidden unit만 있는 linear policy를 사용했다고 합니다.</li>\n</ul>\n<h3 id=\"7-2-3-Task\"><a href=\"#7-2-3-Task\" class=\"headerlink\" title=\"7.2.3 Task\"></a>7.2.3 Task</h3><ul>\n<li>Cartpole<ul>\n<li>한 배치당 20 개의 trajectory를 모았고, maximum length는 1000입니다.</li>\n</ul>\n</li>\n<li>3D biped locomotion<ul>\n<li>33 dim state , 10 dim action</li>\n<li>50000 time step per batch</li>\n</ul>\n</li>\n<li>3D quadruped locomotion<ul>\n<li>29 dim state, 8 dim action</li>\n<li>200000 time step per batch</li>\n</ul>\n</li>\n<li>3D biped locomotion Standing<ul>\n<li>33 dim state , 10 dim action</li>\n<li>200000 time step per batch</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"7-2-3-results\"><a href=\"#7-2-3-results\" class=\"headerlink\" title=\"7.2.3 results\"></a>7.2.3 results</h3><p>cost의 관점에서 결과를 나타내었다고 합니다. Cost는 negative reward와 이것이 최소화 되었는가로 정의되었습니다.</p>\n<h4 id=\"7-2-3-1-Cartpole\"><a href=\"#7-2-3-1-Cartpole\" class=\"headerlink\" title=\"7.2.3.1 Cartpole\"></a>7.2.3.1 Cartpole</h4><center> <img width=\"500px\" src=\"https://www.dropbox.com/s/x9pbms1wvg38lda/Screenshot%202018-07-08%2011.08.22.png?dl=1\"> </center>\n\n<ul>\n<li>왼쪽 그림은 $\\gamma$를 0.99로 고정시켜놓은 상태에서 $\\lambda$를 변화시킴에 따라서 cost를 측정한 것입니다. </li>\n<li>오른쪽은 $\\gamma$와 $\\lambda$를 둘 다 변화 시키면서 성능을 그림으로 나타낸 표입니다. 흰색에 가까울 수록 좋은 성능입니다. </li>\n</ul>\n<h4 id=\"7-2-3-2-3D-BIPEDAL-LOCOMOTINO\"><a href=\"#7-2-3-2-3D-BIPEDAL-LOCOMOTINO\" class=\"headerlink\" title=\"7.2.3.2 3D BIPEDAL LOCOMOTINO\"></a>7.2.3.2 3D BIPEDAL LOCOMOTINO</h4><center> <img width=\"500px\" src=\"https://www.dropbox.com/s/i9wj4p6ijojsy82/Screenshot%202018-07-08%2011.25.08.png?dl=1\"> </center>\n\n\n<ul>\n<li>다른 random seed로 부터 9번 씩 시도한 결과를 mean을 취해서 사용합니다.</li>\n<li>Best performance는   $\\gamma \\in [0.99, 0.995]$ 그리고  $\\lambda \\in [0.96, 0.99]$일때. </li>\n<li>1000 iteration 후에 빠르고 부드럽고 안정적인 걸음거이가 나옵니다.</li>\n<li>실제로 걸린 시간은 0.01(타입스텝당 시간) <em> 50000(배치당 타임스텝) </em> 1000(배치) <em> 3600(초-&gt;시간) </em> 24 = 5.8일 정도가 걸렸습니다.</li>\n</ul>\n<h4 id=\"7-2-3-3-다른-ROBOT-TASKS\"><a href=\"#7-2-3-3-다른-ROBOT-TASKS\" class=\"headerlink\" title=\"7.2.3.3 다른 ROBOT TASKS\"></a>7.2.3.3 다른 ROBOT TASKS</h4><center> <img width=\"500px\" src=\"https://www.dropbox.com/s/fuuat65we52quht/Screenshot%202018-07-08%2011.33.11.png?dl=1\"> </center>\n\n<ul>\n<li>다른 로봇 TASK에 대해서는 아주 제한적인 실험만 진행합니다.(시간이 부족했던듯 하네요..)</li>\n<li>Quadruped에 대해서는 $\\gamma = 0.995$로 fix, $\\lambda \\in {0, 0.96}$</li>\n<li>Standingup에 대해서는 $\\gamma = 0.99$로 fix, $\\lambda \\in {0, 0.96}$</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"8-Discussion\"><a href=\"#8-Discussion\" class=\"headerlink\" title=\"8. Discussion\"></a>8. Discussion</h1><p><br></p>\n<h2 id=\"8-1-Main-discussion\"><a href=\"#8-1-Main-discussion\" class=\"headerlink\" title=\"8.1 Main discussion\"></a>8.1 Main discussion</h2><p>지금까지 복잡하고 어려운 control problem에서 Reinforcement Learning(RL)은 high sample complexity 때문에 제한이 되어왔습니다. 따라서 이 논문에서 그 제한을 풀고자 advantage function의 good estimate를 얻는 “variance reduction”에 대해 연구하였습니다.</p>\n<p>“Generalized Advantage Estimator(GAE)”라는 것을 제안했고, 이것은 bias-variance tradeoff를 조절하는 두 개의 parameter $\\gamma,\\lambda$를 가집니다.<br>또한 어떻게 Trust Region Policy Optimization과 value function을 optimize하는 Trust Region Algorithm의 idea를 합치는 지를 보였습니다.</p>\n<p>이렇게 함으로써 보다 더 복잡하고 어려운 control task들을 해결할 수 있었습니다.</p>\n<p>GAE의 실험적인 입증으로는 robotic locomotion을 simulation하는 domain입니다. 실험에서도 보여준 것처럼 [0.9, 0.99]의 범위에서 $\\lambda$의 적절한 중간의 값을 통해 best performance를 얻습니다. 좀 더 나아가 연구되어야할 점은 adaptive or automatic하도록 estimator parameter $\\gamma,\\lambda$를 조절하는 방법입니다.</p>\n<p><br></p>\n<h2 id=\"8-2-Future-work\"><a href=\"#8-2-Future-work\" class=\"headerlink\" title=\"8.2 Future work\"></a>8.2 Future work</h2><p>Value function estimation error와 Policy gradient estimation error사이의 관계를 알아낸다면, 우리는 Value function fitting에 더 잘 맞는 error metric(policy gradient estimation 의 정확성과 더 잘 맞는 value function)을 사용할 수 있습니다. 여기서 Policy와 Value function의 파라미터를 공유하는 모델을 만드는 것은 아주 흥미롭고 이점이 많습니다. 하지만 수렴을 보장하도록 적절한 numerical optimization을 제시해야 할 것입니다.</p>\n<p>추가적으로 DDPG는 별로라고 합니다. 그리고 TD(0)는 bias가 너무 크고, poor performance로 이끈다고 합니다. 특히나 이 논문에서는 low-dimention의 쉬운 문제들만 해결했습니다.</p>\n<center> <img width=\"500px\" src=\"https://www.dropbox.com/s/nhc7t9psul5lr3x/Screenshot%202018-07-08%2011.45.15.png?dl=1\"> </center>\n\n<p><br></p>\n<h2 id=\"8-3-FAQ\"><a href=\"#8-3-FAQ\" class=\"headerlink\" title=\"8.3 FAQ\"></a>8.3 FAQ</h2><ul>\n<li>Compatible features와는 무슨 관계?<ul>\n<li>Compatible features는 value function을 이용하는 policy gradient 알고리즘들과 함께 자주 언급됩니다.</li>\n<li>Actor Critic의 저자는 policy의 제한된 representation power때문에, policy gradient는 단지 advantage function space의 subspace에만 의존하게 됩니다.</li>\n<li>이 subspace는 compatible features에 의해 span됩니다.</li>\n<li>이 이론은 현재 문제 구조를 어떻게 이용해야 advantage function에 대해 더 나은 estimation을 할 수 있는 지에 대한 지침을 주지 않습니다. GAE 논문의 idea와 orthogonal합니다.</li>\n</ul>\n</li>\n<li>왜 Q function을 사용하지 않는가?<ul>\n<li>먼저 state-value function이 더 낮은 차원의 input을 가진다. 그래서 Q function보다 더 학습하기가 쉽습니다.</li>\n<li>두 번째로 이 논문에서 제안하는 방법으로는 high bias estimator에서 low bias estimator로 $\\lambda$를 통해서 부드럽게 interpolate를 할 수 있습니다.</li>\n<li>반면에 Q를 사용하면 단지 high-bias estimator 밖에 사용할 수 없습니다.</li>\n<li>특히나 return에 대한 one-step estimation은 엄두를 못낼 정도로 bias가 큽니다.</li>\n</ul>\n</li>\n</ul>\n"},{"title":"PG Travel Guide","date":"2018-06-28T16:11:26.000Z","author":"공민서, 김동민, 양혁렬, 이동민, 이웅원, 장수영, 차금강","subtitle":"피지여행에 관한 개략적 기록","_content":"\n---\n# 1. Policy Gradient의 세계로\n\n반갑습니다! 저희는 PG여행을 위해 모인 PG탐험대입니다. 강화학습하면 보통 Q-learning을 떠올립니다. 그렇지만 오래전부터 Policy Gradient라는 Q-learning 못지 않은 훌륭한 방법론이 연구되어 왔고, 최근에는 강화학습의 최정점의 기술로 자리매김하고 있습니다. 강화학습의 아버지인 Sutton의 논문을 필두로 하여 기존의 DQN보다 뛰어난 성능을 내는 DPG와 DDPG, 그리고 현재 가장 주목받는 강화학습 연구자인 John Schulmann의 TRPO, GAE, PPO와 이를 이해하기 위해 필요한 Natural Policy Gardient까지 더불어 살펴보고자 합니다.\n\n<center> <img src=\"https://www.dropbox.com/s/ezxjriu1gfyztma/Policy%20Optimization%20in%20the%20RL%20Algorithm%20Landscape.png?dl=1\" width=\"800\"> </center>\n\n위의 그림은 강화학습 알고리즘 landscape에서 Policy Optimization의 관점을 중점적으로 하여 나타낸 그림입니다. 위의 그림에서 빨간색 작은 숫자로 나타낸 것이 저희 PG여행에서 다룰 논문들입니다. 순서는 다음과 같습니다.\n\n1. Sutton_PG\n2. DPG\n3. DDPG\n4. NPG\n5. TRPO\n6. PPO\n7. GAE\n\n위와 같이 총 7가지 논문들을 리뷰하여 블로그로 정리하였습니다. 이 블로그에는 각각의 기술을 제안한 논문을 PG탐험대분들이 자세하게 리뷰한 포스트들이 있습니다. 우리나라에서 PG에 대해서 이렇게 자세하게 리뷰한 포스트들은 없었다고 감히 말씀드리고 싶습니다. 본 글에서는 이 포스트들을 읽기 전에 전체 내용을 개략적으로 소개하고 각각의 포스트들로 안내하고자 합니다. 자, 저희와 함께 PG여행을 즐겨보시겠습니까?\n\n<br><br>\n# 2. \\[Sutton PG\\] Policy gradient methods for reinforcement learning with function approximation\n[Sutton PG 여행하기](../../../06/15/sutton-pg/)\n[Sutton PG Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\n[Sutton PG 여행하기](../../../06/15/sutton-pg/)\n[Sutton PG Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\n<br><br>\n# 3. \\[DPG\\] Deterministic policy gradient algorithms\n[DPG 여행하기](../../../06/16/dpg/)\n[DPG Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\n[DPG 여행하기](../../../06/16/dpg/)\n[DPG Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\n<br><br>\n# 4. \\[DDPG\\] Continuous control with deep reinforcement learning\n[DDPG 여행하기](../../../06/23/ddpg/)\n[DDPG Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\n[DDPG 여행하기](../../../06/23/ddpg/)\n[DDPG Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\n<br><br>\n# 5. \\[NPG\\] A natural policy gradient\n[NPG 여행하기](../../../06/14/2018-06-15-npg/)\n[NPG Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\n일반적으로 Policy Gradient 기법들은 앞에서 확일할 수 있듯이 Objective funtction, $\\eta(\\pi_\\theta)$을 최대화하는 쪽으로 파라미터를 업데이트하면서 복잡한 비선형 함수을 최적화 합니다. 일반적으로 파라미터를 업데이트하는 방법은 Objective function를 각 파라미터들로 편미분(Gradient를 구하여)하여 그 변수들의 변화에 따른 Objective function의 변화를 따라 파라미터를 업데이트 합니다. 하지만 컴퓨터는 해석학적으로 목적함수를 최적화하는 방법을 따를 수 없어 수치학적으로 접근을 합니다. 수치학적으로 Objective function을 최대화하려면 대부분의 경우 반복적인 방법, $\\theta_{k+1} = \\theta_k + \\bigtriangledown_\\theta \\eta(\\theta)$,을 사용합니다.\n\n해석학적으로는 $\\bigtriangledown_\\theta\\eta(\\theta)$를 편미분하여 직접 Objective function의 Gradient를 구할 수 있겠지만 수치학적으로는 매우 작은 크기를 가지는 $d\\theta$에 대해 $\\eta(\\theta+d\\theta)-\\eta(\\theta)$를 구함으로써 Gradient를 간접적으로 얻을 수 있습니다. 그리고 이것은 간단하게 파라미터들의 집합인 $\\theta$의 Positive-Definite Matrix인 $G(\\theta)$에 의해 $G^{-1}\\eta(\\theta)$로 구해질 수 있습니다.\n\n하지만 여기서 Objective function은 매우 많은 파라미터로 구성되어 있으며 파라미터들은 매 스텝마다 업데이트되기 때문에 $G(\\theta)$를 통해 얻은 Gradient는 매번 달라지며 파라미터들의 성능을 추정하는 정확한 Metric이 될 수 없습니다. 본 논문에서는 Fisher Information Matrix라는 것을 도입하여 파라미터의 업데이트 등에 따라 영향을 받지 않는 Metric을 구해내며 이를 통해 파라미터를 업데이트하는 것을 소개하고 있습니다.\n\n[NPG 여행하기](../../../06/14/2018-06-15-npg/)\n[NPG Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\n<br><br>\n# 6. \\[TRPO\\] Trust region policy optimization\n[TRPO 여행하기](blog link)\n[TRPO Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\nPG기법이 각광을 받게 된 계기는 아마도 TRPO 때문이 아닌가 싶습니다. DQN이라는 인공지능이 아타리게임을 사람보다 잘 플레이했던 영상을 보고 충격을 받았던 사람들이 많았던 것처럼 TRPO가 난제로 여겨졌던 서기, 걷기, 뛰기 같은 로봇에 적용할 수 있는 continuous action을 잘 수행하는 것을 보고 많은 사람들이 놀라워했습니다. (솔직히 처음 아타리게임을 봤을 때 만큼은 아니지만...) 이것들은 DQN으로는 달성할 수 없었던 난제였습니다. 그렇지만 거인의 어깨 위에서 세상을 바라보았던 많은 과학자들이 그랬던 것처럼 Schulmann도 TRPO를 갑자기 생각낸 것은 아닙니다. policy를 업데이트할 때 기존의 policy와 너무 많이 달라지면 문제가 생깁니다. 일단, state distribution이 변합니다. state distribution은 간단히 얘기하면 각각의 state를 방문하는 빈도를 의미한다고 할 수 있습니다. policy가 많이 바뀌면 기존의 state distribution을 그대로 이용할 수 없고 이렇게 되면 policy gradient를 구하기 어려워집니다. 그래서 착안한 것이 policy의 변화 정도를 제한하는 것입니다. 여기서 우리는 stochastic policy, 즉, 확률적 policy를 이용한다고 전제하고 있기 때문에, policy의 변화는 policy 확률 분포의 변화를 의미합니다. 현재 policy와 새로운 policy 사이의 확률 분포의 변화를 어떻게 측정할 수 있을까요? Schulmann은 [KL divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)라는 척도를 이용하였습니다. 이 변화량을 특정값 이하로 제한하고 이것을 만족하는 영역을 trust region이라고 불렀습니다. 그렇지만 문제는 풀기에는 복잡했습니다. 이것을 풀기 위해서 여러 번의 근사화를 적용시켰습니다. objective function에는 linear approximation을 적용하고 trust region constraint에는 second-order approximation을 적용하면 문제가 좀 더 풀기 쉬운 형태로 바뀐다는 것을 알아냈습니다. 이것은 사실 natural gradient를 이용하는 것이었습니다. 이러한 이유로 TRPO를 이해하기 위해서 우리는 natural gradient도 살펴보았던 것입니다.\n\n[TRPO 여행하기](blog link)\n[TRPO Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\n<br><br>\n# 7. \\[PPO\\] Proximal policy optimization algorithms\n[PPO 여행하기](blog link)\n[PPO Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\nPPO는 TRPO의 연장선상에 있는 기술이라고 할 수 있습니다. 사실 Schulmann은 TRPO 논문을 쓸 당시 이미 PPO를 구상하고 있었던 것 같습니다. TRPO 논문에도 PPO와 관련있는 내용이 좀 나옵니다. 아이디어 자체는 간단합니다. 그래서그런지 PPO는 arxiv에만 발표되었고 논문도 비교적 짧습니다. TRPO에서는 문제를 단순하게 만들기 위해서 최적화 문제를 여러 번 변형시켰습니다. PPO는 clip이라는 개념을 사용합니다. TRPO에서 이용했던 surrogate objective function을 reward가 특정값 이상이거나 이하가 될 때 더 이상 변화시키지 않는 것입니다.\n\n[PPO 여행하기](blog link)\n[PPO Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\n<br><br>\n# 8. \\[GAE\\] High-Dimensional Continuous Control Using Generalized Advantage Estimation\n[GAE 여행하기](blog link)\n[GAE Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\nTRPO가 나오고 난 뒤로도 복잡하고 어려운 control problem에서 Reinforcement Learning(RL)은 high sample complexity 때문에 제한이 되어왔습니다. 따라서 이 논문에서 그 제한을 풀고자 advantage function의 good estimate를 얻는 \"variance reduction\"에 대해 연구하였습니다.\n\n\"Generalized Advantage Estimator(GAE)\"라는 것을 제안했고, 이것은 bias-variance tradeoff를 조절하는 두 개의 parameter $\\gamma,\\lambda$를 가집니다.\n또한 어떻게 Trust Region Policy Optimization과 value function을 optimize하는 Trust Region Algorithm의 idea를 합치는 지를 보였습니다.\n\n이렇게 함으로써 보다 더 복잡하고 어려운 control task들을 해결할 수 있었습니다.\n\nGAE의 실험적인 입증으로는 robotic locomotion을 simulation하는 domain입니다. 실험에서도 보여준 것처럼 [0.9, 0.99]의 범위에서 $\\lambda$의 적절한 중간의 값을 통해 best performance를 얻습니다. 좀 더 나아가 연구되어야할 점은 adaptive or automatic하도록 estimator parameter $\\gamma,\\lambda$를 조절하는 방법입니다.\n\n추가적으로 앞으로 연구되어야할 부분은 만약 Value function estimation error와 Policy gradient estimation error 사이의 관계를 알아낸다면, Value function fitting에 더 잘 맞는 error metric(policy gradient estimation의 정확성과 더 잘 맞는 value function)을 사용할 수 있을 것입니다. 여기서 Policy와 Value function의 파라미터를 공유하는 모델을 만드는 것은 아주 흥미롭고 이점이 많습니다. 하지만 수렴을 보장하도록 적절한 numerical optimization을 제시해야 할 것입니다.\n\n[GAE 여행하기](blog link)\n[GAE Code](https://github.com/reinforcement-learning-kr/pg_travel)","source":"_posts/pg-travel-guide.md","raw":"---\ntitle: PG Travel Guide\ndate: 2018-06-29 01:11:26\ntags: [\"프로젝트\", \"피지여행\"]\ncategories: 프로젝트\nauthor: 공민서, 김동민, 양혁렬, 이동민, 이웅원, 장수영, 차금강\nsubtitle: 피지여행에 관한 개략적 기록\n---\n\n---\n# 1. Policy Gradient의 세계로\n\n반갑습니다! 저희는 PG여행을 위해 모인 PG탐험대입니다. 강화학습하면 보통 Q-learning을 떠올립니다. 그렇지만 오래전부터 Policy Gradient라는 Q-learning 못지 않은 훌륭한 방법론이 연구되어 왔고, 최근에는 강화학습의 최정점의 기술로 자리매김하고 있습니다. 강화학습의 아버지인 Sutton의 논문을 필두로 하여 기존의 DQN보다 뛰어난 성능을 내는 DPG와 DDPG, 그리고 현재 가장 주목받는 강화학습 연구자인 John Schulmann의 TRPO, GAE, PPO와 이를 이해하기 위해 필요한 Natural Policy Gardient까지 더불어 살펴보고자 합니다.\n\n<center> <img src=\"https://www.dropbox.com/s/ezxjriu1gfyztma/Policy%20Optimization%20in%20the%20RL%20Algorithm%20Landscape.png?dl=1\" width=\"800\"> </center>\n\n위의 그림은 강화학습 알고리즘 landscape에서 Policy Optimization의 관점을 중점적으로 하여 나타낸 그림입니다. 위의 그림에서 빨간색 작은 숫자로 나타낸 것이 저희 PG여행에서 다룰 논문들입니다. 순서는 다음과 같습니다.\n\n1. Sutton_PG\n2. DPG\n3. DDPG\n4. NPG\n5. TRPO\n6. PPO\n7. GAE\n\n위와 같이 총 7가지 논문들을 리뷰하여 블로그로 정리하였습니다. 이 블로그에는 각각의 기술을 제안한 논문을 PG탐험대분들이 자세하게 리뷰한 포스트들이 있습니다. 우리나라에서 PG에 대해서 이렇게 자세하게 리뷰한 포스트들은 없었다고 감히 말씀드리고 싶습니다. 본 글에서는 이 포스트들을 읽기 전에 전체 내용을 개략적으로 소개하고 각각의 포스트들로 안내하고자 합니다. 자, 저희와 함께 PG여행을 즐겨보시겠습니까?\n\n<br><br>\n# 2. \\[Sutton PG\\] Policy gradient methods for reinforcement learning with function approximation\n[Sutton PG 여행하기](../../../06/15/sutton-pg/)\n[Sutton PG Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\n[Sutton PG 여행하기](../../../06/15/sutton-pg/)\n[Sutton PG Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\n<br><br>\n# 3. \\[DPG\\] Deterministic policy gradient algorithms\n[DPG 여행하기](../../../06/16/dpg/)\n[DPG Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\n[DPG 여행하기](../../../06/16/dpg/)\n[DPG Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\n<br><br>\n# 4. \\[DDPG\\] Continuous control with deep reinforcement learning\n[DDPG 여행하기](../../../06/23/ddpg/)\n[DDPG Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\n[DDPG 여행하기](../../../06/23/ddpg/)\n[DDPG Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\n<br><br>\n# 5. \\[NPG\\] A natural policy gradient\n[NPG 여행하기](../../../06/14/2018-06-15-npg/)\n[NPG Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\n일반적으로 Policy Gradient 기법들은 앞에서 확일할 수 있듯이 Objective funtction, $\\eta(\\pi_\\theta)$을 최대화하는 쪽으로 파라미터를 업데이트하면서 복잡한 비선형 함수을 최적화 합니다. 일반적으로 파라미터를 업데이트하는 방법은 Objective function를 각 파라미터들로 편미분(Gradient를 구하여)하여 그 변수들의 변화에 따른 Objective function의 변화를 따라 파라미터를 업데이트 합니다. 하지만 컴퓨터는 해석학적으로 목적함수를 최적화하는 방법을 따를 수 없어 수치학적으로 접근을 합니다. 수치학적으로 Objective function을 최대화하려면 대부분의 경우 반복적인 방법, $\\theta_{k+1} = \\theta_k + \\bigtriangledown_\\theta \\eta(\\theta)$,을 사용합니다.\n\n해석학적으로는 $\\bigtriangledown_\\theta\\eta(\\theta)$를 편미분하여 직접 Objective function의 Gradient를 구할 수 있겠지만 수치학적으로는 매우 작은 크기를 가지는 $d\\theta$에 대해 $\\eta(\\theta+d\\theta)-\\eta(\\theta)$를 구함으로써 Gradient를 간접적으로 얻을 수 있습니다. 그리고 이것은 간단하게 파라미터들의 집합인 $\\theta$의 Positive-Definite Matrix인 $G(\\theta)$에 의해 $G^{-1}\\eta(\\theta)$로 구해질 수 있습니다.\n\n하지만 여기서 Objective function은 매우 많은 파라미터로 구성되어 있으며 파라미터들은 매 스텝마다 업데이트되기 때문에 $G(\\theta)$를 통해 얻은 Gradient는 매번 달라지며 파라미터들의 성능을 추정하는 정확한 Metric이 될 수 없습니다. 본 논문에서는 Fisher Information Matrix라는 것을 도입하여 파라미터의 업데이트 등에 따라 영향을 받지 않는 Metric을 구해내며 이를 통해 파라미터를 업데이트하는 것을 소개하고 있습니다.\n\n[NPG 여행하기](../../../06/14/2018-06-15-npg/)\n[NPG Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\n<br><br>\n# 6. \\[TRPO\\] Trust region policy optimization\n[TRPO 여행하기](blog link)\n[TRPO Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\nPG기법이 각광을 받게 된 계기는 아마도 TRPO 때문이 아닌가 싶습니다. DQN이라는 인공지능이 아타리게임을 사람보다 잘 플레이했던 영상을 보고 충격을 받았던 사람들이 많았던 것처럼 TRPO가 난제로 여겨졌던 서기, 걷기, 뛰기 같은 로봇에 적용할 수 있는 continuous action을 잘 수행하는 것을 보고 많은 사람들이 놀라워했습니다. (솔직히 처음 아타리게임을 봤을 때 만큼은 아니지만...) 이것들은 DQN으로는 달성할 수 없었던 난제였습니다. 그렇지만 거인의 어깨 위에서 세상을 바라보았던 많은 과학자들이 그랬던 것처럼 Schulmann도 TRPO를 갑자기 생각낸 것은 아닙니다. policy를 업데이트할 때 기존의 policy와 너무 많이 달라지면 문제가 생깁니다. 일단, state distribution이 변합니다. state distribution은 간단히 얘기하면 각각의 state를 방문하는 빈도를 의미한다고 할 수 있습니다. policy가 많이 바뀌면 기존의 state distribution을 그대로 이용할 수 없고 이렇게 되면 policy gradient를 구하기 어려워집니다. 그래서 착안한 것이 policy의 변화 정도를 제한하는 것입니다. 여기서 우리는 stochastic policy, 즉, 확률적 policy를 이용한다고 전제하고 있기 때문에, policy의 변화는 policy 확률 분포의 변화를 의미합니다. 현재 policy와 새로운 policy 사이의 확률 분포의 변화를 어떻게 측정할 수 있을까요? Schulmann은 [KL divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)라는 척도를 이용하였습니다. 이 변화량을 특정값 이하로 제한하고 이것을 만족하는 영역을 trust region이라고 불렀습니다. 그렇지만 문제는 풀기에는 복잡했습니다. 이것을 풀기 위해서 여러 번의 근사화를 적용시켰습니다. objective function에는 linear approximation을 적용하고 trust region constraint에는 second-order approximation을 적용하면 문제가 좀 더 풀기 쉬운 형태로 바뀐다는 것을 알아냈습니다. 이것은 사실 natural gradient를 이용하는 것이었습니다. 이러한 이유로 TRPO를 이해하기 위해서 우리는 natural gradient도 살펴보았던 것입니다.\n\n[TRPO 여행하기](blog link)\n[TRPO Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\n<br><br>\n# 7. \\[PPO\\] Proximal policy optimization algorithms\n[PPO 여행하기](blog link)\n[PPO Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\nPPO는 TRPO의 연장선상에 있는 기술이라고 할 수 있습니다. 사실 Schulmann은 TRPO 논문을 쓸 당시 이미 PPO를 구상하고 있었던 것 같습니다. TRPO 논문에도 PPO와 관련있는 내용이 좀 나옵니다. 아이디어 자체는 간단합니다. 그래서그런지 PPO는 arxiv에만 발표되었고 논문도 비교적 짧습니다. TRPO에서는 문제를 단순하게 만들기 위해서 최적화 문제를 여러 번 변형시켰습니다. PPO는 clip이라는 개념을 사용합니다. TRPO에서 이용했던 surrogate objective function을 reward가 특정값 이상이거나 이하가 될 때 더 이상 변화시키지 않는 것입니다.\n\n[PPO 여행하기](blog link)\n[PPO Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\n<br><br>\n# 8. \\[GAE\\] High-Dimensional Continuous Control Using Generalized Advantage Estimation\n[GAE 여행하기](blog link)\n[GAE Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\nTRPO가 나오고 난 뒤로도 복잡하고 어려운 control problem에서 Reinforcement Learning(RL)은 high sample complexity 때문에 제한이 되어왔습니다. 따라서 이 논문에서 그 제한을 풀고자 advantage function의 good estimate를 얻는 \"variance reduction\"에 대해 연구하였습니다.\n\n\"Generalized Advantage Estimator(GAE)\"라는 것을 제안했고, 이것은 bias-variance tradeoff를 조절하는 두 개의 parameter $\\gamma,\\lambda$를 가집니다.\n또한 어떻게 Trust Region Policy Optimization과 value function을 optimize하는 Trust Region Algorithm의 idea를 합치는 지를 보였습니다.\n\n이렇게 함으로써 보다 더 복잡하고 어려운 control task들을 해결할 수 있었습니다.\n\nGAE의 실험적인 입증으로는 robotic locomotion을 simulation하는 domain입니다. 실험에서도 보여준 것처럼 [0.9, 0.99]의 범위에서 $\\lambda$의 적절한 중간의 값을 통해 best performance를 얻습니다. 좀 더 나아가 연구되어야할 점은 adaptive or automatic하도록 estimator parameter $\\gamma,\\lambda$를 조절하는 방법입니다.\n\n추가적으로 앞으로 연구되어야할 부분은 만약 Value function estimation error와 Policy gradient estimation error 사이의 관계를 알아낸다면, Value function fitting에 더 잘 맞는 error metric(policy gradient estimation의 정확성과 더 잘 맞는 value function)을 사용할 수 있을 것입니다. 여기서 Policy와 Value function의 파라미터를 공유하는 모델을 만드는 것은 아주 흥미롭고 이점이 많습니다. 하지만 수렴을 보장하도록 적절한 numerical optimization을 제시해야 할 것입니다.\n\n[GAE 여행하기](blog link)\n[GAE Code](https://github.com/reinforcement-learning-kr/pg_travel)","slug":"pg-travel-guide","published":1,"updated":"2018-07-23T10:02:21.420Z","_id":"cjjgzyg4a0001xo15qjhkwruc","comments":1,"layout":"post","photos":[],"link":"","content":"<hr>\n<h1 id=\"1-Policy-Gradient의-세계로\"><a href=\"#1-Policy-Gradient의-세계로\" class=\"headerlink\" title=\"1. Policy Gradient의 세계로\"></a>1. Policy Gradient의 세계로</h1><p>반갑습니다! 저희는 PG여행을 위해 모인 PG탐험대입니다. 강화학습하면 보통 Q-learning을 떠올립니다. 그렇지만 오래전부터 Policy Gradient라는 Q-learning 못지 않은 훌륭한 방법론이 연구되어 왔고, 최근에는 강화학습의 최정점의 기술로 자리매김하고 있습니다. 강화학습의 아버지인 Sutton의 논문을 필두로 하여 기존의 DQN보다 뛰어난 성능을 내는 DPG와 DDPG, 그리고 현재 가장 주목받는 강화학습 연구자인 John Schulmann의 TRPO, GAE, PPO와 이를 이해하기 위해 필요한 Natural Policy Gardient까지 더불어 살펴보고자 합니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/ezxjriu1gfyztma/Policy%20Optimization%20in%20the%20RL%20Algorithm%20Landscape.png?dl=1\" width=\"800\"> </center>\n\n<p>위의 그림은 강화학습 알고리즘 landscape에서 Policy Optimization의 관점을 중점적으로 하여 나타낸 그림입니다. 위의 그림에서 빨간색 작은 숫자로 나타낸 것이 저희 PG여행에서 다룰 논문들입니다. 순서는 다음과 같습니다.</p>\n<ol>\n<li>Sutton_PG</li>\n<li>DPG</li>\n<li>DDPG</li>\n<li>NPG</li>\n<li>TRPO</li>\n<li>PPO</li>\n<li>GAE</li>\n</ol>\n<p>위와 같이 총 7가지 논문들을 리뷰하여 블로그로 정리하였습니다. 이 블로그에는 각각의 기술을 제안한 논문을 PG탐험대분들이 자세하게 리뷰한 포스트들이 있습니다. 우리나라에서 PG에 대해서 이렇게 자세하게 리뷰한 포스트들은 없었다고 감히 말씀드리고 싶습니다. 본 글에서는 이 포스트들을 읽기 전에 전체 내용을 개략적으로 소개하고 각각의 포스트들로 안내하고자 합니다. 자, 저희와 함께 PG여행을 즐겨보시겠습니까?</p>\n<p><br><br></p>\n<h1 id=\"2-Sutton-PG-Policy-gradient-methods-for-reinforcement-learning-with-function-approximation\"><a href=\"#2-Sutton-PG-Policy-gradient-methods-for-reinforcement-learning-with-function-approximation\" class=\"headerlink\" title=\"2. [Sutton PG] Policy gradient methods for reinforcement learning with function approximation\"></a>2. [Sutton PG] Policy gradient methods for reinforcement learning with function approximation</h1><p><a href=\"../../../06/15/sutton-pg/\">Sutton PG 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">Sutton PG Code</a></p>\n<p><a href=\"../../../06/15/sutton-pg/\">Sutton PG 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">Sutton PG Code</a></p>\n<p><br><br></p>\n<h1 id=\"3-DPG-Deterministic-policy-gradient-algorithms\"><a href=\"#3-DPG-Deterministic-policy-gradient-algorithms\" class=\"headerlink\" title=\"3. [DPG] Deterministic policy gradient algorithms\"></a>3. [DPG] Deterministic policy gradient algorithms</h1><p><a href=\"../../../06/16/dpg/\">DPG 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">DPG Code</a></p>\n<p><a href=\"../../../06/16/dpg/\">DPG 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">DPG Code</a></p>\n<p><br><br></p>\n<h1 id=\"4-DDPG-Continuous-control-with-deep-reinforcement-learning\"><a href=\"#4-DDPG-Continuous-control-with-deep-reinforcement-learning\" class=\"headerlink\" title=\"4. [DDPG] Continuous control with deep reinforcement learning\"></a>4. [DDPG] Continuous control with deep reinforcement learning</h1><p><a href=\"../../../06/23/ddpg/\">DDPG 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">DDPG Code</a></p>\n<p><a href=\"../../../06/23/ddpg/\">DDPG 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">DDPG Code</a></p>\n<p><br><br></p>\n<h1 id=\"5-NPG-A-natural-policy-gradient\"><a href=\"#5-NPG-A-natural-policy-gradient\" class=\"headerlink\" title=\"5. [NPG] A natural policy gradient\"></a>5. [NPG] A natural policy gradient</h1><p><a href=\"../../../06/14/2018-06-15-npg/\">NPG 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">NPG Code</a></p>\n<p>일반적으로 Policy Gradient 기법들은 앞에서 확일할 수 있듯이 Objective funtction, $\\eta(\\pi_\\theta)$을 최대화하는 쪽으로 파라미터를 업데이트하면서 복잡한 비선형 함수을 최적화 합니다. 일반적으로 파라미터를 업데이트하는 방법은 Objective function를 각 파라미터들로 편미분(Gradient를 구하여)하여 그 변수들의 변화에 따른 Objective function의 변화를 따라 파라미터를 업데이트 합니다. 하지만 컴퓨터는 해석학적으로 목적함수를 최적화하는 방법을 따를 수 없어 수치학적으로 접근을 합니다. 수치학적으로 Objective function을 최대화하려면 대부분의 경우 반복적인 방법, $\\theta_{k+1} = \\theta_k + \\bigtriangledown_\\theta \\eta(\\theta)$,을 사용합니다.</p>\n<p>해석학적으로는 $\\bigtriangledown_\\theta\\eta(\\theta)$를 편미분하여 직접 Objective function의 Gradient를 구할 수 있겠지만 수치학적으로는 매우 작은 크기를 가지는 $d\\theta$에 대해 $\\eta(\\theta+d\\theta)-\\eta(\\theta)$를 구함으로써 Gradient를 간접적으로 얻을 수 있습니다. 그리고 이것은 간단하게 파라미터들의 집합인 $\\theta$의 Positive-Definite Matrix인 $G(\\theta)$에 의해 $G^{-1}\\eta(\\theta)$로 구해질 수 있습니다.</p>\n<p>하지만 여기서 Objective function은 매우 많은 파라미터로 구성되어 있으며 파라미터들은 매 스텝마다 업데이트되기 때문에 $G(\\theta)$를 통해 얻은 Gradient는 매번 달라지며 파라미터들의 성능을 추정하는 정확한 Metric이 될 수 없습니다. 본 논문에서는 Fisher Information Matrix라는 것을 도입하여 파라미터의 업데이트 등에 따라 영향을 받지 않는 Metric을 구해내며 이를 통해 파라미터를 업데이트하는 것을 소개하고 있습니다.</p>\n<p><a href=\"../../../06/14/2018-06-15-npg/\">NPG 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">NPG Code</a></p>\n<p><br><br></p>\n<h1 id=\"6-TRPO-Trust-region-policy-optimization\"><a href=\"#6-TRPO-Trust-region-policy-optimization\" class=\"headerlink\" title=\"6. [TRPO] Trust region policy optimization\"></a>6. [TRPO] Trust region policy optimization</h1><p><a href=\"blog link\">TRPO 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">TRPO Code</a></p>\n<p>PG기법이 각광을 받게 된 계기는 아마도 TRPO 때문이 아닌가 싶습니다. DQN이라는 인공지능이 아타리게임을 사람보다 잘 플레이했던 영상을 보고 충격을 받았던 사람들이 많았던 것처럼 TRPO가 난제로 여겨졌던 서기, 걷기, 뛰기 같은 로봇에 적용할 수 있는 continuous action을 잘 수행하는 것을 보고 많은 사람들이 놀라워했습니다. (솔직히 처음 아타리게임을 봤을 때 만큼은 아니지만…) 이것들은 DQN으로는 달성할 수 없었던 난제였습니다. 그렇지만 거인의 어깨 위에서 세상을 바라보았던 많은 과학자들이 그랬던 것처럼 Schulmann도 TRPO를 갑자기 생각낸 것은 아닙니다. policy를 업데이트할 때 기존의 policy와 너무 많이 달라지면 문제가 생깁니다. 일단, state distribution이 변합니다. state distribution은 간단히 얘기하면 각각의 state를 방문하는 빈도를 의미한다고 할 수 있습니다. policy가 많이 바뀌면 기존의 state distribution을 그대로 이용할 수 없고 이렇게 되면 policy gradient를 구하기 어려워집니다. 그래서 착안한 것이 policy의 변화 정도를 제한하는 것입니다. 여기서 우리는 stochastic policy, 즉, 확률적 policy를 이용한다고 전제하고 있기 때문에, policy의 변화는 policy 확률 분포의 변화를 의미합니다. 현재 policy와 새로운 policy 사이의 확률 분포의 변화를 어떻게 측정할 수 있을까요? Schulmann은 <a href=\"https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\" target=\"_blank\" rel=\"noopener\">KL divergence</a>라는 척도를 이용하였습니다. 이 변화량을 특정값 이하로 제한하고 이것을 만족하는 영역을 trust region이라고 불렀습니다. 그렇지만 문제는 풀기에는 복잡했습니다. 이것을 풀기 위해서 여러 번의 근사화를 적용시켰습니다. objective function에는 linear approximation을 적용하고 trust region constraint에는 second-order approximation을 적용하면 문제가 좀 더 풀기 쉬운 형태로 바뀐다는 것을 알아냈습니다. 이것은 사실 natural gradient를 이용하는 것이었습니다. 이러한 이유로 TRPO를 이해하기 위해서 우리는 natural gradient도 살펴보았던 것입니다.</p>\n<p><a href=\"blog link\">TRPO 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">TRPO Code</a></p>\n<p><br><br></p>\n<h1 id=\"7-PPO-Proximal-policy-optimization-algorithms\"><a href=\"#7-PPO-Proximal-policy-optimization-algorithms\" class=\"headerlink\" title=\"7. [PPO] Proximal policy optimization algorithms\"></a>7. [PPO] Proximal policy optimization algorithms</h1><p><a href=\"blog link\">PPO 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">PPO Code</a></p>\n<p>PPO는 TRPO의 연장선상에 있는 기술이라고 할 수 있습니다. 사실 Schulmann은 TRPO 논문을 쓸 당시 이미 PPO를 구상하고 있었던 것 같습니다. TRPO 논문에도 PPO와 관련있는 내용이 좀 나옵니다. 아이디어 자체는 간단합니다. 그래서그런지 PPO는 arxiv에만 발표되었고 논문도 비교적 짧습니다. TRPO에서는 문제를 단순하게 만들기 위해서 최적화 문제를 여러 번 변형시켰습니다. PPO는 clip이라는 개념을 사용합니다. TRPO에서 이용했던 surrogate objective function을 reward가 특정값 이상이거나 이하가 될 때 더 이상 변화시키지 않는 것입니다.</p>\n<p><a href=\"blog link\">PPO 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">PPO Code</a></p>\n<p><br><br></p>\n<h1 id=\"8-GAE-High-Dimensional-Continuous-Control-Using-Generalized-Advantage-Estimation\"><a href=\"#8-GAE-High-Dimensional-Continuous-Control-Using-Generalized-Advantage-Estimation\" class=\"headerlink\" title=\"8. [GAE] High-Dimensional Continuous Control Using Generalized Advantage Estimation\"></a>8. [GAE] High-Dimensional Continuous Control Using Generalized Advantage Estimation</h1><p><a href=\"blog link\">GAE 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">GAE Code</a></p>\n<p>TRPO가 나오고 난 뒤로도 복잡하고 어려운 control problem에서 Reinforcement Learning(RL)은 high sample complexity 때문에 제한이 되어왔습니다. 따라서 이 논문에서 그 제한을 풀고자 advantage function의 good estimate를 얻는 “variance reduction”에 대해 연구하였습니다.</p>\n<p>“Generalized Advantage Estimator(GAE)”라는 것을 제안했고, 이것은 bias-variance tradeoff를 조절하는 두 개의 parameter $\\gamma,\\lambda$를 가집니다.<br>또한 어떻게 Trust Region Policy Optimization과 value function을 optimize하는 Trust Region Algorithm의 idea를 합치는 지를 보였습니다.</p>\n<p>이렇게 함으로써 보다 더 복잡하고 어려운 control task들을 해결할 수 있었습니다.</p>\n<p>GAE의 실험적인 입증으로는 robotic locomotion을 simulation하는 domain입니다. 실험에서도 보여준 것처럼 [0.9, 0.99]의 범위에서 $\\lambda$의 적절한 중간의 값을 통해 best performance를 얻습니다. 좀 더 나아가 연구되어야할 점은 adaptive or automatic하도록 estimator parameter $\\gamma,\\lambda$를 조절하는 방법입니다.</p>\n<p>추가적으로 앞으로 연구되어야할 부분은 만약 Value function estimation error와 Policy gradient estimation error 사이의 관계를 알아낸다면, Value function fitting에 더 잘 맞는 error metric(policy gradient estimation의 정확성과 더 잘 맞는 value function)을 사용할 수 있을 것입니다. 여기서 Policy와 Value function의 파라미터를 공유하는 모델을 만드는 것은 아주 흥미롭고 이점이 많습니다. 하지만 수렴을 보장하도록 적절한 numerical optimization을 제시해야 할 것입니다.</p>\n<p><a href=\"blog link\">GAE 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">GAE Code</a></p>\n","site":{"data":{}},"excerpt":"","more":"<hr>\n<h1 id=\"1-Policy-Gradient의-세계로\"><a href=\"#1-Policy-Gradient의-세계로\" class=\"headerlink\" title=\"1. Policy Gradient의 세계로\"></a>1. Policy Gradient의 세계로</h1><p>반갑습니다! 저희는 PG여행을 위해 모인 PG탐험대입니다. 강화학습하면 보통 Q-learning을 떠올립니다. 그렇지만 오래전부터 Policy Gradient라는 Q-learning 못지 않은 훌륭한 방법론이 연구되어 왔고, 최근에는 강화학습의 최정점의 기술로 자리매김하고 있습니다. 강화학습의 아버지인 Sutton의 논문을 필두로 하여 기존의 DQN보다 뛰어난 성능을 내는 DPG와 DDPG, 그리고 현재 가장 주목받는 강화학습 연구자인 John Schulmann의 TRPO, GAE, PPO와 이를 이해하기 위해 필요한 Natural Policy Gardient까지 더불어 살펴보고자 합니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/ezxjriu1gfyztma/Policy%20Optimization%20in%20the%20RL%20Algorithm%20Landscape.png?dl=1\" width=\"800\"> </center>\n\n<p>위의 그림은 강화학습 알고리즘 landscape에서 Policy Optimization의 관점을 중점적으로 하여 나타낸 그림입니다. 위의 그림에서 빨간색 작은 숫자로 나타낸 것이 저희 PG여행에서 다룰 논문들입니다. 순서는 다음과 같습니다.</p>\n<ol>\n<li>Sutton_PG</li>\n<li>DPG</li>\n<li>DDPG</li>\n<li>NPG</li>\n<li>TRPO</li>\n<li>PPO</li>\n<li>GAE</li>\n</ol>\n<p>위와 같이 총 7가지 논문들을 리뷰하여 블로그로 정리하였습니다. 이 블로그에는 각각의 기술을 제안한 논문을 PG탐험대분들이 자세하게 리뷰한 포스트들이 있습니다. 우리나라에서 PG에 대해서 이렇게 자세하게 리뷰한 포스트들은 없었다고 감히 말씀드리고 싶습니다. 본 글에서는 이 포스트들을 읽기 전에 전체 내용을 개략적으로 소개하고 각각의 포스트들로 안내하고자 합니다. 자, 저희와 함께 PG여행을 즐겨보시겠습니까?</p>\n<p><br><br></p>\n<h1 id=\"2-Sutton-PG-Policy-gradient-methods-for-reinforcement-learning-with-function-approximation\"><a href=\"#2-Sutton-PG-Policy-gradient-methods-for-reinforcement-learning-with-function-approximation\" class=\"headerlink\" title=\"2. [Sutton PG] Policy gradient methods for reinforcement learning with function approximation\"></a>2. [Sutton PG] Policy gradient methods for reinforcement learning with function approximation</h1><p><a href=\"../../../06/15/sutton-pg/\">Sutton PG 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">Sutton PG Code</a></p>\n<p><a href=\"../../../06/15/sutton-pg/\">Sutton PG 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">Sutton PG Code</a></p>\n<p><br><br></p>\n<h1 id=\"3-DPG-Deterministic-policy-gradient-algorithms\"><a href=\"#3-DPG-Deterministic-policy-gradient-algorithms\" class=\"headerlink\" title=\"3. [DPG] Deterministic policy gradient algorithms\"></a>3. [DPG] Deterministic policy gradient algorithms</h1><p><a href=\"../../../06/16/dpg/\">DPG 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">DPG Code</a></p>\n<p><a href=\"../../../06/16/dpg/\">DPG 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">DPG Code</a></p>\n<p><br><br></p>\n<h1 id=\"4-DDPG-Continuous-control-with-deep-reinforcement-learning\"><a href=\"#4-DDPG-Continuous-control-with-deep-reinforcement-learning\" class=\"headerlink\" title=\"4. [DDPG] Continuous control with deep reinforcement learning\"></a>4. [DDPG] Continuous control with deep reinforcement learning</h1><p><a href=\"../../../06/23/ddpg/\">DDPG 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">DDPG Code</a></p>\n<p><a href=\"../../../06/23/ddpg/\">DDPG 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">DDPG Code</a></p>\n<p><br><br></p>\n<h1 id=\"5-NPG-A-natural-policy-gradient\"><a href=\"#5-NPG-A-natural-policy-gradient\" class=\"headerlink\" title=\"5. [NPG] A natural policy gradient\"></a>5. [NPG] A natural policy gradient</h1><p><a href=\"../../../06/14/2018-06-15-npg/\">NPG 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">NPG Code</a></p>\n<p>일반적으로 Policy Gradient 기법들은 앞에서 확일할 수 있듯이 Objective funtction, $\\eta(\\pi_\\theta)$을 최대화하는 쪽으로 파라미터를 업데이트하면서 복잡한 비선형 함수을 최적화 합니다. 일반적으로 파라미터를 업데이트하는 방법은 Objective function를 각 파라미터들로 편미분(Gradient를 구하여)하여 그 변수들의 변화에 따른 Objective function의 변화를 따라 파라미터를 업데이트 합니다. 하지만 컴퓨터는 해석학적으로 목적함수를 최적화하는 방법을 따를 수 없어 수치학적으로 접근을 합니다. 수치학적으로 Objective function을 최대화하려면 대부분의 경우 반복적인 방법, $\\theta_{k+1} = \\theta_k + \\bigtriangledown_\\theta \\eta(\\theta)$,을 사용합니다.</p>\n<p>해석학적으로는 $\\bigtriangledown_\\theta\\eta(\\theta)$를 편미분하여 직접 Objective function의 Gradient를 구할 수 있겠지만 수치학적으로는 매우 작은 크기를 가지는 $d\\theta$에 대해 $\\eta(\\theta+d\\theta)-\\eta(\\theta)$를 구함으로써 Gradient를 간접적으로 얻을 수 있습니다. 그리고 이것은 간단하게 파라미터들의 집합인 $\\theta$의 Positive-Definite Matrix인 $G(\\theta)$에 의해 $G^{-1}\\eta(\\theta)$로 구해질 수 있습니다.</p>\n<p>하지만 여기서 Objective function은 매우 많은 파라미터로 구성되어 있으며 파라미터들은 매 스텝마다 업데이트되기 때문에 $G(\\theta)$를 통해 얻은 Gradient는 매번 달라지며 파라미터들의 성능을 추정하는 정확한 Metric이 될 수 없습니다. 본 논문에서는 Fisher Information Matrix라는 것을 도입하여 파라미터의 업데이트 등에 따라 영향을 받지 않는 Metric을 구해내며 이를 통해 파라미터를 업데이트하는 것을 소개하고 있습니다.</p>\n<p><a href=\"../../../06/14/2018-06-15-npg/\">NPG 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">NPG Code</a></p>\n<p><br><br></p>\n<h1 id=\"6-TRPO-Trust-region-policy-optimization\"><a href=\"#6-TRPO-Trust-region-policy-optimization\" class=\"headerlink\" title=\"6. [TRPO] Trust region policy optimization\"></a>6. [TRPO] Trust region policy optimization</h1><p><a href=\"blog link\">TRPO 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">TRPO Code</a></p>\n<p>PG기법이 각광을 받게 된 계기는 아마도 TRPO 때문이 아닌가 싶습니다. DQN이라는 인공지능이 아타리게임을 사람보다 잘 플레이했던 영상을 보고 충격을 받았던 사람들이 많았던 것처럼 TRPO가 난제로 여겨졌던 서기, 걷기, 뛰기 같은 로봇에 적용할 수 있는 continuous action을 잘 수행하는 것을 보고 많은 사람들이 놀라워했습니다. (솔직히 처음 아타리게임을 봤을 때 만큼은 아니지만…) 이것들은 DQN으로는 달성할 수 없었던 난제였습니다. 그렇지만 거인의 어깨 위에서 세상을 바라보았던 많은 과학자들이 그랬던 것처럼 Schulmann도 TRPO를 갑자기 생각낸 것은 아닙니다. policy를 업데이트할 때 기존의 policy와 너무 많이 달라지면 문제가 생깁니다. 일단, state distribution이 변합니다. state distribution은 간단히 얘기하면 각각의 state를 방문하는 빈도를 의미한다고 할 수 있습니다. policy가 많이 바뀌면 기존의 state distribution을 그대로 이용할 수 없고 이렇게 되면 policy gradient를 구하기 어려워집니다. 그래서 착안한 것이 policy의 변화 정도를 제한하는 것입니다. 여기서 우리는 stochastic policy, 즉, 확률적 policy를 이용한다고 전제하고 있기 때문에, policy의 변화는 policy 확률 분포의 변화를 의미합니다. 현재 policy와 새로운 policy 사이의 확률 분포의 변화를 어떻게 측정할 수 있을까요? Schulmann은 <a href=\"https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\" target=\"_blank\" rel=\"noopener\">KL divergence</a>라는 척도를 이용하였습니다. 이 변화량을 특정값 이하로 제한하고 이것을 만족하는 영역을 trust region이라고 불렀습니다. 그렇지만 문제는 풀기에는 복잡했습니다. 이것을 풀기 위해서 여러 번의 근사화를 적용시켰습니다. objective function에는 linear approximation을 적용하고 trust region constraint에는 second-order approximation을 적용하면 문제가 좀 더 풀기 쉬운 형태로 바뀐다는 것을 알아냈습니다. 이것은 사실 natural gradient를 이용하는 것이었습니다. 이러한 이유로 TRPO를 이해하기 위해서 우리는 natural gradient도 살펴보았던 것입니다.</p>\n<p><a href=\"blog link\">TRPO 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">TRPO Code</a></p>\n<p><br><br></p>\n<h1 id=\"7-PPO-Proximal-policy-optimization-algorithms\"><a href=\"#7-PPO-Proximal-policy-optimization-algorithms\" class=\"headerlink\" title=\"7. [PPO] Proximal policy optimization algorithms\"></a>7. [PPO] Proximal policy optimization algorithms</h1><p><a href=\"blog link\">PPO 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">PPO Code</a></p>\n<p>PPO는 TRPO의 연장선상에 있는 기술이라고 할 수 있습니다. 사실 Schulmann은 TRPO 논문을 쓸 당시 이미 PPO를 구상하고 있었던 것 같습니다. TRPO 논문에도 PPO와 관련있는 내용이 좀 나옵니다. 아이디어 자체는 간단합니다. 그래서그런지 PPO는 arxiv에만 발표되었고 논문도 비교적 짧습니다. TRPO에서는 문제를 단순하게 만들기 위해서 최적화 문제를 여러 번 변형시켰습니다. PPO는 clip이라는 개념을 사용합니다. TRPO에서 이용했던 surrogate objective function을 reward가 특정값 이상이거나 이하가 될 때 더 이상 변화시키지 않는 것입니다.</p>\n<p><a href=\"blog link\">PPO 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">PPO Code</a></p>\n<p><br><br></p>\n<h1 id=\"8-GAE-High-Dimensional-Continuous-Control-Using-Generalized-Advantage-Estimation\"><a href=\"#8-GAE-High-Dimensional-Continuous-Control-Using-Generalized-Advantage-Estimation\" class=\"headerlink\" title=\"8. [GAE] High-Dimensional Continuous Control Using Generalized Advantage Estimation\"></a>8. [GAE] High-Dimensional Continuous Control Using Generalized Advantage Estimation</h1><p><a href=\"blog link\">GAE 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">GAE Code</a></p>\n<p>TRPO가 나오고 난 뒤로도 복잡하고 어려운 control problem에서 Reinforcement Learning(RL)은 high sample complexity 때문에 제한이 되어왔습니다. 따라서 이 논문에서 그 제한을 풀고자 advantage function의 good estimate를 얻는 “variance reduction”에 대해 연구하였습니다.</p>\n<p>“Generalized Advantage Estimator(GAE)”라는 것을 제안했고, 이것은 bias-variance tradeoff를 조절하는 두 개의 parameter $\\gamma,\\lambda$를 가집니다.<br>또한 어떻게 Trust Region Policy Optimization과 value function을 optimize하는 Trust Region Algorithm의 idea를 합치는 지를 보였습니다.</p>\n<p>이렇게 함으로써 보다 더 복잡하고 어려운 control task들을 해결할 수 있었습니다.</p>\n<p>GAE의 실험적인 입증으로는 robotic locomotion을 simulation하는 domain입니다. 실험에서도 보여준 것처럼 [0.9, 0.99]의 범위에서 $\\lambda$의 적절한 중간의 값을 통해 best performance를 얻습니다. 좀 더 나아가 연구되어야할 점은 adaptive or automatic하도록 estimator parameter $\\gamma,\\lambda$를 조절하는 방법입니다.</p>\n<p>추가적으로 앞으로 연구되어야할 부분은 만약 Value function estimation error와 Policy gradient estimation error 사이의 관계를 알아낸다면, Value function fitting에 더 잘 맞는 error metric(policy gradient estimation의 정확성과 더 잘 맞는 value function)을 사용할 수 있을 것입니다. 여기서 Policy와 Value function의 파라미터를 공유하는 모델을 만드는 것은 아주 흥미롭고 이점이 많습니다. 하지만 수렴을 보장하도록 적절한 numerical optimization을 제시해야 할 것입니다.</p>\n<p><a href=\"blog link\">GAE 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">GAE Code</a></p>\n"},{"title":"Deep Determinstic Policy Gradient (DDPG)","date":"2018-06-26T02:20:45.000Z","author":"양혁렬","subtitle":"피지여행 3번째 논문","_content":"\n<center > <img src=\"https://www.dropbox.com/s/zv8rk0uf87ipiaj/Screenshot%202018-06-23%2010.01.48.png?dl=1\" width=\"600\"> </center>\n\n논문 저자 : Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver & Daan Wierstra\n논문 링크 : https://arxiv.org/pdf/1509.02971.pdf\nProceeding : ??\n정리 : 양혁렬\n\n---\n## 1.Introduction\nDDPG 알고리즘에 대한 개요입니다.\n### 1.1 Success & Limination of DQN  \n-  Success\n    - sensor로 부터 나오는 전처리를 거친 input 대신에 raw pixel input 을 사용 <br>\n     : High dimensional observation space 문제를 풀어냄.\n- Limitation\n    - discrete & low dimensional action space 만 다룰 수 있음 <br>\n     : Continuous action space 를 다루기 위해서는 매 스텝 이를 위한 iterative optimization process 를 거쳐야 함.\n\n### 1.2 Problems of discritization\n<p align=\"center\">\n<img src=\"https://www.dropbox.com/s/nulhzxs8bak2fn6/Screenshot%202018-06-23%2012.22.20.png?dl=1\" width=\"300px\">\n</p>\n\n\n- 만약 7개의 관절을 가진 로봇 팔이 있다면, 가장 간단한 discretization 은 각 관절을 다음과 같이 $a_{i}\\in \\\\{ -k, 0, k \\\\}$ 3 개의 값을 가지도록 하는 것이다.\n- 그렇다면 $3 \\times 3 \\times 3 \\times 3 \\times 3 \\times 3 \\times 3 = 3^{7} = 2187$ 가지의 dimension을 가진 action space가 만들어진다.\n    - Discretization 을 하면 action space 가 exponential 하게 늘어남.\n- 충분히 큰 action space 임에도 discretization으로 인한 정보의 손실도 있다\n    - 섬세한 Control 을 할 수 없다.\n\n### 1.3 New approach for continuous control\n\n- Model-free, Off-policy, Actor-critic algorithm 을 제안.\n- Deep Deterministic Policy(이하 DPG) 를 기반으로 함.\n- Actor-Critic approach 와 DQN의 성공적이었던 부분을 합침\n    - Replay buffer : 샘플들 사이의 상관관계를 줄여줌\n    - target Q Network : Update 동안 target 을 안정적으로 만들어줌.\n\n## 2.Background\n\n### 2.1 Notation\n- Observation : $x_{t}$\n- Action : $a_t \\in {\\rm IR}^N $\n- Reward : $r_t$\n- Discount factor : $\\gamma$\n- Environment : $E$\n- Policy : $\\pi : S \\rightarrow P(A)  $\n- Transition dynamics : $p(s_{t+1} \\vert s_t, a_t) $\n- Reward function : $r(s_t, a_t)$\n- Return : $ \\sum_{ i=t }^{ T } \\gamma^{(i-t)}r(s_i, a_i) $\n- Discounted state visitation distribution for a policy : $\\rho^\\pi $\n\n\n### 2.2 Bellman Equation\n- 상태 $s_t$ 에서 행동 $a_t$를 취했을 때 Expected return\n  $ Q^{\\pi}(s_t, a_t)={\\rm E}_{r_{i \\geqq t},s_{i \\geqq t} \\backsim E, a_{i \\geqq t} \\backsim \\pi } [R_{t} \\vert s_t, a_t  ] $\n  <br>\n- 벨만 방정식을 사용하여 위의 식을 변형\n  $ Q^{\\pi}(s_t, a_t)={\\rm E}_{r_{t},s_{t} \\backsim E } \\[r(s_t,a_t)+\\gamma {\\rm E}_{a_{t+1} \\backsim \\pi } \\[ Q^{\\pi}(s_{t+1}, a_{t+1}) \\] \\] $\n  <br>\n- Determinsitc policy 를 가정\n  $ Q^{\\mu}(s_t, a_t)={\\rm E}_{r_{t},s_{t} \\backsim E } \\[r(s_t,a_t)+\\gamma Q^{\\mu}(s_{t+1}, \\mu (s_{t+1})) \\] $\n  - 위의 식에서 아래 식으로 내려오면서 policy 가 determinstic 하기 때문에 policy 에 dependent 한 Expectation 이 빠진 것을 알 수 있습니다. \n  - Deterministic policy를 가정하기 전의 수식에서는 $a_{t+1}$ 을 골랐던 순간의 policy로 Q에 대한 Expection을 원래 구해야하기 때문에 off-policy가 아니지만, Determinsitic policy를 가정한다면 update 할 당시의 policy로 $a_{t+1}$ 를 구할 수 있기 때문에 off-policy 입니다. \n  <br>\n- Q learning \n$L(\\theta^{Q}) = {\\rm E}_{s_t \\backsim \\rho^\\beta , a_t \\backsim \\beta , r_t \\backsim E} [(Q(s_t, a_t \\vert \\theta^Q)-y_t)^2] $ 참고)  $\\beta$ 는 behavior policy를 의미합니다. \n    - $ y_t = r(s_t, a_t) + \\gamma Q^{\\mu}(s_{t+1},\\mu(s_{t+1})) $  \n    - $\\mu(s) = argmax_{a}Q(s,a)$\n        - Q learning 은 위와 같이 $argmax$ 라는 deterministic policy를 사용하기 때문에 off policy 로 사용할 수 있습니다. \n\n### 2.3 DPG\n<center> $ \\nabla_{\\theta^\\mu} J \\approx  {\\rm E}_{s_t \\backsim \\rho^\\beta} [ \\nabla_{\\theta^\\mu} Q(s, a \\vert \\theta ^ Q) \\vert_{s=s_t, a=\\mu(s_t)}] $ <br> $ = {\\rm E}_{s_t \\backsim \\rho^\\beta} [ \\nabla_{a} Q(s, a \\vert \\theta ^ Q) \\vert_{s=s_t, a=\\mu(s_t)} \\nabla_{\\theta^\\mu} \\mu(s \\vert Q^{\\mu})\\vert_{s=s_t}] $ </center>\n\n- 위의 수식은 피지여행 DPG 글 4-2.Q-learning 을 이용한 off-policy actor-critic 에서 이미 정리 한 바 있습니다.\n\n\n\n## 3.Algorithm\nContinous control 을 위한 새로운 알고리즘을 제안합니다. 제안하는 알고리즘의 특징은 다음과 같습니다. \n\n- Replay buffer 를 사용하였다.\n- \"soft\" target update 를 사용하였다. \n- 각 차원의 scale이 다른 low dimension vector 로 부터 학습할 때 Batch Normalization을 사용하였다. \n- 탐험을 위해 action에 Noise 를 추가하였다. \n\n### 3.1 Replay buffer\n<center>\n<img src=\"https://www.dropbox.com/s/lc61b8nas1clqme/Screenshot%202018-06-23%2016.32.53.png?dl=1\" width=\"600px\"></center>\n\n- 큰 State space를 학습하고 일반화 하기위해서는 Neural Network와 같은 non-linear approximator 가 필수적이지만 수렴한다는 보장이 없음.\n- NFQCA 에서는 수렴의 안정성을 위해서 batch learning을 도입함. 하지만 NFQCA에서는 업데이트시에 policy를 reset 하지 않음.\n- DDPG 는 DQN에서 사용된 Replay buffer를 사용하여 online batch update를 가능하게 했음.\n\n### 3.2 Soft target update\n<center> \n$ \\theta^{Q^{'}} \\leftarrow \\tau \\theta^{Q} + (1-\\tau) \\theta^{Q^{'}}   $<br>\n$ \\theta^{\\mu^{'}} \\leftarrow \\tau \\theta^{\\mu} + (1-\\tau) \\theta^{\\mu^{'}}   $\n</center>\n\n- DQN에서는 일정 주기마다 origin network의 weight 를 target network로 직접 복사해서 사용했음.\n- DDPG 에서는 exponential moving average(지수이동평균) 식으로 대체\n- soft update 가 DQN에서 사용했던 방식에 비해 어떤 장점이 있는지는 명확하게 설명되어있지 않음.\n\n### 3.3 Batch Normalization\n<center> \n<img src=\"https://www.dropbox.com/s/1erxzrgk69x04j8/Screenshot%202018-06-23%2016.51.56.png?dl=1\" width=\"300px\">\n</center>\n\n- 서로 scale 이 다른 feature 를 state 로 사용할 때에 Neural Net이 일반화에서 어려움을 겪는다. \n    - 이걸 해결하기 위해서는 원래 직접 스케일을 조정해주었음.\n- 하지만 각 layer 의 Input 을 Unit Gaussian 이 되도록 강제하는 BatchNormalization을 사용하여 이 문제를 해결하였다.\n\n### 3.4 Noise Process\nDDPG 에서는 Exploration을 위해서 output 으로 나온 행동에 노이즈를 추가해줍니다.\n<center>\nORNSTEIN UHLENBECK PROCESS(이하 OU) : $dx_t = \\theta (\\mu - x_t) dt + \\sigma dW_t$\n</center>\n\n- OU Process는 평균으로 회귀하는 random process 입니다.\n- $\\theta$ 는 얼마나 빨리 평균으로 회귀할 지를 나타내는 파라미터이며  $\\mu$ 는 평균을 의미합니다.\n- $\\sigma$ 는 process 의 변동성을 의미하며 $W_t$ 는 Wiener process 를 의미합니다. <br>\n- 따라서 이전의 noise들과 temporally correlated 입니다.\n- 위와 같은 temporally correlated noise process를 사용하는 이유는 physical control 과 같은 관성이 있는 환경에서 학습 시킬 때 보다 효과적이기 때문입니다.\n\n### 3.5 Diagram & Pseudocode \n\n<img src=\"https://www.dropbox.com/s/0ffb2c9irctjx2n/Screenshot%202018-06-23%2017.58.57.png?dl=1\"> \n\n- DDPG 의 학습 과정을 간단히 도식화 해본 다이어 그램입니다. \n\n<center>\n<img src=\"https://www.dropbox.com/s/fd0nj7goixfnd6z/Screenshot%202018-06-23%2018.02.13.png?dl=1\" width=\"500px\"></center>\n\n- DDPG 의 알고리즘 수도코드입니다. 모든 항목을 위에서 설명했으니 순서대로 보시면 이해에 도움이 될것입니다. \n\n\n## 4. Results\n\n### 4.1 Variants of DPG\n<center>\n<img src=\"https://www.dropbox.com/s/k8q6tih85lgow5l/Screenshot%202018-06-23%2018.08.05.png?dl=1\" width=\"500px\"></center>\n\n- original DPG 에 batchnorm 만 추가 (연한 회색), target network만 추가 (진한 회색), 둘 다 추가 (초록), pixel로만 학습 (파랑) . Target network가 성능을 가장 좌지우지한다.\n\n### 4.2 Q estimation of DDPG\n<center>\n<img src=\"https://www.dropbox.com/s/vvcfoni0lqrisst/Screenshot%202018-06-23%2018.10.23.png?dl=1\" width=\"500px\"></center>\n\n- DQN 은 Q value 를 Over-estimate 하는 경향이 있었지만, DDPG 는 simple task에 대해서는 잘한다. 복잡한 문제에 대해서는 estimation을 잘 못했지만, 여전히 좋은 Policy를 찾아내었다.\n\n### 4.3 Performance Comparison\n<center>\n<img src=\"https://www.dropbox.com/s/u8ibmz9q4kfxh6l/Screenshot%202018-06-23%2018.11.40.png?dl=1\" width=\"500px\"></center>\n\n- Score 는 naive policy 를 0, ILQG (planning algorithm) 의 mean score를 1점으로 놓았을 때의 점수 Torcs 환경에 대해서만 raw reward 를 score로 사용.\n\n## 5. Implementation Details\n\n### 5.1 Hyper parameters\n\n- Optimizer : Adam\n    - actor lr : 0.0001, critic lr : 0.001\n- Weight decay(L2) for critic(Q) = 0.001 \n- discount factor, $\\gamma = 0.99 $\n- soft target updates. $\\tau = 0.001 $\n- Size of replay buffer = 1,000,000\n- Orstein Uhlenbeck Process : $\\theta = 0.15$, $\\sigma = 0.2$\n\n\n### 5.2 Etc.\n\n- Final output layer of actor : tanh (행동의 최소 최대를 맞춰주기 위해서)\n- low-dimentional 문제에서 네트워크는 2개의 hidden layer (1st layer 400 units, 2nd layer 300 units) 를 가진다. \n- 이미지를 통해서 학습시킬 때 : 3 convolutional layers (no pooling) with 32 filters at each layer.\n- actor 와 critic 각각의 final layer(weight, bias 모두 ) 는 다음 범위의 uniform distribution 에서 샘플링한다. [ - 0.003, 0.003], [ - 0.0003 , 0.0003] . 이렇게 하는 이유는 가장 처음의 policy와 value 의 output 이 0에 가깝게 나오도록 하기 위함. \n\n\n<br>\n## 6.Conclusion\n<br>\n\n- 이 연구는 최근 딥러닝의 발전과 강화학습을 엮은 것으로 Continuous action space 를 가지는 문제를 robust 하게 풀어냄.\n- non-linear function approximators 을 쓰는 것은 수렴을 보장하지 않지만, 여러 환경에 대해서 특별한 조작 없이 안정적으로 수렴하는 것을 실험으로 보여냄.\n- Atari 도메인에서 DQN 보다 상당히 적은 step 만에 수렴하는 것을 실험을 통해서 알아냄. \n- model-free 알고리즘은 좋은 solution 을 찾기 위해서는 많은 sample을 필요로 한다는 한계가 있지만 더 큰 시스템에서는 이러한 한계를 물리칠 정도로 중요한 역할을 하게 될 것임.","source":"_posts/ddpg.md","raw":"---\ntitle: Deep Determinstic Policy Gradient (DDPG)\ndate: 2018-06-26 11:20:45\ntags: [\"프로젝트\", \"피지여행\"]\ncategories: 프로젝트\nauthor: 양혁렬\nsubtitle: 피지여행 3번째 논문\n---\n\n<center > <img src=\"https://www.dropbox.com/s/zv8rk0uf87ipiaj/Screenshot%202018-06-23%2010.01.48.png?dl=1\" width=\"600\"> </center>\n\n논문 저자 : Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver & Daan Wierstra\n논문 링크 : https://arxiv.org/pdf/1509.02971.pdf\nProceeding : ??\n정리 : 양혁렬\n\n---\n## 1.Introduction\nDDPG 알고리즘에 대한 개요입니다.\n### 1.1 Success & Limination of DQN  \n-  Success\n    - sensor로 부터 나오는 전처리를 거친 input 대신에 raw pixel input 을 사용 <br>\n     : High dimensional observation space 문제를 풀어냄.\n- Limitation\n    - discrete & low dimensional action space 만 다룰 수 있음 <br>\n     : Continuous action space 를 다루기 위해서는 매 스텝 이를 위한 iterative optimization process 를 거쳐야 함.\n\n### 1.2 Problems of discritization\n<p align=\"center\">\n<img src=\"https://www.dropbox.com/s/nulhzxs8bak2fn6/Screenshot%202018-06-23%2012.22.20.png?dl=1\" width=\"300px\">\n</p>\n\n\n- 만약 7개의 관절을 가진 로봇 팔이 있다면, 가장 간단한 discretization 은 각 관절을 다음과 같이 $a_{i}\\in \\\\{ -k, 0, k \\\\}$ 3 개의 값을 가지도록 하는 것이다.\n- 그렇다면 $3 \\times 3 \\times 3 \\times 3 \\times 3 \\times 3 \\times 3 = 3^{7} = 2187$ 가지의 dimension을 가진 action space가 만들어진다.\n    - Discretization 을 하면 action space 가 exponential 하게 늘어남.\n- 충분히 큰 action space 임에도 discretization으로 인한 정보의 손실도 있다\n    - 섬세한 Control 을 할 수 없다.\n\n### 1.3 New approach for continuous control\n\n- Model-free, Off-policy, Actor-critic algorithm 을 제안.\n- Deep Deterministic Policy(이하 DPG) 를 기반으로 함.\n- Actor-Critic approach 와 DQN의 성공적이었던 부분을 합침\n    - Replay buffer : 샘플들 사이의 상관관계를 줄여줌\n    - target Q Network : Update 동안 target 을 안정적으로 만들어줌.\n\n## 2.Background\n\n### 2.1 Notation\n- Observation : $x_{t}$\n- Action : $a_t \\in {\\rm IR}^N $\n- Reward : $r_t$\n- Discount factor : $\\gamma$\n- Environment : $E$\n- Policy : $\\pi : S \\rightarrow P(A)  $\n- Transition dynamics : $p(s_{t+1} \\vert s_t, a_t) $\n- Reward function : $r(s_t, a_t)$\n- Return : $ \\sum_{ i=t }^{ T } \\gamma^{(i-t)}r(s_i, a_i) $\n- Discounted state visitation distribution for a policy : $\\rho^\\pi $\n\n\n### 2.2 Bellman Equation\n- 상태 $s_t$ 에서 행동 $a_t$를 취했을 때 Expected return\n  $ Q^{\\pi}(s_t, a_t)={\\rm E}_{r_{i \\geqq t},s_{i \\geqq t} \\backsim E, a_{i \\geqq t} \\backsim \\pi } [R_{t} \\vert s_t, a_t  ] $\n  <br>\n- 벨만 방정식을 사용하여 위의 식을 변형\n  $ Q^{\\pi}(s_t, a_t)={\\rm E}_{r_{t},s_{t} \\backsim E } \\[r(s_t,a_t)+\\gamma {\\rm E}_{a_{t+1} \\backsim \\pi } \\[ Q^{\\pi}(s_{t+1}, a_{t+1}) \\] \\] $\n  <br>\n- Determinsitc policy 를 가정\n  $ Q^{\\mu}(s_t, a_t)={\\rm E}_{r_{t},s_{t} \\backsim E } \\[r(s_t,a_t)+\\gamma Q^{\\mu}(s_{t+1}, \\mu (s_{t+1})) \\] $\n  - 위의 식에서 아래 식으로 내려오면서 policy 가 determinstic 하기 때문에 policy 에 dependent 한 Expectation 이 빠진 것을 알 수 있습니다. \n  - Deterministic policy를 가정하기 전의 수식에서는 $a_{t+1}$ 을 골랐던 순간의 policy로 Q에 대한 Expection을 원래 구해야하기 때문에 off-policy가 아니지만, Determinsitic policy를 가정한다면 update 할 당시의 policy로 $a_{t+1}$ 를 구할 수 있기 때문에 off-policy 입니다. \n  <br>\n- Q learning \n$L(\\theta^{Q}) = {\\rm E}_{s_t \\backsim \\rho^\\beta , a_t \\backsim \\beta , r_t \\backsim E} [(Q(s_t, a_t \\vert \\theta^Q)-y_t)^2] $ 참고)  $\\beta$ 는 behavior policy를 의미합니다. \n    - $ y_t = r(s_t, a_t) + \\gamma Q^{\\mu}(s_{t+1},\\mu(s_{t+1})) $  \n    - $\\mu(s) = argmax_{a}Q(s,a)$\n        - Q learning 은 위와 같이 $argmax$ 라는 deterministic policy를 사용하기 때문에 off policy 로 사용할 수 있습니다. \n\n### 2.3 DPG\n<center> $ \\nabla_{\\theta^\\mu} J \\approx  {\\rm E}_{s_t \\backsim \\rho^\\beta} [ \\nabla_{\\theta^\\mu} Q(s, a \\vert \\theta ^ Q) \\vert_{s=s_t, a=\\mu(s_t)}] $ <br> $ = {\\rm E}_{s_t \\backsim \\rho^\\beta} [ \\nabla_{a} Q(s, a \\vert \\theta ^ Q) \\vert_{s=s_t, a=\\mu(s_t)} \\nabla_{\\theta^\\mu} \\mu(s \\vert Q^{\\mu})\\vert_{s=s_t}] $ </center>\n\n- 위의 수식은 피지여행 DPG 글 4-2.Q-learning 을 이용한 off-policy actor-critic 에서 이미 정리 한 바 있습니다.\n\n\n\n## 3.Algorithm\nContinous control 을 위한 새로운 알고리즘을 제안합니다. 제안하는 알고리즘의 특징은 다음과 같습니다. \n\n- Replay buffer 를 사용하였다.\n- \"soft\" target update 를 사용하였다. \n- 각 차원의 scale이 다른 low dimension vector 로 부터 학습할 때 Batch Normalization을 사용하였다. \n- 탐험을 위해 action에 Noise 를 추가하였다. \n\n### 3.1 Replay buffer\n<center>\n<img src=\"https://www.dropbox.com/s/lc61b8nas1clqme/Screenshot%202018-06-23%2016.32.53.png?dl=1\" width=\"600px\"></center>\n\n- 큰 State space를 학습하고 일반화 하기위해서는 Neural Network와 같은 non-linear approximator 가 필수적이지만 수렴한다는 보장이 없음.\n- NFQCA 에서는 수렴의 안정성을 위해서 batch learning을 도입함. 하지만 NFQCA에서는 업데이트시에 policy를 reset 하지 않음.\n- DDPG 는 DQN에서 사용된 Replay buffer를 사용하여 online batch update를 가능하게 했음.\n\n### 3.2 Soft target update\n<center> \n$ \\theta^{Q^{'}} \\leftarrow \\tau \\theta^{Q} + (1-\\tau) \\theta^{Q^{'}}   $<br>\n$ \\theta^{\\mu^{'}} \\leftarrow \\tau \\theta^{\\mu} + (1-\\tau) \\theta^{\\mu^{'}}   $\n</center>\n\n- DQN에서는 일정 주기마다 origin network의 weight 를 target network로 직접 복사해서 사용했음.\n- DDPG 에서는 exponential moving average(지수이동평균) 식으로 대체\n- soft update 가 DQN에서 사용했던 방식에 비해 어떤 장점이 있는지는 명확하게 설명되어있지 않음.\n\n### 3.3 Batch Normalization\n<center> \n<img src=\"https://www.dropbox.com/s/1erxzrgk69x04j8/Screenshot%202018-06-23%2016.51.56.png?dl=1\" width=\"300px\">\n</center>\n\n- 서로 scale 이 다른 feature 를 state 로 사용할 때에 Neural Net이 일반화에서 어려움을 겪는다. \n    - 이걸 해결하기 위해서는 원래 직접 스케일을 조정해주었음.\n- 하지만 각 layer 의 Input 을 Unit Gaussian 이 되도록 강제하는 BatchNormalization을 사용하여 이 문제를 해결하였다.\n\n### 3.4 Noise Process\nDDPG 에서는 Exploration을 위해서 output 으로 나온 행동에 노이즈를 추가해줍니다.\n<center>\nORNSTEIN UHLENBECK PROCESS(이하 OU) : $dx_t = \\theta (\\mu - x_t) dt + \\sigma dW_t$\n</center>\n\n- OU Process는 평균으로 회귀하는 random process 입니다.\n- $\\theta$ 는 얼마나 빨리 평균으로 회귀할 지를 나타내는 파라미터이며  $\\mu$ 는 평균을 의미합니다.\n- $\\sigma$ 는 process 의 변동성을 의미하며 $W_t$ 는 Wiener process 를 의미합니다. <br>\n- 따라서 이전의 noise들과 temporally correlated 입니다.\n- 위와 같은 temporally correlated noise process를 사용하는 이유는 physical control 과 같은 관성이 있는 환경에서 학습 시킬 때 보다 효과적이기 때문입니다.\n\n### 3.5 Diagram & Pseudocode \n\n<img src=\"https://www.dropbox.com/s/0ffb2c9irctjx2n/Screenshot%202018-06-23%2017.58.57.png?dl=1\"> \n\n- DDPG 의 학습 과정을 간단히 도식화 해본 다이어 그램입니다. \n\n<center>\n<img src=\"https://www.dropbox.com/s/fd0nj7goixfnd6z/Screenshot%202018-06-23%2018.02.13.png?dl=1\" width=\"500px\"></center>\n\n- DDPG 의 알고리즘 수도코드입니다. 모든 항목을 위에서 설명했으니 순서대로 보시면 이해에 도움이 될것입니다. \n\n\n## 4. Results\n\n### 4.1 Variants of DPG\n<center>\n<img src=\"https://www.dropbox.com/s/k8q6tih85lgow5l/Screenshot%202018-06-23%2018.08.05.png?dl=1\" width=\"500px\"></center>\n\n- original DPG 에 batchnorm 만 추가 (연한 회색), target network만 추가 (진한 회색), 둘 다 추가 (초록), pixel로만 학습 (파랑) . Target network가 성능을 가장 좌지우지한다.\n\n### 4.2 Q estimation of DDPG\n<center>\n<img src=\"https://www.dropbox.com/s/vvcfoni0lqrisst/Screenshot%202018-06-23%2018.10.23.png?dl=1\" width=\"500px\"></center>\n\n- DQN 은 Q value 를 Over-estimate 하는 경향이 있었지만, DDPG 는 simple task에 대해서는 잘한다. 복잡한 문제에 대해서는 estimation을 잘 못했지만, 여전히 좋은 Policy를 찾아내었다.\n\n### 4.3 Performance Comparison\n<center>\n<img src=\"https://www.dropbox.com/s/u8ibmz9q4kfxh6l/Screenshot%202018-06-23%2018.11.40.png?dl=1\" width=\"500px\"></center>\n\n- Score 는 naive policy 를 0, ILQG (planning algorithm) 의 mean score를 1점으로 놓았을 때의 점수 Torcs 환경에 대해서만 raw reward 를 score로 사용.\n\n## 5. Implementation Details\n\n### 5.1 Hyper parameters\n\n- Optimizer : Adam\n    - actor lr : 0.0001, critic lr : 0.001\n- Weight decay(L2) for critic(Q) = 0.001 \n- discount factor, $\\gamma = 0.99 $\n- soft target updates. $\\tau = 0.001 $\n- Size of replay buffer = 1,000,000\n- Orstein Uhlenbeck Process : $\\theta = 0.15$, $\\sigma = 0.2$\n\n\n### 5.2 Etc.\n\n- Final output layer of actor : tanh (행동의 최소 최대를 맞춰주기 위해서)\n- low-dimentional 문제에서 네트워크는 2개의 hidden layer (1st layer 400 units, 2nd layer 300 units) 를 가진다. \n- 이미지를 통해서 학습시킬 때 : 3 convolutional layers (no pooling) with 32 filters at each layer.\n- actor 와 critic 각각의 final layer(weight, bias 모두 ) 는 다음 범위의 uniform distribution 에서 샘플링한다. [ - 0.003, 0.003], [ - 0.0003 , 0.0003] . 이렇게 하는 이유는 가장 처음의 policy와 value 의 output 이 0에 가깝게 나오도록 하기 위함. \n\n\n<br>\n## 6.Conclusion\n<br>\n\n- 이 연구는 최근 딥러닝의 발전과 강화학습을 엮은 것으로 Continuous action space 를 가지는 문제를 robust 하게 풀어냄.\n- non-linear function approximators 을 쓰는 것은 수렴을 보장하지 않지만, 여러 환경에 대해서 특별한 조작 없이 안정적으로 수렴하는 것을 실험으로 보여냄.\n- Atari 도메인에서 DQN 보다 상당히 적은 step 만에 수렴하는 것을 실험을 통해서 알아냄. \n- model-free 알고리즘은 좋은 solution 을 찾기 위해서는 많은 sample을 필요로 한다는 한계가 있지만 더 큰 시스템에서는 이러한 한계를 물리칠 정도로 중요한 역할을 하게 될 것임.","slug":"ddpg","published":1,"updated":"2018-07-19T12:17:29.265Z","_id":"cjjgzyg4h0008xo15m13f942j","comments":1,"layout":"post","photos":[],"link":"","content":"<center> <img src=\"https://www.dropbox.com/s/zv8rk0uf87ipiaj/Screenshot%202018-06-23%2010.01.48.png?dl=1\" width=\"600\"> </center>\n\n<p>논문 저자 : Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver &amp; Daan Wierstra<br>논문 링크 : <a href=\"https://arxiv.org/pdf/1509.02971.pdf\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/pdf/1509.02971.pdf</a><br>Proceeding : ??<br>정리 : 양혁렬</p>\n<hr>\n<h2 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1.Introduction\"></a>1.Introduction</h2><p>DDPG 알고리즘에 대한 개요입니다.</p>\n<h3 id=\"1-1-Success-amp-Limination-of-DQN\"><a href=\"#1-1-Success-amp-Limination-of-DQN\" class=\"headerlink\" title=\"1.1 Success &amp; Limination of DQN\"></a>1.1 Success &amp; Limination of DQN</h3><ul>\n<li>Success<ul>\n<li>sensor로 부터 나오는 전처리를 거친 input 대신에 raw pixel input 을 사용 <br><br>: High dimensional observation space 문제를 풀어냄.</li>\n</ul>\n</li>\n<li>Limitation<ul>\n<li>discrete &amp; low dimensional action space 만 다룰 수 있음 <br><br>: Continuous action space 를 다루기 위해서는 매 스텝 이를 위한 iterative optimization process 를 거쳐야 함.</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"1-2-Problems-of-discritization\"><a href=\"#1-2-Problems-of-discritization\" class=\"headerlink\" title=\"1.2 Problems of discritization\"></a>1.2 Problems of discritization</h3><p align=\"center\"><br><img src=\"https://www.dropbox.com/s/nulhzxs8bak2fn6/Screenshot%202018-06-23%2012.22.20.png?dl=1\" width=\"300px\"><br></p>\n\n\n<ul>\n<li>만약 7개의 관절을 가진 로봇 팔이 있다면, 가장 간단한 discretization 은 각 관절을 다음과 같이 $a_{i}\\in \\{ -k, 0, k \\}$ 3 개의 값을 가지도록 하는 것이다.</li>\n<li>그렇다면 $3 \\times 3 \\times 3 \\times 3 \\times 3 \\times 3 \\times 3 = 3^{7} = 2187$ 가지의 dimension을 가진 action space가 만들어진다.<ul>\n<li>Discretization 을 하면 action space 가 exponential 하게 늘어남.</li>\n</ul>\n</li>\n<li>충분히 큰 action space 임에도 discretization으로 인한 정보의 손실도 있다<ul>\n<li>섬세한 Control 을 할 수 없다.</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"1-3-New-approach-for-continuous-control\"><a href=\"#1-3-New-approach-for-continuous-control\" class=\"headerlink\" title=\"1.3 New approach for continuous control\"></a>1.3 New approach for continuous control</h3><ul>\n<li>Model-free, Off-policy, Actor-critic algorithm 을 제안.</li>\n<li>Deep Deterministic Policy(이하 DPG) 를 기반으로 함.</li>\n<li>Actor-Critic approach 와 DQN의 성공적이었던 부분을 합침<ul>\n<li>Replay buffer : 샘플들 사이의 상관관계를 줄여줌</li>\n<li>target Q Network : Update 동안 target 을 안정적으로 만들어줌.</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"2-Background\"><a href=\"#2-Background\" class=\"headerlink\" title=\"2.Background\"></a>2.Background</h2><h3 id=\"2-1-Notation\"><a href=\"#2-1-Notation\" class=\"headerlink\" title=\"2.1 Notation\"></a>2.1 Notation</h3><ul>\n<li>Observation : $x_{t}$</li>\n<li>Action : $a_t \\in {\\rm IR}^N $</li>\n<li>Reward : $r_t$</li>\n<li>Discount factor : $\\gamma$</li>\n<li>Environment : $E$</li>\n<li>Policy : $\\pi : S \\rightarrow P(A)  $</li>\n<li>Transition dynamics : $p(s_{t+1} \\vert s_t, a_t) $</li>\n<li>Reward function : $r(s_t, a_t)$</li>\n<li>Return : $ \\sum_{ i=t }^{ T } \\gamma^{(i-t)}r(s_i, a_i) $</li>\n<li>Discounted state visitation distribution for a policy : $\\rho^\\pi $</li>\n</ul>\n<h3 id=\"2-2-Bellman-Equation\"><a href=\"#2-2-Bellman-Equation\" class=\"headerlink\" title=\"2.2 Bellman Equation\"></a>2.2 Bellman Equation</h3><ul>\n<li>상태 $s_t$ 에서 행동 $a_t$를 취했을 때 Expected return<br>$ Q^{\\pi}(s_t, a_t)={\\rm E}_{r_{i \\geqq t},s_{i \\geqq t} \\backsim E, a_{i \\geqq t} \\backsim \\pi } [R_{t} \\vert s_t, a_t  ] $<br><br></li>\n<li>벨만 방정식을 사용하여 위의 식을 변형<br>$ Q^{\\pi}(s_t, a_t)={\\rm E}_{r_{t},s_{t} \\backsim E } [r(s_t,a_t)+\\gamma {\\rm E}_{a_{t+1} \\backsim \\pi } [ Q^{\\pi}(s_{t+1}, a_{t+1}) ] ] $<br><br></li>\n<li>Determinsitc policy 를 가정<br>$ Q^{\\mu}(s_t, a_t)={\\rm E}_{r_{t},s_{t} \\backsim E } [r(s_t,a_t)+\\gamma Q^{\\mu}(s_{t+1}, \\mu (s_{t+1})) ] $<ul>\n<li>위의 식에서 아래 식으로 내려오면서 policy 가 determinstic 하기 때문에 policy 에 dependent 한 Expectation 이 빠진 것을 알 수 있습니다. </li>\n<li>Deterministic policy를 가정하기 전의 수식에서는 $a_{t+1}$ 을 골랐던 순간의 policy로 Q에 대한 Expection을 원래 구해야하기 때문에 off-policy가 아니지만, Determinsitic policy를 가정한다면 update 할 당시의 policy로 $a_{t+1}$ 를 구할 수 있기 때문에 off-policy 입니다.<br><br></li>\n</ul>\n</li>\n<li>Q learning<br>$L(\\theta^{Q}) = {\\rm E}_{s_t \\backsim \\rho^\\beta , a_t \\backsim \\beta , r_t \\backsim E} [(Q(s_t, a_t \\vert \\theta^Q)-y_t)^2] $ 참고)  $\\beta$ 는 behavior policy를 의미합니다. <ul>\n<li>$ y_t = r(s_t, a_t) + \\gamma Q^{\\mu}(s_{t+1},\\mu(s_{t+1})) $  </li>\n<li>$\\mu(s) = argmax_{a}Q(s,a)$<ul>\n<li>Q learning 은 위와 같이 $argmax$ 라는 deterministic policy를 사용하기 때문에 off policy 로 사용할 수 있습니다. </li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"2-3-DPG\"><a href=\"#2-3-DPG\" class=\"headerlink\" title=\"2.3 DPG\"></a>2.3 DPG</h3><center> $ \\nabla_{\\theta^\\mu} J \\approx  {\\rm E}_{s_t \\backsim \\rho^\\beta} [ \\nabla_{\\theta^\\mu} Q(s, a \\vert \\theta ^ Q) \\vert_{s=s_t, a=\\mu(s_t)}] $ <br> $ = {\\rm E}_{s_t \\backsim \\rho^\\beta} [ \\nabla_{a} Q(s, a \\vert \\theta ^ Q) \\vert_{s=s_t, a=\\mu(s_t)} \\nabla_{\\theta^\\mu} \\mu(s \\vert Q^{\\mu})\\vert_{s=s_t}] $ </center>\n\n<ul>\n<li>위의 수식은 피지여행 DPG 글 4-2.Q-learning 을 이용한 off-policy actor-critic 에서 이미 정리 한 바 있습니다.</li>\n</ul>\n<h2 id=\"3-Algorithm\"><a href=\"#3-Algorithm\" class=\"headerlink\" title=\"3.Algorithm\"></a>3.Algorithm</h2><p>Continous control 을 위한 새로운 알고리즘을 제안합니다. 제안하는 알고리즘의 특징은 다음과 같습니다. </p>\n<ul>\n<li>Replay buffer 를 사용하였다.</li>\n<li>“soft” target update 를 사용하였다. </li>\n<li>각 차원의 scale이 다른 low dimension vector 로 부터 학습할 때 Batch Normalization을 사용하였다. </li>\n<li>탐험을 위해 action에 Noise 를 추가하였다. </li>\n</ul>\n<h3 id=\"3-1-Replay-buffer\"><a href=\"#3-1-Replay-buffer\" class=\"headerlink\" title=\"3.1 Replay buffer\"></a>3.1 Replay buffer</h3><center><br><img src=\"https://www.dropbox.com/s/lc61b8nas1clqme/Screenshot%202018-06-23%2016.32.53.png?dl=1\" width=\"600px\"></center>\n\n<ul>\n<li>큰 State space를 학습하고 일반화 하기위해서는 Neural Network와 같은 non-linear approximator 가 필수적이지만 수렴한다는 보장이 없음.</li>\n<li>NFQCA 에서는 수렴의 안정성을 위해서 batch learning을 도입함. 하지만 NFQCA에서는 업데이트시에 policy를 reset 하지 않음.</li>\n<li>DDPG 는 DQN에서 사용된 Replay buffer를 사용하여 online batch update를 가능하게 했음.</li>\n</ul>\n<h3 id=\"3-2-Soft-target-update\"><a href=\"#3-2-Soft-target-update\" class=\"headerlink\" title=\"3.2 Soft target update\"></a>3.2 Soft target update</h3><center><br>$ \\theta^{Q^{‘}} \\leftarrow \\tau \\theta^{Q} + (1-\\tau) \\theta^{Q^{‘}}   $<br><br>$ \\theta^{\\mu^{‘}} \\leftarrow \\tau \\theta^{\\mu} + (1-\\tau) \\theta^{\\mu^{‘}}   $<br></center>\n\n<ul>\n<li>DQN에서는 일정 주기마다 origin network의 weight 를 target network로 직접 복사해서 사용했음.</li>\n<li>DDPG 에서는 exponential moving average(지수이동평균) 식으로 대체</li>\n<li>soft update 가 DQN에서 사용했던 방식에 비해 어떤 장점이 있는지는 명확하게 설명되어있지 않음.</li>\n</ul>\n<h3 id=\"3-3-Batch-Normalization\"><a href=\"#3-3-Batch-Normalization\" class=\"headerlink\" title=\"3.3 Batch Normalization\"></a>3.3 Batch Normalization</h3><center><br><img src=\"https://www.dropbox.com/s/1erxzrgk69x04j8/Screenshot%202018-06-23%2016.51.56.png?dl=1\" width=\"300px\"><br></center>\n\n<ul>\n<li>서로 scale 이 다른 feature 를 state 로 사용할 때에 Neural Net이 일반화에서 어려움을 겪는다. <ul>\n<li>이걸 해결하기 위해서는 원래 직접 스케일을 조정해주었음.</li>\n</ul>\n</li>\n<li>하지만 각 layer 의 Input 을 Unit Gaussian 이 되도록 강제하는 BatchNormalization을 사용하여 이 문제를 해결하였다.</li>\n</ul>\n<h3 id=\"3-4-Noise-Process\"><a href=\"#3-4-Noise-Process\" class=\"headerlink\" title=\"3.4 Noise Process\"></a>3.4 Noise Process</h3><p>DDPG 에서는 Exploration을 위해서 output 으로 나온 행동에 노이즈를 추가해줍니다.</p>\n<center><br>ORNSTEIN UHLENBECK PROCESS(이하 OU) : $dx_t = \\theta (\\mu - x_t) dt + \\sigma dW_t$<br></center>\n\n<ul>\n<li>OU Process는 평균으로 회귀하는 random process 입니다.</li>\n<li>$\\theta$ 는 얼마나 빨리 평균으로 회귀할 지를 나타내는 파라미터이며  $\\mu$ 는 평균을 의미합니다.</li>\n<li>$\\sigma$ 는 process 의 변동성을 의미하며 $W_t$ 는 Wiener process 를 의미합니다. <br></li>\n<li>따라서 이전의 noise들과 temporally correlated 입니다.</li>\n<li>위와 같은 temporally correlated noise process를 사용하는 이유는 physical control 과 같은 관성이 있는 환경에서 학습 시킬 때 보다 효과적이기 때문입니다.</li>\n</ul>\n<h3 id=\"3-5-Diagram-amp-Pseudocode\"><a href=\"#3-5-Diagram-amp-Pseudocode\" class=\"headerlink\" title=\"3.5 Diagram &amp; Pseudocode\"></a>3.5 Diagram &amp; Pseudocode</h3><p><img src=\"https://www.dropbox.com/s/0ffb2c9irctjx2n/Screenshot%202018-06-23%2017.58.57.png?dl=1\"> </p>\n<ul>\n<li>DDPG 의 학습 과정을 간단히 도식화 해본 다이어 그램입니다. </li>\n</ul>\n<center><br><img src=\"https://www.dropbox.com/s/fd0nj7goixfnd6z/Screenshot%202018-06-23%2018.02.13.png?dl=1\" width=\"500px\"></center>\n\n<ul>\n<li>DDPG 의 알고리즘 수도코드입니다. 모든 항목을 위에서 설명했으니 순서대로 보시면 이해에 도움이 될것입니다. </li>\n</ul>\n<h2 id=\"4-Results\"><a href=\"#4-Results\" class=\"headerlink\" title=\"4. Results\"></a>4. Results</h2><h3 id=\"4-1-Variants-of-DPG\"><a href=\"#4-1-Variants-of-DPG\" class=\"headerlink\" title=\"4.1 Variants of DPG\"></a>4.1 Variants of DPG</h3><center><br><img src=\"https://www.dropbox.com/s/k8q6tih85lgow5l/Screenshot%202018-06-23%2018.08.05.png?dl=1\" width=\"500px\"></center>\n\n<ul>\n<li>original DPG 에 batchnorm 만 추가 (연한 회색), target network만 추가 (진한 회색), 둘 다 추가 (초록), pixel로만 학습 (파랑) . Target network가 성능을 가장 좌지우지한다.</li>\n</ul>\n<h3 id=\"4-2-Q-estimation-of-DDPG\"><a href=\"#4-2-Q-estimation-of-DDPG\" class=\"headerlink\" title=\"4.2 Q estimation of DDPG\"></a>4.2 Q estimation of DDPG</h3><center><br><img src=\"https://www.dropbox.com/s/vvcfoni0lqrisst/Screenshot%202018-06-23%2018.10.23.png?dl=1\" width=\"500px\"></center>\n\n<ul>\n<li>DQN 은 Q value 를 Over-estimate 하는 경향이 있었지만, DDPG 는 simple task에 대해서는 잘한다. 복잡한 문제에 대해서는 estimation을 잘 못했지만, 여전히 좋은 Policy를 찾아내었다.</li>\n</ul>\n<h3 id=\"4-3-Performance-Comparison\"><a href=\"#4-3-Performance-Comparison\" class=\"headerlink\" title=\"4.3 Performance Comparison\"></a>4.3 Performance Comparison</h3><center><br><img src=\"https://www.dropbox.com/s/u8ibmz9q4kfxh6l/Screenshot%202018-06-23%2018.11.40.png?dl=1\" width=\"500px\"></center>\n\n<ul>\n<li>Score 는 naive policy 를 0, ILQG (planning algorithm) 의 mean score를 1점으로 놓았을 때의 점수 Torcs 환경에 대해서만 raw reward 를 score로 사용.</li>\n</ul>\n<h2 id=\"5-Implementation-Details\"><a href=\"#5-Implementation-Details\" class=\"headerlink\" title=\"5. Implementation Details\"></a>5. Implementation Details</h2><h3 id=\"5-1-Hyper-parameters\"><a href=\"#5-1-Hyper-parameters\" class=\"headerlink\" title=\"5.1 Hyper parameters\"></a>5.1 Hyper parameters</h3><ul>\n<li>Optimizer : Adam<ul>\n<li>actor lr : 0.0001, critic lr : 0.001</li>\n</ul>\n</li>\n<li>Weight decay(L2) for critic(Q) = 0.001 </li>\n<li>discount factor, $\\gamma = 0.99 $</li>\n<li>soft target updates. $\\tau = 0.001 $</li>\n<li>Size of replay buffer = 1,000,000</li>\n<li>Orstein Uhlenbeck Process : $\\theta = 0.15$, $\\sigma = 0.2$</li>\n</ul>\n<h3 id=\"5-2-Etc\"><a href=\"#5-2-Etc\" class=\"headerlink\" title=\"5.2 Etc.\"></a>5.2 Etc.</h3><ul>\n<li>Final output layer of actor : tanh (행동의 최소 최대를 맞춰주기 위해서)</li>\n<li>low-dimentional 문제에서 네트워크는 2개의 hidden layer (1st layer 400 units, 2nd layer 300 units) 를 가진다. </li>\n<li>이미지를 통해서 학습시킬 때 : 3 convolutional layers (no pooling) with 32 filters at each layer.</li>\n<li>actor 와 critic 각각의 final layer(weight, bias 모두 ) 는 다음 범위의 uniform distribution 에서 샘플링한다. [ - 0.003, 0.003], [ - 0.0003 , 0.0003] . 이렇게 하는 이유는 가장 처음의 policy와 value 의 output 이 0에 가깝게 나오도록 하기 위함. </li>\n</ul>\n<p><br></p>\n<h2 id=\"6-Conclusion\"><a href=\"#6-Conclusion\" class=\"headerlink\" title=\"6.Conclusion\"></a>6.Conclusion</h2><p><br></p>\n<ul>\n<li>이 연구는 최근 딥러닝의 발전과 강화학습을 엮은 것으로 Continuous action space 를 가지는 문제를 robust 하게 풀어냄.</li>\n<li>non-linear function approximators 을 쓰는 것은 수렴을 보장하지 않지만, 여러 환경에 대해서 특별한 조작 없이 안정적으로 수렴하는 것을 실험으로 보여냄.</li>\n<li>Atari 도메인에서 DQN 보다 상당히 적은 step 만에 수렴하는 것을 실험을 통해서 알아냄. </li>\n<li>model-free 알고리즘은 좋은 solution 을 찾기 위해서는 많은 sample을 필요로 한다는 한계가 있지만 더 큰 시스템에서는 이러한 한계를 물리칠 정도로 중요한 역할을 하게 될 것임.</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<center> <img src=\"https://www.dropbox.com/s/zv8rk0uf87ipiaj/Screenshot%202018-06-23%2010.01.48.png?dl=1\" width=\"600\"> </center>\n\n<p>논문 저자 : Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver &amp; Daan Wierstra<br>논문 링크 : <a href=\"https://arxiv.org/pdf/1509.02971.pdf\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/pdf/1509.02971.pdf</a><br>Proceeding : ??<br>정리 : 양혁렬</p>\n<hr>\n<h2 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1.Introduction\"></a>1.Introduction</h2><p>DDPG 알고리즘에 대한 개요입니다.</p>\n<h3 id=\"1-1-Success-amp-Limination-of-DQN\"><a href=\"#1-1-Success-amp-Limination-of-DQN\" class=\"headerlink\" title=\"1.1 Success &amp; Limination of DQN\"></a>1.1 Success &amp; Limination of DQN</h3><ul>\n<li>Success<ul>\n<li>sensor로 부터 나오는 전처리를 거친 input 대신에 raw pixel input 을 사용 <br><br>: High dimensional observation space 문제를 풀어냄.</li>\n</ul>\n</li>\n<li>Limitation<ul>\n<li>discrete &amp; low dimensional action space 만 다룰 수 있음 <br><br>: Continuous action space 를 다루기 위해서는 매 스텝 이를 위한 iterative optimization process 를 거쳐야 함.</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"1-2-Problems-of-discritization\"><a href=\"#1-2-Problems-of-discritization\" class=\"headerlink\" title=\"1.2 Problems of discritization\"></a>1.2 Problems of discritization</h3><p align=\"center\"><br><img src=\"https://www.dropbox.com/s/nulhzxs8bak2fn6/Screenshot%202018-06-23%2012.22.20.png?dl=1\" width=\"300px\"><br></p>\n\n\n<ul>\n<li>만약 7개의 관절을 가진 로봇 팔이 있다면, 가장 간단한 discretization 은 각 관절을 다음과 같이 $a_{i}\\in \\{ -k, 0, k \\}$ 3 개의 값을 가지도록 하는 것이다.</li>\n<li>그렇다면 $3 \\times 3 \\times 3 \\times 3 \\times 3 \\times 3 \\times 3 = 3^{7} = 2187$ 가지의 dimension을 가진 action space가 만들어진다.<ul>\n<li>Discretization 을 하면 action space 가 exponential 하게 늘어남.</li>\n</ul>\n</li>\n<li>충분히 큰 action space 임에도 discretization으로 인한 정보의 손실도 있다<ul>\n<li>섬세한 Control 을 할 수 없다.</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"1-3-New-approach-for-continuous-control\"><a href=\"#1-3-New-approach-for-continuous-control\" class=\"headerlink\" title=\"1.3 New approach for continuous control\"></a>1.3 New approach for continuous control</h3><ul>\n<li>Model-free, Off-policy, Actor-critic algorithm 을 제안.</li>\n<li>Deep Deterministic Policy(이하 DPG) 를 기반으로 함.</li>\n<li>Actor-Critic approach 와 DQN의 성공적이었던 부분을 합침<ul>\n<li>Replay buffer : 샘플들 사이의 상관관계를 줄여줌</li>\n<li>target Q Network : Update 동안 target 을 안정적으로 만들어줌.</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"2-Background\"><a href=\"#2-Background\" class=\"headerlink\" title=\"2.Background\"></a>2.Background</h2><h3 id=\"2-1-Notation\"><a href=\"#2-1-Notation\" class=\"headerlink\" title=\"2.1 Notation\"></a>2.1 Notation</h3><ul>\n<li>Observation : $x_{t}$</li>\n<li>Action : $a_t \\in {\\rm IR}^N $</li>\n<li>Reward : $r_t$</li>\n<li>Discount factor : $\\gamma$</li>\n<li>Environment : $E$</li>\n<li>Policy : $\\pi : S \\rightarrow P(A)  $</li>\n<li>Transition dynamics : $p(s_{t+1} \\vert s_t, a_t) $</li>\n<li>Reward function : $r(s_t, a_t)$</li>\n<li>Return : $ \\sum_{ i=t }^{ T } \\gamma^{(i-t)}r(s_i, a_i) $</li>\n<li>Discounted state visitation distribution for a policy : $\\rho^\\pi $</li>\n</ul>\n<h3 id=\"2-2-Bellman-Equation\"><a href=\"#2-2-Bellman-Equation\" class=\"headerlink\" title=\"2.2 Bellman Equation\"></a>2.2 Bellman Equation</h3><ul>\n<li>상태 $s_t$ 에서 행동 $a_t$를 취했을 때 Expected return<br>$ Q^{\\pi}(s_t, a_t)={\\rm E}_{r_{i \\geqq t},s_{i \\geqq t} \\backsim E, a_{i \\geqq t} \\backsim \\pi } [R_{t} \\vert s_t, a_t  ] $<br><br></li>\n<li>벨만 방정식을 사용하여 위의 식을 변형<br>$ Q^{\\pi}(s_t, a_t)={\\rm E}_{r_{t},s_{t} \\backsim E } [r(s_t,a_t)+\\gamma {\\rm E}_{a_{t+1} \\backsim \\pi } [ Q^{\\pi}(s_{t+1}, a_{t+1}) ] ] $<br><br></li>\n<li>Determinsitc policy 를 가정<br>$ Q^{\\mu}(s_t, a_t)={\\rm E}_{r_{t},s_{t} \\backsim E } [r(s_t,a_t)+\\gamma Q^{\\mu}(s_{t+1}, \\mu (s_{t+1})) ] $<ul>\n<li>위의 식에서 아래 식으로 내려오면서 policy 가 determinstic 하기 때문에 policy 에 dependent 한 Expectation 이 빠진 것을 알 수 있습니다. </li>\n<li>Deterministic policy를 가정하기 전의 수식에서는 $a_{t+1}$ 을 골랐던 순간의 policy로 Q에 대한 Expection을 원래 구해야하기 때문에 off-policy가 아니지만, Determinsitic policy를 가정한다면 update 할 당시의 policy로 $a_{t+1}$ 를 구할 수 있기 때문에 off-policy 입니다.<br><br></li>\n</ul>\n</li>\n<li>Q learning<br>$L(\\theta^{Q}) = {\\rm E}_{s_t \\backsim \\rho^\\beta , a_t \\backsim \\beta , r_t \\backsim E} [(Q(s_t, a_t \\vert \\theta^Q)-y_t)^2] $ 참고)  $\\beta$ 는 behavior policy를 의미합니다. <ul>\n<li>$ y_t = r(s_t, a_t) + \\gamma Q^{\\mu}(s_{t+1},\\mu(s_{t+1})) $  </li>\n<li>$\\mu(s) = argmax_{a}Q(s,a)$<ul>\n<li>Q learning 은 위와 같이 $argmax$ 라는 deterministic policy를 사용하기 때문에 off policy 로 사용할 수 있습니다. </li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"2-3-DPG\"><a href=\"#2-3-DPG\" class=\"headerlink\" title=\"2.3 DPG\"></a>2.3 DPG</h3><center> $ \\nabla_{\\theta^\\mu} J \\approx  {\\rm E}_{s_t \\backsim \\rho^\\beta} [ \\nabla_{\\theta^\\mu} Q(s, a \\vert \\theta ^ Q) \\vert_{s=s_t, a=\\mu(s_t)}] $ <br> $ = {\\rm E}_{s_t \\backsim \\rho^\\beta} [ \\nabla_{a} Q(s, a \\vert \\theta ^ Q) \\vert_{s=s_t, a=\\mu(s_t)} \\nabla_{\\theta^\\mu} \\mu(s \\vert Q^{\\mu})\\vert_{s=s_t}] $ </center>\n\n<ul>\n<li>위의 수식은 피지여행 DPG 글 4-2.Q-learning 을 이용한 off-policy actor-critic 에서 이미 정리 한 바 있습니다.</li>\n</ul>\n<h2 id=\"3-Algorithm\"><a href=\"#3-Algorithm\" class=\"headerlink\" title=\"3.Algorithm\"></a>3.Algorithm</h2><p>Continous control 을 위한 새로운 알고리즘을 제안합니다. 제안하는 알고리즘의 특징은 다음과 같습니다. </p>\n<ul>\n<li>Replay buffer 를 사용하였다.</li>\n<li>“soft” target update 를 사용하였다. </li>\n<li>각 차원의 scale이 다른 low dimension vector 로 부터 학습할 때 Batch Normalization을 사용하였다. </li>\n<li>탐험을 위해 action에 Noise 를 추가하였다. </li>\n</ul>\n<h3 id=\"3-1-Replay-buffer\"><a href=\"#3-1-Replay-buffer\" class=\"headerlink\" title=\"3.1 Replay buffer\"></a>3.1 Replay buffer</h3><center><br><img src=\"https://www.dropbox.com/s/lc61b8nas1clqme/Screenshot%202018-06-23%2016.32.53.png?dl=1\" width=\"600px\"></center>\n\n<ul>\n<li>큰 State space를 학습하고 일반화 하기위해서는 Neural Network와 같은 non-linear approximator 가 필수적이지만 수렴한다는 보장이 없음.</li>\n<li>NFQCA 에서는 수렴의 안정성을 위해서 batch learning을 도입함. 하지만 NFQCA에서는 업데이트시에 policy를 reset 하지 않음.</li>\n<li>DDPG 는 DQN에서 사용된 Replay buffer를 사용하여 online batch update를 가능하게 했음.</li>\n</ul>\n<h3 id=\"3-2-Soft-target-update\"><a href=\"#3-2-Soft-target-update\" class=\"headerlink\" title=\"3.2 Soft target update\"></a>3.2 Soft target update</h3><center><br>$ \\theta^{Q^{‘}} \\leftarrow \\tau \\theta^{Q} + (1-\\tau) \\theta^{Q^{‘}}   $<br><br>$ \\theta^{\\mu^{‘}} \\leftarrow \\tau \\theta^{\\mu} + (1-\\tau) \\theta^{\\mu^{‘}}   $<br></center>\n\n<ul>\n<li>DQN에서는 일정 주기마다 origin network의 weight 를 target network로 직접 복사해서 사용했음.</li>\n<li>DDPG 에서는 exponential moving average(지수이동평균) 식으로 대체</li>\n<li>soft update 가 DQN에서 사용했던 방식에 비해 어떤 장점이 있는지는 명확하게 설명되어있지 않음.</li>\n</ul>\n<h3 id=\"3-3-Batch-Normalization\"><a href=\"#3-3-Batch-Normalization\" class=\"headerlink\" title=\"3.3 Batch Normalization\"></a>3.3 Batch Normalization</h3><center><br><img src=\"https://www.dropbox.com/s/1erxzrgk69x04j8/Screenshot%202018-06-23%2016.51.56.png?dl=1\" width=\"300px\"><br></center>\n\n<ul>\n<li>서로 scale 이 다른 feature 를 state 로 사용할 때에 Neural Net이 일반화에서 어려움을 겪는다. <ul>\n<li>이걸 해결하기 위해서는 원래 직접 스케일을 조정해주었음.</li>\n</ul>\n</li>\n<li>하지만 각 layer 의 Input 을 Unit Gaussian 이 되도록 강제하는 BatchNormalization을 사용하여 이 문제를 해결하였다.</li>\n</ul>\n<h3 id=\"3-4-Noise-Process\"><a href=\"#3-4-Noise-Process\" class=\"headerlink\" title=\"3.4 Noise Process\"></a>3.4 Noise Process</h3><p>DDPG 에서는 Exploration을 위해서 output 으로 나온 행동에 노이즈를 추가해줍니다.</p>\n<center><br>ORNSTEIN UHLENBECK PROCESS(이하 OU) : $dx_t = \\theta (\\mu - x_t) dt + \\sigma dW_t$<br></center>\n\n<ul>\n<li>OU Process는 평균으로 회귀하는 random process 입니다.</li>\n<li>$\\theta$ 는 얼마나 빨리 평균으로 회귀할 지를 나타내는 파라미터이며  $\\mu$ 는 평균을 의미합니다.</li>\n<li>$\\sigma$ 는 process 의 변동성을 의미하며 $W_t$ 는 Wiener process 를 의미합니다. <br></li>\n<li>따라서 이전의 noise들과 temporally correlated 입니다.</li>\n<li>위와 같은 temporally correlated noise process를 사용하는 이유는 physical control 과 같은 관성이 있는 환경에서 학습 시킬 때 보다 효과적이기 때문입니다.</li>\n</ul>\n<h3 id=\"3-5-Diagram-amp-Pseudocode\"><a href=\"#3-5-Diagram-amp-Pseudocode\" class=\"headerlink\" title=\"3.5 Diagram &amp; Pseudocode\"></a>3.5 Diagram &amp; Pseudocode</h3><p><img src=\"https://www.dropbox.com/s/0ffb2c9irctjx2n/Screenshot%202018-06-23%2017.58.57.png?dl=1\"> </p>\n<ul>\n<li>DDPG 의 학습 과정을 간단히 도식화 해본 다이어 그램입니다. </li>\n</ul>\n<center><br><img src=\"https://www.dropbox.com/s/fd0nj7goixfnd6z/Screenshot%202018-06-23%2018.02.13.png?dl=1\" width=\"500px\"></center>\n\n<ul>\n<li>DDPG 의 알고리즘 수도코드입니다. 모든 항목을 위에서 설명했으니 순서대로 보시면 이해에 도움이 될것입니다. </li>\n</ul>\n<h2 id=\"4-Results\"><a href=\"#4-Results\" class=\"headerlink\" title=\"4. Results\"></a>4. Results</h2><h3 id=\"4-1-Variants-of-DPG\"><a href=\"#4-1-Variants-of-DPG\" class=\"headerlink\" title=\"4.1 Variants of DPG\"></a>4.1 Variants of DPG</h3><center><br><img src=\"https://www.dropbox.com/s/k8q6tih85lgow5l/Screenshot%202018-06-23%2018.08.05.png?dl=1\" width=\"500px\"></center>\n\n<ul>\n<li>original DPG 에 batchnorm 만 추가 (연한 회색), target network만 추가 (진한 회색), 둘 다 추가 (초록), pixel로만 학습 (파랑) . Target network가 성능을 가장 좌지우지한다.</li>\n</ul>\n<h3 id=\"4-2-Q-estimation-of-DDPG\"><a href=\"#4-2-Q-estimation-of-DDPG\" class=\"headerlink\" title=\"4.2 Q estimation of DDPG\"></a>4.2 Q estimation of DDPG</h3><center><br><img src=\"https://www.dropbox.com/s/vvcfoni0lqrisst/Screenshot%202018-06-23%2018.10.23.png?dl=1\" width=\"500px\"></center>\n\n<ul>\n<li>DQN 은 Q value 를 Over-estimate 하는 경향이 있었지만, DDPG 는 simple task에 대해서는 잘한다. 복잡한 문제에 대해서는 estimation을 잘 못했지만, 여전히 좋은 Policy를 찾아내었다.</li>\n</ul>\n<h3 id=\"4-3-Performance-Comparison\"><a href=\"#4-3-Performance-Comparison\" class=\"headerlink\" title=\"4.3 Performance Comparison\"></a>4.3 Performance Comparison</h3><center><br><img src=\"https://www.dropbox.com/s/u8ibmz9q4kfxh6l/Screenshot%202018-06-23%2018.11.40.png?dl=1\" width=\"500px\"></center>\n\n<ul>\n<li>Score 는 naive policy 를 0, ILQG (planning algorithm) 의 mean score를 1점으로 놓았을 때의 점수 Torcs 환경에 대해서만 raw reward 를 score로 사용.</li>\n</ul>\n<h2 id=\"5-Implementation-Details\"><a href=\"#5-Implementation-Details\" class=\"headerlink\" title=\"5. Implementation Details\"></a>5. Implementation Details</h2><h3 id=\"5-1-Hyper-parameters\"><a href=\"#5-1-Hyper-parameters\" class=\"headerlink\" title=\"5.1 Hyper parameters\"></a>5.1 Hyper parameters</h3><ul>\n<li>Optimizer : Adam<ul>\n<li>actor lr : 0.0001, critic lr : 0.001</li>\n</ul>\n</li>\n<li>Weight decay(L2) for critic(Q) = 0.001 </li>\n<li>discount factor, $\\gamma = 0.99 $</li>\n<li>soft target updates. $\\tau = 0.001 $</li>\n<li>Size of replay buffer = 1,000,000</li>\n<li>Orstein Uhlenbeck Process : $\\theta = 0.15$, $\\sigma = 0.2$</li>\n</ul>\n<h3 id=\"5-2-Etc\"><a href=\"#5-2-Etc\" class=\"headerlink\" title=\"5.2 Etc.\"></a>5.2 Etc.</h3><ul>\n<li>Final output layer of actor : tanh (행동의 최소 최대를 맞춰주기 위해서)</li>\n<li>low-dimentional 문제에서 네트워크는 2개의 hidden layer (1st layer 400 units, 2nd layer 300 units) 를 가진다. </li>\n<li>이미지를 통해서 학습시킬 때 : 3 convolutional layers (no pooling) with 32 filters at each layer.</li>\n<li>actor 와 critic 각각의 final layer(weight, bias 모두 ) 는 다음 범위의 uniform distribution 에서 샘플링한다. [ - 0.003, 0.003], [ - 0.0003 , 0.0003] . 이렇게 하는 이유는 가장 처음의 policy와 value 의 output 이 0에 가깝게 나오도록 하기 위함. </li>\n</ul>\n<p><br></p>\n<h2 id=\"6-Conclusion\"><a href=\"#6-Conclusion\" class=\"headerlink\" title=\"6.Conclusion\"></a>6.Conclusion</h2><p><br></p>\n<ul>\n<li>이 연구는 최근 딥러닝의 발전과 강화학습을 엮은 것으로 Continuous action space 를 가지는 문제를 robust 하게 풀어냄.</li>\n<li>non-linear function approximators 을 쓰는 것은 수렴을 보장하지 않지만, 여러 환경에 대해서 특별한 조작 없이 안정적으로 수렴하는 것을 실험으로 보여냄.</li>\n<li>Atari 도메인에서 DQN 보다 상당히 적은 step 만에 수렴하는 것을 실험을 통해서 알아냄. </li>\n<li>model-free 알고리즘은 좋은 solution 을 찾기 위해서는 많은 sample을 필요로 한다는 한계가 있지만 더 큰 시스템에서는 이러한 한계를 물리칠 정도로 중요한 역할을 하게 될 것임.</li>\n</ul>\n"},{"title":"Natural Policy Gradient","date":"2018-06-24T02:36:45.000Z","author":"이웅원, 차금강","subtitle":"피지여행 4번째 논문","_content":"\n<center> <img src=\"https://www.dropbox.com/s/yd0x14ljrhpnj1b/Screen%20Shot%202018-07-18%20at%201.08.05%20AM.png?dl=1\" width=\"600\"> </center>\n\n논문 저자 : Sham Kakade\n논문 링크 : https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf\nProceeding : ??\n정리 : 이웅원, 차금강\n\n---\n# 1. 왜 Natural Policy Gradient 인가?\n\n많은 연구들이 좋은 Policy $\\pi$를 Objective function의 Gradient 값을 따라서 찾는데 노력을 하고 있습니다. 하지만 기존의 Gradient descent rule을 따르는 업데이트 방식은 non-covariant 방식입니다. 즉, Gradient descent rule은 $\\bigtriangleup \\theta_i = \\dfrac{\\partial f}{\\partial \\theta_i}$을 이용해서 파라미터를 업데이트 하는데 $\\bigtriangleup \\theta_i$가 $f$를 구성하고 있는 파라미터들에 종속되어 있다는 것입니다. 본 논문에서는 $f$를 구성하고 있는 파라미터들과 독립적인 metric을 정의함으로써 covariant한 gradient을 제시하고 이를 이용하여 파라미터를 업데이트합니다.\n\n---\n# 2. Notation & Natural Gradient\n\n## 2.1 Notation\n\nFinite한 MDP tuple을 먼저 정의합니다. $(S, s_0, A, R, P)$로 정의됩니다. Finite한 상태의 set$(S)$, 에피소드 시작지점의 state$(s_0)$, finite한 행동의 set$(A)$, 보상 함수 $R: S \\times A \\to [0, R_{max}]$, 그리고 transition 모델$(P)$로 이루어져 있습니다. 그리고 모든 정책 $\\pi$는 ergodic하다고 가정합니다. 여기서 어떠한 통계적인 모델(모집단)과 모집단으로부터 추출된 모델(표본집단)이 같은 특성을 가질 때 ergodic하다고 합니다. 고등학교 수학에서 배우는 표본집단의 표준편차, 평균은 모집단의 그것과 같다는 것과 비슷한 개념입니다. 그리고 정책은 파라미터들 $\\theta$에 의해 지배될 때 상태 $s$에서 행동 $a$를 선택할 확률로 정의되며 $\\pi(a;s,\\theta)$와 같이 표현됩니다. 여타 다른 강화학습들과 같이 $Q^\\pi(s,a) = E_\\pi (\\Sigma^\\infty_{t=0}R(s_t, a_t)-\\eta(\\pi)|s_0 = s, a_0 = a)$를 정의합니다.\n\n다음 다른 정책 업데이트와 같이 Object function, $\\eta(\\pi_{\\theta})$을 정의합니다.\n$$\n\\eta(\\pi_{\\theta}) = \\Sigma_{s,a} \\rho^{\\pi}(s)\\pi(a;s,\\theta)Q^\\pi(s,a)\n$$\n\n\n여기서 $\\eta(\\pi_\\theta)$는 $\\pi_\\theta$에 종속되어 있으며, $\\pi_\\theta$는 $\\theta$에 종속되어 있습니다. 결국 $\\eta(\\pi_\\theta)$는 $\\theta$에 종속되어 있다고 할 수 있으며 $\\eta(\\pi_{\\theta})$는 $\\eta(\\theta)$로 표현할 수 있습니다. \n\n## 2.2 Natural Gradient\n\n2.1에 정의된 Objective function을 최대화 하는 방향으로 파라미터들을 업데이트 하는 것이 목적입니다. Euclidean space에서는 Objective function의 일반적인 gradient는 다음과 같이 구할 수 있습니다.\n\n$$\\bigtriangledown\\eta(\\pi_\\theta) = \\Sigma_{s,a}\\rho^\\pi(s)\\bigtriangledown\\pi(a;s,\\theta)Q^\\pi(s,a)$$\n\n하지만 뉴럴 네트워크에서 사용하는 parameter들은 매우 복잡하게 얽혀 있으며 이는 보통 생각하는 직선으로 이루어져 있는 Euclidean space가 아닙니다. 일반적으로 이는 구와 같이 휘어져 있는 평면(곡면)으로 이루어져 있으며 이를 리만 공간(Riemannian space)라고 합니다. 리만 공간에서는 Natural gradient가 steepest direction(업데이트 방향)이며 이를 구하는 방법은 [Amari, Natural gradient works in efficiently in learning](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.452.7280&rep=rep1&type=pdf)에서 자세히 설명을 하고 있습니다. 이를 간단히 설명하면 다음과 같습니다. 파라미터로 구성된 Matrix의 Positive-Definite Matrix인 $G(\\theta)$를 이용하여 구할 수 있습니다. 정확한 리만 공간에서의 steepest gradient(natural gradient)를 $\\widetilde \\bigtriangledown \\eta(\\theta)$라고 한다면 다음과 같이 표현될 수 있습니다.\n\n$$\\widetilde \\bigtriangledown\\eta(\\theta)=G(\\theta)^{-1}\\bigtriangledown\\eta(\\theta)$$\n\n위의 식을 이용하여 리만 공간에서의 파라미터를 업데이트 하면 아래의 식과 같이 표현될 수 있습니다.\n\n$$\\theta_{k+1}=\\theta_k-\\alpha_tG(\\theta)^{-1}\\bigtriangledown\\eta(\\theta)$$\n\n여기서 끝나는 것이 아니라 여전히 문제가 하나 남아 있습니다. 우리는 이 문제를 뉴럴넷을 이용하여 구성하고 해결하고 있습니다. 뉴럴넷은 여러가지 파라미터셋들로 구성될 수 있습니다. 하지만 우연의 일치로 다른 파라미터셋을 가지지만 같은 policy를 가질 수 있습니다. 이 경우 steepest  direction는 같은 policy이기 때문에 같은 방향을 가리키고 있어야 하는데 non-covariant한 경우 그렇지 못합니다. 이 이유로 느린 학습이 야기됩니다. 이 문제를 해결하기 위해 단순히 Positive-Definite Matrix인 $G(\\theta)$를 사용하지 않고 [Fisher Information Matrix](rxiv.org/abs/1707.06347)(이하 FIM)인 $F_s(\\theta)$를 사용하면 이를 해결할 수 있다고 서술하고 있습니다. FIM는 어떤 확률 변수의 관측값으로부터 확률 변수의 분포의 매개변수에 대해 유추할 수 있는 정보의 양입니다. 어떠한 확률변수 $X$가 미지의 매개변수 $\\theta$에 의해 정의되는 분포를 따른다고 하면 $X=x$일때 FIM은 다음과 같이 정의됩니다.\n\n$$F_x(\\theta)=E[(\\dfrac{\\partial}{\\partial\\theta}logPr(x|\\theta))^2]$$\n\n강화학습에서는 정보 $x$는 에피소드에 의해 관측된 상태값 $s$이며 매개변수 $\\theta$에 의해 선택될 수 있는 행동에 대한 분포가 나오게 됩니다. 이에 의해 위의 FIM는 다음과 같이 표현될 수 있습니다.\n\n$$F_s(\\theta) \\equiv E_{\\pi(a;s,\\theta)}[(\\dfrac{\\partial}{\\partial\\theta}log\\pi(a;s,\\theta))^2] =E_{pi(a;s,\\theta)}[\\dfrac{\\partial log\\pi(a;s,\\theta)}{\\partial \\theta_i}\\dfrac{\\partial log\\pi(a;s,\\theta)}{\\partial\\theta_j}]$$\n\n그리고 위의 정리된 식들을 이용하여 Objective function을 정리하면 아래의 식과 같이 표현됩니다.\n\n$$F(\\theta) = E_{\\rho^{\\pi}(s)}[F_s(\\theta)]$$\n\n또한 이 Fisher Information Matrix에 정의된 이 metric은 다음과 같은 성질을 가지고 있습니다. 업데이트되는 파라미터에 의해 구성되는 Manifold(곡률을 가지는)에 기반한 metric입니다. 확률분포(본 논문에서는 $\\pi(a;s,\\theta)$)를 구성하는 파라미터($\\theta$)의 변화에 독립적입니다. 어떠한 coordinate를 선택하느냐에 따라 변화하지 않습니다. 마지막으로 positive-definite한 값을 가집니다. 이렇기 때문에 steepest gradient에서 Objective function의 방향을 알기 위해 사용한 방법과 같은 방법으로 Natural gradient direction을 다음과 같이 구할 수 있습니다.\n\n$$\\widetilde{\\bigtriangledown}\\eta(\\theta) \\equiv F(\\theta)^{-1}\\bigtriangledown\\eta(\\theta)$$\n\n---\n# 3. Natural Gradient와 Policy Iteration\n\n3장에서는 Natural gradient를 통한 policy iteration을 수행하여 실제로 정책의 향상이 있는지를 증명합니다. 본 논문에서 $Q^\\pi(s,a)$는 compatible function approximator $f^\\pi(s,a;w)$에 의해 정의됩니다.\n\n## 3.1 Compatible Function Approximation\n\n3.1절에서는 정책이 업데이트 될 때, value function approximator, $f^\\pi(s,a;w)$도 같이 실제 값과 가까워지는지를 증명합니다.\n\n본 증명을 수행하기 전에 몇가지 가정과 정의가 필요합니다.\n\n파라미터들의 집합, $\\theta$, 그리고 선형 행렬인 $\\omega$가 정의되며 이를 이용하여 다음의 정의가 이루어집니다.\n$$\n\\psi^{\\pi}(s,a) = \\bigtriangledown log\\pi(a;s,\\theta), f^\\pi(s,a;\\omega) = \\omega^T\\psi^\\pi(s,a)\n$$\n여기서, $[\\bigtriangledown log\\pi(a;s,\\theta)]_i=\\partial log\\pi(a;s,\\theta)/\\partial\\theta_i$입니다. 그리고 compatible function approximator는 실제 $Q^\\pi(s,a)$에 가까워져야 하므로 $\\epsilon(\\omega,\\pi)=\\Sigma_{s,a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)(f^\\pi(s,a;\\omega)-Q^\\pi(s,a))^2$가 정의됩니다. 그리고 $\\widetilde \\omega$는 $\\epsilon(\\omega, \\pi_\\theta)$를 최소화하는 $\\omega$라고 가정합니다.\n\n최종적으로 $\\widetilde \\omega = \\widetilde \\bigtriangledown \\eta(\\theta)$이면 function approximator의 gradient 방향과 정책의 gradient 방향이 같다는 의미입니다.\n\n증명) $\\widetilde \\omega$ 가 squared error $\\epsilon(\\omega, \\pi)$를 최소화 한다면, $\\dfrac{\\partial\\epsilon}{\\partial\\omega_i}=0$입니다.\n$$\n\\dfrac{\\partial}{\\partial\\omega_i}\\Sigma_{s,a}\\rho^\\pi(s)\\pi(a;s,\\theta){(\\color{red}f^\\pi(s,a;w)\\color{black}-Q^\\pi(s,a))^2}=0\n$$\n\n$$\n\\dfrac{\\partial}{\\partial\\omega_i}\\Sigma_{s,a}\\rho^\\pi(s)\\pi(a;s,\\theta)(\\color{red}\\psi^\\pi(s,a)\\widetilde\\omega\\color{black}-Q^\\pi(s,a))^2 = 0\n$$\n\n$$\n\\Sigma_{s,a}\\rho^\\pi(s)\\pi(a;s,\\theta)\\psi^\\pi(s,a)\\color{blue}(\\psi^\\pi(s,a)^T\\widetilde\\omega-Q^\\pi(s,a))\\color{black} = 0\n$$\n\n$$\n\\Sigma_{s,a}\\rho^\\pi(s)\\pi(a;s,\\theta)\\psi^\\pi(s,a)\\color{blue}\\psi^\\pi(s,a)^T\\widetilde\\omega=\\color{black}\\Sigma_{s,a}\\rho^\\pi(s)\\pi(a;s,\\theta)\\psi^\\pi(s,a)\\color{blue}Q^\\pi(s,a)\n$$\n\n$$\n\\color{green}\\psi^\\pi(s,a)\\psi^\\pi(s,a)^T\\color{black} = \\bigtriangledown log\\pi(a;s,\\theta) \\bigtriangledown log\\pi(a;s,\\theta) = \\color{green} F_s(\\theta)\n$$\n\n$$\n\\Sigma_{s,a}\\rho^\\pi(s)\\pi(a;s,\\theta)\\psi^\\pi(s,a)\\psi^\\pi(s,a)^T= E_{\\rho^\\pi(s)}[F_s(\\theta)] = F(\\theta)\n$$\n\n$$\n\\widetilde\\bigtriangledown\\eta(\\theta)\\equiv F(\\theta)^{-1}\\bigtriangledown\\eta(\\theta)\n$$\n\n수식을 따라 정리하면 정리가 됩니다.\n\n## 3.2 Greedy Policy Improvement\n\n3.2절에서는 natrual gradient는 다른 policy iteration 방법처럼 단지 현재보다만 좋은 방법을 선택하도록 업데이트하는 것이 아니라 현재보다 좋은 것 중에 가장 좋은 방법을 선택한다고 합니다. 이것을 특수한 정책을 가지는 상황안에서 학습속도($\\alpha$)를 무한대로 가져감으로써 어떤 action을 선택하는지를 알아봅니다.\n\n3.2절에서는 정책이 다음과 같은 형태를 가진다고 가정합니다.\n$$\n\\pi(a;s,\\theta) \\propto exp(\\theta^T\\phi_{sa})\n$$\n그리고 3.1절처럼 $\\widetilde\\omega$가 approximation error인 $\\epsilon$을 최소화하는 파라미터라고 가정하지만 gradient마저 최소화되게 하는 파라미터는 아니라고 가정합니다.($\\widetilde\\bigtriangledown\\eta(\\theta)\\neq0$)\n\n그리고 학습속도가 무한대일 때의 정책의 notation을 다음과 같이 표현합니다.\n$$\n\\pi_\\infty(a;s) = lim_{\\alpha\\rightarrow\\infty}\\pi(a;s,\\theta + \\alpha\\widetilde\\bigtriangledown\\eta(\\theta))\n$$\n증명을 들어가는 시작으로 3.1절의 결과를 이용하여 function approximator($f^\\pi(s,a;\\omega)$)는 다음과 같이 쓸 수 있습니다.\n$$\nf^\\pi(s,a;\\omega) = \\widetilde\\bigtriangledown\\eta(\\theta)^T\\psi^\\pi(s,a)\n$$\n그리고 가정에 의해 다음과 같이 표기될 수 있습니다.\n$$\n\\psi^\\pi(s,a) = \\phi_{sa}-E_{\\pi(a\\prime;s,\\theta)}[\\phi_{sa\\prime}]\n$$\n다음 function approximator는 다음과 같이 다시 쓸 수 있습니다.\n$$\nf_\\pi(s,a;\\omega) = \\widetilde\\bigtriangledown\\eta(\\theta)^T(\\phi_{sa}-E_{\\pi(a\\prime,s,\\theta)}[\\phi_{sa\\prime}])\n$$\nGreedy improvement와 같이 function approxmiator의 가장 높은 행동을 선택하는 것을 정책으로 합니다.($argmax_{a\\prime}f^\\pi(s,a)=argmax_{a\\prime}\\alpha\\widetilde\\bigtriangledown\\eta(\\theta)^T\\phi_{sa\\prime}$)\n\n그리고 정책의 업데이트는 다음과 같이 이루어집니다.\n$$\n\\pi(a;s,\\theta+\\alpha\\widetilde\\bigtriangledown\\eta(\\theta)) \\propto exp(\\theta^T\\phi_{sa}+\\alpha\\widetilde\\bigtriangledown\\eta(\\theta)^T\\phi_{sa})\n$$\n3.2절 초입의 가정($\\widetilde\\bigtriangledown\\eta(\\theta)\\neq0$)에 의해 위의 식에서 $\\alpha$가 무한대로 간다면 우측 식에서 $\\theta^T\\phi_{sa}$의 항은 다른 항에 비해 매우 작은 값을 가지게 되므로 $\\alpha\\widetilde\\bigtriangledown\\eta(\\theta)^T\\phi_{sa}$의 지배를 받게 됩니다.\n\n그러므로 $\\pi_\\infty=0$ 과 $a∉argmax_a\\widetilde\\bigtriangledownη(θ)Tϕsa′$은 필요충분조건이 됩니다. 이것은 npg가 단지 더 좋은 행동이 아니라 가장 좋은 행동을 취하는 것을 뜻합니다.\n\n## 3.3 General Parameterized Policy\n\n일반적인 정책에서 또한 npg는 가장 좋은 행동을 선택하는 쪽으로 학습합니다.\n\n만약 $\\widetilde\\omega$가 approximation error을 최소화 하는 파라미터라고 가정하고 파라미터는 $\\theta\\prime=\\theta+\\alpha\\widetilde\\bigtriangledown\\eta(\\theta)$의 방식으로 업데이트 합니다. 일반적으로 업데이트를 $\\theta\\prime=\\theta+\\bigtriangleup\\theta$로 표현하며 앞 문장의 방식으로 표현을 바꾼다면 $\\bigtriangleup\\theta = \\alpha\\widetilde\\bigtriangledown\\eta(\\theta)$가 됩니다. 그리고 3.1절의 결과에 의해 $\\bigtriangleup\\theta=\\alpha\\widetilde\\omega$로 표현할 수 있습니다. 이를 이용하여 Taylor expansion에 의해 다음과 같이 전개 될 수 있습니다.\n$$\n\\pi(a;s,\\theta\\prime)=\\pi(a;s,\\theta)+\\dfrac{\\partial\\pi(a;s,\\theta^T)}{\\partial\\theta}\\bigtriangleup\\theta+O(\\bigtriangleup\\theta^2)\n$$\n\n$$\n= \\pi(a;s,\\theta)(1+\\alpha\\psi(s,a)^T\\bigtriangleup\\theta)+O(\\bigtriangleup\\theta^2)\n$$\n\n$$\n= \\pi(a;s,\\theta)+(1+\\alpha\\psi(s,a)^T\\widetilde\\omega)+O(\\alpha^2)\n$$\n\n$$\n=\\pi(a;s,\\theta) + (1+\\alpha f^\\pi(s,a;\\widetilde\\omega))+O(\\alpha^2)\n$$\n\n정책 자체가 function approximator의 크기대로 업데이트가 되기 때문에 지역적으로 가장 좋은 행동의 확률은 커지고 다른 확률은 작아질 것입니다. 하지만 탐욕적으로 향상을 하더라도 그게 성능자체를 향상시키지는 않을 수도 있습니다. 그렇기 때문에 line search기법과 함께 사용할 경우 성능 향상을 보장할 수 있습니다.\n\n# 4. Metrics and Curvatures\n\n2절에서 설명하고 있는 Positive-Definite Matrix인 FIM이외의 다른 Matrix도 사용할 수 있습니다. 본 논문에서는 다음과 같이 설명하고 있습니다. 다양한 파라미터 추정에서 FIM은 Hessian Matrix에 수렴하지 않을 수 있다고 합니다. 이 말은 2nd order 수렴이 보장되지 않는다는 말입니다.  [Mackay](http://www.inference.org.uk/mackay/ica.pdf) 논문에서 hessian에서 data independant한 term을 metric으로 가져오는 방법을 제안했다. 그래서 performance를 2번 미분해보면 다음과 같다. 하지만 다음 식에서는 모든 항이 data dependent하다(Q가 있으니까). 첫 번째 항이 그나마 FIM과의 관련성이 있을 수 있지만 Q 값이 curvature에 weight를 주는 방식 때문에 다르다고 할 수 있다.\n\n$\\nabla^2\\eta(\\theta)=\\sum_{sa}\\rho^{\\pi}(s)(\\nabla^2\\pi(a;s)Q^{\\pi}(s,a)+\\nabla\\pi(a;s)\\nabla Q^{\\pi}(s,a)^T+\\nabla Q^{\\pi}(s,a)\\nabla\\pi(a;s)^T)$\n\nHessian Matrix는 무조건 Positive-Definite Matrix인 것이 아니기 때문에, 다라서 국부적 최대점이 될 때 까지 Hessian Matrix를 사용하는 것은 좋은 방법이 아니라고 설명하고 있습니다. 이를 극복하지 위해서 Conjugate methods가 효율적이라고 설명하고 있습니다. \n\n# 5. Experiment\n\n본 논문에서는 LQR, simple MDP, 그리고 tetris MDP에 대해서 실험을 진행했습니다. Practice에서는 FIM은 다음과 같은 식으로 업데이트합니다.\n\n$f\\leftarrow f+\\nabla log \\pi(a_t; s_t, \\theta)\\nabla log \\pi(a_t; s_t, \\theta)^T$\n\nT길이의 경로에 대해서 f/T를 이용해 F의 기대값($E$)를 구합니다.\n\n## 5.1 LQR(Linear Quadratic Regulator)\n\nAgent를 실험할 환경은 다음과 같은 dynamics를 가지고 있습니다.\n\n$x(t+1) = 0.7x(t)+u(t)+\\epsilon(t)$\n\n$u(t)$는 control 신호로서 에이전트의 행동입니다. $\\epsilon$은 noise distribution으로 환경에 가해지는 노이즈입니다. 에이전트의 목표는 적절한 $u(t)$를 통해 $x(t)$를 0으로 유지하는 것입니다. $x(t)$를 0으로 유지하기 위해서 필요한 소모값(cost)는 $x(t)^2$로 정의하며 cost를 최소화하도록 학습합니다. 이 시스템을 linear라고 부르는 것은 아래 그림과 같이 선형의 형태를 띄기 때문입니다. 이 논문에서는 실험할 때 복잡성을 더 해주기 위해 noise distribution인 $\\epsilon$을 dynamics에 추가하였습니다. \n\n이 실험에서 policy는 다음과 같이 설정하였습니다. 파라미터가 2개 밖에 없는 간단한 policy입니다.\n\n$\\pi(u;x,\\theta)\\propto exp(\\theta_1s_1x^2+\\theta_2s_2x)$\n\n이 policy를 간단히 그래프로 그려보면 다음과 같습니다. $\\theta_1$과 $\\theta_2$를 (0.5, 0.5), (1, 0), (0, 1)로 설정하고 $s_1$, $s_2$는, 1로 두었습니다. $x$는 -1~1사이의 범위로 그렸습니다. \n\n<center><img src='https://www.dropbox.com/s/v69qyrwn7zurk8c/Screenshot%202018-06-08%2014.57.07.png?dl=1' width='500px'></center>\n\n아래의 그림은 LQR을 학습한 그래프입니다. cost가 $x^2$이기 때문에 cost가 0으로 갈수록 agent는 0에서 안정적으로 머무른다고 볼 수 있다. 6개의 선 중에서 오른쪽 세 개가 일반적인 gradient 방법을 사용해서 학습한 결과이다. 그리고 왼쪽의 세 개의 선이 natural policy gradient를 통해 학습한 학습 곡선이다. 일반 gradient 방법보다 natural gradient가 훨씬 빠르게 학습한다(time 축이 log scale인 것을 감안하자).\n\n하지만 문제가 있다. npg를 학습한 세 개의 곡선은 $\\theta$를 rescale 한 것이다. $\\theta$앞에 곱해지는 숫자에 따라 학습의 과정이 다르다. 이 것은 coordinate에 따라 steepest gradient가 다르게 측정된다는 것이다. 즉, covariant gradient가 아니라는 뜻이다. 이 논문에서는 natural gradient를 통해 gradient가 covariant하도록 만들고 싶었는데 실패한 것이다. \n\n<center><img src=\"https://www.dropbox.com/s/fhn8cgje0rdws0i/Screenshot%202018-06-08%2023.13.37.png?dl=1\" width=\"300px\"></center>\n\nnatural gradient가 covariant하지 않은 이유는 Fisher Information Matrix가 예상했던 바와는 달리 invariant metric이 아니기 때문이다. 또한 FIM이 invariant metric이 아닌 이유는 FIM을 계산할 때 $\\rho_s$가 곱해지기 때문이다(state distribution에 대한 expectation. $\\rho_s$가 곱해지는 것이 invariant에 미치는 영향이 무엇인지는 모르겠다). 하지만 여전히 의의가 있는 것은 기존 gradient 방법들보다 훨씬 빠르게 학습한다는 것이다.\n\n## 5.2 Simple 2-state MDP\n\n이제 다른 예제에서 NPG를 테스트한다. 2개의 state만 가지는 MDP를 고려해보자. [그림출처](http://repository.cmu.edu/cgi/viewcontent.cgi?article=1080&context=robotics). 그림으로보면 다음과 같다. x=0 상태와 x=1 상태 두 개가 존재한다. 에이전트는 각 상태에서 다시 자신의 상태로 되돌아오는 행동을 하거나 다른 상태로 가는 행동을 할 수 있다. 상태 x=0에서 다시 자기 자신으로 돌아오면 1의 보상을 받고 상태 x=1에서 자기 자신으로 돌아오면 2의 보상을 받는다. 따라서 결국 optimal policy는 상태 x=1에서 계속 자기 자신으로 돌아오는 행동을 취하는 것이다. \n\n<img src=\"https://www.dropbox.com/s/g1x9yknzsrip59i/Screenshot%202018-06-08%2023.06.50.png?dl=1\">\n\n문제를 좀 어렵게 만들기 위해 state distribution을 다음과 같이 설정한다. 즉, 대부분의 경우에 상태 x=0에서 에이전트가 시작하는 것이다. \n$$\n\\rho(x=0)=0.8,  \\rho(x=1)=0.2\n$$\n일반적인 gradient 방법을 사용하면 다음과 같은 policy gradient 식에 따라서 업데이트를 하게 된다. 이 때, $\\rho(s)$가 gradient에 곱해지므로 상태적으로 상태 0에서의 gradient 값이 커지게 된다. 따라서 에이전트는 상태 0에서의 gradient(상태 0에서 스스로에게 돌아오는 행동을 취하도록 정책을 업데이트하는 gradient)를 따라 parameterized policy를 update한다. 따라서 아래 그림의 첫번째 그림에서처럼 Reward가 1에서 오랜 시간동안 머무른다. 즉, 에이전트가 상태 0에서 self-loop를 계속 돌고 있다는 것이다. $\\rho(x=0)$가 $10^{-7}$까지 떨어진다.\n\n$$\\nabla\\eta(\\theta)=\\sum_{s,a}\\rho^{\\pi}(s)\\nabla\\pi(a;s,\\theta)Q^{\\pi}(s,a)$$\n\n<center><img src=\"https://www.dropbox.com/s/xtb77mfazbppnss/Screenshot%202018-06-08%2023.14.24.png?dl=1\" width=\"300px\"></center>\n\n하지만 NPG를 사용할 경우에는 훨씬 빠르게 average reward가 2에 도달한다. gradient 방법이 $1.7X10^(7)$정도의 시간만에 2에 도달한 반면 NPG는 2만에 도달한다. 또한 $\\rho(x=0)$가 $10^{-5}$이하로 떨어지지 않는다.\n\n한 가지 그래프를 더 살펴보자. 다음 그래프는 parameter $\\theta$가 업데이트 되는 과정을 보여준다. 이 그래프에서는 parameter가 2개 있는 것이다. 일반적인 gradient가 아래 그래프에서 실선에 해당한다. 이 실선의 그래프는 보면 처음부터 중반까지 $\\theta_i$만 거의 업데이트하는 것을 볼 수 있다. 그에 비해 NPG는 두 개의 parameter를 균등하게 업데이트하는 것을 볼 수 있다. \n\n<center><img src=\"https://www.dropbox.com/s/g7pazozw2k6rd7x/Screenshot%202018-06-08%2023.23.25.png?dl=1\" width=\"300px\"></center>\n\npolicy가 $\\pi(a;s,\\theta)\\propto exp(\\theta_{sa})$일 때, 다음과 같이 $F_{-1}$이 gradient 앞에 weight로 곱해지는데 이게 $\\rho$와는 달리 각 parameter에 대해 균등하다. 따라서 위 그래프에서와 같이 각 parameter는 비슷한 비율로 업데이트가 되는 것이다.\n\n$$\\bar{\\nabla}\\eta(\\theta) = F^{-1}\\nabla\\eta(\\theta)$$\n\n## 5.3 Tetris\n\nNPG를 테스트할 tetris 예제는 Neuro Dynamic Programming 책에 소개되어있다. 다음 그림은 tetris 예제를 보여준다. 보통 그림에서와 같이 state의 feature를 정해준다. [그림 출처](http://slideplayer.com/slide/5215520/)\n\n<img src=\"https://www.dropbox.com/s/y1halso9yermy8s/Screenshot%202018-06-08%2023.44.34.png?dl=1\">\n\n이 예제에서도 exponantial family로 policy를 표현한다. $\\pi(a;s,\\theta) \\propto exp(\\theta^T\\phi_{sa})$ 로 표현한다.\n\ntetris는 linear function approximator와 greedy policy iteration을 사용할 경우 performance가 갑자기 떨어지는 현상이 있다. 밑의 그림에서 A의 spike가 있는 그래프가 이 경우이다. 그 밑에 낮게 누워있는 그래프는 일반적인 policy gradient 방법이다. 하지만 Natural policy gradient를 사용할 경우 B 그림에서 오른쪽 그래프와 같이 성능개선이 뚜렷하다. Policy Iteration 처럼 성능이 뚝 떨어지지 않고 안정적으로 유지한다. 또한 그림 C에서 보는 것처럼 오른쪽 그래프인 일반적인 gradient 방법보다 훨씬 빠르게 학습하는 것을 볼 수 있다.\n\n<img src=\"https://www.dropbox.com/s/pr6s2qrqaic0wyj/Screenshot%202018-06-08%2023.40.16.png?dl=1\">\n\n# 6. Discussion\n\nNatural Gradient Method는  Policy Iteration에서와 같이 Greedy Action을 선택하도록 학습합니다. Line search 기법과 함께 사용하면 더 Policy Iteration과 비슷해집니다. Greedy Policy Iteration에서와 비교하면 Performance Improvement가 보장됩니다. 하지만 FIM이 asymtotically Hessian으로 수렴하지 않습니다. 그렇기 때문에 Conjugate Gradient Method로 구하는 방법이 더 좋을수 있습니다.\n\n위의 실험과 이론에 의해서 Hessian과 FIM 두 방법은 상황에 따라서 다를 수 있습니다. Conjugate Gradient Method가 조금 더 빠르게 Maximum에 수렴하지만 성능은 Maximum에서 거의 변하지 않으므로 말하기 어렵습니다(무슨말인지 잘 모르겠음..).\n\n위의 몇가지 문제점을 극복하기 위한 추가적인 연구가 필요합니다.","source":"_posts/npg.md","raw":"---\ntitle: Natural Policy Gradient\ndate: 2018-06-24 11:36:45\ntags: [\"프로젝트\", \"피지여행\"]\ncategories: 프로젝트\nauthor: 이웅원, 차금강\nsubtitle: 피지여행 4번째 논문\n---\n\n<center> <img src=\"https://www.dropbox.com/s/yd0x14ljrhpnj1b/Screen%20Shot%202018-07-18%20at%201.08.05%20AM.png?dl=1\" width=\"600\"> </center>\n\n논문 저자 : Sham Kakade\n논문 링크 : https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf\nProceeding : ??\n정리 : 이웅원, 차금강\n\n---\n# 1. 왜 Natural Policy Gradient 인가?\n\n많은 연구들이 좋은 Policy $\\pi$를 Objective function의 Gradient 값을 따라서 찾는데 노력을 하고 있습니다. 하지만 기존의 Gradient descent rule을 따르는 업데이트 방식은 non-covariant 방식입니다. 즉, Gradient descent rule은 $\\bigtriangleup \\theta_i = \\dfrac{\\partial f}{\\partial \\theta_i}$을 이용해서 파라미터를 업데이트 하는데 $\\bigtriangleup \\theta_i$가 $f$를 구성하고 있는 파라미터들에 종속되어 있다는 것입니다. 본 논문에서는 $f$를 구성하고 있는 파라미터들과 독립적인 metric을 정의함으로써 covariant한 gradient을 제시하고 이를 이용하여 파라미터를 업데이트합니다.\n\n---\n# 2. Notation & Natural Gradient\n\n## 2.1 Notation\n\nFinite한 MDP tuple을 먼저 정의합니다. $(S, s_0, A, R, P)$로 정의됩니다. Finite한 상태의 set$(S)$, 에피소드 시작지점의 state$(s_0)$, finite한 행동의 set$(A)$, 보상 함수 $R: S \\times A \\to [0, R_{max}]$, 그리고 transition 모델$(P)$로 이루어져 있습니다. 그리고 모든 정책 $\\pi$는 ergodic하다고 가정합니다. 여기서 어떠한 통계적인 모델(모집단)과 모집단으로부터 추출된 모델(표본집단)이 같은 특성을 가질 때 ergodic하다고 합니다. 고등학교 수학에서 배우는 표본집단의 표준편차, 평균은 모집단의 그것과 같다는 것과 비슷한 개념입니다. 그리고 정책은 파라미터들 $\\theta$에 의해 지배될 때 상태 $s$에서 행동 $a$를 선택할 확률로 정의되며 $\\pi(a;s,\\theta)$와 같이 표현됩니다. 여타 다른 강화학습들과 같이 $Q^\\pi(s,a) = E_\\pi (\\Sigma^\\infty_{t=0}R(s_t, a_t)-\\eta(\\pi)|s_0 = s, a_0 = a)$를 정의합니다.\n\n다음 다른 정책 업데이트와 같이 Object function, $\\eta(\\pi_{\\theta})$을 정의합니다.\n$$\n\\eta(\\pi_{\\theta}) = \\Sigma_{s,a} \\rho^{\\pi}(s)\\pi(a;s,\\theta)Q^\\pi(s,a)\n$$\n\n\n여기서 $\\eta(\\pi_\\theta)$는 $\\pi_\\theta$에 종속되어 있으며, $\\pi_\\theta$는 $\\theta$에 종속되어 있습니다. 결국 $\\eta(\\pi_\\theta)$는 $\\theta$에 종속되어 있다고 할 수 있으며 $\\eta(\\pi_{\\theta})$는 $\\eta(\\theta)$로 표현할 수 있습니다. \n\n## 2.2 Natural Gradient\n\n2.1에 정의된 Objective function을 최대화 하는 방향으로 파라미터들을 업데이트 하는 것이 목적입니다. Euclidean space에서는 Objective function의 일반적인 gradient는 다음과 같이 구할 수 있습니다.\n\n$$\\bigtriangledown\\eta(\\pi_\\theta) = \\Sigma_{s,a}\\rho^\\pi(s)\\bigtriangledown\\pi(a;s,\\theta)Q^\\pi(s,a)$$\n\n하지만 뉴럴 네트워크에서 사용하는 parameter들은 매우 복잡하게 얽혀 있으며 이는 보통 생각하는 직선으로 이루어져 있는 Euclidean space가 아닙니다. 일반적으로 이는 구와 같이 휘어져 있는 평면(곡면)으로 이루어져 있으며 이를 리만 공간(Riemannian space)라고 합니다. 리만 공간에서는 Natural gradient가 steepest direction(업데이트 방향)이며 이를 구하는 방법은 [Amari, Natural gradient works in efficiently in learning](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.452.7280&rep=rep1&type=pdf)에서 자세히 설명을 하고 있습니다. 이를 간단히 설명하면 다음과 같습니다. 파라미터로 구성된 Matrix의 Positive-Definite Matrix인 $G(\\theta)$를 이용하여 구할 수 있습니다. 정확한 리만 공간에서의 steepest gradient(natural gradient)를 $\\widetilde \\bigtriangledown \\eta(\\theta)$라고 한다면 다음과 같이 표현될 수 있습니다.\n\n$$\\widetilde \\bigtriangledown\\eta(\\theta)=G(\\theta)^{-1}\\bigtriangledown\\eta(\\theta)$$\n\n위의 식을 이용하여 리만 공간에서의 파라미터를 업데이트 하면 아래의 식과 같이 표현될 수 있습니다.\n\n$$\\theta_{k+1}=\\theta_k-\\alpha_tG(\\theta)^{-1}\\bigtriangledown\\eta(\\theta)$$\n\n여기서 끝나는 것이 아니라 여전히 문제가 하나 남아 있습니다. 우리는 이 문제를 뉴럴넷을 이용하여 구성하고 해결하고 있습니다. 뉴럴넷은 여러가지 파라미터셋들로 구성될 수 있습니다. 하지만 우연의 일치로 다른 파라미터셋을 가지지만 같은 policy를 가질 수 있습니다. 이 경우 steepest  direction는 같은 policy이기 때문에 같은 방향을 가리키고 있어야 하는데 non-covariant한 경우 그렇지 못합니다. 이 이유로 느린 학습이 야기됩니다. 이 문제를 해결하기 위해 단순히 Positive-Definite Matrix인 $G(\\theta)$를 사용하지 않고 [Fisher Information Matrix](rxiv.org/abs/1707.06347)(이하 FIM)인 $F_s(\\theta)$를 사용하면 이를 해결할 수 있다고 서술하고 있습니다. FIM는 어떤 확률 변수의 관측값으로부터 확률 변수의 분포의 매개변수에 대해 유추할 수 있는 정보의 양입니다. 어떠한 확률변수 $X$가 미지의 매개변수 $\\theta$에 의해 정의되는 분포를 따른다고 하면 $X=x$일때 FIM은 다음과 같이 정의됩니다.\n\n$$F_x(\\theta)=E[(\\dfrac{\\partial}{\\partial\\theta}logPr(x|\\theta))^2]$$\n\n강화학습에서는 정보 $x$는 에피소드에 의해 관측된 상태값 $s$이며 매개변수 $\\theta$에 의해 선택될 수 있는 행동에 대한 분포가 나오게 됩니다. 이에 의해 위의 FIM는 다음과 같이 표현될 수 있습니다.\n\n$$F_s(\\theta) \\equiv E_{\\pi(a;s,\\theta)}[(\\dfrac{\\partial}{\\partial\\theta}log\\pi(a;s,\\theta))^2] =E_{pi(a;s,\\theta)}[\\dfrac{\\partial log\\pi(a;s,\\theta)}{\\partial \\theta_i}\\dfrac{\\partial log\\pi(a;s,\\theta)}{\\partial\\theta_j}]$$\n\n그리고 위의 정리된 식들을 이용하여 Objective function을 정리하면 아래의 식과 같이 표현됩니다.\n\n$$F(\\theta) = E_{\\rho^{\\pi}(s)}[F_s(\\theta)]$$\n\n또한 이 Fisher Information Matrix에 정의된 이 metric은 다음과 같은 성질을 가지고 있습니다. 업데이트되는 파라미터에 의해 구성되는 Manifold(곡률을 가지는)에 기반한 metric입니다. 확률분포(본 논문에서는 $\\pi(a;s,\\theta)$)를 구성하는 파라미터($\\theta$)의 변화에 독립적입니다. 어떠한 coordinate를 선택하느냐에 따라 변화하지 않습니다. 마지막으로 positive-definite한 값을 가집니다. 이렇기 때문에 steepest gradient에서 Objective function의 방향을 알기 위해 사용한 방법과 같은 방법으로 Natural gradient direction을 다음과 같이 구할 수 있습니다.\n\n$$\\widetilde{\\bigtriangledown}\\eta(\\theta) \\equiv F(\\theta)^{-1}\\bigtriangledown\\eta(\\theta)$$\n\n---\n# 3. Natural Gradient와 Policy Iteration\n\n3장에서는 Natural gradient를 통한 policy iteration을 수행하여 실제로 정책의 향상이 있는지를 증명합니다. 본 논문에서 $Q^\\pi(s,a)$는 compatible function approximator $f^\\pi(s,a;w)$에 의해 정의됩니다.\n\n## 3.1 Compatible Function Approximation\n\n3.1절에서는 정책이 업데이트 될 때, value function approximator, $f^\\pi(s,a;w)$도 같이 실제 값과 가까워지는지를 증명합니다.\n\n본 증명을 수행하기 전에 몇가지 가정과 정의가 필요합니다.\n\n파라미터들의 집합, $\\theta$, 그리고 선형 행렬인 $\\omega$가 정의되며 이를 이용하여 다음의 정의가 이루어집니다.\n$$\n\\psi^{\\pi}(s,a) = \\bigtriangledown log\\pi(a;s,\\theta), f^\\pi(s,a;\\omega) = \\omega^T\\psi^\\pi(s,a)\n$$\n여기서, $[\\bigtriangledown log\\pi(a;s,\\theta)]_i=\\partial log\\pi(a;s,\\theta)/\\partial\\theta_i$입니다. 그리고 compatible function approximator는 실제 $Q^\\pi(s,a)$에 가까워져야 하므로 $\\epsilon(\\omega,\\pi)=\\Sigma_{s,a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)(f^\\pi(s,a;\\omega)-Q^\\pi(s,a))^2$가 정의됩니다. 그리고 $\\widetilde \\omega$는 $\\epsilon(\\omega, \\pi_\\theta)$를 최소화하는 $\\omega$라고 가정합니다.\n\n최종적으로 $\\widetilde \\omega = \\widetilde \\bigtriangledown \\eta(\\theta)$이면 function approximator의 gradient 방향과 정책의 gradient 방향이 같다는 의미입니다.\n\n증명) $\\widetilde \\omega$ 가 squared error $\\epsilon(\\omega, \\pi)$를 최소화 한다면, $\\dfrac{\\partial\\epsilon}{\\partial\\omega_i}=0$입니다.\n$$\n\\dfrac{\\partial}{\\partial\\omega_i}\\Sigma_{s,a}\\rho^\\pi(s)\\pi(a;s,\\theta){(\\color{red}f^\\pi(s,a;w)\\color{black}-Q^\\pi(s,a))^2}=0\n$$\n\n$$\n\\dfrac{\\partial}{\\partial\\omega_i}\\Sigma_{s,a}\\rho^\\pi(s)\\pi(a;s,\\theta)(\\color{red}\\psi^\\pi(s,a)\\widetilde\\omega\\color{black}-Q^\\pi(s,a))^2 = 0\n$$\n\n$$\n\\Sigma_{s,a}\\rho^\\pi(s)\\pi(a;s,\\theta)\\psi^\\pi(s,a)\\color{blue}(\\psi^\\pi(s,a)^T\\widetilde\\omega-Q^\\pi(s,a))\\color{black} = 0\n$$\n\n$$\n\\Sigma_{s,a}\\rho^\\pi(s)\\pi(a;s,\\theta)\\psi^\\pi(s,a)\\color{blue}\\psi^\\pi(s,a)^T\\widetilde\\omega=\\color{black}\\Sigma_{s,a}\\rho^\\pi(s)\\pi(a;s,\\theta)\\psi^\\pi(s,a)\\color{blue}Q^\\pi(s,a)\n$$\n\n$$\n\\color{green}\\psi^\\pi(s,a)\\psi^\\pi(s,a)^T\\color{black} = \\bigtriangledown log\\pi(a;s,\\theta) \\bigtriangledown log\\pi(a;s,\\theta) = \\color{green} F_s(\\theta)\n$$\n\n$$\n\\Sigma_{s,a}\\rho^\\pi(s)\\pi(a;s,\\theta)\\psi^\\pi(s,a)\\psi^\\pi(s,a)^T= E_{\\rho^\\pi(s)}[F_s(\\theta)] = F(\\theta)\n$$\n\n$$\n\\widetilde\\bigtriangledown\\eta(\\theta)\\equiv F(\\theta)^{-1}\\bigtriangledown\\eta(\\theta)\n$$\n\n수식을 따라 정리하면 정리가 됩니다.\n\n## 3.2 Greedy Policy Improvement\n\n3.2절에서는 natrual gradient는 다른 policy iteration 방법처럼 단지 현재보다만 좋은 방법을 선택하도록 업데이트하는 것이 아니라 현재보다 좋은 것 중에 가장 좋은 방법을 선택한다고 합니다. 이것을 특수한 정책을 가지는 상황안에서 학습속도($\\alpha$)를 무한대로 가져감으로써 어떤 action을 선택하는지를 알아봅니다.\n\n3.2절에서는 정책이 다음과 같은 형태를 가진다고 가정합니다.\n$$\n\\pi(a;s,\\theta) \\propto exp(\\theta^T\\phi_{sa})\n$$\n그리고 3.1절처럼 $\\widetilde\\omega$가 approximation error인 $\\epsilon$을 최소화하는 파라미터라고 가정하지만 gradient마저 최소화되게 하는 파라미터는 아니라고 가정합니다.($\\widetilde\\bigtriangledown\\eta(\\theta)\\neq0$)\n\n그리고 학습속도가 무한대일 때의 정책의 notation을 다음과 같이 표현합니다.\n$$\n\\pi_\\infty(a;s) = lim_{\\alpha\\rightarrow\\infty}\\pi(a;s,\\theta + \\alpha\\widetilde\\bigtriangledown\\eta(\\theta))\n$$\n증명을 들어가는 시작으로 3.1절의 결과를 이용하여 function approximator($f^\\pi(s,a;\\omega)$)는 다음과 같이 쓸 수 있습니다.\n$$\nf^\\pi(s,a;\\omega) = \\widetilde\\bigtriangledown\\eta(\\theta)^T\\psi^\\pi(s,a)\n$$\n그리고 가정에 의해 다음과 같이 표기될 수 있습니다.\n$$\n\\psi^\\pi(s,a) = \\phi_{sa}-E_{\\pi(a\\prime;s,\\theta)}[\\phi_{sa\\prime}]\n$$\n다음 function approximator는 다음과 같이 다시 쓸 수 있습니다.\n$$\nf_\\pi(s,a;\\omega) = \\widetilde\\bigtriangledown\\eta(\\theta)^T(\\phi_{sa}-E_{\\pi(a\\prime,s,\\theta)}[\\phi_{sa\\prime}])\n$$\nGreedy improvement와 같이 function approxmiator의 가장 높은 행동을 선택하는 것을 정책으로 합니다.($argmax_{a\\prime}f^\\pi(s,a)=argmax_{a\\prime}\\alpha\\widetilde\\bigtriangledown\\eta(\\theta)^T\\phi_{sa\\prime}$)\n\n그리고 정책의 업데이트는 다음과 같이 이루어집니다.\n$$\n\\pi(a;s,\\theta+\\alpha\\widetilde\\bigtriangledown\\eta(\\theta)) \\propto exp(\\theta^T\\phi_{sa}+\\alpha\\widetilde\\bigtriangledown\\eta(\\theta)^T\\phi_{sa})\n$$\n3.2절 초입의 가정($\\widetilde\\bigtriangledown\\eta(\\theta)\\neq0$)에 의해 위의 식에서 $\\alpha$가 무한대로 간다면 우측 식에서 $\\theta^T\\phi_{sa}$의 항은 다른 항에 비해 매우 작은 값을 가지게 되므로 $\\alpha\\widetilde\\bigtriangledown\\eta(\\theta)^T\\phi_{sa}$의 지배를 받게 됩니다.\n\n그러므로 $\\pi_\\infty=0$ 과 $a∉argmax_a\\widetilde\\bigtriangledownη(θ)Tϕsa′$은 필요충분조건이 됩니다. 이것은 npg가 단지 더 좋은 행동이 아니라 가장 좋은 행동을 취하는 것을 뜻합니다.\n\n## 3.3 General Parameterized Policy\n\n일반적인 정책에서 또한 npg는 가장 좋은 행동을 선택하는 쪽으로 학습합니다.\n\n만약 $\\widetilde\\omega$가 approximation error을 최소화 하는 파라미터라고 가정하고 파라미터는 $\\theta\\prime=\\theta+\\alpha\\widetilde\\bigtriangledown\\eta(\\theta)$의 방식으로 업데이트 합니다. 일반적으로 업데이트를 $\\theta\\prime=\\theta+\\bigtriangleup\\theta$로 표현하며 앞 문장의 방식으로 표현을 바꾼다면 $\\bigtriangleup\\theta = \\alpha\\widetilde\\bigtriangledown\\eta(\\theta)$가 됩니다. 그리고 3.1절의 결과에 의해 $\\bigtriangleup\\theta=\\alpha\\widetilde\\omega$로 표현할 수 있습니다. 이를 이용하여 Taylor expansion에 의해 다음과 같이 전개 될 수 있습니다.\n$$\n\\pi(a;s,\\theta\\prime)=\\pi(a;s,\\theta)+\\dfrac{\\partial\\pi(a;s,\\theta^T)}{\\partial\\theta}\\bigtriangleup\\theta+O(\\bigtriangleup\\theta^2)\n$$\n\n$$\n= \\pi(a;s,\\theta)(1+\\alpha\\psi(s,a)^T\\bigtriangleup\\theta)+O(\\bigtriangleup\\theta^2)\n$$\n\n$$\n= \\pi(a;s,\\theta)+(1+\\alpha\\psi(s,a)^T\\widetilde\\omega)+O(\\alpha^2)\n$$\n\n$$\n=\\pi(a;s,\\theta) + (1+\\alpha f^\\pi(s,a;\\widetilde\\omega))+O(\\alpha^2)\n$$\n\n정책 자체가 function approximator의 크기대로 업데이트가 되기 때문에 지역적으로 가장 좋은 행동의 확률은 커지고 다른 확률은 작아질 것입니다. 하지만 탐욕적으로 향상을 하더라도 그게 성능자체를 향상시키지는 않을 수도 있습니다. 그렇기 때문에 line search기법과 함께 사용할 경우 성능 향상을 보장할 수 있습니다.\n\n# 4. Metrics and Curvatures\n\n2절에서 설명하고 있는 Positive-Definite Matrix인 FIM이외의 다른 Matrix도 사용할 수 있습니다. 본 논문에서는 다음과 같이 설명하고 있습니다. 다양한 파라미터 추정에서 FIM은 Hessian Matrix에 수렴하지 않을 수 있다고 합니다. 이 말은 2nd order 수렴이 보장되지 않는다는 말입니다.  [Mackay](http://www.inference.org.uk/mackay/ica.pdf) 논문에서 hessian에서 data independant한 term을 metric으로 가져오는 방법을 제안했다. 그래서 performance를 2번 미분해보면 다음과 같다. 하지만 다음 식에서는 모든 항이 data dependent하다(Q가 있으니까). 첫 번째 항이 그나마 FIM과의 관련성이 있을 수 있지만 Q 값이 curvature에 weight를 주는 방식 때문에 다르다고 할 수 있다.\n\n$\\nabla^2\\eta(\\theta)=\\sum_{sa}\\rho^{\\pi}(s)(\\nabla^2\\pi(a;s)Q^{\\pi}(s,a)+\\nabla\\pi(a;s)\\nabla Q^{\\pi}(s,a)^T+\\nabla Q^{\\pi}(s,a)\\nabla\\pi(a;s)^T)$\n\nHessian Matrix는 무조건 Positive-Definite Matrix인 것이 아니기 때문에, 다라서 국부적 최대점이 될 때 까지 Hessian Matrix를 사용하는 것은 좋은 방법이 아니라고 설명하고 있습니다. 이를 극복하지 위해서 Conjugate methods가 효율적이라고 설명하고 있습니다. \n\n# 5. Experiment\n\n본 논문에서는 LQR, simple MDP, 그리고 tetris MDP에 대해서 실험을 진행했습니다. Practice에서는 FIM은 다음과 같은 식으로 업데이트합니다.\n\n$f\\leftarrow f+\\nabla log \\pi(a_t; s_t, \\theta)\\nabla log \\pi(a_t; s_t, \\theta)^T$\n\nT길이의 경로에 대해서 f/T를 이용해 F의 기대값($E$)를 구합니다.\n\n## 5.1 LQR(Linear Quadratic Regulator)\n\nAgent를 실험할 환경은 다음과 같은 dynamics를 가지고 있습니다.\n\n$x(t+1) = 0.7x(t)+u(t)+\\epsilon(t)$\n\n$u(t)$는 control 신호로서 에이전트의 행동입니다. $\\epsilon$은 noise distribution으로 환경에 가해지는 노이즈입니다. 에이전트의 목표는 적절한 $u(t)$를 통해 $x(t)$를 0으로 유지하는 것입니다. $x(t)$를 0으로 유지하기 위해서 필요한 소모값(cost)는 $x(t)^2$로 정의하며 cost를 최소화하도록 학습합니다. 이 시스템을 linear라고 부르는 것은 아래 그림과 같이 선형의 형태를 띄기 때문입니다. 이 논문에서는 실험할 때 복잡성을 더 해주기 위해 noise distribution인 $\\epsilon$을 dynamics에 추가하였습니다. \n\n이 실험에서 policy는 다음과 같이 설정하였습니다. 파라미터가 2개 밖에 없는 간단한 policy입니다.\n\n$\\pi(u;x,\\theta)\\propto exp(\\theta_1s_1x^2+\\theta_2s_2x)$\n\n이 policy를 간단히 그래프로 그려보면 다음과 같습니다. $\\theta_1$과 $\\theta_2$를 (0.5, 0.5), (1, 0), (0, 1)로 설정하고 $s_1$, $s_2$는, 1로 두었습니다. $x$는 -1~1사이의 범위로 그렸습니다. \n\n<center><img src='https://www.dropbox.com/s/v69qyrwn7zurk8c/Screenshot%202018-06-08%2014.57.07.png?dl=1' width='500px'></center>\n\n아래의 그림은 LQR을 학습한 그래프입니다. cost가 $x^2$이기 때문에 cost가 0으로 갈수록 agent는 0에서 안정적으로 머무른다고 볼 수 있다. 6개의 선 중에서 오른쪽 세 개가 일반적인 gradient 방법을 사용해서 학습한 결과이다. 그리고 왼쪽의 세 개의 선이 natural policy gradient를 통해 학습한 학습 곡선이다. 일반 gradient 방법보다 natural gradient가 훨씬 빠르게 학습한다(time 축이 log scale인 것을 감안하자).\n\n하지만 문제가 있다. npg를 학습한 세 개의 곡선은 $\\theta$를 rescale 한 것이다. $\\theta$앞에 곱해지는 숫자에 따라 학습의 과정이 다르다. 이 것은 coordinate에 따라 steepest gradient가 다르게 측정된다는 것이다. 즉, covariant gradient가 아니라는 뜻이다. 이 논문에서는 natural gradient를 통해 gradient가 covariant하도록 만들고 싶었는데 실패한 것이다. \n\n<center><img src=\"https://www.dropbox.com/s/fhn8cgje0rdws0i/Screenshot%202018-06-08%2023.13.37.png?dl=1\" width=\"300px\"></center>\n\nnatural gradient가 covariant하지 않은 이유는 Fisher Information Matrix가 예상했던 바와는 달리 invariant metric이 아니기 때문이다. 또한 FIM이 invariant metric이 아닌 이유는 FIM을 계산할 때 $\\rho_s$가 곱해지기 때문이다(state distribution에 대한 expectation. $\\rho_s$가 곱해지는 것이 invariant에 미치는 영향이 무엇인지는 모르겠다). 하지만 여전히 의의가 있는 것은 기존 gradient 방법들보다 훨씬 빠르게 학습한다는 것이다.\n\n## 5.2 Simple 2-state MDP\n\n이제 다른 예제에서 NPG를 테스트한다. 2개의 state만 가지는 MDP를 고려해보자. [그림출처](http://repository.cmu.edu/cgi/viewcontent.cgi?article=1080&context=robotics). 그림으로보면 다음과 같다. x=0 상태와 x=1 상태 두 개가 존재한다. 에이전트는 각 상태에서 다시 자신의 상태로 되돌아오는 행동을 하거나 다른 상태로 가는 행동을 할 수 있다. 상태 x=0에서 다시 자기 자신으로 돌아오면 1의 보상을 받고 상태 x=1에서 자기 자신으로 돌아오면 2의 보상을 받는다. 따라서 결국 optimal policy는 상태 x=1에서 계속 자기 자신으로 돌아오는 행동을 취하는 것이다. \n\n<img src=\"https://www.dropbox.com/s/g1x9yknzsrip59i/Screenshot%202018-06-08%2023.06.50.png?dl=1\">\n\n문제를 좀 어렵게 만들기 위해 state distribution을 다음과 같이 설정한다. 즉, 대부분의 경우에 상태 x=0에서 에이전트가 시작하는 것이다. \n$$\n\\rho(x=0)=0.8,  \\rho(x=1)=0.2\n$$\n일반적인 gradient 방법을 사용하면 다음과 같은 policy gradient 식에 따라서 업데이트를 하게 된다. 이 때, $\\rho(s)$가 gradient에 곱해지므로 상태적으로 상태 0에서의 gradient 값이 커지게 된다. 따라서 에이전트는 상태 0에서의 gradient(상태 0에서 스스로에게 돌아오는 행동을 취하도록 정책을 업데이트하는 gradient)를 따라 parameterized policy를 update한다. 따라서 아래 그림의 첫번째 그림에서처럼 Reward가 1에서 오랜 시간동안 머무른다. 즉, 에이전트가 상태 0에서 self-loop를 계속 돌고 있다는 것이다. $\\rho(x=0)$가 $10^{-7}$까지 떨어진다.\n\n$$\\nabla\\eta(\\theta)=\\sum_{s,a}\\rho^{\\pi}(s)\\nabla\\pi(a;s,\\theta)Q^{\\pi}(s,a)$$\n\n<center><img src=\"https://www.dropbox.com/s/xtb77mfazbppnss/Screenshot%202018-06-08%2023.14.24.png?dl=1\" width=\"300px\"></center>\n\n하지만 NPG를 사용할 경우에는 훨씬 빠르게 average reward가 2에 도달한다. gradient 방법이 $1.7X10^(7)$정도의 시간만에 2에 도달한 반면 NPG는 2만에 도달한다. 또한 $\\rho(x=0)$가 $10^{-5}$이하로 떨어지지 않는다.\n\n한 가지 그래프를 더 살펴보자. 다음 그래프는 parameter $\\theta$가 업데이트 되는 과정을 보여준다. 이 그래프에서는 parameter가 2개 있는 것이다. 일반적인 gradient가 아래 그래프에서 실선에 해당한다. 이 실선의 그래프는 보면 처음부터 중반까지 $\\theta_i$만 거의 업데이트하는 것을 볼 수 있다. 그에 비해 NPG는 두 개의 parameter를 균등하게 업데이트하는 것을 볼 수 있다. \n\n<center><img src=\"https://www.dropbox.com/s/g7pazozw2k6rd7x/Screenshot%202018-06-08%2023.23.25.png?dl=1\" width=\"300px\"></center>\n\npolicy가 $\\pi(a;s,\\theta)\\propto exp(\\theta_{sa})$일 때, 다음과 같이 $F_{-1}$이 gradient 앞에 weight로 곱해지는데 이게 $\\rho$와는 달리 각 parameter에 대해 균등하다. 따라서 위 그래프에서와 같이 각 parameter는 비슷한 비율로 업데이트가 되는 것이다.\n\n$$\\bar{\\nabla}\\eta(\\theta) = F^{-1}\\nabla\\eta(\\theta)$$\n\n## 5.3 Tetris\n\nNPG를 테스트할 tetris 예제는 Neuro Dynamic Programming 책에 소개되어있다. 다음 그림은 tetris 예제를 보여준다. 보통 그림에서와 같이 state의 feature를 정해준다. [그림 출처](http://slideplayer.com/slide/5215520/)\n\n<img src=\"https://www.dropbox.com/s/y1halso9yermy8s/Screenshot%202018-06-08%2023.44.34.png?dl=1\">\n\n이 예제에서도 exponantial family로 policy를 표현한다. $\\pi(a;s,\\theta) \\propto exp(\\theta^T\\phi_{sa})$ 로 표현한다.\n\ntetris는 linear function approximator와 greedy policy iteration을 사용할 경우 performance가 갑자기 떨어지는 현상이 있다. 밑의 그림에서 A의 spike가 있는 그래프가 이 경우이다. 그 밑에 낮게 누워있는 그래프는 일반적인 policy gradient 방법이다. 하지만 Natural policy gradient를 사용할 경우 B 그림에서 오른쪽 그래프와 같이 성능개선이 뚜렷하다. Policy Iteration 처럼 성능이 뚝 떨어지지 않고 안정적으로 유지한다. 또한 그림 C에서 보는 것처럼 오른쪽 그래프인 일반적인 gradient 방법보다 훨씬 빠르게 학습하는 것을 볼 수 있다.\n\n<img src=\"https://www.dropbox.com/s/pr6s2qrqaic0wyj/Screenshot%202018-06-08%2023.40.16.png?dl=1\">\n\n# 6. Discussion\n\nNatural Gradient Method는  Policy Iteration에서와 같이 Greedy Action을 선택하도록 학습합니다. Line search 기법과 함께 사용하면 더 Policy Iteration과 비슷해집니다. Greedy Policy Iteration에서와 비교하면 Performance Improvement가 보장됩니다. 하지만 FIM이 asymtotically Hessian으로 수렴하지 않습니다. 그렇기 때문에 Conjugate Gradient Method로 구하는 방법이 더 좋을수 있습니다.\n\n위의 실험과 이론에 의해서 Hessian과 FIM 두 방법은 상황에 따라서 다를 수 있습니다. Conjugate Gradient Method가 조금 더 빠르게 Maximum에 수렴하지만 성능은 Maximum에서 거의 변하지 않으므로 말하기 어렵습니다(무슨말인지 잘 모르겠음..).\n\n위의 몇가지 문제점을 극복하기 위한 추가적인 연구가 필요합니다.","slug":"npg","published":1,"updated":"2018-07-19T14:59:19.372Z","_id":"cjjgzyg4q000cxo15a2yfnmeh","comments":1,"layout":"post","photos":[],"link":"","content":"<center> <img src=\"https://www.dropbox.com/s/yd0x14ljrhpnj1b/Screen%20Shot%202018-07-18%20at%201.08.05%20AM.png?dl=1\" width=\"600\"> </center>\n\n<p>논문 저자 : Sham Kakade<br>논문 링크 : <a href=\"https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf\" target=\"_blank\" rel=\"noopener\">https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf</a><br>Proceeding : ??<br>정리 : 이웅원, 차금강</p>\n<hr>\n<h1 id=\"1-왜-Natural-Policy-Gradient-인가\"><a href=\"#1-왜-Natural-Policy-Gradient-인가\" class=\"headerlink\" title=\"1. 왜 Natural Policy Gradient 인가?\"></a>1. 왜 Natural Policy Gradient 인가?</h1><p>많은 연구들이 좋은 Policy $\\pi$를 Objective function의 Gradient 값을 따라서 찾는데 노력을 하고 있습니다. 하지만 기존의 Gradient descent rule을 따르는 업데이트 방식은 non-covariant 방식입니다. 즉, Gradient descent rule은 $\\bigtriangleup \\theta_i = \\dfrac{\\partial f}{\\partial \\theta_i}$을 이용해서 파라미터를 업데이트 하는데 $\\bigtriangleup \\theta_i$가 $f$를 구성하고 있는 파라미터들에 종속되어 있다는 것입니다. 본 논문에서는 $f$를 구성하고 있는 파라미터들과 독립적인 metric을 정의함으로써 covariant한 gradient을 제시하고 이를 이용하여 파라미터를 업데이트합니다.</p>\n<hr>\n<h1 id=\"2-Notation-amp-Natural-Gradient\"><a href=\"#2-Notation-amp-Natural-Gradient\" class=\"headerlink\" title=\"2. Notation &amp; Natural Gradient\"></a>2. Notation &amp; Natural Gradient</h1><h2 id=\"2-1-Notation\"><a href=\"#2-1-Notation\" class=\"headerlink\" title=\"2.1 Notation\"></a>2.1 Notation</h2><p>Finite한 MDP tuple을 먼저 정의합니다. $(S, s_0, A, R, P)$로 정의됩니다. Finite한 상태의 set$(S)$, 에피소드 시작지점의 state$(s_0)$, finite한 행동의 set$(A)$, 보상 함수 $R: S \\times A \\to [0, R_{max}]$, 그리고 transition 모델$(P)$로 이루어져 있습니다. 그리고 모든 정책 $\\pi$는 ergodic하다고 가정합니다. 여기서 어떠한 통계적인 모델(모집단)과 모집단으로부터 추출된 모델(표본집단)이 같은 특성을 가질 때 ergodic하다고 합니다. 고등학교 수학에서 배우는 표본집단의 표준편차, 평균은 모집단의 그것과 같다는 것과 비슷한 개념입니다. 그리고 정책은 파라미터들 $\\theta$에 의해 지배될 때 상태 $s$에서 행동 $a$를 선택할 확률로 정의되며 $\\pi(a;s,\\theta)$와 같이 표현됩니다. 여타 다른 강화학습들과 같이 $Q^\\pi(s,a) = E_\\pi (\\Sigma^\\infty_{t=0}R(s_t, a_t)-\\eta(\\pi)|s_0 = s, a_0 = a)$를 정의합니다.</p>\n<p>다음 다른 정책 업데이트와 같이 Object function, $\\eta(\\pi_{\\theta})$을 정의합니다.<br>$$<br>\\eta(\\pi_{\\theta}) = \\Sigma_{s,a} \\rho^{\\pi}(s)\\pi(a;s,\\theta)Q^\\pi(s,a)<br>$$</p>\n<p>여기서 $\\eta(\\pi_\\theta)$는 $\\pi_\\theta$에 종속되어 있으며, $\\pi_\\theta$는 $\\theta$에 종속되어 있습니다. 결국 $\\eta(\\pi_\\theta)$는 $\\theta$에 종속되어 있다고 할 수 있으며 $\\eta(\\pi_{\\theta})$는 $\\eta(\\theta)$로 표현할 수 있습니다. </p>\n<h2 id=\"2-2-Natural-Gradient\"><a href=\"#2-2-Natural-Gradient\" class=\"headerlink\" title=\"2.2 Natural Gradient\"></a>2.2 Natural Gradient</h2><p>2.1에 정의된 Objective function을 최대화 하는 방향으로 파라미터들을 업데이트 하는 것이 목적입니다. Euclidean space에서는 Objective function의 일반적인 gradient는 다음과 같이 구할 수 있습니다.</p>\n<p>$$\\bigtriangledown\\eta(\\pi_\\theta) = \\Sigma_{s,a}\\rho^\\pi(s)\\bigtriangledown\\pi(a;s,\\theta)Q^\\pi(s,a)$$</p>\n<p>하지만 뉴럴 네트워크에서 사용하는 parameter들은 매우 복잡하게 얽혀 있으며 이는 보통 생각하는 직선으로 이루어져 있는 Euclidean space가 아닙니다. 일반적으로 이는 구와 같이 휘어져 있는 평면(곡면)으로 이루어져 있으며 이를 리만 공간(Riemannian space)라고 합니다. 리만 공간에서는 Natural gradient가 steepest direction(업데이트 방향)이며 이를 구하는 방법은 <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.452.7280&amp;rep=rep1&amp;type=pdf\" target=\"_blank\" rel=\"noopener\">Amari, Natural gradient works in efficiently in learning</a>에서 자세히 설명을 하고 있습니다. 이를 간단히 설명하면 다음과 같습니다. 파라미터로 구성된 Matrix의 Positive-Definite Matrix인 $G(\\theta)$를 이용하여 구할 수 있습니다. 정확한 리만 공간에서의 steepest gradient(natural gradient)를 $\\widetilde \\bigtriangledown \\eta(\\theta)$라고 한다면 다음과 같이 표현될 수 있습니다.</p>\n<p>$$\\widetilde \\bigtriangledown\\eta(\\theta)=G(\\theta)^{-1}\\bigtriangledown\\eta(\\theta)$$</p>\n<p>위의 식을 이용하여 리만 공간에서의 파라미터를 업데이트 하면 아래의 식과 같이 표현될 수 있습니다.</p>\n<p>$$\\theta_{k+1}=\\theta_k-\\alpha_tG(\\theta)^{-1}\\bigtriangledown\\eta(\\theta)$$</p>\n<p>여기서 끝나는 것이 아니라 여전히 문제가 하나 남아 있습니다. 우리는 이 문제를 뉴럴넷을 이용하여 구성하고 해결하고 있습니다. 뉴럴넷은 여러가지 파라미터셋들로 구성될 수 있습니다. 하지만 우연의 일치로 다른 파라미터셋을 가지지만 같은 policy를 가질 수 있습니다. 이 경우 steepest  direction는 같은 policy이기 때문에 같은 방향을 가리키고 있어야 하는데 non-covariant한 경우 그렇지 못합니다. 이 이유로 느린 학습이 야기됩니다. 이 문제를 해결하기 위해 단순히 Positive-Definite Matrix인 $G(\\theta)$를 사용하지 않고 <a href=\"rxiv.org/abs/1707.06347\">Fisher Information Matrix</a>(이하 FIM)인 $F_s(\\theta)$를 사용하면 이를 해결할 수 있다고 서술하고 있습니다. FIM는 어떤 확률 변수의 관측값으로부터 확률 변수의 분포의 매개변수에 대해 유추할 수 있는 정보의 양입니다. 어떠한 확률변수 $X$가 미지의 매개변수 $\\theta$에 의해 정의되는 분포를 따른다고 하면 $X=x$일때 FIM은 다음과 같이 정의됩니다.</p>\n<p>$$F_x(\\theta)=E[(\\dfrac{\\partial}{\\partial\\theta}logPr(x|\\theta))^2]$$</p>\n<p>강화학습에서는 정보 $x$는 에피소드에 의해 관측된 상태값 $s$이며 매개변수 $\\theta$에 의해 선택될 수 있는 행동에 대한 분포가 나오게 됩니다. 이에 의해 위의 FIM는 다음과 같이 표현될 수 있습니다.</p>\n<p>$$F_s(\\theta) \\equiv E_{\\pi(a;s,\\theta)}[(\\dfrac{\\partial}{\\partial\\theta}log\\pi(a;s,\\theta))^2] =E_{pi(a;s,\\theta)}[\\dfrac{\\partial log\\pi(a;s,\\theta)}{\\partial \\theta_i}\\dfrac{\\partial log\\pi(a;s,\\theta)}{\\partial\\theta_j}]$$</p>\n<p>그리고 위의 정리된 식들을 이용하여 Objective function을 정리하면 아래의 식과 같이 표현됩니다.</p>\n<p>$$F(\\theta) = E_{\\rho^{\\pi}(s)}[F_s(\\theta)]$$</p>\n<p>또한 이 Fisher Information Matrix에 정의된 이 metric은 다음과 같은 성질을 가지고 있습니다. 업데이트되는 파라미터에 의해 구성되는 Manifold(곡률을 가지는)에 기반한 metric입니다. 확률분포(본 논문에서는 $\\pi(a;s,\\theta)$)를 구성하는 파라미터($\\theta$)의 변화에 독립적입니다. 어떠한 coordinate를 선택하느냐에 따라 변화하지 않습니다. 마지막으로 positive-definite한 값을 가집니다. 이렇기 때문에 steepest gradient에서 Objective function의 방향을 알기 위해 사용한 방법과 같은 방법으로 Natural gradient direction을 다음과 같이 구할 수 있습니다.</p>\n<p>$$\\widetilde{\\bigtriangledown}\\eta(\\theta) \\equiv F(\\theta)^{-1}\\bigtriangledown\\eta(\\theta)$$</p>\n<hr>\n<h1 id=\"3-Natural-Gradient와-Policy-Iteration\"><a href=\"#3-Natural-Gradient와-Policy-Iteration\" class=\"headerlink\" title=\"3. Natural Gradient와 Policy Iteration\"></a>3. Natural Gradient와 Policy Iteration</h1><p>3장에서는 Natural gradient를 통한 policy iteration을 수행하여 실제로 정책의 향상이 있는지를 증명합니다. 본 논문에서 $Q^\\pi(s,a)$는 compatible function approximator $f^\\pi(s,a;w)$에 의해 정의됩니다.</p>\n<h2 id=\"3-1-Compatible-Function-Approximation\"><a href=\"#3-1-Compatible-Function-Approximation\" class=\"headerlink\" title=\"3.1 Compatible Function Approximation\"></a>3.1 Compatible Function Approximation</h2><p>3.1절에서는 정책이 업데이트 될 때, value function approximator, $f^\\pi(s,a;w)$도 같이 실제 값과 가까워지는지를 증명합니다.</p>\n<p>본 증명을 수행하기 전에 몇가지 가정과 정의가 필요합니다.</p>\n<p>파라미터들의 집합, $\\theta$, 그리고 선형 행렬인 $\\omega$가 정의되며 이를 이용하여 다음의 정의가 이루어집니다.<br>$$<br>\\psi^{\\pi}(s,a) = \\bigtriangledown log\\pi(a;s,\\theta), f^\\pi(s,a;\\omega) = \\omega^T\\psi^\\pi(s,a)<br>$$<br>여기서, $[\\bigtriangledown log\\pi(a;s,\\theta)]_i=\\partial log\\pi(a;s,\\theta)/\\partial\\theta_i$입니다. 그리고 compatible function approximator는 실제 $Q^\\pi(s,a)$에 가까워져야 하므로 $\\epsilon(\\omega,\\pi)=\\Sigma_{s,a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)(f^\\pi(s,a;\\omega)-Q^\\pi(s,a))^2$가 정의됩니다. 그리고 $\\widetilde \\omega$는 $\\epsilon(\\omega, \\pi_\\theta)$를 최소화하는 $\\omega$라고 가정합니다.</p>\n<p>최종적으로 $\\widetilde \\omega = \\widetilde \\bigtriangledown \\eta(\\theta)$이면 function approximator의 gradient 방향과 정책의 gradient 방향이 같다는 의미입니다.</p>\n<p>증명) $\\widetilde \\omega$ 가 squared error $\\epsilon(\\omega, \\pi)$를 최소화 한다면, $\\dfrac{\\partial\\epsilon}{\\partial\\omega_i}=0$입니다.<br>$$<br>\\dfrac{\\partial}{\\partial\\omega_i}\\Sigma_{s,a}\\rho^\\pi(s)\\pi(a;s,\\theta){(\\color{red}f^\\pi(s,a;w)\\color{black}-Q^\\pi(s,a))^2}=0<br>$$</p>\n<p>$$<br>\\dfrac{\\partial}{\\partial\\omega_i}\\Sigma_{s,a}\\rho^\\pi(s)\\pi(a;s,\\theta)(\\color{red}\\psi^\\pi(s,a)\\widetilde\\omega\\color{black}-Q^\\pi(s,a))^2 = 0<br>$$</p>\n<p>$$<br>\\Sigma_{s,a}\\rho^\\pi(s)\\pi(a;s,\\theta)\\psi^\\pi(s,a)\\color{blue}(\\psi^\\pi(s,a)^T\\widetilde\\omega-Q^\\pi(s,a))\\color{black} = 0<br>$$</p>\n<p>$$<br>\\Sigma_{s,a}\\rho^\\pi(s)\\pi(a;s,\\theta)\\psi^\\pi(s,a)\\color{blue}\\psi^\\pi(s,a)^T\\widetilde\\omega=\\color{black}\\Sigma_{s,a}\\rho^\\pi(s)\\pi(a;s,\\theta)\\psi^\\pi(s,a)\\color{blue}Q^\\pi(s,a)<br>$$</p>\n<p>$$<br>\\color{green}\\psi^\\pi(s,a)\\psi^\\pi(s,a)^T\\color{black} = \\bigtriangledown log\\pi(a;s,\\theta) \\bigtriangledown log\\pi(a;s,\\theta) = \\color{green} F_s(\\theta)<br>$$</p>\n<p>$$<br>\\Sigma_{s,a}\\rho^\\pi(s)\\pi(a;s,\\theta)\\psi^\\pi(s,a)\\psi^\\pi(s,a)^T= E_{\\rho^\\pi(s)}[F_s(\\theta)] = F(\\theta)<br>$$</p>\n<p>$$<br>\\widetilde\\bigtriangledown\\eta(\\theta)\\equiv F(\\theta)^{-1}\\bigtriangledown\\eta(\\theta)<br>$$</p>\n<p>수식을 따라 정리하면 정리가 됩니다.</p>\n<h2 id=\"3-2-Greedy-Policy-Improvement\"><a href=\"#3-2-Greedy-Policy-Improvement\" class=\"headerlink\" title=\"3.2 Greedy Policy Improvement\"></a>3.2 Greedy Policy Improvement</h2><p>3.2절에서는 natrual gradient는 다른 policy iteration 방법처럼 단지 현재보다만 좋은 방법을 선택하도록 업데이트하는 것이 아니라 현재보다 좋은 것 중에 가장 좋은 방법을 선택한다고 합니다. 이것을 특수한 정책을 가지는 상황안에서 학습속도($\\alpha$)를 무한대로 가져감으로써 어떤 action을 선택하는지를 알아봅니다.</p>\n<p>3.2절에서는 정책이 다음과 같은 형태를 가진다고 가정합니다.<br>$$<br>\\pi(a;s,\\theta) \\propto exp(\\theta^T\\phi_{sa})<br>$$<br>그리고 3.1절처럼 $\\widetilde\\omega$가 approximation error인 $\\epsilon$을 최소화하는 파라미터라고 가정하지만 gradient마저 최소화되게 하는 파라미터는 아니라고 가정합니다.($\\widetilde\\bigtriangledown\\eta(\\theta)\\neq0$)</p>\n<p>그리고 학습속도가 무한대일 때의 정책의 notation을 다음과 같이 표현합니다.<br>$$<br>\\pi_\\infty(a;s) = lim_{\\alpha\\rightarrow\\infty}\\pi(a;s,\\theta + \\alpha\\widetilde\\bigtriangledown\\eta(\\theta))<br>$$<br>증명을 들어가는 시작으로 3.1절의 결과를 이용하여 function approximator($f^\\pi(s,a;\\omega)$)는 다음과 같이 쓸 수 있습니다.<br>$$<br>f^\\pi(s,a;\\omega) = \\widetilde\\bigtriangledown\\eta(\\theta)^T\\psi^\\pi(s,a)<br>$$<br>그리고 가정에 의해 다음과 같이 표기될 수 있습니다.<br>$$<br>\\psi^\\pi(s,a) = \\phi_{sa}-E_{\\pi(a\\prime;s,\\theta)}[\\phi_{sa\\prime}]<br>$$<br>다음 function approximator는 다음과 같이 다시 쓸 수 있습니다.<br>$$<br>f_\\pi(s,a;\\omega) = \\widetilde\\bigtriangledown\\eta(\\theta)^T(\\phi_{sa}-E_{\\pi(a\\prime,s,\\theta)}[\\phi_{sa\\prime}])<br>$$<br>Greedy improvement와 같이 function approxmiator의 가장 높은 행동을 선택하는 것을 정책으로 합니다.($argmax_{a\\prime}f^\\pi(s,a)=argmax_{a\\prime}\\alpha\\widetilde\\bigtriangledown\\eta(\\theta)^T\\phi_{sa\\prime}$)</p>\n<p>그리고 정책의 업데이트는 다음과 같이 이루어집니다.<br>$$<br>\\pi(a;s,\\theta+\\alpha\\widetilde\\bigtriangledown\\eta(\\theta)) \\propto exp(\\theta^T\\phi_{sa}+\\alpha\\widetilde\\bigtriangledown\\eta(\\theta)^T\\phi_{sa})<br>$$<br>3.2절 초입의 가정($\\widetilde\\bigtriangledown\\eta(\\theta)\\neq0$)에 의해 위의 식에서 $\\alpha$가 무한대로 간다면 우측 식에서 $\\theta^T\\phi_{sa}$의 항은 다른 항에 비해 매우 작은 값을 가지게 되므로 $\\alpha\\widetilde\\bigtriangledown\\eta(\\theta)^T\\phi_{sa}$의 지배를 받게 됩니다.</p>\n<p>그러므로 $\\pi_\\infty=0$ 과 $a∉argmax_a\\widetilde\\bigtriangledownη(θ)Tϕsa′$은 필요충분조건이 됩니다. 이것은 npg가 단지 더 좋은 행동이 아니라 가장 좋은 행동을 취하는 것을 뜻합니다.</p>\n<h2 id=\"3-3-General-Parameterized-Policy\"><a href=\"#3-3-General-Parameterized-Policy\" class=\"headerlink\" title=\"3.3 General Parameterized Policy\"></a>3.3 General Parameterized Policy</h2><p>일반적인 정책에서 또한 npg는 가장 좋은 행동을 선택하는 쪽으로 학습합니다.</p>\n<p>만약 $\\widetilde\\omega$가 approximation error을 최소화 하는 파라미터라고 가정하고 파라미터는 $\\theta\\prime=\\theta+\\alpha\\widetilde\\bigtriangledown\\eta(\\theta)$의 방식으로 업데이트 합니다. 일반적으로 업데이트를 $\\theta\\prime=\\theta+\\bigtriangleup\\theta$로 표현하며 앞 문장의 방식으로 표현을 바꾼다면 $\\bigtriangleup\\theta = \\alpha\\widetilde\\bigtriangledown\\eta(\\theta)$가 됩니다. 그리고 3.1절의 결과에 의해 $\\bigtriangleup\\theta=\\alpha\\widetilde\\omega$로 표현할 수 있습니다. 이를 이용하여 Taylor expansion에 의해 다음과 같이 전개 될 수 있습니다.<br>$$<br>\\pi(a;s,\\theta\\prime)=\\pi(a;s,\\theta)+\\dfrac{\\partial\\pi(a;s,\\theta^T)}{\\partial\\theta}\\bigtriangleup\\theta+O(\\bigtriangleup\\theta^2)<br>$$</p>\n<p>$$<br>= \\pi(a;s,\\theta)(1+\\alpha\\psi(s,a)^T\\bigtriangleup\\theta)+O(\\bigtriangleup\\theta^2)<br>$$</p>\n<p>$$<br>= \\pi(a;s,\\theta)+(1+\\alpha\\psi(s,a)^T\\widetilde\\omega)+O(\\alpha^2)<br>$$</p>\n<p>$$<br>=\\pi(a;s,\\theta) + (1+\\alpha f^\\pi(s,a;\\widetilde\\omega))+O(\\alpha^2)<br>$$</p>\n<p>정책 자체가 function approximator의 크기대로 업데이트가 되기 때문에 지역적으로 가장 좋은 행동의 확률은 커지고 다른 확률은 작아질 것입니다. 하지만 탐욕적으로 향상을 하더라도 그게 성능자체를 향상시키지는 않을 수도 있습니다. 그렇기 때문에 line search기법과 함께 사용할 경우 성능 향상을 보장할 수 있습니다.</p>\n<h1 id=\"4-Metrics-and-Curvatures\"><a href=\"#4-Metrics-and-Curvatures\" class=\"headerlink\" title=\"4. Metrics and Curvatures\"></a>4. Metrics and Curvatures</h1><p>2절에서 설명하고 있는 Positive-Definite Matrix인 FIM이외의 다른 Matrix도 사용할 수 있습니다. 본 논문에서는 다음과 같이 설명하고 있습니다. 다양한 파라미터 추정에서 FIM은 Hessian Matrix에 수렴하지 않을 수 있다고 합니다. 이 말은 2nd order 수렴이 보장되지 않는다는 말입니다.  <a href=\"http://www.inference.org.uk/mackay/ica.pdf\" target=\"_blank\" rel=\"noopener\">Mackay</a> 논문에서 hessian에서 data independant한 term을 metric으로 가져오는 방법을 제안했다. 그래서 performance를 2번 미분해보면 다음과 같다. 하지만 다음 식에서는 모든 항이 data dependent하다(Q가 있으니까). 첫 번째 항이 그나마 FIM과의 관련성이 있을 수 있지만 Q 값이 curvature에 weight를 주는 방식 때문에 다르다고 할 수 있다.</p>\n<p>$\\nabla^2\\eta(\\theta)=\\sum_{sa}\\rho^{\\pi}(s)(\\nabla^2\\pi(a;s)Q^{\\pi}(s,a)+\\nabla\\pi(a;s)\\nabla Q^{\\pi}(s,a)^T+\\nabla Q^{\\pi}(s,a)\\nabla\\pi(a;s)^T)$</p>\n<p>Hessian Matrix는 무조건 Positive-Definite Matrix인 것이 아니기 때문에, 다라서 국부적 최대점이 될 때 까지 Hessian Matrix를 사용하는 것은 좋은 방법이 아니라고 설명하고 있습니다. 이를 극복하지 위해서 Conjugate methods가 효율적이라고 설명하고 있습니다. </p>\n<h1 id=\"5-Experiment\"><a href=\"#5-Experiment\" class=\"headerlink\" title=\"5. Experiment\"></a>5. Experiment</h1><p>본 논문에서는 LQR, simple MDP, 그리고 tetris MDP에 대해서 실험을 진행했습니다. Practice에서는 FIM은 다음과 같은 식으로 업데이트합니다.</p>\n<p>$f\\leftarrow f+\\nabla log \\pi(a_t; s_t, \\theta)\\nabla log \\pi(a_t; s_t, \\theta)^T$</p>\n<p>T길이의 경로에 대해서 f/T를 이용해 F의 기대값($E$)를 구합니다.</p>\n<h2 id=\"5-1-LQR-Linear-Quadratic-Regulator\"><a href=\"#5-1-LQR-Linear-Quadratic-Regulator\" class=\"headerlink\" title=\"5.1 LQR(Linear Quadratic Regulator)\"></a>5.1 LQR(Linear Quadratic Regulator)</h2><p>Agent를 실험할 환경은 다음과 같은 dynamics를 가지고 있습니다.</p>\n<p>$x(t+1) = 0.7x(t)+u(t)+\\epsilon(t)$</p>\n<p>$u(t)$는 control 신호로서 에이전트의 행동입니다. $\\epsilon$은 noise distribution으로 환경에 가해지는 노이즈입니다. 에이전트의 목표는 적절한 $u(t)$를 통해 $x(t)$를 0으로 유지하는 것입니다. $x(t)$를 0으로 유지하기 위해서 필요한 소모값(cost)는 $x(t)^2$로 정의하며 cost를 최소화하도록 학습합니다. 이 시스템을 linear라고 부르는 것은 아래 그림과 같이 선형의 형태를 띄기 때문입니다. 이 논문에서는 실험할 때 복잡성을 더 해주기 위해 noise distribution인 $\\epsilon$을 dynamics에 추가하였습니다. </p>\n<p>이 실험에서 policy는 다음과 같이 설정하였습니다. 파라미터가 2개 밖에 없는 간단한 policy입니다.</p>\n<p>$\\pi(u;x,\\theta)\\propto exp(\\theta_1s_1x^2+\\theta_2s_2x)$</p>\n<p>이 policy를 간단히 그래프로 그려보면 다음과 같습니다. $\\theta_1$과 $\\theta_2$를 (0.5, 0.5), (1, 0), (0, 1)로 설정하고 $s_1$, $s_2$는, 1로 두었습니다. $x$는 -1~1사이의 범위로 그렸습니다. </p>\n<center><img src=\"https://www.dropbox.com/s/v69qyrwn7zurk8c/Screenshot%202018-06-08%2014.57.07.png?dl=1\" width=\"500px\"></center>\n\n<p>아래의 그림은 LQR을 학습한 그래프입니다. cost가 $x^2$이기 때문에 cost가 0으로 갈수록 agent는 0에서 안정적으로 머무른다고 볼 수 있다. 6개의 선 중에서 오른쪽 세 개가 일반적인 gradient 방법을 사용해서 학습한 결과이다. 그리고 왼쪽의 세 개의 선이 natural policy gradient를 통해 학습한 학습 곡선이다. 일반 gradient 방법보다 natural gradient가 훨씬 빠르게 학습한다(time 축이 log scale인 것을 감안하자).</p>\n<p>하지만 문제가 있다. npg를 학습한 세 개의 곡선은 $\\theta$를 rescale 한 것이다. $\\theta$앞에 곱해지는 숫자에 따라 학습의 과정이 다르다. 이 것은 coordinate에 따라 steepest gradient가 다르게 측정된다는 것이다. 즉, covariant gradient가 아니라는 뜻이다. 이 논문에서는 natural gradient를 통해 gradient가 covariant하도록 만들고 싶었는데 실패한 것이다. </p>\n<center><img src=\"https://www.dropbox.com/s/fhn8cgje0rdws0i/Screenshot%202018-06-08%2023.13.37.png?dl=1\" width=\"300px\"></center>\n\n<p>natural gradient가 covariant하지 않은 이유는 Fisher Information Matrix가 예상했던 바와는 달리 invariant metric이 아니기 때문이다. 또한 FIM이 invariant metric이 아닌 이유는 FIM을 계산할 때 $\\rho_s$가 곱해지기 때문이다(state distribution에 대한 expectation. $\\rho_s$가 곱해지는 것이 invariant에 미치는 영향이 무엇인지는 모르겠다). 하지만 여전히 의의가 있는 것은 기존 gradient 방법들보다 훨씬 빠르게 학습한다는 것이다.</p>\n<h2 id=\"5-2-Simple-2-state-MDP\"><a href=\"#5-2-Simple-2-state-MDP\" class=\"headerlink\" title=\"5.2 Simple 2-state MDP\"></a>5.2 Simple 2-state MDP</h2><p>이제 다른 예제에서 NPG를 테스트한다. 2개의 state만 가지는 MDP를 고려해보자. <a href=\"http://repository.cmu.edu/cgi/viewcontent.cgi?article=1080&amp;context=robotics\" target=\"_blank\" rel=\"noopener\">그림출처</a>. 그림으로보면 다음과 같다. x=0 상태와 x=1 상태 두 개가 존재한다. 에이전트는 각 상태에서 다시 자신의 상태로 되돌아오는 행동을 하거나 다른 상태로 가는 행동을 할 수 있다. 상태 x=0에서 다시 자기 자신으로 돌아오면 1의 보상을 받고 상태 x=1에서 자기 자신으로 돌아오면 2의 보상을 받는다. 따라서 결국 optimal policy는 상태 x=1에서 계속 자기 자신으로 돌아오는 행동을 취하는 것이다. </p>\n<p><img src=\"https://www.dropbox.com/s/g1x9yknzsrip59i/Screenshot%202018-06-08%2023.06.50.png?dl=1\"></p>\n<p>문제를 좀 어렵게 만들기 위해 state distribution을 다음과 같이 설정한다. 즉, 대부분의 경우에 상태 x=0에서 에이전트가 시작하는 것이다.<br>$$<br>\\rho(x=0)=0.8,  \\rho(x=1)=0.2<br>$$<br>일반적인 gradient 방법을 사용하면 다음과 같은 policy gradient 식에 따라서 업데이트를 하게 된다. 이 때, $\\rho(s)$가 gradient에 곱해지므로 상태적으로 상태 0에서의 gradient 값이 커지게 된다. 따라서 에이전트는 상태 0에서의 gradient(상태 0에서 스스로에게 돌아오는 행동을 취하도록 정책을 업데이트하는 gradient)를 따라 parameterized policy를 update한다. 따라서 아래 그림의 첫번째 그림에서처럼 Reward가 1에서 오랜 시간동안 머무른다. 즉, 에이전트가 상태 0에서 self-loop를 계속 돌고 있다는 것이다. $\\rho(x=0)$가 $10^{-7}$까지 떨어진다.</p>\n<p>$$\\nabla\\eta(\\theta)=\\sum_{s,a}\\rho^{\\pi}(s)\\nabla\\pi(a;s,\\theta)Q^{\\pi}(s,a)$$</p>\n<center><img src=\"https://www.dropbox.com/s/xtb77mfazbppnss/Screenshot%202018-06-08%2023.14.24.png?dl=1\" width=\"300px\"></center>\n\n<p>하지만 NPG를 사용할 경우에는 훨씬 빠르게 average reward가 2에 도달한다. gradient 방법이 $1.7X10^(7)$정도의 시간만에 2에 도달한 반면 NPG는 2만에 도달한다. 또한 $\\rho(x=0)$가 $10^{-5}$이하로 떨어지지 않는다.</p>\n<p>한 가지 그래프를 더 살펴보자. 다음 그래프는 parameter $\\theta$가 업데이트 되는 과정을 보여준다. 이 그래프에서는 parameter가 2개 있는 것이다. 일반적인 gradient가 아래 그래프에서 실선에 해당한다. 이 실선의 그래프는 보면 처음부터 중반까지 $\\theta_i$만 거의 업데이트하는 것을 볼 수 있다. 그에 비해 NPG는 두 개의 parameter를 균등하게 업데이트하는 것을 볼 수 있다. </p>\n<center><img src=\"https://www.dropbox.com/s/g7pazozw2k6rd7x/Screenshot%202018-06-08%2023.23.25.png?dl=1\" width=\"300px\"></center>\n\n<p>policy가 $\\pi(a;s,\\theta)\\propto exp(\\theta_{sa})$일 때, 다음과 같이 $F_{-1}$이 gradient 앞에 weight로 곱해지는데 이게 $\\rho$와는 달리 각 parameter에 대해 균등하다. 따라서 위 그래프에서와 같이 각 parameter는 비슷한 비율로 업데이트가 되는 것이다.</p>\n<p>$$\\bar{\\nabla}\\eta(\\theta) = F^{-1}\\nabla\\eta(\\theta)$$</p>\n<h2 id=\"5-3-Tetris\"><a href=\"#5-3-Tetris\" class=\"headerlink\" title=\"5.3 Tetris\"></a>5.3 Tetris</h2><p>NPG를 테스트할 tetris 예제는 Neuro Dynamic Programming 책에 소개되어있다. 다음 그림은 tetris 예제를 보여준다. 보통 그림에서와 같이 state의 feature를 정해준다. <a href=\"http://slideplayer.com/slide/5215520/\" target=\"_blank\" rel=\"noopener\">그림 출처</a></p>\n<p><img src=\"https://www.dropbox.com/s/y1halso9yermy8s/Screenshot%202018-06-08%2023.44.34.png?dl=1\"></p>\n<p>이 예제에서도 exponantial family로 policy를 표현한다. $\\pi(a;s,\\theta) \\propto exp(\\theta^T\\phi_{sa})$ 로 표현한다.</p>\n<p>tetris는 linear function approximator와 greedy policy iteration을 사용할 경우 performance가 갑자기 떨어지는 현상이 있다. 밑의 그림에서 A의 spike가 있는 그래프가 이 경우이다. 그 밑에 낮게 누워있는 그래프는 일반적인 policy gradient 방법이다. 하지만 Natural policy gradient를 사용할 경우 B 그림에서 오른쪽 그래프와 같이 성능개선이 뚜렷하다. Policy Iteration 처럼 성능이 뚝 떨어지지 않고 안정적으로 유지한다. 또한 그림 C에서 보는 것처럼 오른쪽 그래프인 일반적인 gradient 방법보다 훨씬 빠르게 학습하는 것을 볼 수 있다.</p>\n<p><img src=\"https://www.dropbox.com/s/pr6s2qrqaic0wyj/Screenshot%202018-06-08%2023.40.16.png?dl=1\"></p>\n<h1 id=\"6-Discussion\"><a href=\"#6-Discussion\" class=\"headerlink\" title=\"6. Discussion\"></a>6. Discussion</h1><p>Natural Gradient Method는  Policy Iteration에서와 같이 Greedy Action을 선택하도록 학습합니다. Line search 기법과 함께 사용하면 더 Policy Iteration과 비슷해집니다. Greedy Policy Iteration에서와 비교하면 Performance Improvement가 보장됩니다. 하지만 FIM이 asymtotically Hessian으로 수렴하지 않습니다. 그렇기 때문에 Conjugate Gradient Method로 구하는 방법이 더 좋을수 있습니다.</p>\n<p>위의 실험과 이론에 의해서 Hessian과 FIM 두 방법은 상황에 따라서 다를 수 있습니다. Conjugate Gradient Method가 조금 더 빠르게 Maximum에 수렴하지만 성능은 Maximum에서 거의 변하지 않으므로 말하기 어렵습니다(무슨말인지 잘 모르겠음..).</p>\n<p>위의 몇가지 문제점을 극복하기 위한 추가적인 연구가 필요합니다.</p>\n","site":{"data":{}},"excerpt":"","more":"<center> <img src=\"https://www.dropbox.com/s/yd0x14ljrhpnj1b/Screen%20Shot%202018-07-18%20at%201.08.05%20AM.png?dl=1\" width=\"600\"> </center>\n\n<p>논문 저자 : Sham Kakade<br>논문 링크 : <a href=\"https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf\" target=\"_blank\" rel=\"noopener\">https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf</a><br>Proceeding : ??<br>정리 : 이웅원, 차금강</p>\n<hr>\n<h1 id=\"1-왜-Natural-Policy-Gradient-인가\"><a href=\"#1-왜-Natural-Policy-Gradient-인가\" class=\"headerlink\" title=\"1. 왜 Natural Policy Gradient 인가?\"></a>1. 왜 Natural Policy Gradient 인가?</h1><p>많은 연구들이 좋은 Policy $\\pi$를 Objective function의 Gradient 값을 따라서 찾는데 노력을 하고 있습니다. 하지만 기존의 Gradient descent rule을 따르는 업데이트 방식은 non-covariant 방식입니다. 즉, Gradient descent rule은 $\\bigtriangleup \\theta_i = \\dfrac{\\partial f}{\\partial \\theta_i}$을 이용해서 파라미터를 업데이트 하는데 $\\bigtriangleup \\theta_i$가 $f$를 구성하고 있는 파라미터들에 종속되어 있다는 것입니다. 본 논문에서는 $f$를 구성하고 있는 파라미터들과 독립적인 metric을 정의함으로써 covariant한 gradient을 제시하고 이를 이용하여 파라미터를 업데이트합니다.</p>\n<hr>\n<h1 id=\"2-Notation-amp-Natural-Gradient\"><a href=\"#2-Notation-amp-Natural-Gradient\" class=\"headerlink\" title=\"2. Notation &amp; Natural Gradient\"></a>2. Notation &amp; Natural Gradient</h1><h2 id=\"2-1-Notation\"><a href=\"#2-1-Notation\" class=\"headerlink\" title=\"2.1 Notation\"></a>2.1 Notation</h2><p>Finite한 MDP tuple을 먼저 정의합니다. $(S, s_0, A, R, P)$로 정의됩니다. Finite한 상태의 set$(S)$, 에피소드 시작지점의 state$(s_0)$, finite한 행동의 set$(A)$, 보상 함수 $R: S \\times A \\to [0, R_{max}]$, 그리고 transition 모델$(P)$로 이루어져 있습니다. 그리고 모든 정책 $\\pi$는 ergodic하다고 가정합니다. 여기서 어떠한 통계적인 모델(모집단)과 모집단으로부터 추출된 모델(표본집단)이 같은 특성을 가질 때 ergodic하다고 합니다. 고등학교 수학에서 배우는 표본집단의 표준편차, 평균은 모집단의 그것과 같다는 것과 비슷한 개념입니다. 그리고 정책은 파라미터들 $\\theta$에 의해 지배될 때 상태 $s$에서 행동 $a$를 선택할 확률로 정의되며 $\\pi(a;s,\\theta)$와 같이 표현됩니다. 여타 다른 강화학습들과 같이 $Q^\\pi(s,a) = E_\\pi (\\Sigma^\\infty_{t=0}R(s_t, a_t)-\\eta(\\pi)|s_0 = s, a_0 = a)$를 정의합니다.</p>\n<p>다음 다른 정책 업데이트와 같이 Object function, $\\eta(\\pi_{\\theta})$을 정의합니다.<br>$$<br>\\eta(\\pi_{\\theta}) = \\Sigma_{s,a} \\rho^{\\pi}(s)\\pi(a;s,\\theta)Q^\\pi(s,a)<br>$$</p>\n<p>여기서 $\\eta(\\pi_\\theta)$는 $\\pi_\\theta$에 종속되어 있으며, $\\pi_\\theta$는 $\\theta$에 종속되어 있습니다. 결국 $\\eta(\\pi_\\theta)$는 $\\theta$에 종속되어 있다고 할 수 있으며 $\\eta(\\pi_{\\theta})$는 $\\eta(\\theta)$로 표현할 수 있습니다. </p>\n<h2 id=\"2-2-Natural-Gradient\"><a href=\"#2-2-Natural-Gradient\" class=\"headerlink\" title=\"2.2 Natural Gradient\"></a>2.2 Natural Gradient</h2><p>2.1에 정의된 Objective function을 최대화 하는 방향으로 파라미터들을 업데이트 하는 것이 목적입니다. Euclidean space에서는 Objective function의 일반적인 gradient는 다음과 같이 구할 수 있습니다.</p>\n<p>$$\\bigtriangledown\\eta(\\pi_\\theta) = \\Sigma_{s,a}\\rho^\\pi(s)\\bigtriangledown\\pi(a;s,\\theta)Q^\\pi(s,a)$$</p>\n<p>하지만 뉴럴 네트워크에서 사용하는 parameter들은 매우 복잡하게 얽혀 있으며 이는 보통 생각하는 직선으로 이루어져 있는 Euclidean space가 아닙니다. 일반적으로 이는 구와 같이 휘어져 있는 평면(곡면)으로 이루어져 있으며 이를 리만 공간(Riemannian space)라고 합니다. 리만 공간에서는 Natural gradient가 steepest direction(업데이트 방향)이며 이를 구하는 방법은 <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.452.7280&amp;rep=rep1&amp;type=pdf\" target=\"_blank\" rel=\"noopener\">Amari, Natural gradient works in efficiently in learning</a>에서 자세히 설명을 하고 있습니다. 이를 간단히 설명하면 다음과 같습니다. 파라미터로 구성된 Matrix의 Positive-Definite Matrix인 $G(\\theta)$를 이용하여 구할 수 있습니다. 정확한 리만 공간에서의 steepest gradient(natural gradient)를 $\\widetilde \\bigtriangledown \\eta(\\theta)$라고 한다면 다음과 같이 표현될 수 있습니다.</p>\n<p>$$\\widetilde \\bigtriangledown\\eta(\\theta)=G(\\theta)^{-1}\\bigtriangledown\\eta(\\theta)$$</p>\n<p>위의 식을 이용하여 리만 공간에서의 파라미터를 업데이트 하면 아래의 식과 같이 표현될 수 있습니다.</p>\n<p>$$\\theta_{k+1}=\\theta_k-\\alpha_tG(\\theta)^{-1}\\bigtriangledown\\eta(\\theta)$$</p>\n<p>여기서 끝나는 것이 아니라 여전히 문제가 하나 남아 있습니다. 우리는 이 문제를 뉴럴넷을 이용하여 구성하고 해결하고 있습니다. 뉴럴넷은 여러가지 파라미터셋들로 구성될 수 있습니다. 하지만 우연의 일치로 다른 파라미터셋을 가지지만 같은 policy를 가질 수 있습니다. 이 경우 steepest  direction는 같은 policy이기 때문에 같은 방향을 가리키고 있어야 하는데 non-covariant한 경우 그렇지 못합니다. 이 이유로 느린 학습이 야기됩니다. 이 문제를 해결하기 위해 단순히 Positive-Definite Matrix인 $G(\\theta)$를 사용하지 않고 <a href=\"rxiv.org/abs/1707.06347\">Fisher Information Matrix</a>(이하 FIM)인 $F_s(\\theta)$를 사용하면 이를 해결할 수 있다고 서술하고 있습니다. FIM는 어떤 확률 변수의 관측값으로부터 확률 변수의 분포의 매개변수에 대해 유추할 수 있는 정보의 양입니다. 어떠한 확률변수 $X$가 미지의 매개변수 $\\theta$에 의해 정의되는 분포를 따른다고 하면 $X=x$일때 FIM은 다음과 같이 정의됩니다.</p>\n<p>$$F_x(\\theta)=E[(\\dfrac{\\partial}{\\partial\\theta}logPr(x|\\theta))^2]$$</p>\n<p>강화학습에서는 정보 $x$는 에피소드에 의해 관측된 상태값 $s$이며 매개변수 $\\theta$에 의해 선택될 수 있는 행동에 대한 분포가 나오게 됩니다. 이에 의해 위의 FIM는 다음과 같이 표현될 수 있습니다.</p>\n<p>$$F_s(\\theta) \\equiv E_{\\pi(a;s,\\theta)}[(\\dfrac{\\partial}{\\partial\\theta}log\\pi(a;s,\\theta))^2] =E_{pi(a;s,\\theta)}[\\dfrac{\\partial log\\pi(a;s,\\theta)}{\\partial \\theta_i}\\dfrac{\\partial log\\pi(a;s,\\theta)}{\\partial\\theta_j}]$$</p>\n<p>그리고 위의 정리된 식들을 이용하여 Objective function을 정리하면 아래의 식과 같이 표현됩니다.</p>\n<p>$$F(\\theta) = E_{\\rho^{\\pi}(s)}[F_s(\\theta)]$$</p>\n<p>또한 이 Fisher Information Matrix에 정의된 이 metric은 다음과 같은 성질을 가지고 있습니다. 업데이트되는 파라미터에 의해 구성되는 Manifold(곡률을 가지는)에 기반한 metric입니다. 확률분포(본 논문에서는 $\\pi(a;s,\\theta)$)를 구성하는 파라미터($\\theta$)의 변화에 독립적입니다. 어떠한 coordinate를 선택하느냐에 따라 변화하지 않습니다. 마지막으로 positive-definite한 값을 가집니다. 이렇기 때문에 steepest gradient에서 Objective function의 방향을 알기 위해 사용한 방법과 같은 방법으로 Natural gradient direction을 다음과 같이 구할 수 있습니다.</p>\n<p>$$\\widetilde{\\bigtriangledown}\\eta(\\theta) \\equiv F(\\theta)^{-1}\\bigtriangledown\\eta(\\theta)$$</p>\n<hr>\n<h1 id=\"3-Natural-Gradient와-Policy-Iteration\"><a href=\"#3-Natural-Gradient와-Policy-Iteration\" class=\"headerlink\" title=\"3. Natural Gradient와 Policy Iteration\"></a>3. Natural Gradient와 Policy Iteration</h1><p>3장에서는 Natural gradient를 통한 policy iteration을 수행하여 실제로 정책의 향상이 있는지를 증명합니다. 본 논문에서 $Q^\\pi(s,a)$는 compatible function approximator $f^\\pi(s,a;w)$에 의해 정의됩니다.</p>\n<h2 id=\"3-1-Compatible-Function-Approximation\"><a href=\"#3-1-Compatible-Function-Approximation\" class=\"headerlink\" title=\"3.1 Compatible Function Approximation\"></a>3.1 Compatible Function Approximation</h2><p>3.1절에서는 정책이 업데이트 될 때, value function approximator, $f^\\pi(s,a;w)$도 같이 실제 값과 가까워지는지를 증명합니다.</p>\n<p>본 증명을 수행하기 전에 몇가지 가정과 정의가 필요합니다.</p>\n<p>파라미터들의 집합, $\\theta$, 그리고 선형 행렬인 $\\omega$가 정의되며 이를 이용하여 다음의 정의가 이루어집니다.<br>$$<br>\\psi^{\\pi}(s,a) = \\bigtriangledown log\\pi(a;s,\\theta), f^\\pi(s,a;\\omega) = \\omega^T\\psi^\\pi(s,a)<br>$$<br>여기서, $[\\bigtriangledown log\\pi(a;s,\\theta)]_i=\\partial log\\pi(a;s,\\theta)/\\partial\\theta_i$입니다. 그리고 compatible function approximator는 실제 $Q^\\pi(s,a)$에 가까워져야 하므로 $\\epsilon(\\omega,\\pi)=\\Sigma_{s,a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)(f^\\pi(s,a;\\omega)-Q^\\pi(s,a))^2$가 정의됩니다. 그리고 $\\widetilde \\omega$는 $\\epsilon(\\omega, \\pi_\\theta)$를 최소화하는 $\\omega$라고 가정합니다.</p>\n<p>최종적으로 $\\widetilde \\omega = \\widetilde \\bigtriangledown \\eta(\\theta)$이면 function approximator의 gradient 방향과 정책의 gradient 방향이 같다는 의미입니다.</p>\n<p>증명) $\\widetilde \\omega$ 가 squared error $\\epsilon(\\omega, \\pi)$를 최소화 한다면, $\\dfrac{\\partial\\epsilon}{\\partial\\omega_i}=0$입니다.<br>$$<br>\\dfrac{\\partial}{\\partial\\omega_i}\\Sigma_{s,a}\\rho^\\pi(s)\\pi(a;s,\\theta){(\\color{red}f^\\pi(s,a;w)\\color{black}-Q^\\pi(s,a))^2}=0<br>$$</p>\n<p>$$<br>\\dfrac{\\partial}{\\partial\\omega_i}\\Sigma_{s,a}\\rho^\\pi(s)\\pi(a;s,\\theta)(\\color{red}\\psi^\\pi(s,a)\\widetilde\\omega\\color{black}-Q^\\pi(s,a))^2 = 0<br>$$</p>\n<p>$$<br>\\Sigma_{s,a}\\rho^\\pi(s)\\pi(a;s,\\theta)\\psi^\\pi(s,a)\\color{blue}(\\psi^\\pi(s,a)^T\\widetilde\\omega-Q^\\pi(s,a))\\color{black} = 0<br>$$</p>\n<p>$$<br>\\Sigma_{s,a}\\rho^\\pi(s)\\pi(a;s,\\theta)\\psi^\\pi(s,a)\\color{blue}\\psi^\\pi(s,a)^T\\widetilde\\omega=\\color{black}\\Sigma_{s,a}\\rho^\\pi(s)\\pi(a;s,\\theta)\\psi^\\pi(s,a)\\color{blue}Q^\\pi(s,a)<br>$$</p>\n<p>$$<br>\\color{green}\\psi^\\pi(s,a)\\psi^\\pi(s,a)^T\\color{black} = \\bigtriangledown log\\pi(a;s,\\theta) \\bigtriangledown log\\pi(a;s,\\theta) = \\color{green} F_s(\\theta)<br>$$</p>\n<p>$$<br>\\Sigma_{s,a}\\rho^\\pi(s)\\pi(a;s,\\theta)\\psi^\\pi(s,a)\\psi^\\pi(s,a)^T= E_{\\rho^\\pi(s)}[F_s(\\theta)] = F(\\theta)<br>$$</p>\n<p>$$<br>\\widetilde\\bigtriangledown\\eta(\\theta)\\equiv F(\\theta)^{-1}\\bigtriangledown\\eta(\\theta)<br>$$</p>\n<p>수식을 따라 정리하면 정리가 됩니다.</p>\n<h2 id=\"3-2-Greedy-Policy-Improvement\"><a href=\"#3-2-Greedy-Policy-Improvement\" class=\"headerlink\" title=\"3.2 Greedy Policy Improvement\"></a>3.2 Greedy Policy Improvement</h2><p>3.2절에서는 natrual gradient는 다른 policy iteration 방법처럼 단지 현재보다만 좋은 방법을 선택하도록 업데이트하는 것이 아니라 현재보다 좋은 것 중에 가장 좋은 방법을 선택한다고 합니다. 이것을 특수한 정책을 가지는 상황안에서 학습속도($\\alpha$)를 무한대로 가져감으로써 어떤 action을 선택하는지를 알아봅니다.</p>\n<p>3.2절에서는 정책이 다음과 같은 형태를 가진다고 가정합니다.<br>$$<br>\\pi(a;s,\\theta) \\propto exp(\\theta^T\\phi_{sa})<br>$$<br>그리고 3.1절처럼 $\\widetilde\\omega$가 approximation error인 $\\epsilon$을 최소화하는 파라미터라고 가정하지만 gradient마저 최소화되게 하는 파라미터는 아니라고 가정합니다.($\\widetilde\\bigtriangledown\\eta(\\theta)\\neq0$)</p>\n<p>그리고 학습속도가 무한대일 때의 정책의 notation을 다음과 같이 표현합니다.<br>$$<br>\\pi_\\infty(a;s) = lim_{\\alpha\\rightarrow\\infty}\\pi(a;s,\\theta + \\alpha\\widetilde\\bigtriangledown\\eta(\\theta))<br>$$<br>증명을 들어가는 시작으로 3.1절의 결과를 이용하여 function approximator($f^\\pi(s,a;\\omega)$)는 다음과 같이 쓸 수 있습니다.<br>$$<br>f^\\pi(s,a;\\omega) = \\widetilde\\bigtriangledown\\eta(\\theta)^T\\psi^\\pi(s,a)<br>$$<br>그리고 가정에 의해 다음과 같이 표기될 수 있습니다.<br>$$<br>\\psi^\\pi(s,a) = \\phi_{sa}-E_{\\pi(a\\prime;s,\\theta)}[\\phi_{sa\\prime}]<br>$$<br>다음 function approximator는 다음과 같이 다시 쓸 수 있습니다.<br>$$<br>f_\\pi(s,a;\\omega) = \\widetilde\\bigtriangledown\\eta(\\theta)^T(\\phi_{sa}-E_{\\pi(a\\prime,s,\\theta)}[\\phi_{sa\\prime}])<br>$$<br>Greedy improvement와 같이 function approxmiator의 가장 높은 행동을 선택하는 것을 정책으로 합니다.($argmax_{a\\prime}f^\\pi(s,a)=argmax_{a\\prime}\\alpha\\widetilde\\bigtriangledown\\eta(\\theta)^T\\phi_{sa\\prime}$)</p>\n<p>그리고 정책의 업데이트는 다음과 같이 이루어집니다.<br>$$<br>\\pi(a;s,\\theta+\\alpha\\widetilde\\bigtriangledown\\eta(\\theta)) \\propto exp(\\theta^T\\phi_{sa}+\\alpha\\widetilde\\bigtriangledown\\eta(\\theta)^T\\phi_{sa})<br>$$<br>3.2절 초입의 가정($\\widetilde\\bigtriangledown\\eta(\\theta)\\neq0$)에 의해 위의 식에서 $\\alpha$가 무한대로 간다면 우측 식에서 $\\theta^T\\phi_{sa}$의 항은 다른 항에 비해 매우 작은 값을 가지게 되므로 $\\alpha\\widetilde\\bigtriangledown\\eta(\\theta)^T\\phi_{sa}$의 지배를 받게 됩니다.</p>\n<p>그러므로 $\\pi_\\infty=0$ 과 $a∉argmax_a\\widetilde\\bigtriangledownη(θ)Tϕsa′$은 필요충분조건이 됩니다. 이것은 npg가 단지 더 좋은 행동이 아니라 가장 좋은 행동을 취하는 것을 뜻합니다.</p>\n<h2 id=\"3-3-General-Parameterized-Policy\"><a href=\"#3-3-General-Parameterized-Policy\" class=\"headerlink\" title=\"3.3 General Parameterized Policy\"></a>3.3 General Parameterized Policy</h2><p>일반적인 정책에서 또한 npg는 가장 좋은 행동을 선택하는 쪽으로 학습합니다.</p>\n<p>만약 $\\widetilde\\omega$가 approximation error을 최소화 하는 파라미터라고 가정하고 파라미터는 $\\theta\\prime=\\theta+\\alpha\\widetilde\\bigtriangledown\\eta(\\theta)$의 방식으로 업데이트 합니다. 일반적으로 업데이트를 $\\theta\\prime=\\theta+\\bigtriangleup\\theta$로 표현하며 앞 문장의 방식으로 표현을 바꾼다면 $\\bigtriangleup\\theta = \\alpha\\widetilde\\bigtriangledown\\eta(\\theta)$가 됩니다. 그리고 3.1절의 결과에 의해 $\\bigtriangleup\\theta=\\alpha\\widetilde\\omega$로 표현할 수 있습니다. 이를 이용하여 Taylor expansion에 의해 다음과 같이 전개 될 수 있습니다.<br>$$<br>\\pi(a;s,\\theta\\prime)=\\pi(a;s,\\theta)+\\dfrac{\\partial\\pi(a;s,\\theta^T)}{\\partial\\theta}\\bigtriangleup\\theta+O(\\bigtriangleup\\theta^2)<br>$$</p>\n<p>$$<br>= \\pi(a;s,\\theta)(1+\\alpha\\psi(s,a)^T\\bigtriangleup\\theta)+O(\\bigtriangleup\\theta^2)<br>$$</p>\n<p>$$<br>= \\pi(a;s,\\theta)+(1+\\alpha\\psi(s,a)^T\\widetilde\\omega)+O(\\alpha^2)<br>$$</p>\n<p>$$<br>=\\pi(a;s,\\theta) + (1+\\alpha f^\\pi(s,a;\\widetilde\\omega))+O(\\alpha^2)<br>$$</p>\n<p>정책 자체가 function approximator의 크기대로 업데이트가 되기 때문에 지역적으로 가장 좋은 행동의 확률은 커지고 다른 확률은 작아질 것입니다. 하지만 탐욕적으로 향상을 하더라도 그게 성능자체를 향상시키지는 않을 수도 있습니다. 그렇기 때문에 line search기법과 함께 사용할 경우 성능 향상을 보장할 수 있습니다.</p>\n<h1 id=\"4-Metrics-and-Curvatures\"><a href=\"#4-Metrics-and-Curvatures\" class=\"headerlink\" title=\"4. Metrics and Curvatures\"></a>4. Metrics and Curvatures</h1><p>2절에서 설명하고 있는 Positive-Definite Matrix인 FIM이외의 다른 Matrix도 사용할 수 있습니다. 본 논문에서는 다음과 같이 설명하고 있습니다. 다양한 파라미터 추정에서 FIM은 Hessian Matrix에 수렴하지 않을 수 있다고 합니다. 이 말은 2nd order 수렴이 보장되지 않는다는 말입니다.  <a href=\"http://www.inference.org.uk/mackay/ica.pdf\" target=\"_blank\" rel=\"noopener\">Mackay</a> 논문에서 hessian에서 data independant한 term을 metric으로 가져오는 방법을 제안했다. 그래서 performance를 2번 미분해보면 다음과 같다. 하지만 다음 식에서는 모든 항이 data dependent하다(Q가 있으니까). 첫 번째 항이 그나마 FIM과의 관련성이 있을 수 있지만 Q 값이 curvature에 weight를 주는 방식 때문에 다르다고 할 수 있다.</p>\n<p>$\\nabla^2\\eta(\\theta)=\\sum_{sa}\\rho^{\\pi}(s)(\\nabla^2\\pi(a;s)Q^{\\pi}(s,a)+\\nabla\\pi(a;s)\\nabla Q^{\\pi}(s,a)^T+\\nabla Q^{\\pi}(s,a)\\nabla\\pi(a;s)^T)$</p>\n<p>Hessian Matrix는 무조건 Positive-Definite Matrix인 것이 아니기 때문에, 다라서 국부적 최대점이 될 때 까지 Hessian Matrix를 사용하는 것은 좋은 방법이 아니라고 설명하고 있습니다. 이를 극복하지 위해서 Conjugate methods가 효율적이라고 설명하고 있습니다. </p>\n<h1 id=\"5-Experiment\"><a href=\"#5-Experiment\" class=\"headerlink\" title=\"5. Experiment\"></a>5. Experiment</h1><p>본 논문에서는 LQR, simple MDP, 그리고 tetris MDP에 대해서 실험을 진행했습니다. Practice에서는 FIM은 다음과 같은 식으로 업데이트합니다.</p>\n<p>$f\\leftarrow f+\\nabla log \\pi(a_t; s_t, \\theta)\\nabla log \\pi(a_t; s_t, \\theta)^T$</p>\n<p>T길이의 경로에 대해서 f/T를 이용해 F의 기대값($E$)를 구합니다.</p>\n<h2 id=\"5-1-LQR-Linear-Quadratic-Regulator\"><a href=\"#5-1-LQR-Linear-Quadratic-Regulator\" class=\"headerlink\" title=\"5.1 LQR(Linear Quadratic Regulator)\"></a>5.1 LQR(Linear Quadratic Regulator)</h2><p>Agent를 실험할 환경은 다음과 같은 dynamics를 가지고 있습니다.</p>\n<p>$x(t+1) = 0.7x(t)+u(t)+\\epsilon(t)$</p>\n<p>$u(t)$는 control 신호로서 에이전트의 행동입니다. $\\epsilon$은 noise distribution으로 환경에 가해지는 노이즈입니다. 에이전트의 목표는 적절한 $u(t)$를 통해 $x(t)$를 0으로 유지하는 것입니다. $x(t)$를 0으로 유지하기 위해서 필요한 소모값(cost)는 $x(t)^2$로 정의하며 cost를 최소화하도록 학습합니다. 이 시스템을 linear라고 부르는 것은 아래 그림과 같이 선형의 형태를 띄기 때문입니다. 이 논문에서는 실험할 때 복잡성을 더 해주기 위해 noise distribution인 $\\epsilon$을 dynamics에 추가하였습니다. </p>\n<p>이 실험에서 policy는 다음과 같이 설정하였습니다. 파라미터가 2개 밖에 없는 간단한 policy입니다.</p>\n<p>$\\pi(u;x,\\theta)\\propto exp(\\theta_1s_1x^2+\\theta_2s_2x)$</p>\n<p>이 policy를 간단히 그래프로 그려보면 다음과 같습니다. $\\theta_1$과 $\\theta_2$를 (0.5, 0.5), (1, 0), (0, 1)로 설정하고 $s_1$, $s_2$는, 1로 두었습니다. $x$는 -1~1사이의 범위로 그렸습니다. </p>\n<center><img src=\"https://www.dropbox.com/s/v69qyrwn7zurk8c/Screenshot%202018-06-08%2014.57.07.png?dl=1\" width=\"500px\"></center>\n\n<p>아래의 그림은 LQR을 학습한 그래프입니다. cost가 $x^2$이기 때문에 cost가 0으로 갈수록 agent는 0에서 안정적으로 머무른다고 볼 수 있다. 6개의 선 중에서 오른쪽 세 개가 일반적인 gradient 방법을 사용해서 학습한 결과이다. 그리고 왼쪽의 세 개의 선이 natural policy gradient를 통해 학습한 학습 곡선이다. 일반 gradient 방법보다 natural gradient가 훨씬 빠르게 학습한다(time 축이 log scale인 것을 감안하자).</p>\n<p>하지만 문제가 있다. npg를 학습한 세 개의 곡선은 $\\theta$를 rescale 한 것이다. $\\theta$앞에 곱해지는 숫자에 따라 학습의 과정이 다르다. 이 것은 coordinate에 따라 steepest gradient가 다르게 측정된다는 것이다. 즉, covariant gradient가 아니라는 뜻이다. 이 논문에서는 natural gradient를 통해 gradient가 covariant하도록 만들고 싶었는데 실패한 것이다. </p>\n<center><img src=\"https://www.dropbox.com/s/fhn8cgje0rdws0i/Screenshot%202018-06-08%2023.13.37.png?dl=1\" width=\"300px\"></center>\n\n<p>natural gradient가 covariant하지 않은 이유는 Fisher Information Matrix가 예상했던 바와는 달리 invariant metric이 아니기 때문이다. 또한 FIM이 invariant metric이 아닌 이유는 FIM을 계산할 때 $\\rho_s$가 곱해지기 때문이다(state distribution에 대한 expectation. $\\rho_s$가 곱해지는 것이 invariant에 미치는 영향이 무엇인지는 모르겠다). 하지만 여전히 의의가 있는 것은 기존 gradient 방법들보다 훨씬 빠르게 학습한다는 것이다.</p>\n<h2 id=\"5-2-Simple-2-state-MDP\"><a href=\"#5-2-Simple-2-state-MDP\" class=\"headerlink\" title=\"5.2 Simple 2-state MDP\"></a>5.2 Simple 2-state MDP</h2><p>이제 다른 예제에서 NPG를 테스트한다. 2개의 state만 가지는 MDP를 고려해보자. <a href=\"http://repository.cmu.edu/cgi/viewcontent.cgi?article=1080&amp;context=robotics\" target=\"_blank\" rel=\"noopener\">그림출처</a>. 그림으로보면 다음과 같다. x=0 상태와 x=1 상태 두 개가 존재한다. 에이전트는 각 상태에서 다시 자신의 상태로 되돌아오는 행동을 하거나 다른 상태로 가는 행동을 할 수 있다. 상태 x=0에서 다시 자기 자신으로 돌아오면 1의 보상을 받고 상태 x=1에서 자기 자신으로 돌아오면 2의 보상을 받는다. 따라서 결국 optimal policy는 상태 x=1에서 계속 자기 자신으로 돌아오는 행동을 취하는 것이다. </p>\n<p><img src=\"https://www.dropbox.com/s/g1x9yknzsrip59i/Screenshot%202018-06-08%2023.06.50.png?dl=1\"></p>\n<p>문제를 좀 어렵게 만들기 위해 state distribution을 다음과 같이 설정한다. 즉, 대부분의 경우에 상태 x=0에서 에이전트가 시작하는 것이다.<br>$$<br>\\rho(x=0)=0.8,  \\rho(x=1)=0.2<br>$$<br>일반적인 gradient 방법을 사용하면 다음과 같은 policy gradient 식에 따라서 업데이트를 하게 된다. 이 때, $\\rho(s)$가 gradient에 곱해지므로 상태적으로 상태 0에서의 gradient 값이 커지게 된다. 따라서 에이전트는 상태 0에서의 gradient(상태 0에서 스스로에게 돌아오는 행동을 취하도록 정책을 업데이트하는 gradient)를 따라 parameterized policy를 update한다. 따라서 아래 그림의 첫번째 그림에서처럼 Reward가 1에서 오랜 시간동안 머무른다. 즉, 에이전트가 상태 0에서 self-loop를 계속 돌고 있다는 것이다. $\\rho(x=0)$가 $10^{-7}$까지 떨어진다.</p>\n<p>$$\\nabla\\eta(\\theta)=\\sum_{s,a}\\rho^{\\pi}(s)\\nabla\\pi(a;s,\\theta)Q^{\\pi}(s,a)$$</p>\n<center><img src=\"https://www.dropbox.com/s/xtb77mfazbppnss/Screenshot%202018-06-08%2023.14.24.png?dl=1\" width=\"300px\"></center>\n\n<p>하지만 NPG를 사용할 경우에는 훨씬 빠르게 average reward가 2에 도달한다. gradient 방법이 $1.7X10^(7)$정도의 시간만에 2에 도달한 반면 NPG는 2만에 도달한다. 또한 $\\rho(x=0)$가 $10^{-5}$이하로 떨어지지 않는다.</p>\n<p>한 가지 그래프를 더 살펴보자. 다음 그래프는 parameter $\\theta$가 업데이트 되는 과정을 보여준다. 이 그래프에서는 parameter가 2개 있는 것이다. 일반적인 gradient가 아래 그래프에서 실선에 해당한다. 이 실선의 그래프는 보면 처음부터 중반까지 $\\theta_i$만 거의 업데이트하는 것을 볼 수 있다. 그에 비해 NPG는 두 개의 parameter를 균등하게 업데이트하는 것을 볼 수 있다. </p>\n<center><img src=\"https://www.dropbox.com/s/g7pazozw2k6rd7x/Screenshot%202018-06-08%2023.23.25.png?dl=1\" width=\"300px\"></center>\n\n<p>policy가 $\\pi(a;s,\\theta)\\propto exp(\\theta_{sa})$일 때, 다음과 같이 $F_{-1}$이 gradient 앞에 weight로 곱해지는데 이게 $\\rho$와는 달리 각 parameter에 대해 균등하다. 따라서 위 그래프에서와 같이 각 parameter는 비슷한 비율로 업데이트가 되는 것이다.</p>\n<p>$$\\bar{\\nabla}\\eta(\\theta) = F^{-1}\\nabla\\eta(\\theta)$$</p>\n<h2 id=\"5-3-Tetris\"><a href=\"#5-3-Tetris\" class=\"headerlink\" title=\"5.3 Tetris\"></a>5.3 Tetris</h2><p>NPG를 테스트할 tetris 예제는 Neuro Dynamic Programming 책에 소개되어있다. 다음 그림은 tetris 예제를 보여준다. 보통 그림에서와 같이 state의 feature를 정해준다. <a href=\"http://slideplayer.com/slide/5215520/\" target=\"_blank\" rel=\"noopener\">그림 출처</a></p>\n<p><img src=\"https://www.dropbox.com/s/y1halso9yermy8s/Screenshot%202018-06-08%2023.44.34.png?dl=1\"></p>\n<p>이 예제에서도 exponantial family로 policy를 표현한다. $\\pi(a;s,\\theta) \\propto exp(\\theta^T\\phi_{sa})$ 로 표현한다.</p>\n<p>tetris는 linear function approximator와 greedy policy iteration을 사용할 경우 performance가 갑자기 떨어지는 현상이 있다. 밑의 그림에서 A의 spike가 있는 그래프가 이 경우이다. 그 밑에 낮게 누워있는 그래프는 일반적인 policy gradient 방법이다. 하지만 Natural policy gradient를 사용할 경우 B 그림에서 오른쪽 그래프와 같이 성능개선이 뚜렷하다. Policy Iteration 처럼 성능이 뚝 떨어지지 않고 안정적으로 유지한다. 또한 그림 C에서 보는 것처럼 오른쪽 그래프인 일반적인 gradient 방법보다 훨씬 빠르게 학습하는 것을 볼 수 있다.</p>\n<p><img src=\"https://www.dropbox.com/s/pr6s2qrqaic0wyj/Screenshot%202018-06-08%2023.40.16.png?dl=1\"></p>\n<h1 id=\"6-Discussion\"><a href=\"#6-Discussion\" class=\"headerlink\" title=\"6. Discussion\"></a>6. Discussion</h1><p>Natural Gradient Method는  Policy Iteration에서와 같이 Greedy Action을 선택하도록 학습합니다. Line search 기법과 함께 사용하면 더 Policy Iteration과 비슷해집니다. Greedy Policy Iteration에서와 비교하면 Performance Improvement가 보장됩니다. 하지만 FIM이 asymtotically Hessian으로 수렴하지 않습니다. 그렇기 때문에 Conjugate Gradient Method로 구하는 방법이 더 좋을수 있습니다.</p>\n<p>위의 실험과 이론에 의해서 Hessian과 FIM 두 방법은 상황에 따라서 다를 수 있습니다. Conjugate Gradient Method가 조금 더 빠르게 Maximum에 수렴하지만 성능은 Maximum에서 거의 변하지 않으므로 말하기 어렵습니다(무슨말인지 잘 모르겠음..).</p>\n<p>위의 몇가지 문제점을 극복하기 위한 추가적인 연구가 필요합니다.</p>\n"},{"title":"Proximal Policy Optimization","date":"2018-06-22T07:53:12.000Z","author":"장수영, 차금강","subtitle":"피지여행 6번째 논문","_content":"\n<center> <img src=\"https://www.dropbox.com/s/145van5kldfvvd5/Screen%20Shot%202018-07-18%20at%201.19.30%20AM.png?dl=1\" width=\"700\"> </center>\n\n논문 저자 : John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov\n논문 링크 : https://arxiv.org/pdf/1707.06347.pdf\nProceeding : ??\n정리 : 장수영, 차금강\n\n---\n\n# 1. 들어가며...\n\n이전의 Trust Region Policy Optimization(TRPO)의 핵심적인 부분을 살펴보자면 다음과 같습니다.\n\n$$maximize_\\theta \\hat{E}_t [\\frac{\\pi_\\theta(a_t|s_t)} \\hat{A}_t ]-\\hat{E}_t [D_{KL}(\\pi_{\\theta old}(*|s) || \\pi_{\\theta(*|s)})]$$\n\n$$maximize_\\theta \\hat{E}_t [\\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta old}(a_t|s_t)} \\hat{A}_t ]-\\hat{E}_t [D_{KL}(\\pi_{\\theta old}(*|s)||\\pi_{\\theta(*|s)})]$$\n\n위의 식을 살펴보면 KL-Divergence를 구하는 과정에서 굉장한 연산량이 필요합니다.\n$$\nD_{KL}(\\pi_{\\theta old}(*|s)||\\pi_\\theta(*|s)) = -\\int{\\pi_{\\theta old}(*|s)ln\\pi_\\theta(*|s)}+\\int{\\pi_\\theta(*|s)ln\\pi_{\\theta old}(*|s)}\n$$\n특히 위의 식에서 볼 수 있듯이 $ln()$을 계산하는데 있어 많은 계산량이 필요로 하며 매번 파라미터들을 업데이트할 때마다 이 복잡한 연산을 반복적으로 수행해야 합니다. TRPO에서 KL-Divergence의 역할을 쉽게 풀이하자면 복잡한 비선형 최적화에서 조금의 파라미터 변화에도 성능이 크게 변화할 수 있는데 이를 방지하며 기존의 성능을 잘 보존할 수 있게 하는 장치입니다. 꼭 TRPO만이 아니라 일반적인 Policy Gradient 방법에서도 Objective Function은 $ln()$으로 되어 있으며 이를 어떤 연산량이 적은 방법으로 쉽게 적용할 수 있을까에 대해 고민하는 것이 PPO의 핵심입니다.\n\nPPO 논문에서는 TRPO에서 제시한 surrogate function을 두 가지로 나누어서 접근하며 이를 서로, 그리고 TRPO와 비교 분석하고 있습니다. 본 논문에서 제시하고 있는 방법 두 가지 중 첫번째는 surrogate function을 매우 작은 값으로 clipping함으로써 gradient 자체를 강제적으로 적게 만들어주는 방법이며 두번째는 TRPO의 KL-Divergence의 크기에 따라 Adaptive한 파라미터를 적용하는 방법입니다.\n\n<br><br>\n\n# 2. Introduction\n\n<br>\n## 2.1 대표적인 방법들\n\n- DQN\n    - Discrete action space를 가지는 문제들에는 효과적으로 적용 가능하지만, continuous control에도 잘 작동하는지는 검증되지 않았습니다.\n- A3C - \"Vanilla\" policy gradient methods\n    - Data efficiency와 robustness 측면이 좋지 않습니다.\n- TRPO\n    - 간단히 말해 복잡합니다.\n    - 또한 noise(ex. dropout)나 parameter sharing(policy와 value function 간 혹은 auxiliary tasks와의)를 포함하는 architecture와의 호환성 없습니다.\n\n<br>\n## 2.2 대표적인 방법들 대비 개선사항\n\n- Scalability\n    - large models and parallel implementations\n- Data Efficiency\n- Robustness\n    - hyperparameter tuning없이 다양한 문제들에 적용되어 해결\n\n<br>\n## 2.3 제한하는 알고리즘\n\n이 논문에는 Clipped probability ratios를 포함하는 objective function 제안하였습니다.\n\n- TRPO의 data efficiency와 robustness를 유지하면서도 first-order approximation만 사용합니다.\n- Policy 성능에 대한 lower bound를 제공합니다.\n\n따라서 Policy로부터의 data sampling과 sampled data를 이용한 최적화를 번갈아가면서 수행합니다.\n\n<br>\n## 2.4 실험 결과\n\n다양한 버전의 surrogate objectives 중에는 clipped probability ratio가 가장 성능이 좋았습니다.\n\n- Continuous control tasks에서 기존 알고리즘 대비 성능이 좋습니다.\n- Atari에서는 A2C 대비 sampling efficiency 측면에서는 성능이 월등히 좋으며, ACER 대비 훨씬 간단하지만 성능은 비슷합니다.\n\n<br><br>\n\n# 3. Backgroud: Policy Optimization\n\n<br>\n## 3.1 Policy Gradient Methods\n\n일반적인 PG Method들은 실험적으로 destructive large policy updates가 발생합니다.\n\n- 더 자세히 말하자면, PG Method가 수행하는 parameter space에서의 gradual update가 policy space에서는 큰 변화를 유발할 수 있다는 의미입니다.\n\n<br>\n## 3.2 Trust Region Methods\n\nPolicy update 크기에 대한 contraint하에 objective function(\"surrogate\" function)을 최대화하는 것이 목표입니다. 수식은 아래와 같습니다.\n\n$$$$\n$$$$\n\n위의 수식은 contraint로 인해 excessive large policy update가 방지됩니다.\n\nTRPO에서는 constrained optimization problem을 풀기 위해서는 다음과 같은 방법들이 필요합니다.\n\n1. Fisher Information Matrix인 second-order derivative of KL divergence를 사용하거나,\n    - 여기서 second-order matrixes를 구하기 위해서는 많은 계산량 필요합니다.\n2. Conjugate Gradient를 사용합니다. \n    - Conjugate Gradient는 현하기가 어렵습니다.\n\n$$$$\n\n원래 이론적으로 위의 수식과 같이 \"contraint\"가 아니라 objective에 \"penalty\"를 부여하는 형태입니다. 하지만 다양한 문제들(혹은 학습 중에 특성이 변하는 문제)에서 모두 잘 동작하는 single value $\\beta$를 찾는 것(robustness)이 어렵기 때문에, TRPO에서는 penalty대신 contraint를 취하는 방식을 택한 것입니다.\n\n<br><br>\n\n# 4. Clipped Surrogate Objective\n\n이번 section에서는 TRPO의 surrogate obejctive function을 강제적으로 clipping하는 방법에 대하여 말합니다.\n\n먼저 기존의 TRPO의 surrogate function을 다음과 같이 표현합니다. \n$$r_t(\\theta)=\\dfrac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta old}(a_t|s_t)}, \\, \\, r_t(\\theta old) = 1$$\n\n위의 수식을 이용하여 TRPO의 surrogate object를 최대화합니다.\n\n- 실험중..\n\n$$L^{CPI} (\\theta) = \\hat{E}_t [\\frac{a}{a}]$$\n$$L^{CPI} (\\theta) = \\hat{E}_t [\\frac{\\pi}{a}]$$\n$$L^{CPI} (\\theta) = \\hat{E}_t [\\frac{\\pi\\_{\\theta}}{a}]$$\n$$L^{CPI} (\\theta) = \\hat{E}_t [\\frac{\\pi\\_{\\theta}}{a}]$$\n$$L^{CPI} (\\theta) = \\hat{E}_t [\\frac{\\pi}{a}]$$\n$$L^{CPI} (\\theta) = \\hat{E}_t [\\frac{\\pi_\\theta}{a}]$$\n$$L^{CPI} (\\theta) = \\hat{E}_t [\\frac{\\pi_\\theta (a_t|s_t)}{\\pi_\\theta}]$$\n$$L^{CPI} (\\theta) = \\hat{E}_t [\\frac{\\pi_\\theta (a_t \\vert s_t)}{\\pi_{\\theta old} (a_t \\vert s_t)} \\hat{A}_t] = \\hat{E}_t [r_t (\\theta\\hat{A}_t)]$$\n$$L^{CPI}(\\theta) = \\hat{E}_t [\\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta old} (a_t|s_t)}\\hat{A}_t]=\\hat{E}_t[r_t(\\theta)\\hat{A}_t]$$\n\n위의 수식을 그대로 사용한다면 excessively large policy update가 됩니다. 따라서 penalty를 이용하여 필요 이상의 policy update를 방지합니다. TRPO에서는 KL-Divergence를 이용하여 penalty를 적용하지만 PPO에서는 computation적으로 효율적인 penalty를 적용하고 excessively large policy update를 방지하기 위해 아래와 같은 clipping 기법을 사용합니다.\n\n$$L^{CLIP}(\\theta) = \\hat{E}_t [min(r_t(\\theta)\\hat{A}_t, clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_t)]$$\n\n위의 수식에서 $\\epsilon$은 hyperparameter로서 continuous control에서는 0.2일 때 성능이 가장 좋았으며, Atari Game에서는 0.1 x $\\alpha$ 값을 택함. ( $\\alpha$ 는 학습률로 1 에서 시작하여 학습이 진행됨에 따라 0 으로 감소)\n• Clipped 와 unclipped objective 중 min 값을 택함으로써 LCLIP($\\theta$)는 unclipped objective에 대 한 lowerbound가 됨\n\n이를 논문에서는 두 가지의 그래프($\\hat{A}_t$의 부호에 따라)로 설명하고 있습니다.  $\\hat{A}_t$가 양수일 때는 Advantage가 현재보다 높다라는 뜻이며 파라미터를 +의 방향으로 업데이트 하여야 합니다. $\\hat{A}_t$가 음수일 때는 Advantage가 현재보다 좋지 않다라는 뜻이며 그의 반대 방향으로 업데이트 하여야 합니다. $r_t(\\theta)$은 확률을 뜻하는 두개의 함수를 분자 분모로 가지고 있으며 분수로 구성되어 있기 때문에 무조건 양수로 이루어져 있으며 Advantage function인 $\\hat{A}_t$와 곱해져 Objective function인 $L^{CLIP}$은 Advantage function과 방향이 같아집니다.\n\n<br><br>\n\n# 5. Adaptive KL Penalty Coefficient\n\n2절에서 설명한 Clipped Surrogate Objective 방법과 달리 기존의 TRPO의 방법에 Adaptive한 파라미터를 적용한 방법 또한 제시하고 설명하고 있습니다.\n\n기존의 TRPO의 Objective function에 $\\beta$를 적용하여 다음과 같이 $\\beta$를 Adaptive하게 조절하고 있습니다.\n$$\nL^{KLPEN}(\\theta) = \\hat{E}_t [\\dfrac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta old}(a_t|s_t)}\\hat{A}_t-\\beta D_{KL}(\\pi_{\\theta old}(*|s)||\\pi_\\theta(*|s))]\n$$\n$d = \\hat{E}_t[ D_{KL}(\\pi_{\\theta old}(*|s)||\\pi_\\theta(*|s))]$, \n\n$d<d_{targ}/1.5, \\beta \\xleftarrow{}\\beta/2$ \n\n$ d>d_{targ}, \\beta \\xleftarrow{}\\beta\\times2$\n\n$\\beta$를 조절하는 방법을 해석하자면 다음과 같습니다. 만약 $\\theta old$와 $\\theta$간의 파라미터 차이가 크다면 penalty를 강하게 부여해 적은 변화를 야기하며, 파라미터 차이가 적다면 penalty를 완화시켜 주어 더 큰 변화를 야기하는 것을 위의 수식으로 표현할 수 있습니다.\n\n<br><br>\n\n# 6. Algorithm\n\n2절, 3절의 내용을 보면 policy만을 어떻게 업데이트 하는지에 대해 설명하고 있습니다. 4절에서는 다른 알고리즘과 같이 value, entropy-exploration과 합쳐 어떻게 통합적으로 업데이트 하는지에 대해 설명하고 있습니다.\n\n본 논문에서 설명하기를 state-value function $V(s)$는 [generalized advantage estimation](https://arxiv.org/abs/1506.02438)를 선택하여 사용하고 있으며 일반적인 방식인 Mean-square error를 통해 업데이트하고 있습니다. 이에 대한 Objective function은 본 논문 내에서 $L^{VF}$로 표현하고 있습니다.\n\n또한 일반적인 policy gradient 방법에서 [exploration](\"Asynchronous methods for deep reinforcement learning\" )의 기능을 추가하기 위해 다음과 같은 항, $S[\\pi_\\theta(s_t)]=\\pi_\\theta(a|s_t)log\\pi_\\theta(a|s_t)$, 을 추가합니다. \n\n2절, 3절에서 제시한 Objective function을 $L^{PPO}$라고 표현한다면($L^{PPO}$는 $L^{KLPEN}$, $L^{CLIP}$일 수 있습니다.) PPO에서 제시하는 통합적으로 최대화해야하는 함수는 다음과 같이 표현할 수 있습니다.\n$$\nL^{PPO+VF+S}(\\theta) = \\hat{E}_t[L^{PPO}(\\theta)-c_1L^{VF}(\\theta)+c_2S[\\pi_\\theta(s_t)]\n$$\n위의 Objective function을 최대화하면 Reinforcement Learning을 통한 policy 학습, state-value function의 학습, exploration을 할 수 있습니다.\n\n<br><br>\n\n# 7. Experimnet\n\n<br>\n## 7.1 Surrogate Objectives의 비교\n\n5.1절에서는 3가지 Surrogate Objectives를 비교분석 하고 있습니다.\n\n* No clipping or penalty: $L_t(\\theta) = r_t(\\theta)\\hat{A}_t$\n* Clipping: $L_t(\\theta) = min(r_t(\\theta)\\hat{A}_t, clip(r_t(\\theta), 1-\\epsilon,1+\\epsilon)\\hat{A}_t)$\n* KL penalty(fixed or adaptive): $L_t(\\theta)=r_t(\\theta)\\hat{A}_t-\\beta KL[\\pi_{\\theta old}, \\pi_\\theta]$\n\n그리고 환경은 MuJoCo 엔진을 사용하며 OpenAI Gym 환경에서 실험을 진행하였습니다. 그리고 네트워크들은 2개의 hidden layer를 가지며 각각 64개의 unit들을 가지고 있습니다. 그리고 tanh를 활성화 함수로써 사용하고 있으며 네트워크는 Gaussian distribution을 출력하는 것으로 구성되어 있습니다. 그리고 Policy Network와 Value Network는 파라미터들을 공유하고 있지 않으며 위에서 설명한 Exploration을 위한 entropy는 사용하지 않습니다.\n\n7개의 환경에서 실험하였으며 각 환경들은 HalfCheetah, Hopper, InvertedDoublePendulum, InvertedPerdulum, Reacher, Swimmer, Walker2d를 사용하였으며 모든 환경들은 v1 버전을 사용하였습니다.\n\n| Algorithm                      | avg. normalized score |\n| ------------------------------ | --------------------- |\n| No clipping or penalty         | -0.39                 |\n| Clipping, $\\epsilon=0.1$       | 0.76                  |\n| Clipping, $\\epsilon=0.2$       | 0.82                  |\n| Clipping, $\\epsilon=0.3$       | 0.70                  |\n| Adaptive KL $d_{targ}=0.003$   | 0.68                  |\n| Adaptive KL $d_{targ} = 0.001$ | 0.74                  |\n| Adaptive KL  $d_{targ} = 0.03$ | 0.71                  |\n| Fixed KL, $\\beta=0.3$          | 0.62                  |\n| Fixed KL, $\\beta=1$            | 0.71                  |\n| Fixed KL, $\\beta=3$            | 0.72                  |\n| Fixed KL, $\\beta=10$           | 0.69                  |\n\n위의 표는 실험 결과입니다. 세 가지 Surrogate Objective에 대해 각 파라미터들을 테이블에 표기된 방식대로 변화하며 실험하였습니다. \n\n<br>\n## 7.2 Comparison to other Algorithms in the Continuous Domain\n\n5.2절에서는 Clipping 버전의 PPO와 다른 알고리즘들 간의 비교를 연속적인 Action을 가지는 환경에서 실험을 한 결과를 보여주고 있습니다. 다른 알고리즘은 TRPO, cross-entropy method, vanilla policy gradient with adaptive stepsize, A2C, A2C with trust region를 사용하였습니다. 환경과 사용한 네트워크의 구성은 5.1절에서 사용한 것과 동일합니다.\n\n여타 다른 알고리즘보다 좋은 성능을 보이는 것을 알 수 있습니다.\n\n<br>\n## 7.3 Showcase in the Continuous Domain: Humanoid Running and Steering\n\n5.3절에서는 Humanoid-v0, HumanoidFlagrun-v0, HumanoidFlagrunHarder-v0의 환경에서 Roboschool엔진을 사용하여 실험하였습니다. Humanoid-v0는 단순히 앞으로 걸어나가는 환경, HumanoidFlagrun-v0은 200스텝마다 혹은 목적지에 도착할 때 마다 위치가 바뀌는 목적지에 걸어서 도달하는 환경입니다. HumanoidFlagrunHarder-v0 환경은 환경이 초기화될 때 마다 특정 경계안에 Humanoid가 위치하게 되며 특정 영역 밖으로 걸어나가는 것을 수행하는 환경입니다.\n\n<br>\n## 7.4 Comparison to Other Algorithms on the Atari Domain\n\n5.4절에서는 Atari Domain에서 PPO, A2C, ACER(Actor Critic with Experience Replay)의 3가지 알고리즘을 비교합니다.\n\n|                                                | A2C  | ACER | PPO  | Tie  |\n| ---------------------------------------------- | ---- | ---- | ---- | ---- |\n| (1) avg. episode reward over all of training   | 1    | 18   | 30   | 0    |\n| (2) avg. episode reward over last 100 episodes | 1    | 28   | 19   | 1    |\n\n모든 학습하는 도중에는 PPO가 ACER보다 좋은 성능을 나타내지만 마지막 100 에피소드만을 비교했을 때는 PPO보다 ACER이 더 좋은 성능을 나타냅니다. 이는 PPO가 더 빨리 최종 능력에 도달하지만 ACER이 가진 잠재력이 더 높다는 것을 나타냅니다.\n\n<br><br>\n\n# 8. Conclusion\n\n본 논문에서는 TRPO를 컴퓨터 친화적?으로 알고리즘을 수정하고 다른 알고리즘들과의 성능비교를 통해 PPO가 좋다라는 것을 보여주고 있습니다.","source":"_posts/ppo.md","raw":"---\ntitle: Proximal Policy Optimization\ndate: 2018-06-22 16:53:12\ntags: [\"프로젝트\", \"피지여행\"]\ncategories: 프로젝트\nauthor: 장수영, 차금강\nsubtitle: 피지여행 6번째 논문\n---\n\n<center> <img src=\"https://www.dropbox.com/s/145van5kldfvvd5/Screen%20Shot%202018-07-18%20at%201.19.30%20AM.png?dl=1\" width=\"700\"> </center>\n\n논문 저자 : John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov\n논문 링크 : https://arxiv.org/pdf/1707.06347.pdf\nProceeding : ??\n정리 : 장수영, 차금강\n\n---\n\n# 1. 들어가며...\n\n이전의 Trust Region Policy Optimization(TRPO)의 핵심적인 부분을 살펴보자면 다음과 같습니다.\n\n$$maximize_\\theta \\hat{E}_t [\\frac{\\pi_\\theta(a_t|s_t)} \\hat{A}_t ]-\\hat{E}_t [D_{KL}(\\pi_{\\theta old}(*|s) || \\pi_{\\theta(*|s)})]$$\n\n$$maximize_\\theta \\hat{E}_t [\\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta old}(a_t|s_t)} \\hat{A}_t ]-\\hat{E}_t [D_{KL}(\\pi_{\\theta old}(*|s)||\\pi_{\\theta(*|s)})]$$\n\n위의 식을 살펴보면 KL-Divergence를 구하는 과정에서 굉장한 연산량이 필요합니다.\n$$\nD_{KL}(\\pi_{\\theta old}(*|s)||\\pi_\\theta(*|s)) = -\\int{\\pi_{\\theta old}(*|s)ln\\pi_\\theta(*|s)}+\\int{\\pi_\\theta(*|s)ln\\pi_{\\theta old}(*|s)}\n$$\n특히 위의 식에서 볼 수 있듯이 $ln()$을 계산하는데 있어 많은 계산량이 필요로 하며 매번 파라미터들을 업데이트할 때마다 이 복잡한 연산을 반복적으로 수행해야 합니다. TRPO에서 KL-Divergence의 역할을 쉽게 풀이하자면 복잡한 비선형 최적화에서 조금의 파라미터 변화에도 성능이 크게 변화할 수 있는데 이를 방지하며 기존의 성능을 잘 보존할 수 있게 하는 장치입니다. 꼭 TRPO만이 아니라 일반적인 Policy Gradient 방법에서도 Objective Function은 $ln()$으로 되어 있으며 이를 어떤 연산량이 적은 방법으로 쉽게 적용할 수 있을까에 대해 고민하는 것이 PPO의 핵심입니다.\n\nPPO 논문에서는 TRPO에서 제시한 surrogate function을 두 가지로 나누어서 접근하며 이를 서로, 그리고 TRPO와 비교 분석하고 있습니다. 본 논문에서 제시하고 있는 방법 두 가지 중 첫번째는 surrogate function을 매우 작은 값으로 clipping함으로써 gradient 자체를 강제적으로 적게 만들어주는 방법이며 두번째는 TRPO의 KL-Divergence의 크기에 따라 Adaptive한 파라미터를 적용하는 방법입니다.\n\n<br><br>\n\n# 2. Introduction\n\n<br>\n## 2.1 대표적인 방법들\n\n- DQN\n    - Discrete action space를 가지는 문제들에는 효과적으로 적용 가능하지만, continuous control에도 잘 작동하는지는 검증되지 않았습니다.\n- A3C - \"Vanilla\" policy gradient methods\n    - Data efficiency와 robustness 측면이 좋지 않습니다.\n- TRPO\n    - 간단히 말해 복잡합니다.\n    - 또한 noise(ex. dropout)나 parameter sharing(policy와 value function 간 혹은 auxiliary tasks와의)를 포함하는 architecture와의 호환성 없습니다.\n\n<br>\n## 2.2 대표적인 방법들 대비 개선사항\n\n- Scalability\n    - large models and parallel implementations\n- Data Efficiency\n- Robustness\n    - hyperparameter tuning없이 다양한 문제들에 적용되어 해결\n\n<br>\n## 2.3 제한하는 알고리즘\n\n이 논문에는 Clipped probability ratios를 포함하는 objective function 제안하였습니다.\n\n- TRPO의 data efficiency와 robustness를 유지하면서도 first-order approximation만 사용합니다.\n- Policy 성능에 대한 lower bound를 제공합니다.\n\n따라서 Policy로부터의 data sampling과 sampled data를 이용한 최적화를 번갈아가면서 수행합니다.\n\n<br>\n## 2.4 실험 결과\n\n다양한 버전의 surrogate objectives 중에는 clipped probability ratio가 가장 성능이 좋았습니다.\n\n- Continuous control tasks에서 기존 알고리즘 대비 성능이 좋습니다.\n- Atari에서는 A2C 대비 sampling efficiency 측면에서는 성능이 월등히 좋으며, ACER 대비 훨씬 간단하지만 성능은 비슷합니다.\n\n<br><br>\n\n# 3. Backgroud: Policy Optimization\n\n<br>\n## 3.1 Policy Gradient Methods\n\n일반적인 PG Method들은 실험적으로 destructive large policy updates가 발생합니다.\n\n- 더 자세히 말하자면, PG Method가 수행하는 parameter space에서의 gradual update가 policy space에서는 큰 변화를 유발할 수 있다는 의미입니다.\n\n<br>\n## 3.2 Trust Region Methods\n\nPolicy update 크기에 대한 contraint하에 objective function(\"surrogate\" function)을 최대화하는 것이 목표입니다. 수식은 아래와 같습니다.\n\n$$$$\n$$$$\n\n위의 수식은 contraint로 인해 excessive large policy update가 방지됩니다.\n\nTRPO에서는 constrained optimization problem을 풀기 위해서는 다음과 같은 방법들이 필요합니다.\n\n1. Fisher Information Matrix인 second-order derivative of KL divergence를 사용하거나,\n    - 여기서 second-order matrixes를 구하기 위해서는 많은 계산량 필요합니다.\n2. Conjugate Gradient를 사용합니다. \n    - Conjugate Gradient는 현하기가 어렵습니다.\n\n$$$$\n\n원래 이론적으로 위의 수식과 같이 \"contraint\"가 아니라 objective에 \"penalty\"를 부여하는 형태입니다. 하지만 다양한 문제들(혹은 학습 중에 특성이 변하는 문제)에서 모두 잘 동작하는 single value $\\beta$를 찾는 것(robustness)이 어렵기 때문에, TRPO에서는 penalty대신 contraint를 취하는 방식을 택한 것입니다.\n\n<br><br>\n\n# 4. Clipped Surrogate Objective\n\n이번 section에서는 TRPO의 surrogate obejctive function을 강제적으로 clipping하는 방법에 대하여 말합니다.\n\n먼저 기존의 TRPO의 surrogate function을 다음과 같이 표현합니다. \n$$r_t(\\theta)=\\dfrac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta old}(a_t|s_t)}, \\, \\, r_t(\\theta old) = 1$$\n\n위의 수식을 이용하여 TRPO의 surrogate object를 최대화합니다.\n\n- 실험중..\n\n$$L^{CPI} (\\theta) = \\hat{E}_t [\\frac{a}{a}]$$\n$$L^{CPI} (\\theta) = \\hat{E}_t [\\frac{\\pi}{a}]$$\n$$L^{CPI} (\\theta) = \\hat{E}_t [\\frac{\\pi\\_{\\theta}}{a}]$$\n$$L^{CPI} (\\theta) = \\hat{E}_t [\\frac{\\pi\\_{\\theta}}{a}]$$\n$$L^{CPI} (\\theta) = \\hat{E}_t [\\frac{\\pi}{a}]$$\n$$L^{CPI} (\\theta) = \\hat{E}_t [\\frac{\\pi_\\theta}{a}]$$\n$$L^{CPI} (\\theta) = \\hat{E}_t [\\frac{\\pi_\\theta (a_t|s_t)}{\\pi_\\theta}]$$\n$$L^{CPI} (\\theta) = \\hat{E}_t [\\frac{\\pi_\\theta (a_t \\vert s_t)}{\\pi_{\\theta old} (a_t \\vert s_t)} \\hat{A}_t] = \\hat{E}_t [r_t (\\theta\\hat{A}_t)]$$\n$$L^{CPI}(\\theta) = \\hat{E}_t [\\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta old} (a_t|s_t)}\\hat{A}_t]=\\hat{E}_t[r_t(\\theta)\\hat{A}_t]$$\n\n위의 수식을 그대로 사용한다면 excessively large policy update가 됩니다. 따라서 penalty를 이용하여 필요 이상의 policy update를 방지합니다. TRPO에서는 KL-Divergence를 이용하여 penalty를 적용하지만 PPO에서는 computation적으로 효율적인 penalty를 적용하고 excessively large policy update를 방지하기 위해 아래와 같은 clipping 기법을 사용합니다.\n\n$$L^{CLIP}(\\theta) = \\hat{E}_t [min(r_t(\\theta)\\hat{A}_t, clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_t)]$$\n\n위의 수식에서 $\\epsilon$은 hyperparameter로서 continuous control에서는 0.2일 때 성능이 가장 좋았으며, Atari Game에서는 0.1 x $\\alpha$ 값을 택함. ( $\\alpha$ 는 학습률로 1 에서 시작하여 학습이 진행됨에 따라 0 으로 감소)\n• Clipped 와 unclipped objective 중 min 값을 택함으로써 LCLIP($\\theta$)는 unclipped objective에 대 한 lowerbound가 됨\n\n이를 논문에서는 두 가지의 그래프($\\hat{A}_t$의 부호에 따라)로 설명하고 있습니다.  $\\hat{A}_t$가 양수일 때는 Advantage가 현재보다 높다라는 뜻이며 파라미터를 +의 방향으로 업데이트 하여야 합니다. $\\hat{A}_t$가 음수일 때는 Advantage가 현재보다 좋지 않다라는 뜻이며 그의 반대 방향으로 업데이트 하여야 합니다. $r_t(\\theta)$은 확률을 뜻하는 두개의 함수를 분자 분모로 가지고 있으며 분수로 구성되어 있기 때문에 무조건 양수로 이루어져 있으며 Advantage function인 $\\hat{A}_t$와 곱해져 Objective function인 $L^{CLIP}$은 Advantage function과 방향이 같아집니다.\n\n<br><br>\n\n# 5. Adaptive KL Penalty Coefficient\n\n2절에서 설명한 Clipped Surrogate Objective 방법과 달리 기존의 TRPO의 방법에 Adaptive한 파라미터를 적용한 방법 또한 제시하고 설명하고 있습니다.\n\n기존의 TRPO의 Objective function에 $\\beta$를 적용하여 다음과 같이 $\\beta$를 Adaptive하게 조절하고 있습니다.\n$$\nL^{KLPEN}(\\theta) = \\hat{E}_t [\\dfrac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta old}(a_t|s_t)}\\hat{A}_t-\\beta D_{KL}(\\pi_{\\theta old}(*|s)||\\pi_\\theta(*|s))]\n$$\n$d = \\hat{E}_t[ D_{KL}(\\pi_{\\theta old}(*|s)||\\pi_\\theta(*|s))]$, \n\n$d<d_{targ}/1.5, \\beta \\xleftarrow{}\\beta/2$ \n\n$ d>d_{targ}, \\beta \\xleftarrow{}\\beta\\times2$\n\n$\\beta$를 조절하는 방법을 해석하자면 다음과 같습니다. 만약 $\\theta old$와 $\\theta$간의 파라미터 차이가 크다면 penalty를 강하게 부여해 적은 변화를 야기하며, 파라미터 차이가 적다면 penalty를 완화시켜 주어 더 큰 변화를 야기하는 것을 위의 수식으로 표현할 수 있습니다.\n\n<br><br>\n\n# 6. Algorithm\n\n2절, 3절의 내용을 보면 policy만을 어떻게 업데이트 하는지에 대해 설명하고 있습니다. 4절에서는 다른 알고리즘과 같이 value, entropy-exploration과 합쳐 어떻게 통합적으로 업데이트 하는지에 대해 설명하고 있습니다.\n\n본 논문에서 설명하기를 state-value function $V(s)$는 [generalized advantage estimation](https://arxiv.org/abs/1506.02438)를 선택하여 사용하고 있으며 일반적인 방식인 Mean-square error를 통해 업데이트하고 있습니다. 이에 대한 Objective function은 본 논문 내에서 $L^{VF}$로 표현하고 있습니다.\n\n또한 일반적인 policy gradient 방법에서 [exploration](\"Asynchronous methods for deep reinforcement learning\" )의 기능을 추가하기 위해 다음과 같은 항, $S[\\pi_\\theta(s_t)]=\\pi_\\theta(a|s_t)log\\pi_\\theta(a|s_t)$, 을 추가합니다. \n\n2절, 3절에서 제시한 Objective function을 $L^{PPO}$라고 표현한다면($L^{PPO}$는 $L^{KLPEN}$, $L^{CLIP}$일 수 있습니다.) PPO에서 제시하는 통합적으로 최대화해야하는 함수는 다음과 같이 표현할 수 있습니다.\n$$\nL^{PPO+VF+S}(\\theta) = \\hat{E}_t[L^{PPO}(\\theta)-c_1L^{VF}(\\theta)+c_2S[\\pi_\\theta(s_t)]\n$$\n위의 Objective function을 최대화하면 Reinforcement Learning을 통한 policy 학습, state-value function의 학습, exploration을 할 수 있습니다.\n\n<br><br>\n\n# 7. Experimnet\n\n<br>\n## 7.1 Surrogate Objectives의 비교\n\n5.1절에서는 3가지 Surrogate Objectives를 비교분석 하고 있습니다.\n\n* No clipping or penalty: $L_t(\\theta) = r_t(\\theta)\\hat{A}_t$\n* Clipping: $L_t(\\theta) = min(r_t(\\theta)\\hat{A}_t, clip(r_t(\\theta), 1-\\epsilon,1+\\epsilon)\\hat{A}_t)$\n* KL penalty(fixed or adaptive): $L_t(\\theta)=r_t(\\theta)\\hat{A}_t-\\beta KL[\\pi_{\\theta old}, \\pi_\\theta]$\n\n그리고 환경은 MuJoCo 엔진을 사용하며 OpenAI Gym 환경에서 실험을 진행하였습니다. 그리고 네트워크들은 2개의 hidden layer를 가지며 각각 64개의 unit들을 가지고 있습니다. 그리고 tanh를 활성화 함수로써 사용하고 있으며 네트워크는 Gaussian distribution을 출력하는 것으로 구성되어 있습니다. 그리고 Policy Network와 Value Network는 파라미터들을 공유하고 있지 않으며 위에서 설명한 Exploration을 위한 entropy는 사용하지 않습니다.\n\n7개의 환경에서 실험하였으며 각 환경들은 HalfCheetah, Hopper, InvertedDoublePendulum, InvertedPerdulum, Reacher, Swimmer, Walker2d를 사용하였으며 모든 환경들은 v1 버전을 사용하였습니다.\n\n| Algorithm                      | avg. normalized score |\n| ------------------------------ | --------------------- |\n| No clipping or penalty         | -0.39                 |\n| Clipping, $\\epsilon=0.1$       | 0.76                  |\n| Clipping, $\\epsilon=0.2$       | 0.82                  |\n| Clipping, $\\epsilon=0.3$       | 0.70                  |\n| Adaptive KL $d_{targ}=0.003$   | 0.68                  |\n| Adaptive KL $d_{targ} = 0.001$ | 0.74                  |\n| Adaptive KL  $d_{targ} = 0.03$ | 0.71                  |\n| Fixed KL, $\\beta=0.3$          | 0.62                  |\n| Fixed KL, $\\beta=1$            | 0.71                  |\n| Fixed KL, $\\beta=3$            | 0.72                  |\n| Fixed KL, $\\beta=10$           | 0.69                  |\n\n위의 표는 실험 결과입니다. 세 가지 Surrogate Objective에 대해 각 파라미터들을 테이블에 표기된 방식대로 변화하며 실험하였습니다. \n\n<br>\n## 7.2 Comparison to other Algorithms in the Continuous Domain\n\n5.2절에서는 Clipping 버전의 PPO와 다른 알고리즘들 간의 비교를 연속적인 Action을 가지는 환경에서 실험을 한 결과를 보여주고 있습니다. 다른 알고리즘은 TRPO, cross-entropy method, vanilla policy gradient with adaptive stepsize, A2C, A2C with trust region를 사용하였습니다. 환경과 사용한 네트워크의 구성은 5.1절에서 사용한 것과 동일합니다.\n\n여타 다른 알고리즘보다 좋은 성능을 보이는 것을 알 수 있습니다.\n\n<br>\n## 7.3 Showcase in the Continuous Domain: Humanoid Running and Steering\n\n5.3절에서는 Humanoid-v0, HumanoidFlagrun-v0, HumanoidFlagrunHarder-v0의 환경에서 Roboschool엔진을 사용하여 실험하였습니다. Humanoid-v0는 단순히 앞으로 걸어나가는 환경, HumanoidFlagrun-v0은 200스텝마다 혹은 목적지에 도착할 때 마다 위치가 바뀌는 목적지에 걸어서 도달하는 환경입니다. HumanoidFlagrunHarder-v0 환경은 환경이 초기화될 때 마다 특정 경계안에 Humanoid가 위치하게 되며 특정 영역 밖으로 걸어나가는 것을 수행하는 환경입니다.\n\n<br>\n## 7.4 Comparison to Other Algorithms on the Atari Domain\n\n5.4절에서는 Atari Domain에서 PPO, A2C, ACER(Actor Critic with Experience Replay)의 3가지 알고리즘을 비교합니다.\n\n|                                                | A2C  | ACER | PPO  | Tie  |\n| ---------------------------------------------- | ---- | ---- | ---- | ---- |\n| (1) avg. episode reward over all of training   | 1    | 18   | 30   | 0    |\n| (2) avg. episode reward over last 100 episodes | 1    | 28   | 19   | 1    |\n\n모든 학습하는 도중에는 PPO가 ACER보다 좋은 성능을 나타내지만 마지막 100 에피소드만을 비교했을 때는 PPO보다 ACER이 더 좋은 성능을 나타냅니다. 이는 PPO가 더 빨리 최종 능력에 도달하지만 ACER이 가진 잠재력이 더 높다는 것을 나타냅니다.\n\n<br><br>\n\n# 8. Conclusion\n\n본 논문에서는 TRPO를 컴퓨터 친화적?으로 알고리즘을 수정하고 다른 알고리즘들과의 성능비교를 통해 PPO가 좋다라는 것을 보여주고 있습니다.","slug":"ppo","published":1,"updated":"2018-07-24T06:51:45.037Z","_id":"cjjptjn050000ue153nep5uvv","comments":1,"layout":"post","photos":[],"link":"","content":"<center> <img src=\"https://www.dropbox.com/s/145van5kldfvvd5/Screen%20Shot%202018-07-18%20at%201.19.30%20AM.png?dl=1\" width=\"700\"> </center>\n\n<p>논문 저자 : John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov<br>논문 링크 : <a href=\"https://arxiv.org/pdf/1707.06347.pdf\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/pdf/1707.06347.pdf</a><br>Proceeding : ??<br>정리 : 장수영, 차금강</p>\n<hr>\n<h1 id=\"1-들어가며…\"><a href=\"#1-들어가며…\" class=\"headerlink\" title=\"1. 들어가며…\"></a>1. 들어가며…</h1><p>이전의 Trust Region Policy Optimization(TRPO)의 핵심적인 부분을 살펴보자면 다음과 같습니다.</p>\n<p>$$maximize_\\theta \\hat{E}<em>t [\\frac{\\pi</em>\\theta(a_t|s_t)} \\hat{A}_t ]-\\hat{E}<em>t [D</em>{KL}(\\pi_{\\theta old}(<em>|s) || \\pi_{\\theta(</em>|s)})]$$</p>\n<p>$$maximize_\\theta \\hat{E}<em>t [\\frac{\\pi</em>\\theta(a_t|s_t)}{\\pi_{\\theta old}(a_t|s_t)} \\hat{A}_t ]-\\hat{E}<em>t [D</em>{KL}(\\pi_{\\theta old}(<em>|s)||\\pi_{\\theta(</em>|s)})]$$</p>\n<p>위의 식을 살펴보면 KL-Divergence를 구하는 과정에서 굉장한 연산량이 필요합니다.<br>$$<br>D_{KL}(\\pi_{\\theta old}(<em>|s)||\\pi_\\theta(</em>|s)) = -\\int{\\pi_{\\theta old}(<em>|s)ln\\pi_\\theta(</em>|s)}+\\int{\\pi_\\theta(<em>|s)ln\\pi_{\\theta old}(</em>|s)}<br>$$<br>특히 위의 식에서 볼 수 있듯이 $ln()$을 계산하는데 있어 많은 계산량이 필요로 하며 매번 파라미터들을 업데이트할 때마다 이 복잡한 연산을 반복적으로 수행해야 합니다. TRPO에서 KL-Divergence의 역할을 쉽게 풀이하자면 복잡한 비선형 최적화에서 조금의 파라미터 변화에도 성능이 크게 변화할 수 있는데 이를 방지하며 기존의 성능을 잘 보존할 수 있게 하는 장치입니다. 꼭 TRPO만이 아니라 일반적인 Policy Gradient 방법에서도 Objective Function은 $ln()$으로 되어 있으며 이를 어떤 연산량이 적은 방법으로 쉽게 적용할 수 있을까에 대해 고민하는 것이 PPO의 핵심입니다.</p>\n<p>PPO 논문에서는 TRPO에서 제시한 surrogate function을 두 가지로 나누어서 접근하며 이를 서로, 그리고 TRPO와 비교 분석하고 있습니다. 본 논문에서 제시하고 있는 방법 두 가지 중 첫번째는 surrogate function을 매우 작은 값으로 clipping함으로써 gradient 자체를 강제적으로 적게 만들어주는 방법이며 두번째는 TRPO의 KL-Divergence의 크기에 따라 Adaptive한 파라미터를 적용하는 방법입니다.</p>\n<p><br><br></p>\n<h1 id=\"2-Introduction\"><a href=\"#2-Introduction\" class=\"headerlink\" title=\"2. Introduction\"></a>2. Introduction</h1><p><br></p>\n<h2 id=\"2-1-대표적인-방법들\"><a href=\"#2-1-대표적인-방법들\" class=\"headerlink\" title=\"2.1 대표적인 방법들\"></a>2.1 대표적인 방법들</h2><ul>\n<li>DQN<ul>\n<li>Discrete action space를 가지는 문제들에는 효과적으로 적용 가능하지만, continuous control에도 잘 작동하는지는 검증되지 않았습니다.</li>\n</ul>\n</li>\n<li>A3C - “Vanilla” policy gradient methods<ul>\n<li>Data efficiency와 robustness 측면이 좋지 않습니다.</li>\n</ul>\n</li>\n<li>TRPO<ul>\n<li>간단히 말해 복잡합니다.</li>\n<li>또한 noise(ex. dropout)나 parameter sharing(policy와 value function 간 혹은 auxiliary tasks와의)를 포함하는 architecture와의 호환성 없습니다.</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-2-대표적인-방법들-대비-개선사항\"><a href=\"#2-2-대표적인-방법들-대비-개선사항\" class=\"headerlink\" title=\"2.2 대표적인 방법들 대비 개선사항\"></a>2.2 대표적인 방법들 대비 개선사항</h2><ul>\n<li>Scalability<ul>\n<li>large models and parallel implementations</li>\n</ul>\n</li>\n<li>Data Efficiency</li>\n<li>Robustness<ul>\n<li>hyperparameter tuning없이 다양한 문제들에 적용되어 해결</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-3-제한하는-알고리즘\"><a href=\"#2-3-제한하는-알고리즘\" class=\"headerlink\" title=\"2.3 제한하는 알고리즘\"></a>2.3 제한하는 알고리즘</h2><p>이 논문에는 Clipped probability ratios를 포함하는 objective function 제안하였습니다.</p>\n<ul>\n<li>TRPO의 data efficiency와 robustness를 유지하면서도 first-order approximation만 사용합니다.</li>\n<li>Policy 성능에 대한 lower bound를 제공합니다.</li>\n</ul>\n<p>따라서 Policy로부터의 data sampling과 sampled data를 이용한 최적화를 번갈아가면서 수행합니다.</p>\n<p><br></p>\n<h2 id=\"2-4-실험-결과\"><a href=\"#2-4-실험-결과\" class=\"headerlink\" title=\"2.4 실험 결과\"></a>2.4 실험 결과</h2><p>다양한 버전의 surrogate objectives 중에는 clipped probability ratio가 가장 성능이 좋았습니다.</p>\n<ul>\n<li>Continuous control tasks에서 기존 알고리즘 대비 성능이 좋습니다.</li>\n<li>Atari에서는 A2C 대비 sampling efficiency 측면에서는 성능이 월등히 좋으며, ACER 대비 훨씬 간단하지만 성능은 비슷합니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"3-Backgroud-Policy-Optimization\"><a href=\"#3-Backgroud-Policy-Optimization\" class=\"headerlink\" title=\"3. Backgroud: Policy Optimization\"></a>3. Backgroud: Policy Optimization</h1><p><br></p>\n<h2 id=\"3-1-Policy-Gradient-Methods\"><a href=\"#3-1-Policy-Gradient-Methods\" class=\"headerlink\" title=\"3.1 Policy Gradient Methods\"></a>3.1 Policy Gradient Methods</h2><p>일반적인 PG Method들은 실험적으로 destructive large policy updates가 발생합니다.</p>\n<ul>\n<li>더 자세히 말하자면, PG Method가 수행하는 parameter space에서의 gradual update가 policy space에서는 큰 변화를 유발할 수 있다는 의미입니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-2-Trust-Region-Methods\"><a href=\"#3-2-Trust-Region-Methods\" class=\"headerlink\" title=\"3.2 Trust Region Methods\"></a>3.2 Trust Region Methods</h2><p>Policy update 크기에 대한 contraint하에 objective function(“surrogate” function)을 최대화하는 것이 목표입니다. 수식은 아래와 같습니다.</p>\n<p>$$$$<br>$$$$</p>\n<p>위의 수식은 contraint로 인해 excessive large policy update가 방지됩니다.</p>\n<p>TRPO에서는 constrained optimization problem을 풀기 위해서는 다음과 같은 방법들이 필요합니다.</p>\n<ol>\n<li>Fisher Information Matrix인 second-order derivative of KL divergence를 사용하거나,<ul>\n<li>여기서 second-order matrixes를 구하기 위해서는 많은 계산량 필요합니다.</li>\n</ul>\n</li>\n<li>Conjugate Gradient를 사용합니다. <ul>\n<li>Conjugate Gradient는 현하기가 어렵습니다.</li>\n</ul>\n</li>\n</ol>\n<p>$$$$</p>\n<p>원래 이론적으로 위의 수식과 같이 “contraint”가 아니라 objective에 “penalty”를 부여하는 형태입니다. 하지만 다양한 문제들(혹은 학습 중에 특성이 변하는 문제)에서 모두 잘 동작하는 single value $\\beta$를 찾는 것(robustness)이 어렵기 때문에, TRPO에서는 penalty대신 contraint를 취하는 방식을 택한 것입니다.</p>\n<p><br><br></p>\n<h1 id=\"4-Clipped-Surrogate-Objective\"><a href=\"#4-Clipped-Surrogate-Objective\" class=\"headerlink\" title=\"4. Clipped Surrogate Objective\"></a>4. Clipped Surrogate Objective</h1><p>이번 section에서는 TRPO의 surrogate obejctive function을 강제적으로 clipping하는 방법에 대하여 말합니다.</p>\n<p>먼저 기존의 TRPO의 surrogate function을 다음과 같이 표현합니다.<br>$$r_t(\\theta)=\\dfrac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta old}(a_t|s_t)}, \\, \\, r_t(\\theta old) = 1$$</p>\n<p>위의 수식을 이용하여 TRPO의 surrogate object를 최대화합니다.</p>\n<ul>\n<li>실험중..</li>\n</ul>\n<p>$$L^{CPI} (\\theta) = \\hat{E}_t [\\frac{a}{a}]$$<br>$$L^{CPI} (\\theta) = \\hat{E}_t [\\frac{\\pi}{a}]$$<br>$$L^{CPI} (\\theta) = \\hat{E}<em>t [\\frac{\\pi\\</em>{\\theta}}{a}]$$<br>$$L^{CPI} (\\theta) = \\hat{E}<em>t [\\frac{\\pi\\</em>{\\theta}}{a}]$$<br>$$L^{CPI} (\\theta) = \\hat{E}_t [\\frac{\\pi}{a}]$$<br>$$L^{CPI} (\\theta) = \\hat{E}<em>t [\\frac{\\pi</em>\\theta}{a}]$$<br>$$L^{CPI} (\\theta) = \\hat{E}<em>t [\\frac{\\pi</em>\\theta (a_t|s_t)}{\\pi_\\theta}]$$<br>$$L^{CPI} (\\theta) = \\hat{E}<em>t [\\frac{\\pi</em>\\theta (a_t \\vert s_t)}{\\pi_{\\theta old} (a_t \\vert s_t)} \\hat{A}_t] = \\hat{E}_t [r_t (\\theta\\hat{A}_t)]$$<br>$$L^{CPI}(\\theta) = \\hat{E}<em>t [\\frac{\\pi</em>\\theta(a_t|s_t)}{\\pi_{\\theta old} (a_t|s_t)}\\hat{A}_t]=\\hat{E}_t[r_t(\\theta)\\hat{A}_t]$$</p>\n<p>위의 수식을 그대로 사용한다면 excessively large policy update가 됩니다. 따라서 penalty를 이용하여 필요 이상의 policy update를 방지합니다. TRPO에서는 KL-Divergence를 이용하여 penalty를 적용하지만 PPO에서는 computation적으로 효율적인 penalty를 적용하고 excessively large policy update를 방지하기 위해 아래와 같은 clipping 기법을 사용합니다.</p>\n<p>$$L^{CLIP}(\\theta) = \\hat{E}_t [min(r_t(\\theta)\\hat{A}_t, clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_t)]$$</p>\n<p>위의 수식에서 $\\epsilon$은 hyperparameter로서 continuous control에서는 0.2일 때 성능이 가장 좋았으며, Atari Game에서는 0.1 x $\\alpha$ 값을 택함. ( $\\alpha$ 는 학습률로 1 에서 시작하여 학습이 진행됨에 따라 0 으로 감소)<br>• Clipped 와 unclipped objective 중 min 값을 택함으로써 LCLIP($\\theta$)는 unclipped objective에 대 한 lowerbound가 됨</p>\n<p>이를 논문에서는 두 가지의 그래프($\\hat{A}_t$의 부호에 따라)로 설명하고 있습니다.  $\\hat{A}_t$가 양수일 때는 Advantage가 현재보다 높다라는 뜻이며 파라미터를 +의 방향으로 업데이트 하여야 합니다. $\\hat{A}_t$가 음수일 때는 Advantage가 현재보다 좋지 않다라는 뜻이며 그의 반대 방향으로 업데이트 하여야 합니다. $r_t(\\theta)$은 확률을 뜻하는 두개의 함수를 분자 분모로 가지고 있으며 분수로 구성되어 있기 때문에 무조건 양수로 이루어져 있으며 Advantage function인 $\\hat{A}_t$와 곱해져 Objective function인 $L^{CLIP}$은 Advantage function과 방향이 같아집니다.</p>\n<p><br><br></p>\n<h1 id=\"5-Adaptive-KL-Penalty-Coefficient\"><a href=\"#5-Adaptive-KL-Penalty-Coefficient\" class=\"headerlink\" title=\"5. Adaptive KL Penalty Coefficient\"></a>5. Adaptive KL Penalty Coefficient</h1><p>2절에서 설명한 Clipped Surrogate Objective 방법과 달리 기존의 TRPO의 방법에 Adaptive한 파라미터를 적용한 방법 또한 제시하고 설명하고 있습니다.</p>\n<p>기존의 TRPO의 Objective function에 $\\beta$를 적용하여 다음과 같이 $\\beta$를 Adaptive하게 조절하고 있습니다.<br>$$<br>L^{KLPEN}(\\theta) = \\hat{E}<em>t [\\dfrac{\\pi</em>\\theta(a_t|s_t)}{\\pi_{\\theta old}(a_t|s_t)}\\hat{A}<em>t-\\beta D</em>{KL}(\\pi_{\\theta old}(<em>|s)||\\pi_\\theta(</em>|s))]<br>$$<br>$d = \\hat{E}<em>t[ D</em>{KL}(\\pi_{\\theta old}(<em>|s)||\\pi_\\theta(</em>|s))]$, </p>\n<p>$d&lt;d_{targ}/1.5, \\beta \\xleftarrow{}\\beta/2$ </p>\n<p>$ d&gt;d_{targ}, \\beta \\xleftarrow{}\\beta\\times2$</p>\n<p>$\\beta$를 조절하는 방법을 해석하자면 다음과 같습니다. 만약 $\\theta old$와 $\\theta$간의 파라미터 차이가 크다면 penalty를 강하게 부여해 적은 변화를 야기하며, 파라미터 차이가 적다면 penalty를 완화시켜 주어 더 큰 변화를 야기하는 것을 위의 수식으로 표현할 수 있습니다.</p>\n<p><br><br></p>\n<h1 id=\"6-Algorithm\"><a href=\"#6-Algorithm\" class=\"headerlink\" title=\"6. Algorithm\"></a>6. Algorithm</h1><p>2절, 3절의 내용을 보면 policy만을 어떻게 업데이트 하는지에 대해 설명하고 있습니다. 4절에서는 다른 알고리즘과 같이 value, entropy-exploration과 합쳐 어떻게 통합적으로 업데이트 하는지에 대해 설명하고 있습니다.</p>\n<p>본 논문에서 설명하기를 state-value function $V(s)$는 <a href=\"https://arxiv.org/abs/1506.02438\" target=\"_blank\" rel=\"noopener\">generalized advantage estimation</a>를 선택하여 사용하고 있으며 일반적인 방식인 Mean-square error를 통해 업데이트하고 있습니다. 이에 대한 Objective function은 본 논문 내에서 $L^{VF}$로 표현하고 있습니다.</p>\n<p>또한 일반적인 policy gradient 방법에서 <a href=\"&quot;Asynchronous methods for deep reinforcement learning&quot;\">exploration</a>의 기능을 추가하기 위해 다음과 같은 항, $S[\\pi_\\theta(s_t)]=\\pi_\\theta(a|s_t)log\\pi_\\theta(a|s_t)$, 을 추가합니다. </p>\n<p>2절, 3절에서 제시한 Objective function을 $L^{PPO}$라고 표현한다면($L^{PPO}$는 $L^{KLPEN}$, $L^{CLIP}$일 수 있습니다.) PPO에서 제시하는 통합적으로 최대화해야하는 함수는 다음과 같이 표현할 수 있습니다.<br>$$<br>L^{PPO+VF+S}(\\theta) = \\hat{E}_t[L^{PPO}(\\theta)-c_1L^{VF}(\\theta)+c_2S[\\pi_\\theta(s_t)]<br>$$<br>위의 Objective function을 최대화하면 Reinforcement Learning을 통한 policy 학습, state-value function의 학습, exploration을 할 수 있습니다.</p>\n<p><br><br></p>\n<h1 id=\"7-Experimnet\"><a href=\"#7-Experimnet\" class=\"headerlink\" title=\"7. Experimnet\"></a>7. Experimnet</h1><p><br></p>\n<h2 id=\"7-1-Surrogate-Objectives의-비교\"><a href=\"#7-1-Surrogate-Objectives의-비교\" class=\"headerlink\" title=\"7.1 Surrogate Objectives의 비교\"></a>7.1 Surrogate Objectives의 비교</h2><p>5.1절에서는 3가지 Surrogate Objectives를 비교분석 하고 있습니다.</p>\n<ul>\n<li>No clipping or penalty: $L_t(\\theta) = r_t(\\theta)\\hat{A}_t$</li>\n<li>Clipping: $L_t(\\theta) = min(r_t(\\theta)\\hat{A}_t, clip(r_t(\\theta), 1-\\epsilon,1+\\epsilon)\\hat{A}_t)$</li>\n<li>KL penalty(fixed or adaptive): $L_t(\\theta)=r_t(\\theta)\\hat{A}<em>t-\\beta KL[\\pi</em>{\\theta old}, \\pi_\\theta]$</li>\n</ul>\n<p>그리고 환경은 MuJoCo 엔진을 사용하며 OpenAI Gym 환경에서 실험을 진행하였습니다. 그리고 네트워크들은 2개의 hidden layer를 가지며 각각 64개의 unit들을 가지고 있습니다. 그리고 tanh를 활성화 함수로써 사용하고 있으며 네트워크는 Gaussian distribution을 출력하는 것으로 구성되어 있습니다. 그리고 Policy Network와 Value Network는 파라미터들을 공유하고 있지 않으며 위에서 설명한 Exploration을 위한 entropy는 사용하지 않습니다.</p>\n<p>7개의 환경에서 실험하였으며 각 환경들은 HalfCheetah, Hopper, InvertedDoublePendulum, InvertedPerdulum, Reacher, Swimmer, Walker2d를 사용하였으며 모든 환경들은 v1 버전을 사용하였습니다.</p>\n<table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th>avg. normalized score</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>No clipping or penalty</td>\n<td>-0.39</td>\n</tr>\n<tr>\n<td>Clipping, $\\epsilon=0.1$</td>\n<td>0.76</td>\n</tr>\n<tr>\n<td>Clipping, $\\epsilon=0.2$</td>\n<td>0.82</td>\n</tr>\n<tr>\n<td>Clipping, $\\epsilon=0.3$</td>\n<td>0.70</td>\n</tr>\n<tr>\n<td>Adaptive KL $d_{targ}=0.003$</td>\n<td>0.68</td>\n</tr>\n<tr>\n<td>Adaptive KL $d_{targ} = 0.001$</td>\n<td>0.74</td>\n</tr>\n<tr>\n<td>Adaptive KL  $d_{targ} = 0.03$</td>\n<td>0.71</td>\n</tr>\n<tr>\n<td>Fixed KL, $\\beta=0.3$</td>\n<td>0.62</td>\n</tr>\n<tr>\n<td>Fixed KL, $\\beta=1$</td>\n<td>0.71</td>\n</tr>\n<tr>\n<td>Fixed KL, $\\beta=3$</td>\n<td>0.72</td>\n</tr>\n<tr>\n<td>Fixed KL, $\\beta=10$</td>\n<td>0.69</td>\n</tr>\n</tbody>\n</table>\n<p>위의 표는 실험 결과입니다. 세 가지 Surrogate Objective에 대해 각 파라미터들을 테이블에 표기된 방식대로 변화하며 실험하였습니다. </p>\n<p><br></p>\n<h2 id=\"7-2-Comparison-to-other-Algorithms-in-the-Continuous-Domain\"><a href=\"#7-2-Comparison-to-other-Algorithms-in-the-Continuous-Domain\" class=\"headerlink\" title=\"7.2 Comparison to other Algorithms in the Continuous Domain\"></a>7.2 Comparison to other Algorithms in the Continuous Domain</h2><p>5.2절에서는 Clipping 버전의 PPO와 다른 알고리즘들 간의 비교를 연속적인 Action을 가지는 환경에서 실험을 한 결과를 보여주고 있습니다. 다른 알고리즘은 TRPO, cross-entropy method, vanilla policy gradient with adaptive stepsize, A2C, A2C with trust region를 사용하였습니다. 환경과 사용한 네트워크의 구성은 5.1절에서 사용한 것과 동일합니다.</p>\n<p>여타 다른 알고리즘보다 좋은 성능을 보이는 것을 알 수 있습니다.</p>\n<p><br></p>\n<h2 id=\"7-3-Showcase-in-the-Continuous-Domain-Humanoid-Running-and-Steering\"><a href=\"#7-3-Showcase-in-the-Continuous-Domain-Humanoid-Running-and-Steering\" class=\"headerlink\" title=\"7.3 Showcase in the Continuous Domain: Humanoid Running and Steering\"></a>7.3 Showcase in the Continuous Domain: Humanoid Running and Steering</h2><p>5.3절에서는 Humanoid-v0, HumanoidFlagrun-v0, HumanoidFlagrunHarder-v0의 환경에서 Roboschool엔진을 사용하여 실험하였습니다. Humanoid-v0는 단순히 앞으로 걸어나가는 환경, HumanoidFlagrun-v0은 200스텝마다 혹은 목적지에 도착할 때 마다 위치가 바뀌는 목적지에 걸어서 도달하는 환경입니다. HumanoidFlagrunHarder-v0 환경은 환경이 초기화될 때 마다 특정 경계안에 Humanoid가 위치하게 되며 특정 영역 밖으로 걸어나가는 것을 수행하는 환경입니다.</p>\n<p><br></p>\n<h2 id=\"7-4-Comparison-to-Other-Algorithms-on-the-Atari-Domain\"><a href=\"#7-4-Comparison-to-Other-Algorithms-on-the-Atari-Domain\" class=\"headerlink\" title=\"7.4 Comparison to Other Algorithms on the Atari Domain\"></a>7.4 Comparison to Other Algorithms on the Atari Domain</h2><p>5.4절에서는 Atari Domain에서 PPO, A2C, ACER(Actor Critic with Experience Replay)의 3가지 알고리즘을 비교합니다.</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>A2C</th>\n<th>ACER</th>\n<th>PPO</th>\n<th>Tie</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>(1) avg. episode reward over all of training</td>\n<td>1</td>\n<td>18</td>\n<td>30</td>\n<td>0</td>\n</tr>\n<tr>\n<td>(2) avg. episode reward over last 100 episodes</td>\n<td>1</td>\n<td>28</td>\n<td>19</td>\n<td>1</td>\n</tr>\n</tbody>\n</table>\n<p>모든 학습하는 도중에는 PPO가 ACER보다 좋은 성능을 나타내지만 마지막 100 에피소드만을 비교했을 때는 PPO보다 ACER이 더 좋은 성능을 나타냅니다. 이는 PPO가 더 빨리 최종 능력에 도달하지만 ACER이 가진 잠재력이 더 높다는 것을 나타냅니다.</p>\n<p><br><br></p>\n<h1 id=\"8-Conclusion\"><a href=\"#8-Conclusion\" class=\"headerlink\" title=\"8. Conclusion\"></a>8. Conclusion</h1><p>본 논문에서는 TRPO를 컴퓨터 친화적?으로 알고리즘을 수정하고 다른 알고리즘들과의 성능비교를 통해 PPO가 좋다라는 것을 보여주고 있습니다.</p>\n","site":{"data":{}},"excerpt":"","more":"<center> <img src=\"https://www.dropbox.com/s/145van5kldfvvd5/Screen%20Shot%202018-07-18%20at%201.19.30%20AM.png?dl=1\" width=\"700\"> </center>\n\n<p>논문 저자 : John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov<br>논문 링크 : <a href=\"https://arxiv.org/pdf/1707.06347.pdf\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/pdf/1707.06347.pdf</a><br>Proceeding : ??<br>정리 : 장수영, 차금강</p>\n<hr>\n<h1 id=\"1-들어가며…\"><a href=\"#1-들어가며…\" class=\"headerlink\" title=\"1. 들어가며…\"></a>1. 들어가며…</h1><p>이전의 Trust Region Policy Optimization(TRPO)의 핵심적인 부분을 살펴보자면 다음과 같습니다.</p>\n<p>$$maximize_\\theta \\hat{E}<em>t [\\frac{\\pi</em>\\theta(a_t|s_t)} \\hat{A}_t ]-\\hat{E}<em>t [D</em>{KL}(\\pi_{\\theta old}(<em>|s) || \\pi_{\\theta(</em>|s)})]$$</p>\n<p>$$maximize_\\theta \\hat{E}<em>t [\\frac{\\pi</em>\\theta(a_t|s_t)}{\\pi_{\\theta old}(a_t|s_t)} \\hat{A}_t ]-\\hat{E}<em>t [D</em>{KL}(\\pi_{\\theta old}(<em>|s)||\\pi_{\\theta(</em>|s)})]$$</p>\n<p>위의 식을 살펴보면 KL-Divergence를 구하는 과정에서 굉장한 연산량이 필요합니다.<br>$$<br>D_{KL}(\\pi_{\\theta old}(<em>|s)||\\pi_\\theta(</em>|s)) = -\\int{\\pi_{\\theta old}(<em>|s)ln\\pi_\\theta(</em>|s)}+\\int{\\pi_\\theta(<em>|s)ln\\pi_{\\theta old}(</em>|s)}<br>$$<br>특히 위의 식에서 볼 수 있듯이 $ln()$을 계산하는데 있어 많은 계산량이 필요로 하며 매번 파라미터들을 업데이트할 때마다 이 복잡한 연산을 반복적으로 수행해야 합니다. TRPO에서 KL-Divergence의 역할을 쉽게 풀이하자면 복잡한 비선형 최적화에서 조금의 파라미터 변화에도 성능이 크게 변화할 수 있는데 이를 방지하며 기존의 성능을 잘 보존할 수 있게 하는 장치입니다. 꼭 TRPO만이 아니라 일반적인 Policy Gradient 방법에서도 Objective Function은 $ln()$으로 되어 있으며 이를 어떤 연산량이 적은 방법으로 쉽게 적용할 수 있을까에 대해 고민하는 것이 PPO의 핵심입니다.</p>\n<p>PPO 논문에서는 TRPO에서 제시한 surrogate function을 두 가지로 나누어서 접근하며 이를 서로, 그리고 TRPO와 비교 분석하고 있습니다. 본 논문에서 제시하고 있는 방법 두 가지 중 첫번째는 surrogate function을 매우 작은 값으로 clipping함으로써 gradient 자체를 강제적으로 적게 만들어주는 방법이며 두번째는 TRPO의 KL-Divergence의 크기에 따라 Adaptive한 파라미터를 적용하는 방법입니다.</p>\n<p><br><br></p>\n<h1 id=\"2-Introduction\"><a href=\"#2-Introduction\" class=\"headerlink\" title=\"2. Introduction\"></a>2. Introduction</h1><p><br></p>\n<h2 id=\"2-1-대표적인-방법들\"><a href=\"#2-1-대표적인-방법들\" class=\"headerlink\" title=\"2.1 대표적인 방법들\"></a>2.1 대표적인 방법들</h2><ul>\n<li>DQN<ul>\n<li>Discrete action space를 가지는 문제들에는 효과적으로 적용 가능하지만, continuous control에도 잘 작동하는지는 검증되지 않았습니다.</li>\n</ul>\n</li>\n<li>A3C - “Vanilla” policy gradient methods<ul>\n<li>Data efficiency와 robustness 측면이 좋지 않습니다.</li>\n</ul>\n</li>\n<li>TRPO<ul>\n<li>간단히 말해 복잡합니다.</li>\n<li>또한 noise(ex. dropout)나 parameter sharing(policy와 value function 간 혹은 auxiliary tasks와의)를 포함하는 architecture와의 호환성 없습니다.</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-2-대표적인-방법들-대비-개선사항\"><a href=\"#2-2-대표적인-방법들-대비-개선사항\" class=\"headerlink\" title=\"2.2 대표적인 방법들 대비 개선사항\"></a>2.2 대표적인 방법들 대비 개선사항</h2><ul>\n<li>Scalability<ul>\n<li>large models and parallel implementations</li>\n</ul>\n</li>\n<li>Data Efficiency</li>\n<li>Robustness<ul>\n<li>hyperparameter tuning없이 다양한 문제들에 적용되어 해결</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-3-제한하는-알고리즘\"><a href=\"#2-3-제한하는-알고리즘\" class=\"headerlink\" title=\"2.3 제한하는 알고리즘\"></a>2.3 제한하는 알고리즘</h2><p>이 논문에는 Clipped probability ratios를 포함하는 objective function 제안하였습니다.</p>\n<ul>\n<li>TRPO의 data efficiency와 robustness를 유지하면서도 first-order approximation만 사용합니다.</li>\n<li>Policy 성능에 대한 lower bound를 제공합니다.</li>\n</ul>\n<p>따라서 Policy로부터의 data sampling과 sampled data를 이용한 최적화를 번갈아가면서 수행합니다.</p>\n<p><br></p>\n<h2 id=\"2-4-실험-결과\"><a href=\"#2-4-실험-결과\" class=\"headerlink\" title=\"2.4 실험 결과\"></a>2.4 실험 결과</h2><p>다양한 버전의 surrogate objectives 중에는 clipped probability ratio가 가장 성능이 좋았습니다.</p>\n<ul>\n<li>Continuous control tasks에서 기존 알고리즘 대비 성능이 좋습니다.</li>\n<li>Atari에서는 A2C 대비 sampling efficiency 측면에서는 성능이 월등히 좋으며, ACER 대비 훨씬 간단하지만 성능은 비슷합니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"3-Backgroud-Policy-Optimization\"><a href=\"#3-Backgroud-Policy-Optimization\" class=\"headerlink\" title=\"3. Backgroud: Policy Optimization\"></a>3. Backgroud: Policy Optimization</h1><p><br></p>\n<h2 id=\"3-1-Policy-Gradient-Methods\"><a href=\"#3-1-Policy-Gradient-Methods\" class=\"headerlink\" title=\"3.1 Policy Gradient Methods\"></a>3.1 Policy Gradient Methods</h2><p>일반적인 PG Method들은 실험적으로 destructive large policy updates가 발생합니다.</p>\n<ul>\n<li>더 자세히 말하자면, PG Method가 수행하는 parameter space에서의 gradual update가 policy space에서는 큰 변화를 유발할 수 있다는 의미입니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-2-Trust-Region-Methods\"><a href=\"#3-2-Trust-Region-Methods\" class=\"headerlink\" title=\"3.2 Trust Region Methods\"></a>3.2 Trust Region Methods</h2><p>Policy update 크기에 대한 contraint하에 objective function(“surrogate” function)을 최대화하는 것이 목표입니다. 수식은 아래와 같습니다.</p>\n<p>$$$$<br>$$$$</p>\n<p>위의 수식은 contraint로 인해 excessive large policy update가 방지됩니다.</p>\n<p>TRPO에서는 constrained optimization problem을 풀기 위해서는 다음과 같은 방법들이 필요합니다.</p>\n<ol>\n<li>Fisher Information Matrix인 second-order derivative of KL divergence를 사용하거나,<ul>\n<li>여기서 second-order matrixes를 구하기 위해서는 많은 계산량 필요합니다.</li>\n</ul>\n</li>\n<li>Conjugate Gradient를 사용합니다. <ul>\n<li>Conjugate Gradient는 현하기가 어렵습니다.</li>\n</ul>\n</li>\n</ol>\n<p>$$$$</p>\n<p>원래 이론적으로 위의 수식과 같이 “contraint”가 아니라 objective에 “penalty”를 부여하는 형태입니다. 하지만 다양한 문제들(혹은 학습 중에 특성이 변하는 문제)에서 모두 잘 동작하는 single value $\\beta$를 찾는 것(robustness)이 어렵기 때문에, TRPO에서는 penalty대신 contraint를 취하는 방식을 택한 것입니다.</p>\n<p><br><br></p>\n<h1 id=\"4-Clipped-Surrogate-Objective\"><a href=\"#4-Clipped-Surrogate-Objective\" class=\"headerlink\" title=\"4. Clipped Surrogate Objective\"></a>4. Clipped Surrogate Objective</h1><p>이번 section에서는 TRPO의 surrogate obejctive function을 강제적으로 clipping하는 방법에 대하여 말합니다.</p>\n<p>먼저 기존의 TRPO의 surrogate function을 다음과 같이 표현합니다.<br>$$r_t(\\theta)=\\dfrac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta old}(a_t|s_t)}, \\, \\, r_t(\\theta old) = 1$$</p>\n<p>위의 수식을 이용하여 TRPO의 surrogate object를 최대화합니다.</p>\n<ul>\n<li>실험중..</li>\n</ul>\n<p>$$L^{CPI} (\\theta) = \\hat{E}_t [\\frac{a}{a}]$$<br>$$L^{CPI} (\\theta) = \\hat{E}_t [\\frac{\\pi}{a}]$$<br>$$L^{CPI} (\\theta) = \\hat{E}<em>t [\\frac{\\pi\\</em>{\\theta}}{a}]$$<br>$$L^{CPI} (\\theta) = \\hat{E}<em>t [\\frac{\\pi\\</em>{\\theta}}{a}]$$<br>$$L^{CPI} (\\theta) = \\hat{E}_t [\\frac{\\pi}{a}]$$<br>$$L^{CPI} (\\theta) = \\hat{E}<em>t [\\frac{\\pi</em>\\theta}{a}]$$<br>$$L^{CPI} (\\theta) = \\hat{E}<em>t [\\frac{\\pi</em>\\theta (a_t|s_t)}{\\pi_\\theta}]$$<br>$$L^{CPI} (\\theta) = \\hat{E}<em>t [\\frac{\\pi</em>\\theta (a_t \\vert s_t)}{\\pi_{\\theta old} (a_t \\vert s_t)} \\hat{A}_t] = \\hat{E}_t [r_t (\\theta\\hat{A}_t)]$$<br>$$L^{CPI}(\\theta) = \\hat{E}<em>t [\\frac{\\pi</em>\\theta(a_t|s_t)}{\\pi_{\\theta old} (a_t|s_t)}\\hat{A}_t]=\\hat{E}_t[r_t(\\theta)\\hat{A}_t]$$</p>\n<p>위의 수식을 그대로 사용한다면 excessively large policy update가 됩니다. 따라서 penalty를 이용하여 필요 이상의 policy update를 방지합니다. TRPO에서는 KL-Divergence를 이용하여 penalty를 적용하지만 PPO에서는 computation적으로 효율적인 penalty를 적용하고 excessively large policy update를 방지하기 위해 아래와 같은 clipping 기법을 사용합니다.</p>\n<p>$$L^{CLIP}(\\theta) = \\hat{E}_t [min(r_t(\\theta)\\hat{A}_t, clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_t)]$$</p>\n<p>위의 수식에서 $\\epsilon$은 hyperparameter로서 continuous control에서는 0.2일 때 성능이 가장 좋았으며, Atari Game에서는 0.1 x $\\alpha$ 값을 택함. ( $\\alpha$ 는 학습률로 1 에서 시작하여 학습이 진행됨에 따라 0 으로 감소)<br>• Clipped 와 unclipped objective 중 min 값을 택함으로써 LCLIP($\\theta$)는 unclipped objective에 대 한 lowerbound가 됨</p>\n<p>이를 논문에서는 두 가지의 그래프($\\hat{A}_t$의 부호에 따라)로 설명하고 있습니다.  $\\hat{A}_t$가 양수일 때는 Advantage가 현재보다 높다라는 뜻이며 파라미터를 +의 방향으로 업데이트 하여야 합니다. $\\hat{A}_t$가 음수일 때는 Advantage가 현재보다 좋지 않다라는 뜻이며 그의 반대 방향으로 업데이트 하여야 합니다. $r_t(\\theta)$은 확률을 뜻하는 두개의 함수를 분자 분모로 가지고 있으며 분수로 구성되어 있기 때문에 무조건 양수로 이루어져 있으며 Advantage function인 $\\hat{A}_t$와 곱해져 Objective function인 $L^{CLIP}$은 Advantage function과 방향이 같아집니다.</p>\n<p><br><br></p>\n<h1 id=\"5-Adaptive-KL-Penalty-Coefficient\"><a href=\"#5-Adaptive-KL-Penalty-Coefficient\" class=\"headerlink\" title=\"5. Adaptive KL Penalty Coefficient\"></a>5. Adaptive KL Penalty Coefficient</h1><p>2절에서 설명한 Clipped Surrogate Objective 방법과 달리 기존의 TRPO의 방법에 Adaptive한 파라미터를 적용한 방법 또한 제시하고 설명하고 있습니다.</p>\n<p>기존의 TRPO의 Objective function에 $\\beta$를 적용하여 다음과 같이 $\\beta$를 Adaptive하게 조절하고 있습니다.<br>$$<br>L^{KLPEN}(\\theta) = \\hat{E}<em>t [\\dfrac{\\pi</em>\\theta(a_t|s_t)}{\\pi_{\\theta old}(a_t|s_t)}\\hat{A}<em>t-\\beta D</em>{KL}(\\pi_{\\theta old}(<em>|s)||\\pi_\\theta(</em>|s))]<br>$$<br>$d = \\hat{E}<em>t[ D</em>{KL}(\\pi_{\\theta old}(<em>|s)||\\pi_\\theta(</em>|s))]$, </p>\n<p>$d&lt;d_{targ}/1.5, \\beta \\xleftarrow{}\\beta/2$ </p>\n<p>$ d&gt;d_{targ}, \\beta \\xleftarrow{}\\beta\\times2$</p>\n<p>$\\beta$를 조절하는 방법을 해석하자면 다음과 같습니다. 만약 $\\theta old$와 $\\theta$간의 파라미터 차이가 크다면 penalty를 강하게 부여해 적은 변화를 야기하며, 파라미터 차이가 적다면 penalty를 완화시켜 주어 더 큰 변화를 야기하는 것을 위의 수식으로 표현할 수 있습니다.</p>\n<p><br><br></p>\n<h1 id=\"6-Algorithm\"><a href=\"#6-Algorithm\" class=\"headerlink\" title=\"6. Algorithm\"></a>6. Algorithm</h1><p>2절, 3절의 내용을 보면 policy만을 어떻게 업데이트 하는지에 대해 설명하고 있습니다. 4절에서는 다른 알고리즘과 같이 value, entropy-exploration과 합쳐 어떻게 통합적으로 업데이트 하는지에 대해 설명하고 있습니다.</p>\n<p>본 논문에서 설명하기를 state-value function $V(s)$는 <a href=\"https://arxiv.org/abs/1506.02438\" target=\"_blank\" rel=\"noopener\">generalized advantage estimation</a>를 선택하여 사용하고 있으며 일반적인 방식인 Mean-square error를 통해 업데이트하고 있습니다. 이에 대한 Objective function은 본 논문 내에서 $L^{VF}$로 표현하고 있습니다.</p>\n<p>또한 일반적인 policy gradient 방법에서 <a href=\"&quot;Asynchronous methods for deep reinforcement learning&quot;\">exploration</a>의 기능을 추가하기 위해 다음과 같은 항, $S[\\pi_\\theta(s_t)]=\\pi_\\theta(a|s_t)log\\pi_\\theta(a|s_t)$, 을 추가합니다. </p>\n<p>2절, 3절에서 제시한 Objective function을 $L^{PPO}$라고 표현한다면($L^{PPO}$는 $L^{KLPEN}$, $L^{CLIP}$일 수 있습니다.) PPO에서 제시하는 통합적으로 최대화해야하는 함수는 다음과 같이 표현할 수 있습니다.<br>$$<br>L^{PPO+VF+S}(\\theta) = \\hat{E}_t[L^{PPO}(\\theta)-c_1L^{VF}(\\theta)+c_2S[\\pi_\\theta(s_t)]<br>$$<br>위의 Objective function을 최대화하면 Reinforcement Learning을 통한 policy 학습, state-value function의 학습, exploration을 할 수 있습니다.</p>\n<p><br><br></p>\n<h1 id=\"7-Experimnet\"><a href=\"#7-Experimnet\" class=\"headerlink\" title=\"7. Experimnet\"></a>7. Experimnet</h1><p><br></p>\n<h2 id=\"7-1-Surrogate-Objectives의-비교\"><a href=\"#7-1-Surrogate-Objectives의-비교\" class=\"headerlink\" title=\"7.1 Surrogate Objectives의 비교\"></a>7.1 Surrogate Objectives의 비교</h2><p>5.1절에서는 3가지 Surrogate Objectives를 비교분석 하고 있습니다.</p>\n<ul>\n<li>No clipping or penalty: $L_t(\\theta) = r_t(\\theta)\\hat{A}_t$</li>\n<li>Clipping: $L_t(\\theta) = min(r_t(\\theta)\\hat{A}_t, clip(r_t(\\theta), 1-\\epsilon,1+\\epsilon)\\hat{A}_t)$</li>\n<li>KL penalty(fixed or adaptive): $L_t(\\theta)=r_t(\\theta)\\hat{A}<em>t-\\beta KL[\\pi</em>{\\theta old}, \\pi_\\theta]$</li>\n</ul>\n<p>그리고 환경은 MuJoCo 엔진을 사용하며 OpenAI Gym 환경에서 실험을 진행하였습니다. 그리고 네트워크들은 2개의 hidden layer를 가지며 각각 64개의 unit들을 가지고 있습니다. 그리고 tanh를 활성화 함수로써 사용하고 있으며 네트워크는 Gaussian distribution을 출력하는 것으로 구성되어 있습니다. 그리고 Policy Network와 Value Network는 파라미터들을 공유하고 있지 않으며 위에서 설명한 Exploration을 위한 entropy는 사용하지 않습니다.</p>\n<p>7개의 환경에서 실험하였으며 각 환경들은 HalfCheetah, Hopper, InvertedDoublePendulum, InvertedPerdulum, Reacher, Swimmer, Walker2d를 사용하였으며 모든 환경들은 v1 버전을 사용하였습니다.</p>\n<table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th>avg. normalized score</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>No clipping or penalty</td>\n<td>-0.39</td>\n</tr>\n<tr>\n<td>Clipping, $\\epsilon=0.1$</td>\n<td>0.76</td>\n</tr>\n<tr>\n<td>Clipping, $\\epsilon=0.2$</td>\n<td>0.82</td>\n</tr>\n<tr>\n<td>Clipping, $\\epsilon=0.3$</td>\n<td>0.70</td>\n</tr>\n<tr>\n<td>Adaptive KL $d_{targ}=0.003$</td>\n<td>0.68</td>\n</tr>\n<tr>\n<td>Adaptive KL $d_{targ} = 0.001$</td>\n<td>0.74</td>\n</tr>\n<tr>\n<td>Adaptive KL  $d_{targ} = 0.03$</td>\n<td>0.71</td>\n</tr>\n<tr>\n<td>Fixed KL, $\\beta=0.3$</td>\n<td>0.62</td>\n</tr>\n<tr>\n<td>Fixed KL, $\\beta=1$</td>\n<td>0.71</td>\n</tr>\n<tr>\n<td>Fixed KL, $\\beta=3$</td>\n<td>0.72</td>\n</tr>\n<tr>\n<td>Fixed KL, $\\beta=10$</td>\n<td>0.69</td>\n</tr>\n</tbody>\n</table>\n<p>위의 표는 실험 결과입니다. 세 가지 Surrogate Objective에 대해 각 파라미터들을 테이블에 표기된 방식대로 변화하며 실험하였습니다. </p>\n<p><br></p>\n<h2 id=\"7-2-Comparison-to-other-Algorithms-in-the-Continuous-Domain\"><a href=\"#7-2-Comparison-to-other-Algorithms-in-the-Continuous-Domain\" class=\"headerlink\" title=\"7.2 Comparison to other Algorithms in the Continuous Domain\"></a>7.2 Comparison to other Algorithms in the Continuous Domain</h2><p>5.2절에서는 Clipping 버전의 PPO와 다른 알고리즘들 간의 비교를 연속적인 Action을 가지는 환경에서 실험을 한 결과를 보여주고 있습니다. 다른 알고리즘은 TRPO, cross-entropy method, vanilla policy gradient with adaptive stepsize, A2C, A2C with trust region를 사용하였습니다. 환경과 사용한 네트워크의 구성은 5.1절에서 사용한 것과 동일합니다.</p>\n<p>여타 다른 알고리즘보다 좋은 성능을 보이는 것을 알 수 있습니다.</p>\n<p><br></p>\n<h2 id=\"7-3-Showcase-in-the-Continuous-Domain-Humanoid-Running-and-Steering\"><a href=\"#7-3-Showcase-in-the-Continuous-Domain-Humanoid-Running-and-Steering\" class=\"headerlink\" title=\"7.3 Showcase in the Continuous Domain: Humanoid Running and Steering\"></a>7.3 Showcase in the Continuous Domain: Humanoid Running and Steering</h2><p>5.3절에서는 Humanoid-v0, HumanoidFlagrun-v0, HumanoidFlagrunHarder-v0의 환경에서 Roboschool엔진을 사용하여 실험하였습니다. Humanoid-v0는 단순히 앞으로 걸어나가는 환경, HumanoidFlagrun-v0은 200스텝마다 혹은 목적지에 도착할 때 마다 위치가 바뀌는 목적지에 걸어서 도달하는 환경입니다. HumanoidFlagrunHarder-v0 환경은 환경이 초기화될 때 마다 특정 경계안에 Humanoid가 위치하게 되며 특정 영역 밖으로 걸어나가는 것을 수행하는 환경입니다.</p>\n<p><br></p>\n<h2 id=\"7-4-Comparison-to-Other-Algorithms-on-the-Atari-Domain\"><a href=\"#7-4-Comparison-to-Other-Algorithms-on-the-Atari-Domain\" class=\"headerlink\" title=\"7.4 Comparison to Other Algorithms on the Atari Domain\"></a>7.4 Comparison to Other Algorithms on the Atari Domain</h2><p>5.4절에서는 Atari Domain에서 PPO, A2C, ACER(Actor Critic with Experience Replay)의 3가지 알고리즘을 비교합니다.</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>A2C</th>\n<th>ACER</th>\n<th>PPO</th>\n<th>Tie</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>(1) avg. episode reward over all of training</td>\n<td>1</td>\n<td>18</td>\n<td>30</td>\n<td>0</td>\n</tr>\n<tr>\n<td>(2) avg. episode reward over last 100 episodes</td>\n<td>1</td>\n<td>28</td>\n<td>19</td>\n<td>1</td>\n</tr>\n</tbody>\n</table>\n<p>모든 학습하는 도중에는 PPO가 ACER보다 좋은 성능을 나타내지만 마지막 100 에피소드만을 비교했을 때는 PPO보다 ACER이 더 좋은 성능을 나타냅니다. 이는 PPO가 더 빨리 최종 능력에 도달하지만 ACER이 가진 잠재력이 더 높다는 것을 나타냅니다.</p>\n<p><br><br></p>\n<h1 id=\"8-Conclusion\"><a href=\"#8-Conclusion\" class=\"headerlink\" title=\"8. Conclusion\"></a>8. Conclusion</h1><p>본 논문에서는 TRPO를 컴퓨터 친화적?으로 알고리즘을 수정하고 다른 알고리즘들과의 성능비교를 통해 PPO가 좋다라는 것을 보여주고 있습니다.</p>\n"},{"title":"Trust Region Policy Optimization","date":"2018-06-23T07:53:12.000Z","author":"공민서, 김동민","subtitle":"피지여행 5번째 논문","_content":"\n<center> <img src=\"https://www.dropbox.com/s/o7cjcn0e17mpizr/Screen%20Shot%202018-07-18%20at%201.14.22%20AM.png?dl=1\" width=\"700\"> </center>\n\n논문 저자 : John Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, Pieter Abbeel\n논문 링크 : https://arxiv.org/pdf/1502.05477.pdf\nProceeding : International Conference on Machine Learning (ICML) 2015\n정리 : 공민서, 김동민\n\n---\n\n# 1. 들어가며...\n\nTrust region policy optimization (TRPO)는 상당히 우수한 성능을 보여주는 policy gradient 기법으로 알려져 있습니다. 높은 차원의 action space를 가진 robot locomotion부터 action은 적지만 화면을 그대로 처리하여 플레이하기 때문에 control parameter가 매우 많은 Atari game까지 각 application에 세부적으로 hyperparameter들을 특화시키지 않아도 두루두루 좋은 성능을 나타내기 때문에 일반화성능이 매우 좋은 기법입니다. 이 TRPO에 대해서 알아보겠습니다.\n\n※TRPO를 매우 재밌게 설명한 Crazymuse AI의 [Youtube video](https://www.youtube.com/watch?v=CKaN5PgkSBc&t=90s)에서 일부 그림을 차용했습니다. 이 비디오를 시청하시는 것을 강력하게 추천합니다!\n\n<br>\n## 1.1 TRPO 흐름 잡기\n\nTRPO 논문은 많은 수식이 등장하여 이 수식들을 따라가다보면 큰 그림을 놓칠 수도 있습니다. 세부적인 내용을 살펴보기 전에 기존 연구에서 출발해서 TRPO로 어떻게 발전해나가는지 간략하게 살펴보겠습니다.\n\n### 1.1.1 Original Problem\n\n$$\\max\\_\\pi \\eta(\\pi)$$\n\n모든 강화학습이 그렇듯이 expected discounted reward를 최대화하는 policy를 찾는 문제로부터 출발합니다.\n\n### 1.1.2 Conservative policy iteration\n\n$$\\max L\\_\\pi(\\tilde\\pi) = \\eta(\\pi) + \\sum\\_s \\rho_\\pi(s)\\sum\\_a\\tilde\\pi(a\\vert s)A\\_\\pi(s,a)$$\n\n$\\eta(\\pi)$를 바로 최대화하는 것은 많은 경우 어렵습니다. $\\eta(\\pi)$의 성능향상을 보장하면서 policy를 update하는 conservative policy iteration 기법이 [Kakade와 Langford](http://www.cs.cmu.edu/~./jcl/papers/aoarl/Final.pdf)에 의하여 제안되었습니다. 이 기법을 이용하면 policy update가 성능을 향상시키는지는 못하더라도 최소한 성능을 악화시키지는 않는다는 것이 이론적으로 보장됩니다.\n\n### 1.1.3 Theorem 1 of TRPO\n\n$$\\max L\\_{\\pi\\_\\mathrm{old} }\\left(\\pi\\_\\mathrm{new}\\right) - \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2,\\quad\\left(\\alpha=D\\_\\mathrm{TV}^\\max\\left(\\pi\\_\\mathrm{old},\\pi\\_\\mathrm{new}\\right)\\right)$$\n\n기존의 conservative policy iteration은 과거 policy와 새로운 policy를 섞어서 사용해서 실용적이지 않다는 단점이 있었는데 이것을 보완하여 온전히 새로운 policy만으로 update할 수 있는 기법을 제안합니다. \n\n### 1.1.4 KL divergence version of Theorem 1\n\n$$\\max L\\_{\\pi}\\left(\\tilde\\pi\\right) - C\\cdot D_\\mathrm{KL}^\\max\\left(\\pi,\\tilde\\pi\\right),\\quad\\left(C = \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\right)$$\n\ndistance metric을 KL divergence로 바꿀 수 있습니다.\n\n### 1.1.5 Using parameterized policy\n\n$$\\max\\_\\theta L\\_{\\theta\\_\\mathrm{old} }(\\theta) - C\\cdot D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right)$$\n\n최적화문제를 더욱 편리하게 풀 수 있도록 낮은 dimension을 가지는 parameter들로 parameterized된 policy를 사용할 수 있습니다.\n\n### 1.1.6 Trust region constraint\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ }&D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right) \\leq \\delta\n\\end{align}$$\n\n1.1.5까지는 아직 conservative policy iteration을 약간 변형시킨 것입니다. policy를 update할 때 지나치게 많이 변하는 것을 방지하기 위하여 [trust region](https://en.wikipedia.org/wiki/Trust_region)을 constraint로 설정할 수 있습니다. 이 아이디어로 인해서 TRPO라는 명칭을 가지게 됩니다.\n\n### 1.1.7 Heuristic approximation\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ }&\\overline{D}\\_\\mathrm{KL}^{\\rho\\_\\mathrm{old} }\\left(\\theta\\_\\mathrm{old}, \\theta\\right) \\leq \\delta\n\\end{align}$$\n\n사실 1.1.6의 constraint는 모든 state에 대해서 성립해야 하기 때문에 문제를 매우 어렵게 만듭니다. 이것을 좀 더 다루기 쉽게 state distribution에 대한 평균을 취한 것으로 변형합니다.\n\n### 1.1.8 Monte Carlo simulation\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi\\_\\theta(a\\vert s)}{q(a\\vert s)}Q\\_{\\theta\\_\\mathrm{old} }(s,a)\\right] \\\\\\\\\n\\mathrm{s.t.\\ }&E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} }}\\left[D\\_\\mathrm{KL}\\left(\\pi\\_{\\theta\\_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi\\_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta\n\\end{align}$$\n\nSampling을 통한 계산이 가능하도록 식을 다시 표현할 수 있습니다. \n\n### 1.1.9 Efficiently solving TRPO\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &\\nabla\\_\\theta \\left. L\\_{\\theta\\_\\mathrm{old} }(\\theta)\\right\\vert \\_{\\theta=\\theta\\_\\mathrm{old} } \\left(\\theta - \\theta\\_\\mathrm{old}\\right) \\\\\\\\\n\\mathrm{s.t.\\ }&\\frac{1}{2}\\left(\\theta\\_\\mathrm{old} - \\theta \\right)^T A\\left(\\theta\\_\\mathrm{old}\\right)\\left(\\theta\\_\\mathrm{old} - \\theta \\right) \\leq \\delta \\\\\\\\\n&A\\_{ij} = \\frac{\\partial}{\\partial \\theta\\_i}\\frac{\\partial}{\\partial \\theta\\_j}\\overline{D}\\_\\mathrm{KL}^{\\rho\\_\\mathrm{old} }\\left(\\theta\\_\\mathrm{old}, \\theta\\right)\n\\end{align}$$\n\n문제를 효율적으로 풀기 위하여 approximation을 적용할 수 있습니다. objective function은 first order approximation, constraint는 quadratic approximation을 취하면 효율적으로 문제를 풀 수 있는 형태로 바뀌는데 이것을 natural gradient로 풀 수도 있고 conjugate gradient로 풀 수도 있습니다.\n\nTRPO는 이렇게 다양한 방법으로 문제를 변형한 것입니다! 이제 좀 더 자세히 살펴보겠습니다.\n\n<br><br>\n# 2. Preliminaries\n\n다음과 같은 파라미터를 가지는 infinite-horizon discounted Markov decision process (MDP)를 고려합니다. \n\n* $\\mathcal{S}$: finite set of states\n* $\\\\mathcal{A}$: finite set of actions\n* $P: \\mathcal{S}\\times\\mathcal{A}\\rightarrow\\mathbb{R}$: transition probability distribution\n* $r: \\mathcal{S}\\rightarrow\\mathbb{R}$: reward function\n* $\\rho\\_0:\\mathcal{S}\\rightarrow\\mathbb{R}$: distribution of the initial state $s\\_0$\n* $\\gamma\\in(0,1)$: discount factor\n* $\\eta(\\pi)=E\\_{s\\_0,a_0,\\ldots}\\left[\\sum\\_{t=0}^\\infty\\gamma^t r\\left(s\\_t\\right)\\right]$, where $s\\_0\\sim\\rho\\_0\\left(s\\_0\\right), a\\_t\\sim\\pi\\left(\\left.a\\_t \\right\\vert s\\_t\\right), s\\_{t+1}\\sim P\\left(\\left.s\\_{t+1}\\right\\vert s\\_t,a\\_t\\right)$\n* $Q\\_\\pi\\left(s\\_t,a\\_t\\right)=E\\_{ { \\color{red}{s\\_{t+1}} },a\\_{t+1},\\ldots}\\left[\\sum\\_{l=0}^\\infty \\gamma^l r\\left(s\\_{t+l}\\right)\\right]$: action value function\n* $V\\_\\pi\\left(s\\_t\\right)=E\\_{ { \\color{red}{a\\_t} }, s\\_{t+1},a\\_{t+1},\\ldots}\\left[\\sum\\_{l=0}^\\infty \\gamma^l r\\left(s\\_{t+l}\\right)\\right]$: value function (action value function과 expectation의 첨자가 다른 부분을 유의하세요.)\n* $A\\_\\pi(s,a) = Q\\_\\pi(s,a) - V\\_\\pi(s)$: advantage function\n\n<br>\n## 2.1 Useful identity [Kakade & Langford 2002]\n\n우리의 목표는 $\\eta\\(\\pi\\)$가 최대화되도록 만드는 것입니다. 하지만 $\\pi$의 변화에 따라 $\\eta$가 어떻게 변하는지 알아내는 것도 쉽지 않습니다. $\\pi$는 기존의 policy이고 $\\tilde\\pi$는 새로운 policy를 나타낸다고 할 때 $\\eta$와 policy update 사이에 다음과 같은 관계가 있다는 것이 밝혀졌습니다.\n\n**Lemma 1.** $\\eta\\left(\\tilde\\pi\\right) = \\eta(\\pi) + E\\_{s\\_{0},a\\_{0},\\ldots\\sim\\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t A\\_\\pi\\left(s\\_t,a\\_t\\right)\\right]$\n\n*Proof.* $A\\_\\pi(s,a)=E\\_{s'\\sim P\\left(s'\\vert s,a\\right)}\\left[r(s)+\\gamma V\\_\\pi\\left(s'\\right)-V\\_\\pi(s)\\right]$로 다시 표현할 수 있습니다. 표기의 편의를 위하여 $\\tau:=\\left(s\\_0,a\\_0,s\\_1,a\\_1,\\ldots\\right)$를 정의하겠습니다. 다음과 같은 수식 전개가 가능합니다.\n$$\n\\begin{align}\n&E\\_{\\tau\\vert \\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t A\\_\\pi\\left(s\\_t,a\\_t\\right)\\right] \\\\\\\\\n &= E\\_{\\tau\\vert \\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t \\left(r(s\\_t)+\\gamma V\\_\\pi\\left(s\\_{t+1}\\right)-V\\_\\pi(s\\_t)\\right)\\right] \\\\\\\\\n&= E\\_{\\tau\\vert \\tilde\\pi}\\left[\\left(\\sum\\_{t=0}^\\infty\\gamma^t r(s\\_t)\\right)+\\gamma V\\_\\pi\\left(s\\_{1}\\right)-V\\_\\pi(s\\_0)+\\gamma^2 V\\_\\pi\\left(s\\_{2}\\right)-\\gamma V\\_\\pi(s\\_1)+\\gamma^3 V\\_\\pi\\left(s\\_{3}\\right)-\\gamma^2 V\\_\\pi(s\\_2)+\\cdots\\right] \\\\\\\\\n&= E\\_{\\tau\\vert \\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t r(s\\_t)+\\color{red}{\\gamma V\\_\\pi\\left(s\\_{1}\\right)}-V\\_\\pi(s\\_0)+\\color{red}{\\gamma^2 V\\_\\pi\\left(s\\_{2}\\right)}-\\color{red}{\\gamma V\\_\\pi(s\\_1)}+\\cdots\\right] \\\\\\\\\n&= E\\_{\\tau\\vert \\tilde\\pi}\\left[-V\\_\\pi(s\\_0)+\\sum\\_{t=0}^\\infty\\gamma^t r(s\\_t)\\right] \\\\\\\\\n&\\overset{\\underset{\\mathrm{(a)} }{} }{=} -E\\_{\\color{red}{s\\_0}}\\left[V\\_\\pi(s\\_0)\\right]+E\\_{\\tau\\vert \\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t r(s\\_t)\\right] \\\\\\\\\n&=-\\eta(\\pi) + \\eta\\left(\\tilde\\pi\\right)\\\\\\\\\n&\\quad\\therefore \\eta\\left(\\tilde\\pi\\right) = \\eta(\\pi) + E\\_{s\\_{0},a\\_{0},\\ldots\\sim\\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t A\\_\\pi\\left(s\\_t,a\\_t\\right)\\right]\n\\end{align}\n$$\n위의 전개의 (a) 부분은 $V\\_\\pi\\left(s\\_0\\right)=E\\_{a\\_0,a\\_1,s\\_1,\\ldots}\\left[\\sum\\_{t=0}^\\infty\\gamma^l r\\left(s\\_l\\right)\\right]$이므로 $\\tau$ 중 많은 부분들이 이미 expectation이 취해진 값이므로 무시됩니다. 오직 $s\\_0$만 expectation이 취해지지 않았기 때문에 남아있습니다.\n\n**Lemma 1**은 새로운 policy와 기존의 policy 사이의 관계를 규정합니다. 다음과 같은 식을 정의하고 이것을 이용해서 **Lemma 1**을 변형시켜 봅시다.\n\n* $\\rho\\_\\pi(s) = P\\left(s\\_0=s\\right) + \\gamma P\\left(s\\_1=s\\right)\\ + \\gamma^2 P\\left(s\\_2=s\\right) + \\cdots $: (unnormalized) discounted visitation frequencies\n\n\n\n$$\n\\begin{align}\n\\eta\\left(\\tilde\\pi\\right) &= \\eta(\\pi) + \\sum\\_{t=0}^\\infty\\sum\\_{s}P\\left(\\left.s\\_t=s\\right\\vert \\tilde\\pi\\right)\\sum\\_a\\tilde\\pi(a\\vert s)\\gamma^t A\\_\\pi(s,a) \\\\\\\\\n&= \\eta(\\pi) + \\sum\\_{s}{\\color{red}{\\sum\\_{t=0}^\\infty \\gamma^t P\\left(\\left.s\\_t=s\\right\\vert \\tilde\\pi\\right)} } \\sum\\_a\\tilde\\pi(a\\vert s) A\\_\\pi(s,a) \\\\\\\\\n&= \\eta(\\pi) + \\sum\\_{s}{\\color{red}{\\rho\\_\\tilde\\pi(s)} }\\sum\\_a\\tilde\\pi(a\\vert s) A\\_\\pi(s,a) \\\\\\\\\n\\end{align}\n$$\n\n이 수식의 의미가 무엇일까요? 만약 $\\sum\\_a\\tilde\\pi(a\\vert s) A\\_\\pi(s,a) \\geq 0$이라면 $\\eta(\\tilde\\pi)$는 항상 $\\eta(\\pi)$보다 큽니다. 즉, policy를 업데이트함으로써 항상 개선됩니다. 즉, 이 수식을 통해서 항상 더 좋은 성능을 내는 policy update가 가져야 할 특징을 알 수 있습니다. 다음과 같은 deterministic policy가 있다고 합시다.\n\n$$\n\\tilde\\pi(s) = \\arg\\max_a A\\_\\pi(s,a)\n$$\n\n이 policy는 적어도 하나의 state-action pair에서 0보다 큰 값을 가지는 advantage가 있고 그 때의 확률이 0이 아니라면 항상 성능을 개선시킵니다. 어려운 점은 policy를 바꾸면 $\\rho$도 바뀐다는 점입니다. \n\n다음 그림을 봅시다. Starting Point에서 여러가지 경로를 거쳐서 Destination으로 갈 수 있습니다. 다른 policy를 이용하는 것은 다른 경로를 이용한 것입니다.\n\n[![policy_change](../../../../img/policy_change.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=16s)\n\n이 때 다른 policy를 이용함에 따라 state visitation frequency도 변하게 됩니다.\n\n[![state_visitation_change](../../../../img/state_visitation_change.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=36s)\n\n이로 인해서 policy를 최적화하는 것은 어려워집니다. 그래서 이러한 변화를 무시하는 다음과 같은 approximation을 취합니다.\n\n$$\n\\begin{align}\nL_\\pi\\left(\\tilde\\pi\\right) &= \\eta(\\pi) + \\sum\\_{s}{\\color{red}{\\rho\\_\\pi(s)} } \\sum\\_a\\tilde\\pi(a\\vert s) A\\_\\pi(s,a)\n\\end{align}\n$$\n\npolicy가 바뀌었음에도 이전의 state distribution을 계속 이용하는 것입니다. 이것은 policy의 변화가 크지 않다면 어느 정도 허용될 수 있을 것입니다. 그렇지만 얼마나 많은 변화가 허용될까요? 이것을 정하기 위해서 이용하는 것이 trust region입니다.\n\n<br>\n## 2.2 Conservative Policy Iteration\n\npolicy의 변화를 다루기 용이하게 하기 위해서 policy를 다음과 같이 파라미터를 이용해서 표현합시다.\n\n* $\\pi\\_\\theta(a\\vert s)$: parameterized policy\n\n$\\pi\\_\\theta$는 $\\theta$에 대하여 미분가능한 함수입니다. $L\\_\\pi\\left(\\tilde\\pi\\right)$을 $\\theta\\_0$에서 $\\eta(\\pi)$에 대한 first order approximation이라고 하면 다음 식이 성립합니다.\n\n$$\n\\begin{align}\nL\\_{\\pi\\_{\\theta\\_0} }\\left(\\pi\\_{\\theta_0}\\right) &= \\eta\\left(\\pi\\_{\\theta\\_0}\\right) \\\\\\\\\n\\nabla\\_\\theta \\left.L\\_{\\pi\\_{\\theta\\_0} }\\left(\\pi\\_{\\theta\\_0}\\right)\\right\\vert \\_{\\theta=\\theta\\_0} &= \\nabla\\_\\theta\\left.\\eta(\\pi\\_{\\theta\\_0})\\right\\vert \\_{\\theta=\\theta\\_0}\n\\end{align}\n$$\n\n이것의 의미는 $\\pi\\_{\\theta\\_0}$가 매우 작게 변한다면 $L\\_{\\pi\\_{\\theta\\_0} }$를 개선시키는 것이 $\\eta$를 개선시키는 것이라는 것입니다. 그러나 지금까지의 설명만으로는 $\\pi\\_{\\theta\\_0}$를 얼마나 작게 변화시켜야 할지에 대해서는 알 수 없습니다.\n\nKakade & Langford가 2002년 발표한 논문에서도 이것에 대해서 고민했습니다. 그 논문에서 *conservative policy iteration*이라는 기법을 제안합니다. 그 논문의 contribution은 다음과 같습니다.\n\n* $\\eta$ 개선의 lower bound를 제공\n* 기존의 policy를 $\\pi\\_\\mathrm{old}$라고 하고 $\\pi'$를 $\\pi'=\\arg\\max\\_\\{\\pi'}L\\_{\\pi\\_\\mathrm{old} }\\left(\\pi'\\right)$과 같이 정의할 때, 새로운 mixture policy $\\pi\\_\\mathrm{new}$를 다음과 같이 제안\n$$\n\\pi\\_\\mathrm{new}(a\\vert s) = (1-\\alpha)\\pi\\_\\mathrm{old}(a\\vert s) + \\alpha \\pi'(a\\vert s)\n$$\n\n그림으로 표현하면 다음과 같습니다.\n\n[![mixure_policy](../../../../img/mixure_policy.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=2m46s)\n\n* 다음과 같은 lower bound를 정의\n$$\n\\eta\\left(\\pi\\_\\mathrm{new}\\right) \\geq L\\_{\\pi\\_{\\theta\\_\\mathrm{old} }}\\left(\\pi\\_\\mathrm{new}\\right) - \\frac{2\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2,\\\\\\\\\n\\mathrm{where\\ }\\epsilon = \\max_s\\left\\vert E\\_{a\\sim\\pi'(a\\vert s)}\\left[A\\_\\pi(s,a)\\right]\\right\\vert \n$$\n\n하지만 mixture된 policy라는 것은 실용적이지 않습니다. \n\n<br><br>\n# 3. Monotonic Improvement Guarantee for General Stochastic Policies\n\n이전 장에서 설명한 lower bound는 오직 mixture policy에 대해서만 성립하고 더 많이 사용되는 stochastic policy에는 적용되지 않습니다. 따라서 stochastic policy를 이용할 수 있도록 개선할 필요가 있습니다. 아래와 같이 기존 수식에서 두 가지를 바꿈으로써 이것이 가능합니다.\n\n* $\\alpha\\rightarrow$ distance measure between $\\pi$ and $\\tilde\\pi$\n* constant $\\epsilon\\rightarrow\\max\\_{s,a}\\left\\vert A\\_\\pi(s,a)\\right\\vert $\n\n여기서 distance measure로 total variation divergence를 이용합니다. discrete porbability distribution $p$와 $q$에 대하여 다음과 같이 정의됩니다.\n$$\nD\\_\\mathrm{TV}(p\\parallel q) = \\frac{1}{2}\\sum\\_i\\left\\vert p\\_i - q\\_i\\right\\vert \n$$\n\n이것을 이용하여 $D\\_\\mathrm{TV}^\\max$를 다음과 같이 정의합니다.\n$$\nD\\_\\mathrm{TV}^\\max(\\pi\\parallel \\tilde\\pi) = \\max_s D\\_\\mathrm{TV}\\left(\\pi(\\cdot\\vert s)\\parallel\\tilde\\pi(\\cdot\\vert s)\\right)\n$$\n\n그림으로 표현하면 다음과 같습니다.\n\n[![tvd](../../../../img/tvd.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=3m15s)\n\n이것을 이용하여 다음과 같은 관계식을 얻을 수 있습니다.\n\n**Theorem 1.** Let $\\alpha=D\\_\\mathrm{TV}^\\max(\\pi\\parallel \\tilde\\pi)$. Then the following bound holds:\n$$\n\\begin{align}\n\\eta\\left(\\pi\\_\\mathrm{new}\\right) &\\geq L\\_{\\pi\\_\\mathrm{old} }\\left(\\pi\\_\\mathrm{new}\\right) - \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2, \\\\\\\\\n\\mathrm{where\\ } \\epsilon&= \\max\\_{s,a}\\left\\vert A\\_\\pi(s,a)\\right\\vert \n\\end{align}\n$$\n<!--*Proof.* TBD.-->\n\n또다른 distance metric으로 아래 그림과 같은 KL divergence가 있습니다.\n(그런데 왜 하필 KL divergence로 바꿀까요? 논문의 뒤쪽에서 계산효율을 위해서 conjugate gradient method를 이용하는데 이를 위해서 바꾼게 아닐까 싶습니다. Wasserstein distance 같은 다른 방향으로 발전시킬 수도 있을 것 같습니다. Schulmann이 아마 해봤겠죠?^^a)\n\n[![kld](../../../../img/kld.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=3m34s)\n\ntotal variation divergence와 KL divergence 사이에는 다음과 같은 관계가 있습니다.\n$$\nD\\_\\mathrm{TV}(p\\parallel q)^2 \\leq D\\_\\mathrm{KL}(p\\parallel q)\n$$\n\n다음 수식을 정의합니다.\n$$\nD\\_\\mathrm{KL}^\\max(\\pi\\parallel \\tilde\\pi) = \\max_s D\\_\\mathrm{KL}\\left(\\pi(\\cdot\\vert s)\\parallel\\tilde\\pi(\\cdot\\vert s)\\right)\n$$\n\n**Theorem 1**을 이용하여 다음과 같은 수식이 성립함을 알 수 있습니다.\n$$\n\\begin{align}\n\\eta\\left(\\tilde\\pi\\right) &\\geq L\\_{\\pi}\\left(\\tilde\\pi\\right) - C\\cdot D\\_\\mathrm{KL}^\\max(\\pi, \\tilde\\pi), \\\\\\\\\n\\mathrm{where\\ } C&= \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\n\\end{align}\n$$\n\n이러한 policy imporvement bound를 기반으로 다음과 같은 approximate policy iteration 알고리듬을 고안해낼 수 있습니다.\n\n**Algorithm 1** Policy iteration algorithm guaranteeing non-decreasing expected return $\\eta$\n> Initialize $\\pi\\_0$   \n> **for** $i=0,1,2,\\ldots$ until convergence **do**   \n> $\\quad$Compute all advantage values $A\\_{\\pi\\_i}(s,a)$    \n> $\\quad$Solve the contstrained optimization problem    \n> $\\quad\\pi\\_{i+1}=\\arg\\max\\_\\pi\\left[L\\_{\\pi\\_i} - CD\\_\\mathrm{KL}^\\max\\left(\\pi\\_i,\\pi\\right)\\right]$     \n> $\\quad\\quad$where $C = 4\\epsilon\\gamma / (1-\\gamma)^2$    \n> $\\quad\\quad$and $L\\_{\\pi\\_i}\\left(\\pi\\right) = \\eta(\\pi\\_i) + \\sum\\_{s}\\rho\\_{\\pi\\_i}(s)\\sum\\_a\\pi(a\\vert s) A\\_{\\pi\\_i}(s,a)$    \n> **end for**\n\n**Algorithm 1**은 advantage를 정확하게 계산할 수 있다고 가정하고 있습니다. 이 알고리듬은  monotonical한 성능 증가($\\eta(\\pi\\_0)\\leq\\eta(\\pi\\_1)\\leq\\cdots$)를 한다는 것을 다음과 같이 보일 수 있습니다. $M\\_i(\\pi)=L\\_{\\pi\\_i}(\\pi) - CD\\_\\mathrm{KL}^\\max\\left(\\pi\\_i,\\pi\\right)$라고 합시다.\n\n$$\n\\begin{align}\n\\eta \\left(\\pi\\_{i+1}\\right) &\\geq M\\_i\\left(\\pi\\_{i+1}\\right)\\\\\\\\\n\\eta \\left(\\pi\\_{i}\\right) &= M\\_i\\left(\\pi\\_{i}\\right) \\\\\\\\\n\\eta \\left(\\pi\\_{i+1}\\right) - \\eta \\left(\\pi\\_{i}\\right) &\\geq M\\_i\\left(\\pi\\_{i+1}\\right) - M\\_i\\left(\\pi\\_{i}\\right)\n\\end{align}\n$$\n\n위 수식과 같이 매 iteration 마다 $M\\_i$를 최대화함으로써, $\\eta$가 감소하지 않는다는 것을 보장할 수 있습니다. 이와 같은 타입의 알고리듬을 [minorization-maximization (MM) algorithm](https://www.jstor.org/stable/pdf/27643496.pdf?casa_token=0qHamcl60WoAAAAA:-fkZ9JcA_nrY3-zbCUpqvPOgcAMgw7Gr96MajCZg2byHf8m5GU1KTSxyJJcBy1lPZbBTZVCjHHUXilh4k-iuwF91Wka4B5qdltC1IR2qMWk8q1FoV6__)이라고 합니다. EM 알고리듬도 이 타입의 알고리듬 중 하나입니다.\n\n**Algorithm 1**은 아래 그림과 같이 동작합니다.\n\n[![surrogate](../../../../img/surrogate.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=3m52s)\n\n\n$M\\_i$는 $\\pi\\_i$일 때 equality가 되는 $\\eta$에 대한  surrogate function입니다. TRPO는 이 surrogate function을 최대화하고 KL divergence를 penalty가 아닌 constraint로 두는 알고리듬입니다.\n\n<br><br>\n# 4. Optimization of Parameterized Policies\n\n표기의 편의를 위해 다음과 같이 notation들을 더 간략하게 정의합니다.\n\n* $\\eta(\\theta):=\\eta\\left(\\pi\\_\\theta\\right)$\n* $L\\_\\theta\\left(\\tilde\\theta\\right):=L\\_{\\pi\\_\\theta}\\left(\\pi\\_{\\tilde\\theta}\\right)$\n* $D\\_\\mathrm{KL}\\left(\\theta\\parallel\\tilde\\theta\\right):=D\\_\\mathrm{KL}\\left(\\pi\\_\\theta\\parallel\\pi_{\\tilde\\theta}\\right)$\n* $\\theta\\_\\mathrm{old}$: previous policy parameter\n\n## 4.1 Trust Region Policy Optimization\n\n이전 장의 중요 결과를 위의 notation으로 다시 표기하면 아래와 같습니다.\n\n$$\n\\eta\\left(\\theta\\right) \\geq L\\_{\\theta\\_\\mathrm{old} }\\left(\\theta\\right) - C\\cdot D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right)\n$$\n\n$\\eta$의 성능 향상을 보장하기 위해서 그것의 lower bound를 최대화할 수 있습니다.\n\n$$\n\\max\\_\\theta \\left[L\\_{\\theta\\_\\mathrm{old} }\\left(\\theta\\right) - C D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right)\\right]\n$$\n\n이 최적화 문제는 step size를 매우 작게 해야 올바른 동작을 합니다. 위에서 살펴봤듯이 first order approximation이기 때문입니다. 좀 더 큰 step size를 가질 수 있도록 이 최적화 문제를 trust region constraint를 도입하여 다음과 같이 바꿉니다.\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ }&D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right) \\leq \\delta\n\\end{align}$$\n\n이 최적화문제의 constraint는 모든 state space에 대해서 성립해야 합니다. 또한 maximum값을 매번 찾아야 합니다. state가 많은 경우 constraint의 수가 매우 많아져서 문제를 풀기 어렵게 만듭니다. constraint의 수를 줄이기 위하여 다음과 같은 avergae KL divergence를 이용하는 heuristic approximation을 취합니다. 이것이 최선의 방법은 아닐 수 있지만 실용적인 방법입니다.\n\n$$\n\\overline D\\_\\mathrm{KL}^{\\rho}\\left(\\theta\\_1, \\theta\\_2\\right):=E\\_{s\\sim\\rho}\\left[D\\_\\mathrm{KL}\\left(\\pi\\_{\\theta_1}(\\cdot\\vert s)\\parallel\\pi\\_{\\theta\\_2}(\\cdot\\vert s)\\right)\\right]\n$$\n\n이것을 기반으로 다음과 같은 최적화 문제를 풀 수 있습니다.\n\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ }&\\overline{D}\\_\\mathrm{KL}^{\\rho\\_\\mathrm{old} }\\left(\\theta\\_\\mathrm{old}, \\theta\\right) \\leq \\delta\n\\end{align}$$\n\n\n아래 그림처럼 step size에 대해서 고려할 수 있습니다.\n\n[![heuristic_approx](../../../../img/heuristic_approx.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=4m35s)\n\n<br><br>\n# 5. Sample-Based Estimation of the Objective and Constraint\n\n실용적인 알고리듬을 만들려고 하는 노력은 아직 끝나지 않았습니다. 이제 앞의 알고리듬을 sample-based estimation 즉, Monte Carlo estimation을 할 수 있도록 바꿔보겠습니다. sampling을 편하게 할 수 있도록 아래와 같이 바꿔줍니다.\n\n* $\\sum\\_s \\rho\\_{\\theta\\_\\mathrm{old} }(s)[\\ldots]\\rightarrow\\frac{1}{1-\\gamma}E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} }}[\\ldots]$\n* $A\\_{\\theta\\_\\mathrm{old} }\\rightarrow Q\\_{\\theta\\_\\mathrm{old} }$\n* $\\sum\\_a\\pi\\_{\\theta\\_\\mathrm{old} }(a\\vert s)A\\_{\\theta\\_\\mathrm{old} } \\rightarrow E\\_{a\\sim q}\\left[\\frac{\\pi\\_\\theta(a\\vert s)}{q(a\\vert s)}A\\_{\\theta\\_\\mathrm{old} }\\right]$\n\n이러한 변화를 그림처럼 도식화할 수 있습니다.\n\n[![sample-based](../../../../img/sample-based.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=5m31s)\n\n한 가지 짚고 넘어가야 할 점은 action sampling을 할 때 importance sampling을 사용한다는 것입니다.\n\n[![importance_sampling](../../../../img/importance_sampling.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=5m53s)\n\n바뀐 최적화문제는 아래와 같습니다.\n\n- **Equation (14).**\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi\\_\\theta(a\\vert s)}{q(a\\vert s)}Q\\_{\\theta\\_\\mathrm{old} }(s,a)\\right] \\\\\\\\\n\\mathrm{s.t.\\ }&E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} }}\\left[D\\_\\mathrm{KL}\\left(\\pi\\_{\\theta\\_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi\\_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta\n\\end{align}\n$$\n\n이 때 sampling하는 두 가지 방법이 있습니다.\n\n<br>\n## 5.1 Single Path\n\n*single path*는 개별 trajectory들을 이용하는 방법입니다.\n\n[![single](../../../../img/single.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=6m15s)\n\n<br>\n## 5.2 Vine\n\n*vine*은 한 state에서 rollout을 이용하여 여러 action을 수행하는 방법입니다.\n\n[![vine1](../../../../img/vine1.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=6m32s)\n\n[![vine2](../../../../img/vine2.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=6m49s)\n\nestimation의 variance를 낮출 수 있지만 계산량이 많고 한 state에서 여러 action을 수행할 수 있어야 하기 때문에 현실적인 문제에 적용하기에는 어려움이 있습니다.\n\n\n<br><br>\n# 6. Practical Algorithm\n\n앞서 single-path, vine 샘플링을 사용하는 두가지 방식의 policy optimization 알고리즘을 살펴봤습니다. 실용적인 알고리듬은 아래의 과정을 반복해서 수행합니다.\n\n1) Q-values의 몬테카를로 추정을 통해 state-action 쌍 집합을 single path 또는 vine 과정을 통해 수집함.\n\n2) 샘플 평균으로, (14)식의 목적함수와 제약식을 추정함\n + **Equation (14).**\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi\\_\\theta(a\\vert s)}{q(a\\vert s)}Q\\_{\\theta\\_\\mathrm{old} }(s,a)\\right] \\\\\\\\\n\\mathrm{s.t.\\ }&E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} }}\\left[D\\_\\mathrm{KL}\\left(\\pi\\_{\\theta\\_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi\\_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta\n\\end{align}\n$$\n\n3) policy parameter vector인 $\\theta$를 업데이트 하면서 제약조건이 있는 최적화 문제를 근사적으로 풂. 본 논문에서는 gradient를 직접 계산하는 것보다는 약간 더 계산량이 있는 line search와 conjugate gradient algorithm을 사용했습니다.\n\n3)에 대해서,  gradient의 covariance matrix를 사용하지 않고 KL divergence의 Hessian을 해석적으로 계산하여 Fisher Information Matrix를 구성했습니다.\n\n다시말해서\n\n$\\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial}{\\partial \\theta_i} \\log \\pi_{\\theta}(a_n\\vert s_n) \\frac{\\partial}{\\partial \\theta_j} \\log \\pi_{\\theta}(a_n\\vert s_n)$ 대신 $\\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_j} D_{KL}(\\pi_{\\theta_{old} }(\\cdot \\vert  s_n) \\vert \\vert  \\pi_{\\theta}(\\cdot \\vert  s_n))$를 계산한 것입니다.\n\n이 analytic estimator는 Hessian이나 trajectories의 모든 gradient를 저장하지 않아도 되기 때문에 대규모 환경을 고려할 경우 계산 상 이점이 있습니다. 좀 더 자세하게 설명한 Appendix C 부분을 살펴보겠습니다.\n\n\n<br>\n## Appendix C. Efficiently Solving the Trust-Region Constrained Optimization Problem\n\n우리가 풀고자 하는 식은 다음과 같습니다.\n\n$$\n\\begin{align}\n\\max\\quad &L(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ } &\\overline{D}\\_{KL}(\\theta\\_\\mathrm{old}, \\theta) \\leq \\delta\n\\end{align}\n$$\n\n이 최적화 문제를 풀기 위하여 우리는 gradient 기법을 이용할 것입니다. gradient 기법은 결국 update할 방향과 크기를 결정하는 문제로 생각할 수 있습니다. 이에 따라 아래와 같은 두가지 과정으로 진행합니다.\n\n1) 탐색 방향을 계산, 목적함수의 first-order approximation(선형근사)와 제약식의 quadratic approximation(2차 근사).\n\n2) 이동거리 계산을 위해 해당 방향으로 line search 수행.\n\n탐색 방향은 $Ax = g$ 수식을 근사적으로 풀어서 구합니다. 여기서 $A$ 는 Fisher Information Matrix이고, 이것은 KL divergence 제약식의 2차 근사 $\\overline{D}\\_\\mathrm{KL} (\\theta\\_\\mathrm{old}, \\theta) \\approx \\frac{1}{2}(\\theta - \\theta\\_\\mathrm{old})^T A(\\theta - \\theta\\_\\mathrm{old})$를 푸는 것입니다. 여기서 $A\\_{ij} = \\frac{\\partial}{\\partial \\theta\\_i} \\frac{\\partial}{\\partial \\theta\\_j} \\overline{D}\\_\\mathrm{KL}(\\theta\\_\\mathrm{old}, \\theta)$입니다.\n\n논문에서 이 부분에 대한 설명이 약간 부족해서 좀 더 내용을 추가하자면, $\\frac{1}{2}x^T A x - x^T g + b = 0$과 같은 quadratic equation의 해는 1차 derivative를 0으로 만드는 $x$를 찾음으로써 알 수 있습니다. 위에서 우리는 목적함수를 1차 근사시키고 제약조건은 2차 근사를 시킴으로써 quadratic equation 형태로 변형했기 때문에 같은 방식으로 풀 수 있습니다. 따라서 $g$는 1차근사시킨 목적함수의 계수가 될 것입니다.\n\n대규모 문제에서 $A$ 혹은 $A^{-1}$ 을 계산하는 것은 엄청난 계산량이 필요합니다. 하지만 conjugate gradient algorithm을 잘 이용하면 전체 $A$행렬(FIM)을 계산하지 않아도 $Ax=g$ 를 근사적으로 해결할 수 있게 해줍니다.\n\n탐색 방향 $s \\approx A^{-1}g$과 더불어 최대스텝길이(step size) $\\beta$도 계산할 필요가 있습니다.\n\n즉, $\\delta = \\overline{D}\\_\\mathrm{KL} \\approx \\frac{1}{2}(\\beta s)^T A(\\beta s) = \\frac{1}{2}\\beta^2s^TAs$ 이고, \n\n$\\beta = \\sqrt{2\\delta / s^T As}$가 됩니다. 이때 $\\delta$는 KL divergence의 boundary term 입니다.\n\n$s^T As$ 항은 Hessian-vector product로 계산할 수 있고, conjugate gradient 과정에서 계산됩니다.\n\n마지막으로, surrogate objective와 KL divergence 제약식을 위해 다음과 같은 함수를 정의하고 line search를 사용합니다.\n\n$$\nL\\_{\\theta\\_\\mathrm{old} }(\\theta) - \\chi [\\overline{D}\\_{KL}(\\theta\\_\\mathrm{old}, \\theta) \\leq \\delta]\n$$\n\n$\\chi[\\cdot]$ 함수는 []안의 조건이 맞으면 0이고 틀리면 무한으로 발산하는 함수입니다.\n\n위에서 계산한 $\\beta$ 의 최대 값부터 시작해서 목적함수가 개선될때까지 exponential하게 줄여 나갑니다. line search가 없었다면 이 알고리즘은 매우 큰 스텝으로 계산될 것이고 심각한 성능 저하가 발생할 것입니다.\n\n(여기서 살짝 예고를 하자면, 나중에 이 KL 기반 기법을 개선한 더 단순한 방법이 제안됩니다. PPO라고...)\n\n\n<br><br>\n# 7. Connections with Prior Work\n\nNatural Policy Gradient는 $L$의 선형 근사와 $\\overline D\\_\\mathrm{KL}$ 제약식을 2차근사 하는 아래의 식의 special case입니다.\n\n- Equation (17).\n\n$L\\_{\\pi}(\\tilde\\pi) = \\eta(\\pi) + \\underset{s}{\\sum}\\rho\\_{\\pi}(s) \\underset{a}{\\sum}\\tilde\\pi(a\\vert s)A\\_\\pi(s,a)$\n\n$\\underset{\\theta}{\\max}\\quad \\[\\nabla\\_{\\theta}L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\ \\rvert\\_{\\theta=\\theta\\_\\mathrm{old} } \\cdot (\\theta - \\theta\\_\\mathrm{old})]$\n\n$\\mathrm{s.t.}\\quad\\frac{1}{2}(\\theta\\_\\mathrm{old} - \\theta)^{T} A(\\theta\\_\\mathrm{old})(\\theta\\_\\mathrm{old}-\\theta) \\leq \\delta$\n\n$\\mathrm{where\\ }A(\\theta_{old})_{ij} = $\n\n$\\Large \\frac{\\partial}{\\partial \\theta\\_{i} } \\frac{\\partial}{\\partial \\theta\\_{j} }\nE\\_{s \\sim \\rho\\_{\\pi} }[D\\_\\mathrm{KL}(\\pi(\\cdot\\rvert s, \\theta\\_\\mathrm{old})  \\rvert\\rvert \\pi(\\cdot \\rvert s, \\theta))] \\rvert\\_{\\theta=\\theta\\_\\mathrm{old} }$\n\n업데이트 식은 다음과 같습니다.\n\n$\\theta\\_\\mathrm{new} = \\theta\\_\\mathrm{old} +  \\color{Red}{\\frac{1}{\\lambda} } A(\\theta\\_\\mathrm{old})^{-1}\\nabla\\_{\\theta}L(\\theta) \\rvert\\_{\\theta=\\theta\\_\\mathrm{old} }$\n\n여기서 step size인 $\\frac{1}{\\lambda}$ 일반적으로 알고리즘의 파라미터로 취급되지만 TRPO는 각 업데이트 마다 제약식으로 사용한다는 점이 다릅니다. 사소한 차이처럼 보이지만, 큰 차원을 다루는 실험에서 성능을 크게 향상시켰습니다.\n\n또한 $L^2$ 제약식(혹은 페널티) 를 사용하면 표준 Policy Gradient 업데이트 식을 얻었습니다.\n\n- Equation (18).\n\n$\\underset{\\theta}{\\max}\\quad[\\nabla\\_{\\theta} L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\rvert\\_{\\theta=\\theta\\_\\mathrm{old} } \\cdot (\\theta - \\theta\\_\\mathrm{old})]$\n\n$\\mathrm{s.t.}\\quad \\frac{1}{2} \\vert\\vert \\theta - \\theta\\_\\mathrm{old} \\vert\\vert^2 \\leq \\delta$\n\n\n\n$L\\_{\\pi}(\\tilde\\pi) = \\eta(\\pi) + \\underset{s}{\\sum}\\rho\\_{\\pi}(s) \\underset{a}{\\sum}\\tilde\\pi(a\\vert s)A\\_\\pi(s,a) $를 이용해서 제약조건없이 $\\underset{\\pi}{\\max}\\quad L\\_{\\pi\\_\\mathrm{old} }(\\pi)$를 풀면\nPolicy Iteration update를 하는 것과 같습니다.\n\n\n<br><br>\n# 8. Experiments\n\n- 다음과 같은 3 가지 궁금증을 해결하기 위하여 실험을 설계하였습니다.\n\n    1. Single-path 와 vine 의 성능면에서 특징을 알고싶음.\n\n    2. Fixed penalty coefficient(NPG) 보다 fixed KL divergence를 사용하는 것(TRPO)으로    \n    변경한 것이 얼마나 뚜렷한 차이를 보일지, 성능 면에서 어떤 영향을 미치는지 알고싶음.\n\n    3. TRPO가 큰 스케일의 문제를 해결할 수 있는지, 기존에 연구/적용되어왔던 방법들과\n    성능, 계산시간, 샘플 복잡도 면에서 비교하고 싶음.\n\n\n- 1, 2의 궁금증에 대해서 실험 환경에 single path, vine을 비롯한 여러 사전방법들을 함께 실험하였습니다.\n\n- 3에 대해선, robotic locomotion과 Atari game에 TRPO를 실험하였습니다.\n\n\n<br>\n## 8.1 Simulated Robotic Locomotion\n\n- MuJoCo 시뮬레이터를 활용해 로봇 locomotion 실험 진행하였습니다.\n- Robot State: positions and velocities, joint torques\n\n![](https://i.imgur.com/bs5ATS3.png)\n\n- 수영, 점프, 걷기 행동을 학습시키고자 합니다.\n\n    - 수영: 10 dimensional state space , reward: $r(x,u)=v_x - 10^{-5}\\vert \\vert u\\vert \\vert ^2$, quadratic penalty.\n\n    - 점프: 12 dimensional state space, episode의 끝은 높이와 각도 threshol로 점프를 판단, non-terminal state에 대해 +1 보상 추가.\n\n    - 걷기: 18 dimensional state space, 뜀뛰는 듯한 보법(점프에이전트)보다 부드러운 보법을 유도하기위해 페널티를 추가함.\n\n**Detailed Experiment Setup and used parameters, used network model**\n![](https://i.imgur.com/zgnsbw6.png)\n\n![](https://i.imgur.com/FqdWC53.png)\n\nequation (12) : $\\max\\quad L\\_{\\theta\\_\\mathrm{old}(\\theta)}, \\ \\ \\mathrm{s.t.\\ }\\overline{D}\\_\\mathrm{KL}^{\\rho\\_{\\theta\\_\\mathrm{old} }}(\\theta\\_\\mathrm{old}, \\theta) \\leq \\delta$\n\n이 실험에서 $\\delta = 0.01$입니다.\n\n- 비교에 사용된 모델들\n\n    - TRPO Single-path\n    - TRPO vine\n    - CEM(cross-entropy method)\n    - CMA(covariance matrix adaptation)\n    - NPG (Lagrange Multiplier penalty coefficien를 사용하는 것이 차이점)\n    - Empirical FIM\n    - maximum KL(cartpole)\n\n![](https://i.imgur.com/wSqolvS.png)\n\n- 제안한 single path, vine 모두 task를 잘 학습하는 것을 확인할 수 있습니다.\n\n- 페널티를 고정하는 것보다, KL divergenc를 제약하는 것이 step size를 선택하는 부분에서 Robust합니다.\n\n- CEM/CMA는 derivative free 알고리즘으로, 샘플 복잡도가 네트워크 파라미터수 만큼 확장되어 high dimension 문제에서는 속도와 성능이 약한 모습을 보입니다.\n\n- maxKL은 제안방법보다 느린 학습성능을 보입니다.\n\nTRPO video link: https://www.youtube.com/watch?v=jeid0wIrSn4\n\n- 사전지식이 적은 상태에서 여러 액션을 학습하는 실험으로 TRPO의 성능을 보입니다.\n\n- 지금까지의 균형이나 걸음 같은 개념을 명시적으로 인코딩한 robotic locomotion에서의 사전 연구들과 차이를 보였습니다.\n\n<br>\n## 8.2 Playing Games from Images\n\n- 부분관찰가능하며 복잡한 observation인 환경에서 TRPO를 평가하기위해 raw image를 입력으로 하는 아타리게임 환경에서 실험하였습니다.\n\n* Challenging point :\n    - High dimensional space\n    - delayed reward\n    - non-stationary image (Enduro는 배경 이미지가 바뀌고, 반전처리가 됨)\n    - complex action sequence (Q*bert는 21개의 다른 발판에서 점프 해야함)\n\n- ALE(Arcade Learning Environment) 에서 테스트\n- DQN과 전처리 방식은 동일\n    210 x 160 pixel -> gray-scaling & down-sampling -> 110 x 84  -> crop -> 84 x 84\n\n- 2 convolution layer ( 16 channels) , stride 2, FC layer with 20 units\n\n![](https://i.imgur.com/NJBC69d.png) \n\n![](https://i.imgur.com/wTe1OEW.png)\n\n![](https://i.imgur.com/j22VtNQ.png)\n\n- 30시간 동안 500 iteration 학습을 하였습니다.\n\n- DQN과 MCST를 사용한 모델(UCC-I), 사람이 플레이한 것(Human) 과의 비교하였습니다.\n\n- 이전 방법들에 비해 압도적인 점수를 기록하진 못했으나, 다양한 게임에서 적절한 게임 기록하였습니다.\n\n- 이전 방법들과 다른 점은, 특정 task에 초점을 맞춰 설계하지않았는데 robotics나 game playing Task에도 적절한 성능을 내는 점에서 **TRPO의 일반화 성능**을 확인하였습니다.\n\n<br>\n# 9. Discussion\n\n- Trust Region Policy Optimization 을 제안하였습니다.\n\n- KL Divergence 페널티로 $\\eta(\\pi)$를 최적화하는 알고리즘이 monotonically improve 함을 증명하였습니다.\n\n- Locomotion 도메인에서 여러 행동(수영, 걷기, 점프)을 제어하는 controller를 학습하였습니다.\n\n- Robotics와 게임 실험을 결합해, 시각정보와 센서데이터를 사용하는 로봇제어 정책을 학습시킬 수 있는 가능성을 보았습니다.\n\n- 샘플의 복잡도를 상당히 줄일 수 있어 실제상황에 적용가능성을 보았습니다.\n\n<br><br>\n# END","source":"_posts/trpo.md","raw":"---\ntitle: Trust Region Policy Optimization\ndate: 2018-06-23 16:53:12\ntags: [\"프로젝트\", \"피지여행\"]\ncategories: 프로젝트\nauthor: 공민서, 김동민\nsubtitle: 피지여행 5번째 논문\n---\n\n<center> <img src=\"https://www.dropbox.com/s/o7cjcn0e17mpizr/Screen%20Shot%202018-07-18%20at%201.14.22%20AM.png?dl=1\" width=\"700\"> </center>\n\n논문 저자 : John Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, Pieter Abbeel\n논문 링크 : https://arxiv.org/pdf/1502.05477.pdf\nProceeding : International Conference on Machine Learning (ICML) 2015\n정리 : 공민서, 김동민\n\n---\n\n# 1. 들어가며...\n\nTrust region policy optimization (TRPO)는 상당히 우수한 성능을 보여주는 policy gradient 기법으로 알려져 있습니다. 높은 차원의 action space를 가진 robot locomotion부터 action은 적지만 화면을 그대로 처리하여 플레이하기 때문에 control parameter가 매우 많은 Atari game까지 각 application에 세부적으로 hyperparameter들을 특화시키지 않아도 두루두루 좋은 성능을 나타내기 때문에 일반화성능이 매우 좋은 기법입니다. 이 TRPO에 대해서 알아보겠습니다.\n\n※TRPO를 매우 재밌게 설명한 Crazymuse AI의 [Youtube video](https://www.youtube.com/watch?v=CKaN5PgkSBc&t=90s)에서 일부 그림을 차용했습니다. 이 비디오를 시청하시는 것을 강력하게 추천합니다!\n\n<br>\n## 1.1 TRPO 흐름 잡기\n\nTRPO 논문은 많은 수식이 등장하여 이 수식들을 따라가다보면 큰 그림을 놓칠 수도 있습니다. 세부적인 내용을 살펴보기 전에 기존 연구에서 출발해서 TRPO로 어떻게 발전해나가는지 간략하게 살펴보겠습니다.\n\n### 1.1.1 Original Problem\n\n$$\\max\\_\\pi \\eta(\\pi)$$\n\n모든 강화학습이 그렇듯이 expected discounted reward를 최대화하는 policy를 찾는 문제로부터 출발합니다.\n\n### 1.1.2 Conservative policy iteration\n\n$$\\max L\\_\\pi(\\tilde\\pi) = \\eta(\\pi) + \\sum\\_s \\rho_\\pi(s)\\sum\\_a\\tilde\\pi(a\\vert s)A\\_\\pi(s,a)$$\n\n$\\eta(\\pi)$를 바로 최대화하는 것은 많은 경우 어렵습니다. $\\eta(\\pi)$의 성능향상을 보장하면서 policy를 update하는 conservative policy iteration 기법이 [Kakade와 Langford](http://www.cs.cmu.edu/~./jcl/papers/aoarl/Final.pdf)에 의하여 제안되었습니다. 이 기법을 이용하면 policy update가 성능을 향상시키는지는 못하더라도 최소한 성능을 악화시키지는 않는다는 것이 이론적으로 보장됩니다.\n\n### 1.1.3 Theorem 1 of TRPO\n\n$$\\max L\\_{\\pi\\_\\mathrm{old} }\\left(\\pi\\_\\mathrm{new}\\right) - \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2,\\quad\\left(\\alpha=D\\_\\mathrm{TV}^\\max\\left(\\pi\\_\\mathrm{old},\\pi\\_\\mathrm{new}\\right)\\right)$$\n\n기존의 conservative policy iteration은 과거 policy와 새로운 policy를 섞어서 사용해서 실용적이지 않다는 단점이 있었는데 이것을 보완하여 온전히 새로운 policy만으로 update할 수 있는 기법을 제안합니다. \n\n### 1.1.4 KL divergence version of Theorem 1\n\n$$\\max L\\_{\\pi}\\left(\\tilde\\pi\\right) - C\\cdot D_\\mathrm{KL}^\\max\\left(\\pi,\\tilde\\pi\\right),\\quad\\left(C = \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\right)$$\n\ndistance metric을 KL divergence로 바꿀 수 있습니다.\n\n### 1.1.5 Using parameterized policy\n\n$$\\max\\_\\theta L\\_{\\theta\\_\\mathrm{old} }(\\theta) - C\\cdot D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right)$$\n\n최적화문제를 더욱 편리하게 풀 수 있도록 낮은 dimension을 가지는 parameter들로 parameterized된 policy를 사용할 수 있습니다.\n\n### 1.1.6 Trust region constraint\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ }&D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right) \\leq \\delta\n\\end{align}$$\n\n1.1.5까지는 아직 conservative policy iteration을 약간 변형시킨 것입니다. policy를 update할 때 지나치게 많이 변하는 것을 방지하기 위하여 [trust region](https://en.wikipedia.org/wiki/Trust_region)을 constraint로 설정할 수 있습니다. 이 아이디어로 인해서 TRPO라는 명칭을 가지게 됩니다.\n\n### 1.1.7 Heuristic approximation\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ }&\\overline{D}\\_\\mathrm{KL}^{\\rho\\_\\mathrm{old} }\\left(\\theta\\_\\mathrm{old}, \\theta\\right) \\leq \\delta\n\\end{align}$$\n\n사실 1.1.6의 constraint는 모든 state에 대해서 성립해야 하기 때문에 문제를 매우 어렵게 만듭니다. 이것을 좀 더 다루기 쉽게 state distribution에 대한 평균을 취한 것으로 변형합니다.\n\n### 1.1.8 Monte Carlo simulation\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi\\_\\theta(a\\vert s)}{q(a\\vert s)}Q\\_{\\theta\\_\\mathrm{old} }(s,a)\\right] \\\\\\\\\n\\mathrm{s.t.\\ }&E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} }}\\left[D\\_\\mathrm{KL}\\left(\\pi\\_{\\theta\\_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi\\_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta\n\\end{align}$$\n\nSampling을 통한 계산이 가능하도록 식을 다시 표현할 수 있습니다. \n\n### 1.1.9 Efficiently solving TRPO\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &\\nabla\\_\\theta \\left. L\\_{\\theta\\_\\mathrm{old} }(\\theta)\\right\\vert \\_{\\theta=\\theta\\_\\mathrm{old} } \\left(\\theta - \\theta\\_\\mathrm{old}\\right) \\\\\\\\\n\\mathrm{s.t.\\ }&\\frac{1}{2}\\left(\\theta\\_\\mathrm{old} - \\theta \\right)^T A\\left(\\theta\\_\\mathrm{old}\\right)\\left(\\theta\\_\\mathrm{old} - \\theta \\right) \\leq \\delta \\\\\\\\\n&A\\_{ij} = \\frac{\\partial}{\\partial \\theta\\_i}\\frac{\\partial}{\\partial \\theta\\_j}\\overline{D}\\_\\mathrm{KL}^{\\rho\\_\\mathrm{old} }\\left(\\theta\\_\\mathrm{old}, \\theta\\right)\n\\end{align}$$\n\n문제를 효율적으로 풀기 위하여 approximation을 적용할 수 있습니다. objective function은 first order approximation, constraint는 quadratic approximation을 취하면 효율적으로 문제를 풀 수 있는 형태로 바뀌는데 이것을 natural gradient로 풀 수도 있고 conjugate gradient로 풀 수도 있습니다.\n\nTRPO는 이렇게 다양한 방법으로 문제를 변형한 것입니다! 이제 좀 더 자세히 살펴보겠습니다.\n\n<br><br>\n# 2. Preliminaries\n\n다음과 같은 파라미터를 가지는 infinite-horizon discounted Markov decision process (MDP)를 고려합니다. \n\n* $\\mathcal{S}$: finite set of states\n* $\\\\mathcal{A}$: finite set of actions\n* $P: \\mathcal{S}\\times\\mathcal{A}\\rightarrow\\mathbb{R}$: transition probability distribution\n* $r: \\mathcal{S}\\rightarrow\\mathbb{R}$: reward function\n* $\\rho\\_0:\\mathcal{S}\\rightarrow\\mathbb{R}$: distribution of the initial state $s\\_0$\n* $\\gamma\\in(0,1)$: discount factor\n* $\\eta(\\pi)=E\\_{s\\_0,a_0,\\ldots}\\left[\\sum\\_{t=0}^\\infty\\gamma^t r\\left(s\\_t\\right)\\right]$, where $s\\_0\\sim\\rho\\_0\\left(s\\_0\\right), a\\_t\\sim\\pi\\left(\\left.a\\_t \\right\\vert s\\_t\\right), s\\_{t+1}\\sim P\\left(\\left.s\\_{t+1}\\right\\vert s\\_t,a\\_t\\right)$\n* $Q\\_\\pi\\left(s\\_t,a\\_t\\right)=E\\_{ { \\color{red}{s\\_{t+1}} },a\\_{t+1},\\ldots}\\left[\\sum\\_{l=0}^\\infty \\gamma^l r\\left(s\\_{t+l}\\right)\\right]$: action value function\n* $V\\_\\pi\\left(s\\_t\\right)=E\\_{ { \\color{red}{a\\_t} }, s\\_{t+1},a\\_{t+1},\\ldots}\\left[\\sum\\_{l=0}^\\infty \\gamma^l r\\left(s\\_{t+l}\\right)\\right]$: value function (action value function과 expectation의 첨자가 다른 부분을 유의하세요.)\n* $A\\_\\pi(s,a) = Q\\_\\pi(s,a) - V\\_\\pi(s)$: advantage function\n\n<br>\n## 2.1 Useful identity [Kakade & Langford 2002]\n\n우리의 목표는 $\\eta\\(\\pi\\)$가 최대화되도록 만드는 것입니다. 하지만 $\\pi$의 변화에 따라 $\\eta$가 어떻게 변하는지 알아내는 것도 쉽지 않습니다. $\\pi$는 기존의 policy이고 $\\tilde\\pi$는 새로운 policy를 나타낸다고 할 때 $\\eta$와 policy update 사이에 다음과 같은 관계가 있다는 것이 밝혀졌습니다.\n\n**Lemma 1.** $\\eta\\left(\\tilde\\pi\\right) = \\eta(\\pi) + E\\_{s\\_{0},a\\_{0},\\ldots\\sim\\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t A\\_\\pi\\left(s\\_t,a\\_t\\right)\\right]$\n\n*Proof.* $A\\_\\pi(s,a)=E\\_{s'\\sim P\\left(s'\\vert s,a\\right)}\\left[r(s)+\\gamma V\\_\\pi\\left(s'\\right)-V\\_\\pi(s)\\right]$로 다시 표현할 수 있습니다. 표기의 편의를 위하여 $\\tau:=\\left(s\\_0,a\\_0,s\\_1,a\\_1,\\ldots\\right)$를 정의하겠습니다. 다음과 같은 수식 전개가 가능합니다.\n$$\n\\begin{align}\n&E\\_{\\tau\\vert \\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t A\\_\\pi\\left(s\\_t,a\\_t\\right)\\right] \\\\\\\\\n &= E\\_{\\tau\\vert \\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t \\left(r(s\\_t)+\\gamma V\\_\\pi\\left(s\\_{t+1}\\right)-V\\_\\pi(s\\_t)\\right)\\right] \\\\\\\\\n&= E\\_{\\tau\\vert \\tilde\\pi}\\left[\\left(\\sum\\_{t=0}^\\infty\\gamma^t r(s\\_t)\\right)+\\gamma V\\_\\pi\\left(s\\_{1}\\right)-V\\_\\pi(s\\_0)+\\gamma^2 V\\_\\pi\\left(s\\_{2}\\right)-\\gamma V\\_\\pi(s\\_1)+\\gamma^3 V\\_\\pi\\left(s\\_{3}\\right)-\\gamma^2 V\\_\\pi(s\\_2)+\\cdots\\right] \\\\\\\\\n&= E\\_{\\tau\\vert \\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t r(s\\_t)+\\color{red}{\\gamma V\\_\\pi\\left(s\\_{1}\\right)}-V\\_\\pi(s\\_0)+\\color{red}{\\gamma^2 V\\_\\pi\\left(s\\_{2}\\right)}-\\color{red}{\\gamma V\\_\\pi(s\\_1)}+\\cdots\\right] \\\\\\\\\n&= E\\_{\\tau\\vert \\tilde\\pi}\\left[-V\\_\\pi(s\\_0)+\\sum\\_{t=0}^\\infty\\gamma^t r(s\\_t)\\right] \\\\\\\\\n&\\overset{\\underset{\\mathrm{(a)} }{} }{=} -E\\_{\\color{red}{s\\_0}}\\left[V\\_\\pi(s\\_0)\\right]+E\\_{\\tau\\vert \\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t r(s\\_t)\\right] \\\\\\\\\n&=-\\eta(\\pi) + \\eta\\left(\\tilde\\pi\\right)\\\\\\\\\n&\\quad\\therefore \\eta\\left(\\tilde\\pi\\right) = \\eta(\\pi) + E\\_{s\\_{0},a\\_{0},\\ldots\\sim\\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t A\\_\\pi\\left(s\\_t,a\\_t\\right)\\right]\n\\end{align}\n$$\n위의 전개의 (a) 부분은 $V\\_\\pi\\left(s\\_0\\right)=E\\_{a\\_0,a\\_1,s\\_1,\\ldots}\\left[\\sum\\_{t=0}^\\infty\\gamma^l r\\left(s\\_l\\right)\\right]$이므로 $\\tau$ 중 많은 부분들이 이미 expectation이 취해진 값이므로 무시됩니다. 오직 $s\\_0$만 expectation이 취해지지 않았기 때문에 남아있습니다.\n\n**Lemma 1**은 새로운 policy와 기존의 policy 사이의 관계를 규정합니다. 다음과 같은 식을 정의하고 이것을 이용해서 **Lemma 1**을 변형시켜 봅시다.\n\n* $\\rho\\_\\pi(s) = P\\left(s\\_0=s\\right) + \\gamma P\\left(s\\_1=s\\right)\\ + \\gamma^2 P\\left(s\\_2=s\\right) + \\cdots $: (unnormalized) discounted visitation frequencies\n\n\n\n$$\n\\begin{align}\n\\eta\\left(\\tilde\\pi\\right) &= \\eta(\\pi) + \\sum\\_{t=0}^\\infty\\sum\\_{s}P\\left(\\left.s\\_t=s\\right\\vert \\tilde\\pi\\right)\\sum\\_a\\tilde\\pi(a\\vert s)\\gamma^t A\\_\\pi(s,a) \\\\\\\\\n&= \\eta(\\pi) + \\sum\\_{s}{\\color{red}{\\sum\\_{t=0}^\\infty \\gamma^t P\\left(\\left.s\\_t=s\\right\\vert \\tilde\\pi\\right)} } \\sum\\_a\\tilde\\pi(a\\vert s) A\\_\\pi(s,a) \\\\\\\\\n&= \\eta(\\pi) + \\sum\\_{s}{\\color{red}{\\rho\\_\\tilde\\pi(s)} }\\sum\\_a\\tilde\\pi(a\\vert s) A\\_\\pi(s,a) \\\\\\\\\n\\end{align}\n$$\n\n이 수식의 의미가 무엇일까요? 만약 $\\sum\\_a\\tilde\\pi(a\\vert s) A\\_\\pi(s,a) \\geq 0$이라면 $\\eta(\\tilde\\pi)$는 항상 $\\eta(\\pi)$보다 큽니다. 즉, policy를 업데이트함으로써 항상 개선됩니다. 즉, 이 수식을 통해서 항상 더 좋은 성능을 내는 policy update가 가져야 할 특징을 알 수 있습니다. 다음과 같은 deterministic policy가 있다고 합시다.\n\n$$\n\\tilde\\pi(s) = \\arg\\max_a A\\_\\pi(s,a)\n$$\n\n이 policy는 적어도 하나의 state-action pair에서 0보다 큰 값을 가지는 advantage가 있고 그 때의 확률이 0이 아니라면 항상 성능을 개선시킵니다. 어려운 점은 policy를 바꾸면 $\\rho$도 바뀐다는 점입니다. \n\n다음 그림을 봅시다. Starting Point에서 여러가지 경로를 거쳐서 Destination으로 갈 수 있습니다. 다른 policy를 이용하는 것은 다른 경로를 이용한 것입니다.\n\n[![policy_change](../../../../img/policy_change.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=16s)\n\n이 때 다른 policy를 이용함에 따라 state visitation frequency도 변하게 됩니다.\n\n[![state_visitation_change](../../../../img/state_visitation_change.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=36s)\n\n이로 인해서 policy를 최적화하는 것은 어려워집니다. 그래서 이러한 변화를 무시하는 다음과 같은 approximation을 취합니다.\n\n$$\n\\begin{align}\nL_\\pi\\left(\\tilde\\pi\\right) &= \\eta(\\pi) + \\sum\\_{s}{\\color{red}{\\rho\\_\\pi(s)} } \\sum\\_a\\tilde\\pi(a\\vert s) A\\_\\pi(s,a)\n\\end{align}\n$$\n\npolicy가 바뀌었음에도 이전의 state distribution을 계속 이용하는 것입니다. 이것은 policy의 변화가 크지 않다면 어느 정도 허용될 수 있을 것입니다. 그렇지만 얼마나 많은 변화가 허용될까요? 이것을 정하기 위해서 이용하는 것이 trust region입니다.\n\n<br>\n## 2.2 Conservative Policy Iteration\n\npolicy의 변화를 다루기 용이하게 하기 위해서 policy를 다음과 같이 파라미터를 이용해서 표현합시다.\n\n* $\\pi\\_\\theta(a\\vert s)$: parameterized policy\n\n$\\pi\\_\\theta$는 $\\theta$에 대하여 미분가능한 함수입니다. $L\\_\\pi\\left(\\tilde\\pi\\right)$을 $\\theta\\_0$에서 $\\eta(\\pi)$에 대한 first order approximation이라고 하면 다음 식이 성립합니다.\n\n$$\n\\begin{align}\nL\\_{\\pi\\_{\\theta\\_0} }\\left(\\pi\\_{\\theta_0}\\right) &= \\eta\\left(\\pi\\_{\\theta\\_0}\\right) \\\\\\\\\n\\nabla\\_\\theta \\left.L\\_{\\pi\\_{\\theta\\_0} }\\left(\\pi\\_{\\theta\\_0}\\right)\\right\\vert \\_{\\theta=\\theta\\_0} &= \\nabla\\_\\theta\\left.\\eta(\\pi\\_{\\theta\\_0})\\right\\vert \\_{\\theta=\\theta\\_0}\n\\end{align}\n$$\n\n이것의 의미는 $\\pi\\_{\\theta\\_0}$가 매우 작게 변한다면 $L\\_{\\pi\\_{\\theta\\_0} }$를 개선시키는 것이 $\\eta$를 개선시키는 것이라는 것입니다. 그러나 지금까지의 설명만으로는 $\\pi\\_{\\theta\\_0}$를 얼마나 작게 변화시켜야 할지에 대해서는 알 수 없습니다.\n\nKakade & Langford가 2002년 발표한 논문에서도 이것에 대해서 고민했습니다. 그 논문에서 *conservative policy iteration*이라는 기법을 제안합니다. 그 논문의 contribution은 다음과 같습니다.\n\n* $\\eta$ 개선의 lower bound를 제공\n* 기존의 policy를 $\\pi\\_\\mathrm{old}$라고 하고 $\\pi'$를 $\\pi'=\\arg\\max\\_\\{\\pi'}L\\_{\\pi\\_\\mathrm{old} }\\left(\\pi'\\right)$과 같이 정의할 때, 새로운 mixture policy $\\pi\\_\\mathrm{new}$를 다음과 같이 제안\n$$\n\\pi\\_\\mathrm{new}(a\\vert s) = (1-\\alpha)\\pi\\_\\mathrm{old}(a\\vert s) + \\alpha \\pi'(a\\vert s)\n$$\n\n그림으로 표현하면 다음과 같습니다.\n\n[![mixure_policy](../../../../img/mixure_policy.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=2m46s)\n\n* 다음과 같은 lower bound를 정의\n$$\n\\eta\\left(\\pi\\_\\mathrm{new}\\right) \\geq L\\_{\\pi\\_{\\theta\\_\\mathrm{old} }}\\left(\\pi\\_\\mathrm{new}\\right) - \\frac{2\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2,\\\\\\\\\n\\mathrm{where\\ }\\epsilon = \\max_s\\left\\vert E\\_{a\\sim\\pi'(a\\vert s)}\\left[A\\_\\pi(s,a)\\right]\\right\\vert \n$$\n\n하지만 mixture된 policy라는 것은 실용적이지 않습니다. \n\n<br><br>\n# 3. Monotonic Improvement Guarantee for General Stochastic Policies\n\n이전 장에서 설명한 lower bound는 오직 mixture policy에 대해서만 성립하고 더 많이 사용되는 stochastic policy에는 적용되지 않습니다. 따라서 stochastic policy를 이용할 수 있도록 개선할 필요가 있습니다. 아래와 같이 기존 수식에서 두 가지를 바꿈으로써 이것이 가능합니다.\n\n* $\\alpha\\rightarrow$ distance measure between $\\pi$ and $\\tilde\\pi$\n* constant $\\epsilon\\rightarrow\\max\\_{s,a}\\left\\vert A\\_\\pi(s,a)\\right\\vert $\n\n여기서 distance measure로 total variation divergence를 이용합니다. discrete porbability distribution $p$와 $q$에 대하여 다음과 같이 정의됩니다.\n$$\nD\\_\\mathrm{TV}(p\\parallel q) = \\frac{1}{2}\\sum\\_i\\left\\vert p\\_i - q\\_i\\right\\vert \n$$\n\n이것을 이용하여 $D\\_\\mathrm{TV}^\\max$를 다음과 같이 정의합니다.\n$$\nD\\_\\mathrm{TV}^\\max(\\pi\\parallel \\tilde\\pi) = \\max_s D\\_\\mathrm{TV}\\left(\\pi(\\cdot\\vert s)\\parallel\\tilde\\pi(\\cdot\\vert s)\\right)\n$$\n\n그림으로 표현하면 다음과 같습니다.\n\n[![tvd](../../../../img/tvd.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=3m15s)\n\n이것을 이용하여 다음과 같은 관계식을 얻을 수 있습니다.\n\n**Theorem 1.** Let $\\alpha=D\\_\\mathrm{TV}^\\max(\\pi\\parallel \\tilde\\pi)$. Then the following bound holds:\n$$\n\\begin{align}\n\\eta\\left(\\pi\\_\\mathrm{new}\\right) &\\geq L\\_{\\pi\\_\\mathrm{old} }\\left(\\pi\\_\\mathrm{new}\\right) - \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2, \\\\\\\\\n\\mathrm{where\\ } \\epsilon&= \\max\\_{s,a}\\left\\vert A\\_\\pi(s,a)\\right\\vert \n\\end{align}\n$$\n<!--*Proof.* TBD.-->\n\n또다른 distance metric으로 아래 그림과 같은 KL divergence가 있습니다.\n(그런데 왜 하필 KL divergence로 바꿀까요? 논문의 뒤쪽에서 계산효율을 위해서 conjugate gradient method를 이용하는데 이를 위해서 바꾼게 아닐까 싶습니다. Wasserstein distance 같은 다른 방향으로 발전시킬 수도 있을 것 같습니다. Schulmann이 아마 해봤겠죠?^^a)\n\n[![kld](../../../../img/kld.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=3m34s)\n\ntotal variation divergence와 KL divergence 사이에는 다음과 같은 관계가 있습니다.\n$$\nD\\_\\mathrm{TV}(p\\parallel q)^2 \\leq D\\_\\mathrm{KL}(p\\parallel q)\n$$\n\n다음 수식을 정의합니다.\n$$\nD\\_\\mathrm{KL}^\\max(\\pi\\parallel \\tilde\\pi) = \\max_s D\\_\\mathrm{KL}\\left(\\pi(\\cdot\\vert s)\\parallel\\tilde\\pi(\\cdot\\vert s)\\right)\n$$\n\n**Theorem 1**을 이용하여 다음과 같은 수식이 성립함을 알 수 있습니다.\n$$\n\\begin{align}\n\\eta\\left(\\tilde\\pi\\right) &\\geq L\\_{\\pi}\\left(\\tilde\\pi\\right) - C\\cdot D\\_\\mathrm{KL}^\\max(\\pi, \\tilde\\pi), \\\\\\\\\n\\mathrm{where\\ } C&= \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\n\\end{align}\n$$\n\n이러한 policy imporvement bound를 기반으로 다음과 같은 approximate policy iteration 알고리듬을 고안해낼 수 있습니다.\n\n**Algorithm 1** Policy iteration algorithm guaranteeing non-decreasing expected return $\\eta$\n> Initialize $\\pi\\_0$   \n> **for** $i=0,1,2,\\ldots$ until convergence **do**   \n> $\\quad$Compute all advantage values $A\\_{\\pi\\_i}(s,a)$    \n> $\\quad$Solve the contstrained optimization problem    \n> $\\quad\\pi\\_{i+1}=\\arg\\max\\_\\pi\\left[L\\_{\\pi\\_i} - CD\\_\\mathrm{KL}^\\max\\left(\\pi\\_i,\\pi\\right)\\right]$     \n> $\\quad\\quad$where $C = 4\\epsilon\\gamma / (1-\\gamma)^2$    \n> $\\quad\\quad$and $L\\_{\\pi\\_i}\\left(\\pi\\right) = \\eta(\\pi\\_i) + \\sum\\_{s}\\rho\\_{\\pi\\_i}(s)\\sum\\_a\\pi(a\\vert s) A\\_{\\pi\\_i}(s,a)$    \n> **end for**\n\n**Algorithm 1**은 advantage를 정확하게 계산할 수 있다고 가정하고 있습니다. 이 알고리듬은  monotonical한 성능 증가($\\eta(\\pi\\_0)\\leq\\eta(\\pi\\_1)\\leq\\cdots$)를 한다는 것을 다음과 같이 보일 수 있습니다. $M\\_i(\\pi)=L\\_{\\pi\\_i}(\\pi) - CD\\_\\mathrm{KL}^\\max\\left(\\pi\\_i,\\pi\\right)$라고 합시다.\n\n$$\n\\begin{align}\n\\eta \\left(\\pi\\_{i+1}\\right) &\\geq M\\_i\\left(\\pi\\_{i+1}\\right)\\\\\\\\\n\\eta \\left(\\pi\\_{i}\\right) &= M\\_i\\left(\\pi\\_{i}\\right) \\\\\\\\\n\\eta \\left(\\pi\\_{i+1}\\right) - \\eta \\left(\\pi\\_{i}\\right) &\\geq M\\_i\\left(\\pi\\_{i+1}\\right) - M\\_i\\left(\\pi\\_{i}\\right)\n\\end{align}\n$$\n\n위 수식과 같이 매 iteration 마다 $M\\_i$를 최대화함으로써, $\\eta$가 감소하지 않는다는 것을 보장할 수 있습니다. 이와 같은 타입의 알고리듬을 [minorization-maximization (MM) algorithm](https://www.jstor.org/stable/pdf/27643496.pdf?casa_token=0qHamcl60WoAAAAA:-fkZ9JcA_nrY3-zbCUpqvPOgcAMgw7Gr96MajCZg2byHf8m5GU1KTSxyJJcBy1lPZbBTZVCjHHUXilh4k-iuwF91Wka4B5qdltC1IR2qMWk8q1FoV6__)이라고 합니다. EM 알고리듬도 이 타입의 알고리듬 중 하나입니다.\n\n**Algorithm 1**은 아래 그림과 같이 동작합니다.\n\n[![surrogate](../../../../img/surrogate.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=3m52s)\n\n\n$M\\_i$는 $\\pi\\_i$일 때 equality가 되는 $\\eta$에 대한  surrogate function입니다. TRPO는 이 surrogate function을 최대화하고 KL divergence를 penalty가 아닌 constraint로 두는 알고리듬입니다.\n\n<br><br>\n# 4. Optimization of Parameterized Policies\n\n표기의 편의를 위해 다음과 같이 notation들을 더 간략하게 정의합니다.\n\n* $\\eta(\\theta):=\\eta\\left(\\pi\\_\\theta\\right)$\n* $L\\_\\theta\\left(\\tilde\\theta\\right):=L\\_{\\pi\\_\\theta}\\left(\\pi\\_{\\tilde\\theta}\\right)$\n* $D\\_\\mathrm{KL}\\left(\\theta\\parallel\\tilde\\theta\\right):=D\\_\\mathrm{KL}\\left(\\pi\\_\\theta\\parallel\\pi_{\\tilde\\theta}\\right)$\n* $\\theta\\_\\mathrm{old}$: previous policy parameter\n\n## 4.1 Trust Region Policy Optimization\n\n이전 장의 중요 결과를 위의 notation으로 다시 표기하면 아래와 같습니다.\n\n$$\n\\eta\\left(\\theta\\right) \\geq L\\_{\\theta\\_\\mathrm{old} }\\left(\\theta\\right) - C\\cdot D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right)\n$$\n\n$\\eta$의 성능 향상을 보장하기 위해서 그것의 lower bound를 최대화할 수 있습니다.\n\n$$\n\\max\\_\\theta \\left[L\\_{\\theta\\_\\mathrm{old} }\\left(\\theta\\right) - C D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right)\\right]\n$$\n\n이 최적화 문제는 step size를 매우 작게 해야 올바른 동작을 합니다. 위에서 살펴봤듯이 first order approximation이기 때문입니다. 좀 더 큰 step size를 가질 수 있도록 이 최적화 문제를 trust region constraint를 도입하여 다음과 같이 바꿉니다.\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ }&D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right) \\leq \\delta\n\\end{align}$$\n\n이 최적화문제의 constraint는 모든 state space에 대해서 성립해야 합니다. 또한 maximum값을 매번 찾아야 합니다. state가 많은 경우 constraint의 수가 매우 많아져서 문제를 풀기 어렵게 만듭니다. constraint의 수를 줄이기 위하여 다음과 같은 avergae KL divergence를 이용하는 heuristic approximation을 취합니다. 이것이 최선의 방법은 아닐 수 있지만 실용적인 방법입니다.\n\n$$\n\\overline D\\_\\mathrm{KL}^{\\rho}\\left(\\theta\\_1, \\theta\\_2\\right):=E\\_{s\\sim\\rho}\\left[D\\_\\mathrm{KL}\\left(\\pi\\_{\\theta_1}(\\cdot\\vert s)\\parallel\\pi\\_{\\theta\\_2}(\\cdot\\vert s)\\right)\\right]\n$$\n\n이것을 기반으로 다음과 같은 최적화 문제를 풀 수 있습니다.\n\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ }&\\overline{D}\\_\\mathrm{KL}^{\\rho\\_\\mathrm{old} }\\left(\\theta\\_\\mathrm{old}, \\theta\\right) \\leq \\delta\n\\end{align}$$\n\n\n아래 그림처럼 step size에 대해서 고려할 수 있습니다.\n\n[![heuristic_approx](../../../../img/heuristic_approx.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=4m35s)\n\n<br><br>\n# 5. Sample-Based Estimation of the Objective and Constraint\n\n실용적인 알고리듬을 만들려고 하는 노력은 아직 끝나지 않았습니다. 이제 앞의 알고리듬을 sample-based estimation 즉, Monte Carlo estimation을 할 수 있도록 바꿔보겠습니다. sampling을 편하게 할 수 있도록 아래와 같이 바꿔줍니다.\n\n* $\\sum\\_s \\rho\\_{\\theta\\_\\mathrm{old} }(s)[\\ldots]\\rightarrow\\frac{1}{1-\\gamma}E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} }}[\\ldots]$\n* $A\\_{\\theta\\_\\mathrm{old} }\\rightarrow Q\\_{\\theta\\_\\mathrm{old} }$\n* $\\sum\\_a\\pi\\_{\\theta\\_\\mathrm{old} }(a\\vert s)A\\_{\\theta\\_\\mathrm{old} } \\rightarrow E\\_{a\\sim q}\\left[\\frac{\\pi\\_\\theta(a\\vert s)}{q(a\\vert s)}A\\_{\\theta\\_\\mathrm{old} }\\right]$\n\n이러한 변화를 그림처럼 도식화할 수 있습니다.\n\n[![sample-based](../../../../img/sample-based.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=5m31s)\n\n한 가지 짚고 넘어가야 할 점은 action sampling을 할 때 importance sampling을 사용한다는 것입니다.\n\n[![importance_sampling](../../../../img/importance_sampling.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=5m53s)\n\n바뀐 최적화문제는 아래와 같습니다.\n\n- **Equation (14).**\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi\\_\\theta(a\\vert s)}{q(a\\vert s)}Q\\_{\\theta\\_\\mathrm{old} }(s,a)\\right] \\\\\\\\\n\\mathrm{s.t.\\ }&E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} }}\\left[D\\_\\mathrm{KL}\\left(\\pi\\_{\\theta\\_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi\\_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta\n\\end{align}\n$$\n\n이 때 sampling하는 두 가지 방법이 있습니다.\n\n<br>\n## 5.1 Single Path\n\n*single path*는 개별 trajectory들을 이용하는 방법입니다.\n\n[![single](../../../../img/single.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=6m15s)\n\n<br>\n## 5.2 Vine\n\n*vine*은 한 state에서 rollout을 이용하여 여러 action을 수행하는 방법입니다.\n\n[![vine1](../../../../img/vine1.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=6m32s)\n\n[![vine2](../../../../img/vine2.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=6m49s)\n\nestimation의 variance를 낮출 수 있지만 계산량이 많고 한 state에서 여러 action을 수행할 수 있어야 하기 때문에 현실적인 문제에 적용하기에는 어려움이 있습니다.\n\n\n<br><br>\n# 6. Practical Algorithm\n\n앞서 single-path, vine 샘플링을 사용하는 두가지 방식의 policy optimization 알고리즘을 살펴봤습니다. 실용적인 알고리듬은 아래의 과정을 반복해서 수행합니다.\n\n1) Q-values의 몬테카를로 추정을 통해 state-action 쌍 집합을 single path 또는 vine 과정을 통해 수집함.\n\n2) 샘플 평균으로, (14)식의 목적함수와 제약식을 추정함\n + **Equation (14).**\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi\\_\\theta(a\\vert s)}{q(a\\vert s)}Q\\_{\\theta\\_\\mathrm{old} }(s,a)\\right] \\\\\\\\\n\\mathrm{s.t.\\ }&E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} }}\\left[D\\_\\mathrm{KL}\\left(\\pi\\_{\\theta\\_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi\\_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta\n\\end{align}\n$$\n\n3) policy parameter vector인 $\\theta$를 업데이트 하면서 제약조건이 있는 최적화 문제를 근사적으로 풂. 본 논문에서는 gradient를 직접 계산하는 것보다는 약간 더 계산량이 있는 line search와 conjugate gradient algorithm을 사용했습니다.\n\n3)에 대해서,  gradient의 covariance matrix를 사용하지 않고 KL divergence의 Hessian을 해석적으로 계산하여 Fisher Information Matrix를 구성했습니다.\n\n다시말해서\n\n$\\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial}{\\partial \\theta_i} \\log \\pi_{\\theta}(a_n\\vert s_n) \\frac{\\partial}{\\partial \\theta_j} \\log \\pi_{\\theta}(a_n\\vert s_n)$ 대신 $\\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_j} D_{KL}(\\pi_{\\theta_{old} }(\\cdot \\vert  s_n) \\vert \\vert  \\pi_{\\theta}(\\cdot \\vert  s_n))$를 계산한 것입니다.\n\n이 analytic estimator는 Hessian이나 trajectories의 모든 gradient를 저장하지 않아도 되기 때문에 대규모 환경을 고려할 경우 계산 상 이점이 있습니다. 좀 더 자세하게 설명한 Appendix C 부분을 살펴보겠습니다.\n\n\n<br>\n## Appendix C. Efficiently Solving the Trust-Region Constrained Optimization Problem\n\n우리가 풀고자 하는 식은 다음과 같습니다.\n\n$$\n\\begin{align}\n\\max\\quad &L(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ } &\\overline{D}\\_{KL}(\\theta\\_\\mathrm{old}, \\theta) \\leq \\delta\n\\end{align}\n$$\n\n이 최적화 문제를 풀기 위하여 우리는 gradient 기법을 이용할 것입니다. gradient 기법은 결국 update할 방향과 크기를 결정하는 문제로 생각할 수 있습니다. 이에 따라 아래와 같은 두가지 과정으로 진행합니다.\n\n1) 탐색 방향을 계산, 목적함수의 first-order approximation(선형근사)와 제약식의 quadratic approximation(2차 근사).\n\n2) 이동거리 계산을 위해 해당 방향으로 line search 수행.\n\n탐색 방향은 $Ax = g$ 수식을 근사적으로 풀어서 구합니다. 여기서 $A$ 는 Fisher Information Matrix이고, 이것은 KL divergence 제약식의 2차 근사 $\\overline{D}\\_\\mathrm{KL} (\\theta\\_\\mathrm{old}, \\theta) \\approx \\frac{1}{2}(\\theta - \\theta\\_\\mathrm{old})^T A(\\theta - \\theta\\_\\mathrm{old})$를 푸는 것입니다. 여기서 $A\\_{ij} = \\frac{\\partial}{\\partial \\theta\\_i} \\frac{\\partial}{\\partial \\theta\\_j} \\overline{D}\\_\\mathrm{KL}(\\theta\\_\\mathrm{old}, \\theta)$입니다.\n\n논문에서 이 부분에 대한 설명이 약간 부족해서 좀 더 내용을 추가하자면, $\\frac{1}{2}x^T A x - x^T g + b = 0$과 같은 quadratic equation의 해는 1차 derivative를 0으로 만드는 $x$를 찾음으로써 알 수 있습니다. 위에서 우리는 목적함수를 1차 근사시키고 제약조건은 2차 근사를 시킴으로써 quadratic equation 형태로 변형했기 때문에 같은 방식으로 풀 수 있습니다. 따라서 $g$는 1차근사시킨 목적함수의 계수가 될 것입니다.\n\n대규모 문제에서 $A$ 혹은 $A^{-1}$ 을 계산하는 것은 엄청난 계산량이 필요합니다. 하지만 conjugate gradient algorithm을 잘 이용하면 전체 $A$행렬(FIM)을 계산하지 않아도 $Ax=g$ 를 근사적으로 해결할 수 있게 해줍니다.\n\n탐색 방향 $s \\approx A^{-1}g$과 더불어 최대스텝길이(step size) $\\beta$도 계산할 필요가 있습니다.\n\n즉, $\\delta = \\overline{D}\\_\\mathrm{KL} \\approx \\frac{1}{2}(\\beta s)^T A(\\beta s) = \\frac{1}{2}\\beta^2s^TAs$ 이고, \n\n$\\beta = \\sqrt{2\\delta / s^T As}$가 됩니다. 이때 $\\delta$는 KL divergence의 boundary term 입니다.\n\n$s^T As$ 항은 Hessian-vector product로 계산할 수 있고, conjugate gradient 과정에서 계산됩니다.\n\n마지막으로, surrogate objective와 KL divergence 제약식을 위해 다음과 같은 함수를 정의하고 line search를 사용합니다.\n\n$$\nL\\_{\\theta\\_\\mathrm{old} }(\\theta) - \\chi [\\overline{D}\\_{KL}(\\theta\\_\\mathrm{old}, \\theta) \\leq \\delta]\n$$\n\n$\\chi[\\cdot]$ 함수는 []안의 조건이 맞으면 0이고 틀리면 무한으로 발산하는 함수입니다.\n\n위에서 계산한 $\\beta$ 의 최대 값부터 시작해서 목적함수가 개선될때까지 exponential하게 줄여 나갑니다. line search가 없었다면 이 알고리즘은 매우 큰 스텝으로 계산될 것이고 심각한 성능 저하가 발생할 것입니다.\n\n(여기서 살짝 예고를 하자면, 나중에 이 KL 기반 기법을 개선한 더 단순한 방법이 제안됩니다. PPO라고...)\n\n\n<br><br>\n# 7. Connections with Prior Work\n\nNatural Policy Gradient는 $L$의 선형 근사와 $\\overline D\\_\\mathrm{KL}$ 제약식을 2차근사 하는 아래의 식의 special case입니다.\n\n- Equation (17).\n\n$L\\_{\\pi}(\\tilde\\pi) = \\eta(\\pi) + \\underset{s}{\\sum}\\rho\\_{\\pi}(s) \\underset{a}{\\sum}\\tilde\\pi(a\\vert s)A\\_\\pi(s,a)$\n\n$\\underset{\\theta}{\\max}\\quad \\[\\nabla\\_{\\theta}L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\ \\rvert\\_{\\theta=\\theta\\_\\mathrm{old} } \\cdot (\\theta - \\theta\\_\\mathrm{old})]$\n\n$\\mathrm{s.t.}\\quad\\frac{1}{2}(\\theta\\_\\mathrm{old} - \\theta)^{T} A(\\theta\\_\\mathrm{old})(\\theta\\_\\mathrm{old}-\\theta) \\leq \\delta$\n\n$\\mathrm{where\\ }A(\\theta_{old})_{ij} = $\n\n$\\Large \\frac{\\partial}{\\partial \\theta\\_{i} } \\frac{\\partial}{\\partial \\theta\\_{j} }\nE\\_{s \\sim \\rho\\_{\\pi} }[D\\_\\mathrm{KL}(\\pi(\\cdot\\rvert s, \\theta\\_\\mathrm{old})  \\rvert\\rvert \\pi(\\cdot \\rvert s, \\theta))] \\rvert\\_{\\theta=\\theta\\_\\mathrm{old} }$\n\n업데이트 식은 다음과 같습니다.\n\n$\\theta\\_\\mathrm{new} = \\theta\\_\\mathrm{old} +  \\color{Red}{\\frac{1}{\\lambda} } A(\\theta\\_\\mathrm{old})^{-1}\\nabla\\_{\\theta}L(\\theta) \\rvert\\_{\\theta=\\theta\\_\\mathrm{old} }$\n\n여기서 step size인 $\\frac{1}{\\lambda}$ 일반적으로 알고리즘의 파라미터로 취급되지만 TRPO는 각 업데이트 마다 제약식으로 사용한다는 점이 다릅니다. 사소한 차이처럼 보이지만, 큰 차원을 다루는 실험에서 성능을 크게 향상시켰습니다.\n\n또한 $L^2$ 제약식(혹은 페널티) 를 사용하면 표준 Policy Gradient 업데이트 식을 얻었습니다.\n\n- Equation (18).\n\n$\\underset{\\theta}{\\max}\\quad[\\nabla\\_{\\theta} L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\rvert\\_{\\theta=\\theta\\_\\mathrm{old} } \\cdot (\\theta - \\theta\\_\\mathrm{old})]$\n\n$\\mathrm{s.t.}\\quad \\frac{1}{2} \\vert\\vert \\theta - \\theta\\_\\mathrm{old} \\vert\\vert^2 \\leq \\delta$\n\n\n\n$L\\_{\\pi}(\\tilde\\pi) = \\eta(\\pi) + \\underset{s}{\\sum}\\rho\\_{\\pi}(s) \\underset{a}{\\sum}\\tilde\\pi(a\\vert s)A\\_\\pi(s,a) $를 이용해서 제약조건없이 $\\underset{\\pi}{\\max}\\quad L\\_{\\pi\\_\\mathrm{old} }(\\pi)$를 풀면\nPolicy Iteration update를 하는 것과 같습니다.\n\n\n<br><br>\n# 8. Experiments\n\n- 다음과 같은 3 가지 궁금증을 해결하기 위하여 실험을 설계하였습니다.\n\n    1. Single-path 와 vine 의 성능면에서 특징을 알고싶음.\n\n    2. Fixed penalty coefficient(NPG) 보다 fixed KL divergence를 사용하는 것(TRPO)으로    \n    변경한 것이 얼마나 뚜렷한 차이를 보일지, 성능 면에서 어떤 영향을 미치는지 알고싶음.\n\n    3. TRPO가 큰 스케일의 문제를 해결할 수 있는지, 기존에 연구/적용되어왔던 방법들과\n    성능, 계산시간, 샘플 복잡도 면에서 비교하고 싶음.\n\n\n- 1, 2의 궁금증에 대해서 실험 환경에 single path, vine을 비롯한 여러 사전방법들을 함께 실험하였습니다.\n\n- 3에 대해선, robotic locomotion과 Atari game에 TRPO를 실험하였습니다.\n\n\n<br>\n## 8.1 Simulated Robotic Locomotion\n\n- MuJoCo 시뮬레이터를 활용해 로봇 locomotion 실험 진행하였습니다.\n- Robot State: positions and velocities, joint torques\n\n![](https://i.imgur.com/bs5ATS3.png)\n\n- 수영, 점프, 걷기 행동을 학습시키고자 합니다.\n\n    - 수영: 10 dimensional state space , reward: $r(x,u)=v_x - 10^{-5}\\vert \\vert u\\vert \\vert ^2$, quadratic penalty.\n\n    - 점프: 12 dimensional state space, episode의 끝은 높이와 각도 threshol로 점프를 판단, non-terminal state에 대해 +1 보상 추가.\n\n    - 걷기: 18 dimensional state space, 뜀뛰는 듯한 보법(점프에이전트)보다 부드러운 보법을 유도하기위해 페널티를 추가함.\n\n**Detailed Experiment Setup and used parameters, used network model**\n![](https://i.imgur.com/zgnsbw6.png)\n\n![](https://i.imgur.com/FqdWC53.png)\n\nequation (12) : $\\max\\quad L\\_{\\theta\\_\\mathrm{old}(\\theta)}, \\ \\ \\mathrm{s.t.\\ }\\overline{D}\\_\\mathrm{KL}^{\\rho\\_{\\theta\\_\\mathrm{old} }}(\\theta\\_\\mathrm{old}, \\theta) \\leq \\delta$\n\n이 실험에서 $\\delta = 0.01$입니다.\n\n- 비교에 사용된 모델들\n\n    - TRPO Single-path\n    - TRPO vine\n    - CEM(cross-entropy method)\n    - CMA(covariance matrix adaptation)\n    - NPG (Lagrange Multiplier penalty coefficien를 사용하는 것이 차이점)\n    - Empirical FIM\n    - maximum KL(cartpole)\n\n![](https://i.imgur.com/wSqolvS.png)\n\n- 제안한 single path, vine 모두 task를 잘 학습하는 것을 확인할 수 있습니다.\n\n- 페널티를 고정하는 것보다, KL divergenc를 제약하는 것이 step size를 선택하는 부분에서 Robust합니다.\n\n- CEM/CMA는 derivative free 알고리즘으로, 샘플 복잡도가 네트워크 파라미터수 만큼 확장되어 high dimension 문제에서는 속도와 성능이 약한 모습을 보입니다.\n\n- maxKL은 제안방법보다 느린 학습성능을 보입니다.\n\nTRPO video link: https://www.youtube.com/watch?v=jeid0wIrSn4\n\n- 사전지식이 적은 상태에서 여러 액션을 학습하는 실험으로 TRPO의 성능을 보입니다.\n\n- 지금까지의 균형이나 걸음 같은 개념을 명시적으로 인코딩한 robotic locomotion에서의 사전 연구들과 차이를 보였습니다.\n\n<br>\n## 8.2 Playing Games from Images\n\n- 부분관찰가능하며 복잡한 observation인 환경에서 TRPO를 평가하기위해 raw image를 입력으로 하는 아타리게임 환경에서 실험하였습니다.\n\n* Challenging point :\n    - High dimensional space\n    - delayed reward\n    - non-stationary image (Enduro는 배경 이미지가 바뀌고, 반전처리가 됨)\n    - complex action sequence (Q*bert는 21개의 다른 발판에서 점프 해야함)\n\n- ALE(Arcade Learning Environment) 에서 테스트\n- DQN과 전처리 방식은 동일\n    210 x 160 pixel -> gray-scaling & down-sampling -> 110 x 84  -> crop -> 84 x 84\n\n- 2 convolution layer ( 16 channels) , stride 2, FC layer with 20 units\n\n![](https://i.imgur.com/NJBC69d.png) \n\n![](https://i.imgur.com/wTe1OEW.png)\n\n![](https://i.imgur.com/j22VtNQ.png)\n\n- 30시간 동안 500 iteration 학습을 하였습니다.\n\n- DQN과 MCST를 사용한 모델(UCC-I), 사람이 플레이한 것(Human) 과의 비교하였습니다.\n\n- 이전 방법들에 비해 압도적인 점수를 기록하진 못했으나, 다양한 게임에서 적절한 게임 기록하였습니다.\n\n- 이전 방법들과 다른 점은, 특정 task에 초점을 맞춰 설계하지않았는데 robotics나 game playing Task에도 적절한 성능을 내는 점에서 **TRPO의 일반화 성능**을 확인하였습니다.\n\n<br>\n# 9. Discussion\n\n- Trust Region Policy Optimization 을 제안하였습니다.\n\n- KL Divergence 페널티로 $\\eta(\\pi)$를 최적화하는 알고리즘이 monotonically improve 함을 증명하였습니다.\n\n- Locomotion 도메인에서 여러 행동(수영, 걷기, 점프)을 제어하는 controller를 학습하였습니다.\n\n- Robotics와 게임 실험을 결합해, 시각정보와 센서데이터를 사용하는 로봇제어 정책을 학습시킬 수 있는 가능성을 보았습니다.\n\n- 샘플의 복잡도를 상당히 줄일 수 있어 실제상황에 적용가능성을 보았습니다.\n\n<br><br>\n# END","slug":"trpo","published":1,"updated":"2018-07-24T06:42:49.773Z","_id":"cjjr006cr0001ie15li5wd6lu","comments":1,"layout":"post","photos":[],"link":"","content":"<center> <img src=\"https://www.dropbox.com/s/o7cjcn0e17mpizr/Screen%20Shot%202018-07-18%20at%201.14.22%20AM.png?dl=1\" width=\"700\"> </center>\n\n<p>논문 저자 : John Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, Pieter Abbeel<br>논문 링크 : <a href=\"https://arxiv.org/pdf/1502.05477.pdf\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/pdf/1502.05477.pdf</a><br>Proceeding : International Conference on Machine Learning (ICML) 2015<br>정리 : 공민서, 김동민</p>\n<hr>\n<h1 id=\"1-들어가며…\"><a href=\"#1-들어가며…\" class=\"headerlink\" title=\"1. 들어가며…\"></a>1. 들어가며…</h1><p>Trust region policy optimization (TRPO)는 상당히 우수한 성능을 보여주는 policy gradient 기법으로 알려져 있습니다. 높은 차원의 action space를 가진 robot locomotion부터 action은 적지만 화면을 그대로 처리하여 플레이하기 때문에 control parameter가 매우 많은 Atari game까지 각 application에 세부적으로 hyperparameter들을 특화시키지 않아도 두루두루 좋은 성능을 나타내기 때문에 일반화성능이 매우 좋은 기법입니다. 이 TRPO에 대해서 알아보겠습니다.</p>\n<p>※TRPO를 매우 재밌게 설명한 Crazymuse AI의 <a href=\"https://www.youtube.com/watch?v=CKaN5PgkSBc&amp;t=90s\" target=\"_blank\" rel=\"noopener\">Youtube video</a>에서 일부 그림을 차용했습니다. 이 비디오를 시청하시는 것을 강력하게 추천합니다!</p>\n<p><br></p>\n<h2 id=\"1-1-TRPO-흐름-잡기\"><a href=\"#1-1-TRPO-흐름-잡기\" class=\"headerlink\" title=\"1.1 TRPO 흐름 잡기\"></a>1.1 TRPO 흐름 잡기</h2><p>TRPO 논문은 많은 수식이 등장하여 이 수식들을 따라가다보면 큰 그림을 놓칠 수도 있습니다. 세부적인 내용을 살펴보기 전에 기존 연구에서 출발해서 TRPO로 어떻게 발전해나가는지 간략하게 살펴보겠습니다.</p>\n<h3 id=\"1-1-1-Original-Problem\"><a href=\"#1-1-1-Original-Problem\" class=\"headerlink\" title=\"1.1.1 Original Problem\"></a>1.1.1 Original Problem</h3><p>$$\\max_\\pi \\eta(\\pi)$$</p>\n<p>모든 강화학습이 그렇듯이 expected discounted reward를 최대화하는 policy를 찾는 문제로부터 출발합니다.</p>\n<h3 id=\"1-1-2-Conservative-policy-iteration\"><a href=\"#1-1-2-Conservative-policy-iteration\" class=\"headerlink\" title=\"1.1.2 Conservative policy iteration\"></a>1.1.2 Conservative policy iteration</h3><p>$$\\max L_\\pi(\\tilde\\pi) = \\eta(\\pi) + \\sum_s \\rho_\\pi(s)\\sum_a\\tilde\\pi(a\\vert s)A_\\pi(s,a)$$</p>\n<p>$\\eta(\\pi)$를 바로 최대화하는 것은 많은 경우 어렵습니다. $\\eta(\\pi)$의 성능향상을 보장하면서 policy를 update하는 conservative policy iteration 기법이 <a href=\"http://www.cs.cmu.edu/~./jcl/papers/aoarl/Final.pdf\" target=\"_blank\" rel=\"noopener\">Kakade와 Langford</a>에 의하여 제안되었습니다. 이 기법을 이용하면 policy update가 성능을 향상시키는지는 못하더라도 최소한 성능을 악화시키지는 않는다는 것이 이론적으로 보장됩니다.</p>\n<h3 id=\"1-1-3-Theorem-1-of-TRPO\"><a href=\"#1-1-3-Theorem-1-of-TRPO\" class=\"headerlink\" title=\"1.1.3 Theorem 1 of TRPO\"></a>1.1.3 Theorem 1 of TRPO</h3><p>$$\\max L_{\\pi_\\mathrm{old} }\\left(\\pi_\\mathrm{new}\\right) - \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2,\\quad\\left(\\alpha=D_\\mathrm{TV}^\\max\\left(\\pi_\\mathrm{old},\\pi_\\mathrm{new}\\right)\\right)$$</p>\n<p>기존의 conservative policy iteration은 과거 policy와 새로운 policy를 섞어서 사용해서 실용적이지 않다는 단점이 있었는데 이것을 보완하여 온전히 새로운 policy만으로 update할 수 있는 기법을 제안합니다. </p>\n<h3 id=\"1-1-4-KL-divergence-version-of-Theorem-1\"><a href=\"#1-1-4-KL-divergence-version-of-Theorem-1\" class=\"headerlink\" title=\"1.1.4 KL divergence version of Theorem 1\"></a>1.1.4 KL divergence version of Theorem 1</h3><p>$$\\max L_{\\pi}\\left(\\tilde\\pi\\right) - C\\cdot D_\\mathrm{KL}^\\max\\left(\\pi,\\tilde\\pi\\right),\\quad\\left(C = \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\right)$$</p>\n<p>distance metric을 KL divergence로 바꿀 수 있습니다.</p>\n<h3 id=\"1-1-5-Using-parameterized-policy\"><a href=\"#1-1-5-Using-parameterized-policy\" class=\"headerlink\" title=\"1.1.5 Using parameterized policy\"></a>1.1.5 Using parameterized policy</h3><p>$$\\max_\\theta L_{\\theta_\\mathrm{old} }(\\theta) - C\\cdot D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right)$$</p>\n<p>최적화문제를 더욱 편리하게 풀 수 있도록 낮은 dimension을 가지는 parameter들로 parameterized된 policy를 사용할 수 있습니다.</p>\n<h3 id=\"1-1-6-Trust-region-constraint\"><a href=\"#1-1-6-Trust-region-constraint\" class=\"headerlink\" title=\"1.1.6 Trust region constraint\"></a>1.1.6 Trust region constraint</h3><p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;L_{\\theta_\\mathrm{old} }(\\theta) \\\\<br>\\mathrm{s.t.\\ }&amp;D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right) \\leq \\delta<br>\\end{align}$$</p>\n<p>1.1.5까지는 아직 conservative policy iteration을 약간 변형시킨 것입니다. policy를 update할 때 지나치게 많이 변하는 것을 방지하기 위하여 <a href=\"https://en.wikipedia.org/wiki/Trust_region\" target=\"_blank\" rel=\"noopener\">trust region</a>을 constraint로 설정할 수 있습니다. 이 아이디어로 인해서 TRPO라는 명칭을 가지게 됩니다.</p>\n<h3 id=\"1-1-7-Heuristic-approximation\"><a href=\"#1-1-7-Heuristic-approximation\" class=\"headerlink\" title=\"1.1.7 Heuristic approximation\"></a>1.1.7 Heuristic approximation</h3><p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;L_{\\theta_\\mathrm{old} }(\\theta) \\\\<br>\\mathrm{s.t.\\ }&amp;\\overline{D}_\\mathrm{KL}^{\\rho_\\mathrm{old} }\\left(\\theta_\\mathrm{old}, \\theta\\right) \\leq \\delta<br>\\end{align}$$</p>\n<p>사실 1.1.6의 constraint는 모든 state에 대해서 성립해야 하기 때문에 문제를 매우 어렵게 만듭니다. 이것을 좀 더 다루기 쉽게 state distribution에 대한 평균을 취한 것으로 변형합니다.</p>\n<h3 id=\"1-1-8-Monte-Carlo-simulation\"><a href=\"#1-1-8-Monte-Carlo-simulation\" class=\"headerlink\" title=\"1.1.8 Monte Carlo simulation\"></a>1.1.8 Monte Carlo simulation</h3><p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}Q_{\\theta_\\mathrm{old} }(s,a)\\right] \\\\<br>\\mathrm{s.t.\\ }&amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} }}\\left[D_\\mathrm{KL}\\left(\\pi_{\\theta_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta<br>\\end{align}$$</p>\n<p>Sampling을 통한 계산이 가능하도록 식을 다시 표현할 수 있습니다. </p>\n<h3 id=\"1-1-9-Efficiently-solving-TRPO\"><a href=\"#1-1-9-Efficiently-solving-TRPO\" class=\"headerlink\" title=\"1.1.9 Efficiently solving TRPO\"></a>1.1.9 Efficiently solving TRPO</h3><p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;\\nabla_\\theta \\left. L_{\\theta_\\mathrm{old} }(\\theta)\\right\\vert _{\\theta=\\theta_\\mathrm{old} } \\left(\\theta - \\theta_\\mathrm{old}\\right) \\\\<br>\\mathrm{s.t.\\ }&amp;\\frac{1}{2}\\left(\\theta_\\mathrm{old} - \\theta \\right)^T A\\left(\\theta_\\mathrm{old}\\right)\\left(\\theta_\\mathrm{old} - \\theta \\right) \\leq \\delta \\\\<br>&amp;A_{ij} = \\frac{\\partial}{\\partial \\theta_i}\\frac{\\partial}{\\partial \\theta_j}\\overline{D}_\\mathrm{KL}^{\\rho_\\mathrm{old} }\\left(\\theta_\\mathrm{old}, \\theta\\right)<br>\\end{align}$$</p>\n<p>문제를 효율적으로 풀기 위하여 approximation을 적용할 수 있습니다. objective function은 first order approximation, constraint는 quadratic approximation을 취하면 효율적으로 문제를 풀 수 있는 형태로 바뀌는데 이것을 natural gradient로 풀 수도 있고 conjugate gradient로 풀 수도 있습니다.</p>\n<p>TRPO는 이렇게 다양한 방법으로 문제를 변형한 것입니다! 이제 좀 더 자세히 살펴보겠습니다.</p>\n<p><br><br></p>\n<h1 id=\"2-Preliminaries\"><a href=\"#2-Preliminaries\" class=\"headerlink\" title=\"2. Preliminaries\"></a>2. Preliminaries</h1><p>다음과 같은 파라미터를 가지는 infinite-horizon discounted Markov decision process (MDP)를 고려합니다. </p>\n<ul>\n<li>$\\mathcal{S}$: finite set of states</li>\n<li>$\\mathcal{A}$: finite set of actions</li>\n<li>$P: \\mathcal{S}\\times\\mathcal{A}\\rightarrow\\mathbb{R}$: transition probability distribution</li>\n<li>$r: \\mathcal{S}\\rightarrow\\mathbb{R}$: reward function</li>\n<li>$\\rho_0:\\mathcal{S}\\rightarrow\\mathbb{R}$: distribution of the initial state $s_0$</li>\n<li>$\\gamma\\in(0,1)$: discount factor</li>\n<li>$\\eta(\\pi)=E_{s_0,a_0,\\ldots}\\left[\\sum_{t=0}^\\infty\\gamma^t r\\left(s_t\\right)\\right]$, where $s_0\\sim\\rho_0\\left(s_0\\right), a_t\\sim\\pi\\left(\\left.a_t \\right\\vert s_t\\right), s_{t+1}\\sim P\\left(\\left.s_{t+1}\\right\\vert s_t,a_t\\right)$</li>\n<li>$Q_\\pi\\left(s_t,a_t\\right)=E_{ { \\color{red}{s_{t+1}} },a_{t+1},\\ldots}\\left[\\sum_{l=0}^\\infty \\gamma^l r\\left(s_{t+l}\\right)\\right]$: action value function</li>\n<li>$V_\\pi\\left(s_t\\right)=E_{ { \\color{red}{a_t} }, s_{t+1},a_{t+1},\\ldots}\\left[\\sum_{l=0}^\\infty \\gamma^l r\\left(s_{t+l}\\right)\\right]$: value function (action value function과 expectation의 첨자가 다른 부분을 유의하세요.)</li>\n<li>$A_\\pi(s,a) = Q_\\pi(s,a) - V_\\pi(s)$: advantage function</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-1-Useful-identity-Kakade-amp-Langford-2002\"><a href=\"#2-1-Useful-identity-Kakade-amp-Langford-2002\" class=\"headerlink\" title=\"2.1 Useful identity [Kakade &amp; Langford 2002]\"></a>2.1 Useful identity [Kakade &amp; Langford 2002]</h2><p>우리의 목표는 $\\eta(\\pi)$가 최대화되도록 만드는 것입니다. 하지만 $\\pi$의 변화에 따라 $\\eta$가 어떻게 변하는지 알아내는 것도 쉽지 않습니다. $\\pi$는 기존의 policy이고 $\\tilde\\pi$는 새로운 policy를 나타낸다고 할 때 $\\eta$와 policy update 사이에 다음과 같은 관계가 있다는 것이 밝혀졌습니다.</p>\n<p><strong>Lemma 1.</strong> $\\eta\\left(\\tilde\\pi\\right) = \\eta(\\pi) + E_{s_{0},a_{0},\\ldots\\sim\\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t A_\\pi\\left(s_t,a_t\\right)\\right]$</p>\n<p><em>Proof.</em> $A_\\pi(s,a)=E_{s’\\sim P\\left(s’\\vert s,a\\right)}\\left[r(s)+\\gamma V_\\pi\\left(s’\\right)-V_\\pi(s)\\right]$로 다시 표현할 수 있습니다. 표기의 편의를 위하여 $\\tau:=\\left(s_0,a_0,s_1,a_1,\\ldots\\right)$를 정의하겠습니다. 다음과 같은 수식 전개가 가능합니다.<br>$$<br>\\begin{align}<br>&amp;E_{\\tau\\vert \\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t A_\\pi\\left(s_t,a_t\\right)\\right] \\\\<br> &amp;= E_{\\tau\\vert \\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t \\left(r(s_t)+\\gamma V_\\pi\\left(s_{t+1}\\right)-V_\\pi(s_t)\\right)\\right] \\\\<br>&amp;= E_{\\tau\\vert \\tilde\\pi}\\left[\\left(\\sum_{t=0}^\\infty\\gamma^t r(s_t)\\right)+\\gamma V_\\pi\\left(s_{1}\\right)-V_\\pi(s_0)+\\gamma^2 V_\\pi\\left(s_{2}\\right)-\\gamma V_\\pi(s_1)+\\gamma^3 V_\\pi\\left(s_{3}\\right)-\\gamma^2 V_\\pi(s_2)+\\cdots\\right] \\\\<br>&amp;= E_{\\tau\\vert \\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t r(s_t)+\\color{red}{\\gamma V_\\pi\\left(s_{1}\\right)}-V_\\pi(s_0)+\\color{red}{\\gamma^2 V_\\pi\\left(s_{2}\\right)}-\\color{red}{\\gamma V_\\pi(s_1)}+\\cdots\\right] \\\\<br>&amp;= E_{\\tau\\vert \\tilde\\pi}\\left[-V_\\pi(s_0)+\\sum_{t=0}^\\infty\\gamma^t r(s_t)\\right] \\\\<br>&amp;\\overset{\\underset{\\mathrm{(a)} }{} }{=} -E_{\\color{red}{s_0}}\\left[V_\\pi(s_0)\\right]+E_{\\tau\\vert \\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t r(s_t)\\right] \\\\<br>&amp;=-\\eta(\\pi) + \\eta\\left(\\tilde\\pi\\right)\\\\<br>&amp;\\quad\\therefore \\eta\\left(\\tilde\\pi\\right) = \\eta(\\pi) + E_{s_{0},a_{0},\\ldots\\sim\\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t A_\\pi\\left(s_t,a_t\\right)\\right]<br>\\end{align}<br>$$<br>위의 전개의 (a) 부분은 $V_\\pi\\left(s_0\\right)=E_{a_0,a_1,s_1,\\ldots}\\left[\\sum_{t=0}^\\infty\\gamma^l r\\left(s_l\\right)\\right]$이므로 $\\tau$ 중 많은 부분들이 이미 expectation이 취해진 값이므로 무시됩니다. 오직 $s_0$만 expectation이 취해지지 않았기 때문에 남아있습니다.</p>\n<p><strong>Lemma 1</strong>은 새로운 policy와 기존의 policy 사이의 관계를 규정합니다. 다음과 같은 식을 정의하고 이것을 이용해서 <strong>Lemma 1</strong>을 변형시켜 봅시다.</p>\n<ul>\n<li>$\\rho_\\pi(s) = P\\left(s_0=s\\right) + \\gamma P\\left(s_1=s\\right)\\ + \\gamma^2 P\\left(s_2=s\\right) + \\cdots $: (unnormalized) discounted visitation frequencies</li>\n</ul>\n<p>$$<br>\\begin{align}<br>\\eta\\left(\\tilde\\pi\\right) &amp;= \\eta(\\pi) + \\sum_{t=0}^\\infty\\sum_{s}P\\left(\\left.s_t=s\\right\\vert \\tilde\\pi\\right)\\sum_a\\tilde\\pi(a\\vert s)\\gamma^t A_\\pi(s,a) \\\\<br>&amp;= \\eta(\\pi) + \\sum_{s}{\\color{red}{\\sum_{t=0}^\\infty \\gamma^t P\\left(\\left.s_t=s\\right\\vert \\tilde\\pi\\right)} } \\sum_a\\tilde\\pi(a\\vert s) A_\\pi(s,a) \\\\<br>&amp;= \\eta(\\pi) + \\sum_{s}{\\color{red}{\\rho_\\tilde\\pi(s)} }\\sum_a\\tilde\\pi(a\\vert s) A_\\pi(s,a) \\\\<br>\\end{align}<br>$$</p>\n<p>이 수식의 의미가 무엇일까요? 만약 $\\sum_a\\tilde\\pi(a\\vert s) A_\\pi(s,a) \\geq 0$이라면 $\\eta(\\tilde\\pi)$는 항상 $\\eta(\\pi)$보다 큽니다. 즉, policy를 업데이트함으로써 항상 개선됩니다. 즉, 이 수식을 통해서 항상 더 좋은 성능을 내는 policy update가 가져야 할 특징을 알 수 있습니다. 다음과 같은 deterministic policy가 있다고 합시다.</p>\n<p>$$<br>\\tilde\\pi(s) = \\arg\\max_a A_\\pi(s,a)<br>$$</p>\n<p>이 policy는 적어도 하나의 state-action pair에서 0보다 큰 값을 가지는 advantage가 있고 그 때의 확률이 0이 아니라면 항상 성능을 개선시킵니다. 어려운 점은 policy를 바꾸면 $\\rho$도 바뀐다는 점입니다. </p>\n<p>다음 그림을 봅시다. Starting Point에서 여러가지 경로를 거쳐서 Destination으로 갈 수 있습니다. 다른 policy를 이용하는 것은 다른 경로를 이용한 것입니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=16s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/policy_change.png\" alt=\"policy_change\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>이 때 다른 policy를 이용함에 따라 state visitation frequency도 변하게 됩니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=36s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/state_visitation_change.png\" alt=\"state_visitation_change\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>이로 인해서 policy를 최적화하는 것은 어려워집니다. 그래서 이러한 변화를 무시하는 다음과 같은 approximation을 취합니다.</p>\n<p>$$<br>\\begin{align}<br>L_\\pi\\left(\\tilde\\pi\\right) &amp;= \\eta(\\pi) + \\sum_{s}{\\color{red}{\\rho_\\pi(s)} } \\sum_a\\tilde\\pi(a\\vert s) A_\\pi(s,a)<br>\\end{align}<br>$$</p>\n<p>policy가 바뀌었음에도 이전의 state distribution을 계속 이용하는 것입니다. 이것은 policy의 변화가 크지 않다면 어느 정도 허용될 수 있을 것입니다. 그렇지만 얼마나 많은 변화가 허용될까요? 이것을 정하기 위해서 이용하는 것이 trust region입니다.</p>\n<p><br></p>\n<h2 id=\"2-2-Conservative-Policy-Iteration\"><a href=\"#2-2-Conservative-Policy-Iteration\" class=\"headerlink\" title=\"2.2 Conservative Policy Iteration\"></a>2.2 Conservative Policy Iteration</h2><p>policy의 변화를 다루기 용이하게 하기 위해서 policy를 다음과 같이 파라미터를 이용해서 표현합시다.</p>\n<ul>\n<li>$\\pi_\\theta(a\\vert s)$: parameterized policy</li>\n</ul>\n<p>$\\pi_\\theta$는 $\\theta$에 대하여 미분가능한 함수입니다. $L_\\pi\\left(\\tilde\\pi\\right)$을 $\\theta_0$에서 $\\eta(\\pi)$에 대한 first order approximation이라고 하면 다음 식이 성립합니다.</p>\n<p>$$<br>\\begin{align}<br>L_{\\pi_{\\theta_0} }\\left(\\pi_{\\theta_0}\\right) &amp;= \\eta\\left(\\pi_{\\theta_0}\\right) \\\\<br>\\nabla_\\theta \\left.L_{\\pi_{\\theta_0} }\\left(\\pi_{\\theta_0}\\right)\\right\\vert _{\\theta=\\theta_0} &amp;= \\nabla_\\theta\\left.\\eta(\\pi_{\\theta_0})\\right\\vert _{\\theta=\\theta_0}<br>\\end{align}<br>$$</p>\n<p>이것의 의미는 $\\pi_{\\theta_0}$가 매우 작게 변한다면 $L_{\\pi_{\\theta_0} }$를 개선시키는 것이 $\\eta$를 개선시키는 것이라는 것입니다. 그러나 지금까지의 설명만으로는 $\\pi_{\\theta_0}$를 얼마나 작게 변화시켜야 할지에 대해서는 알 수 없습니다.</p>\n<p>Kakade &amp; Langford가 2002년 발표한 논문에서도 이것에 대해서 고민했습니다. 그 논문에서 <em>conservative policy iteration</em>이라는 기법을 제안합니다. 그 논문의 contribution은 다음과 같습니다.</p>\n<ul>\n<li>$\\eta$ 개선의 lower bound를 제공</li>\n<li>기존의 policy를 $\\pi_\\mathrm{old}$라고 하고 $\\pi’$를 $\\pi’=\\arg\\max_{\\pi’}L_{\\pi_\\mathrm{old} }\\left(\\pi’\\right)$과 같이 정의할 때, 새로운 mixture policy $\\pi_\\mathrm{new}$를 다음과 같이 제안<br>$$<br>\\pi_\\mathrm{new}(a\\vert s) = (1-\\alpha)\\pi_\\mathrm{old}(a\\vert s) + \\alpha \\pi’(a\\vert s)<br>$$</li>\n</ul>\n<p>그림으로 표현하면 다음과 같습니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=2m46s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/mixure_policy.png\" alt=\"mixure_policy\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<ul>\n<li>다음과 같은 lower bound를 정의<br>$$<br>\\eta\\left(\\pi_\\mathrm{new}\\right) \\geq L_{\\pi_{\\theta_\\mathrm{old} }}\\left(\\pi_\\mathrm{new}\\right) - \\frac{2\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2,\\\\<br>\\mathrm{where\\ }\\epsilon = \\max_s\\left\\vert E_{a\\sim\\pi’(a\\vert s)}\\left[A_\\pi(s,a)\\right]\\right\\vert<br>$$</li>\n</ul>\n<p>하지만 mixture된 policy라는 것은 실용적이지 않습니다. </p>\n<p><br><br></p>\n<h1 id=\"3-Monotonic-Improvement-Guarantee-for-General-Stochastic-Policies\"><a href=\"#3-Monotonic-Improvement-Guarantee-for-General-Stochastic-Policies\" class=\"headerlink\" title=\"3. Monotonic Improvement Guarantee for General Stochastic Policies\"></a>3. Monotonic Improvement Guarantee for General Stochastic Policies</h1><p>이전 장에서 설명한 lower bound는 오직 mixture policy에 대해서만 성립하고 더 많이 사용되는 stochastic policy에는 적용되지 않습니다. 따라서 stochastic policy를 이용할 수 있도록 개선할 필요가 있습니다. 아래와 같이 기존 수식에서 두 가지를 바꿈으로써 이것이 가능합니다.</p>\n<ul>\n<li>$\\alpha\\rightarrow$ distance measure between $\\pi$ and $\\tilde\\pi$</li>\n<li>constant $\\epsilon\\rightarrow\\max_{s,a}\\left\\vert A_\\pi(s,a)\\right\\vert $</li>\n</ul>\n<p>여기서 distance measure로 total variation divergence를 이용합니다. discrete porbability distribution $p$와 $q$에 대하여 다음과 같이 정의됩니다.<br>$$<br>D_\\mathrm{TV}(p\\parallel q) = \\frac{1}{2}\\sum_i\\left\\vert p_i - q_i\\right\\vert<br>$$</p>\n<p>이것을 이용하여 $D_\\mathrm{TV}^\\max$를 다음과 같이 정의합니다.<br>$$<br>D_\\mathrm{TV}^\\max(\\pi\\parallel \\tilde\\pi) = \\max_s D_\\mathrm{TV}\\left(\\pi(\\cdot\\vert s)\\parallel\\tilde\\pi(\\cdot\\vert s)\\right)<br>$$</p>\n<p>그림으로 표현하면 다음과 같습니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=3m15s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/tvd.png\" alt=\"tvd\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>이것을 이용하여 다음과 같은 관계식을 얻을 수 있습니다.</p>\n<p><strong>Theorem 1.</strong> Let $\\alpha=D_\\mathrm{TV}^\\max(\\pi\\parallel \\tilde\\pi)$. Then the following bound holds:<br>$$<br>\\begin{align}<br>\\eta\\left(\\pi_\\mathrm{new}\\right) &amp;\\geq L_{\\pi_\\mathrm{old} }\\left(\\pi_\\mathrm{new}\\right) - \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2, \\\\<br>\\mathrm{where\\ } \\epsilon&amp;= \\max_{s,a}\\left\\vert A_\\pi(s,a)\\right\\vert<br>\\end{align}<br>$$<br><!--*Proof.* TBD.--></p>\n<p>또다른 distance metric으로 아래 그림과 같은 KL divergence가 있습니다.<br>(그런데 왜 하필 KL divergence로 바꿀까요? 논문의 뒤쪽에서 계산효율을 위해서 conjugate gradient method를 이용하는데 이를 위해서 바꾼게 아닐까 싶습니다. Wasserstein distance 같은 다른 방향으로 발전시킬 수도 있을 것 같습니다. Schulmann이 아마 해봤겠죠?^^a)</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=3m34s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/kld.png\" alt=\"kld\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>total variation divergence와 KL divergence 사이에는 다음과 같은 관계가 있습니다.<br>$$<br>D_\\mathrm{TV}(p\\parallel q)^2 \\leq D_\\mathrm{KL}(p\\parallel q)<br>$$</p>\n<p>다음 수식을 정의합니다.<br>$$<br>D_\\mathrm{KL}^\\max(\\pi\\parallel \\tilde\\pi) = \\max_s D_\\mathrm{KL}\\left(\\pi(\\cdot\\vert s)\\parallel\\tilde\\pi(\\cdot\\vert s)\\right)<br>$$</p>\n<p><strong>Theorem 1</strong>을 이용하여 다음과 같은 수식이 성립함을 알 수 있습니다.<br>$$<br>\\begin{align}<br>\\eta\\left(\\tilde\\pi\\right) &amp;\\geq L_{\\pi}\\left(\\tilde\\pi\\right) - C\\cdot D_\\mathrm{KL}^\\max(\\pi, \\tilde\\pi), \\\\<br>\\mathrm{where\\ } C&amp;= \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}<br>\\end{align}<br>$$</p>\n<p>이러한 policy imporvement bound를 기반으로 다음과 같은 approximate policy iteration 알고리듬을 고안해낼 수 있습니다.</p>\n<p><strong>Algorithm 1</strong> Policy iteration algorithm guaranteeing non-decreasing expected return $\\eta$</p>\n<blockquote>\n<p>Initialize $\\pi_0$<br><strong>for</strong> $i=0,1,2,\\ldots$ until convergence <strong>do</strong><br>$\\quad$Compute all advantage values $A_{\\pi_i}(s,a)$<br>$\\quad$Solve the contstrained optimization problem<br>$\\quad\\pi_{i+1}=\\arg\\max_\\pi\\left[L_{\\pi_i} - CD_\\mathrm{KL}^\\max\\left(\\pi_i,\\pi\\right)\\right]$<br>$\\quad\\quad$where $C = 4\\epsilon\\gamma / (1-\\gamma)^2$<br>$\\quad\\quad$and $L_{\\pi_i}\\left(\\pi\\right) = \\eta(\\pi_i) + \\sum_{s}\\rho_{\\pi_i}(s)\\sum_a\\pi(a\\vert s) A_{\\pi_i}(s,a)$<br><strong>end for</strong></p>\n</blockquote>\n<p><strong>Algorithm 1</strong>은 advantage를 정확하게 계산할 수 있다고 가정하고 있습니다. 이 알고리듬은  monotonical한 성능 증가($\\eta(\\pi_0)\\leq\\eta(\\pi_1)\\leq\\cdots$)를 한다는 것을 다음과 같이 보일 수 있습니다. $M_i(\\pi)=L_{\\pi_i}(\\pi) - CD_\\mathrm{KL}^\\max\\left(\\pi_i,\\pi\\right)$라고 합시다.</p>\n<p>$$<br>\\begin{align}<br>\\eta \\left(\\pi_{i+1}\\right) &amp;\\geq M_i\\left(\\pi_{i+1}\\right)\\\\<br>\\eta \\left(\\pi_{i}\\right) &amp;= M_i\\left(\\pi_{i}\\right) \\\\<br>\\eta \\left(\\pi_{i+1}\\right) - \\eta \\left(\\pi_{i}\\right) &amp;\\geq M_i\\left(\\pi_{i+1}\\right) - M_i\\left(\\pi_{i}\\right)<br>\\end{align}<br>$$</p>\n<p>위 수식과 같이 매 iteration 마다 $M_i$를 최대화함으로써, $\\eta$가 감소하지 않는다는 것을 보장할 수 있습니다. 이와 같은 타입의 알고리듬을 <a href=\"https://www.jstor.org/stable/pdf/27643496.pdf?casa_token=0qHamcl60WoAAAAA:-fkZ9JcA_nrY3-zbCUpqvPOgcAMgw7Gr96MajCZg2byHf8m5GU1KTSxyJJcBy1lPZbBTZVCjHHUXilh4k-iuwF91Wka4B5qdltC1IR2qMWk8q1FoV6__\" target=\"_blank\" rel=\"noopener\">minorization-maximization (MM) algorithm</a>이라고 합니다. EM 알고리듬도 이 타입의 알고리듬 중 하나입니다.</p>\n<p><strong>Algorithm 1</strong>은 아래 그림과 같이 동작합니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=3m52s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/surrogate.png\" alt=\"surrogate\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>$M_i$는 $\\pi_i$일 때 equality가 되는 $\\eta$에 대한  surrogate function입니다. TRPO는 이 surrogate function을 최대화하고 KL divergence를 penalty가 아닌 constraint로 두는 알고리듬입니다.</p>\n<p><br><br></p>\n<h1 id=\"4-Optimization-of-Parameterized-Policies\"><a href=\"#4-Optimization-of-Parameterized-Policies\" class=\"headerlink\" title=\"4. Optimization of Parameterized Policies\"></a>4. Optimization of Parameterized Policies</h1><p>표기의 편의를 위해 다음과 같이 notation들을 더 간략하게 정의합니다.</p>\n<ul>\n<li>$\\eta(\\theta):=\\eta\\left(\\pi_\\theta\\right)$</li>\n<li>$L_\\theta\\left(\\tilde\\theta\\right):=L_{\\pi_\\theta}\\left(\\pi_{\\tilde\\theta}\\right)$</li>\n<li>$D_\\mathrm{KL}\\left(\\theta\\parallel\\tilde\\theta\\right):=D_\\mathrm{KL}\\left(\\pi_\\theta\\parallel\\pi_{\\tilde\\theta}\\right)$</li>\n<li>$\\theta_\\mathrm{old}$: previous policy parameter</li>\n</ul>\n<h2 id=\"4-1-Trust-Region-Policy-Optimization\"><a href=\"#4-1-Trust-Region-Policy-Optimization\" class=\"headerlink\" title=\"4.1 Trust Region Policy Optimization\"></a>4.1 Trust Region Policy Optimization</h2><p>이전 장의 중요 결과를 위의 notation으로 다시 표기하면 아래와 같습니다.</p>\n<p>$$<br>\\eta\\left(\\theta\\right) \\geq L_{\\theta_\\mathrm{old} }\\left(\\theta\\right) - C\\cdot D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right)<br>$$</p>\n<p>$\\eta$의 성능 향상을 보장하기 위해서 그것의 lower bound를 최대화할 수 있습니다.</p>\n<p>$$<br>\\max_\\theta \\left[L_{\\theta_\\mathrm{old} }\\left(\\theta\\right) - C D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right)\\right]<br>$$</p>\n<p>이 최적화 문제는 step size를 매우 작게 해야 올바른 동작을 합니다. 위에서 살펴봤듯이 first order approximation이기 때문입니다. 좀 더 큰 step size를 가질 수 있도록 이 최적화 문제를 trust region constraint를 도입하여 다음과 같이 바꿉니다.</p>\n<p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;L_{\\theta_\\mathrm{old} }(\\theta) \\\\<br>\\mathrm{s.t.\\ }&amp;D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right) \\leq \\delta<br>\\end{align}$$</p>\n<p>이 최적화문제의 constraint는 모든 state space에 대해서 성립해야 합니다. 또한 maximum값을 매번 찾아야 합니다. state가 많은 경우 constraint의 수가 매우 많아져서 문제를 풀기 어렵게 만듭니다. constraint의 수를 줄이기 위하여 다음과 같은 avergae KL divergence를 이용하는 heuristic approximation을 취합니다. 이것이 최선의 방법은 아닐 수 있지만 실용적인 방법입니다.</p>\n<p>$$<br>\\overline D_\\mathrm{KL}^{\\rho}\\left(\\theta_1, \\theta_2\\right):=E_{s\\sim\\rho}\\left[D_\\mathrm{KL}\\left(\\pi_{\\theta_1}(\\cdot\\vert s)\\parallel\\pi_{\\theta_2}(\\cdot\\vert s)\\right)\\right]<br>$$</p>\n<p>이것을 기반으로 다음과 같은 최적화 문제를 풀 수 있습니다.</p>\n<p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;L_{\\theta_\\mathrm{old} }(\\theta) \\\\<br>\\mathrm{s.t.\\ }&amp;\\overline{D}_\\mathrm{KL}^{\\rho_\\mathrm{old} }\\left(\\theta_\\mathrm{old}, \\theta\\right) \\leq \\delta<br>\\end{align}$$</p>\n<p>아래 그림처럼 step size에 대해서 고려할 수 있습니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=4m35s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/heuristic_approx.png\" alt=\"heuristic_approx\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p><br><br></p>\n<h1 id=\"5-Sample-Based-Estimation-of-the-Objective-and-Constraint\"><a href=\"#5-Sample-Based-Estimation-of-the-Objective-and-Constraint\" class=\"headerlink\" title=\"5. Sample-Based Estimation of the Objective and Constraint\"></a>5. Sample-Based Estimation of the Objective and Constraint</h1><p>실용적인 알고리듬을 만들려고 하는 노력은 아직 끝나지 않았습니다. 이제 앞의 알고리듬을 sample-based estimation 즉, Monte Carlo estimation을 할 수 있도록 바꿔보겠습니다. sampling을 편하게 할 수 있도록 아래와 같이 바꿔줍니다.</p>\n<ul>\n<li>$\\sum_s \\rho_{\\theta_\\mathrm{old} }(s)[\\ldots]\\rightarrow\\frac{1}{1-\\gamma}E_{s\\sim\\rho_{\\theta_\\mathrm{old} }}[\\ldots]$</li>\n<li>$A_{\\theta_\\mathrm{old} }\\rightarrow Q_{\\theta_\\mathrm{old} }$</li>\n<li>$\\sum_a\\pi_{\\theta_\\mathrm{old} }(a\\vert s)A_{\\theta_\\mathrm{old} } \\rightarrow E_{a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}A_{\\theta_\\mathrm{old} }\\right]$</li>\n</ul>\n<p>이러한 변화를 그림처럼 도식화할 수 있습니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=5m31s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/sample-based.png\" alt=\"sample-based\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>한 가지 짚고 넘어가야 할 점은 action sampling을 할 때 importance sampling을 사용한다는 것입니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=5m53s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/importance_sampling.png\" alt=\"importance_sampling\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>바뀐 최적화문제는 아래와 같습니다.</p>\n<ul>\n<li><strong>Equation (14).</strong></li>\n</ul>\n<p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}Q_{\\theta_\\mathrm{old} }(s,a)\\right] \\\\<br>\\mathrm{s.t.\\ }&amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} }}\\left[D_\\mathrm{KL}\\left(\\pi_{\\theta_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta<br>\\end{align}<br>$$</p>\n<p>이 때 sampling하는 두 가지 방법이 있습니다.</p>\n<p><br></p>\n<h2 id=\"5-1-Single-Path\"><a href=\"#5-1-Single-Path\" class=\"headerlink\" title=\"5.1 Single Path\"></a>5.1 Single Path</h2><p><em>single path</em>는 개별 trajectory들을 이용하는 방법입니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=6m15s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/single.png\" alt=\"single\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p><br></p>\n<h2 id=\"5-2-Vine\"><a href=\"#5-2-Vine\" class=\"headerlink\" title=\"5.2 Vine\"></a>5.2 Vine</h2><p><em>vine</em>은 한 state에서 rollout을 이용하여 여러 action을 수행하는 방법입니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=6m32s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/vine1.png\" alt=\"vine1\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=6m49s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/vine2.png\" alt=\"vine2\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>estimation의 variance를 낮출 수 있지만 계산량이 많고 한 state에서 여러 action을 수행할 수 있어야 하기 때문에 현실적인 문제에 적용하기에는 어려움이 있습니다.</p>\n<p><br><br></p>\n<h1 id=\"6-Practical-Algorithm\"><a href=\"#6-Practical-Algorithm\" class=\"headerlink\" title=\"6. Practical Algorithm\"></a>6. Practical Algorithm</h1><p>앞서 single-path, vine 샘플링을 사용하는 두가지 방식의 policy optimization 알고리즘을 살펴봤습니다. 실용적인 알고리듬은 아래의 과정을 반복해서 수행합니다.</p>\n<p>1) Q-values의 몬테카를로 추정을 통해 state-action 쌍 집합을 single path 또는 vine 과정을 통해 수집함.</p>\n<p>2) 샘플 평균으로, (14)식의 목적함수와 제약식을 추정함</p>\n<ul>\n<li><strong>Equation (14).</strong><br>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}Q_{\\theta_\\mathrm{old} }(s,a)\\right] \\\\<br>\\mathrm{s.t.\\ }&amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} }}\\left[D_\\mathrm{KL}\\left(\\pi_{\\theta_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta<br>\\end{align}<br>$$</li>\n</ul>\n<p>3) policy parameter vector인 $\\theta$를 업데이트 하면서 제약조건이 있는 최적화 문제를 근사적으로 풂. 본 논문에서는 gradient를 직접 계산하는 것보다는 약간 더 계산량이 있는 line search와 conjugate gradient algorithm을 사용했습니다.</p>\n<p>3)에 대해서,  gradient의 covariance matrix를 사용하지 않고 KL divergence의 Hessian을 해석적으로 계산하여 Fisher Information Matrix를 구성했습니다.</p>\n<p>다시말해서</p>\n<p>$\\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial}{\\partial \\theta_i} \\log \\pi_{\\theta}(a_n\\vert s_n) \\frac{\\partial}{\\partial \\theta_j} \\log \\pi_{\\theta}(a_n\\vert s_n)$ 대신 $\\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_j} D_{KL}(\\pi_{\\theta_{old} }(\\cdot \\vert  s_n) \\vert \\vert  \\pi_{\\theta}(\\cdot \\vert  s_n))$를 계산한 것입니다.</p>\n<p>이 analytic estimator는 Hessian이나 trajectories의 모든 gradient를 저장하지 않아도 되기 때문에 대규모 환경을 고려할 경우 계산 상 이점이 있습니다. 좀 더 자세하게 설명한 Appendix C 부분을 살펴보겠습니다.</p>\n<p><br></p>\n<h2 id=\"Appendix-C-Efficiently-Solving-the-Trust-Region-Constrained-Optimization-Problem\"><a href=\"#Appendix-C-Efficiently-Solving-the-Trust-Region-Constrained-Optimization-Problem\" class=\"headerlink\" title=\"Appendix C. Efficiently Solving the Trust-Region Constrained Optimization Problem\"></a>Appendix C. Efficiently Solving the Trust-Region Constrained Optimization Problem</h2><p>우리가 풀고자 하는 식은 다음과 같습니다.</p>\n<p>$$<br>\\begin{align}<br>\\max\\quad &amp;L(\\theta) \\\\<br>\\mathrm{s.t.\\ } &amp;\\overline{D}_{KL}(\\theta_\\mathrm{old}, \\theta) \\leq \\delta<br>\\end{align}<br>$$</p>\n<p>이 최적화 문제를 풀기 위하여 우리는 gradient 기법을 이용할 것입니다. gradient 기법은 결국 update할 방향과 크기를 결정하는 문제로 생각할 수 있습니다. 이에 따라 아래와 같은 두가지 과정으로 진행합니다.</p>\n<p>1) 탐색 방향을 계산, 목적함수의 first-order approximation(선형근사)와 제약식의 quadratic approximation(2차 근사).</p>\n<p>2) 이동거리 계산을 위해 해당 방향으로 line search 수행.</p>\n<p>탐색 방향은 $Ax = g$ 수식을 근사적으로 풀어서 구합니다. 여기서 $A$ 는 Fisher Information Matrix이고, 이것은 KL divergence 제약식의 2차 근사 $\\overline{D}_\\mathrm{KL} (\\theta_\\mathrm{old}, \\theta) \\approx \\frac{1}{2}(\\theta - \\theta_\\mathrm{old})^T A(\\theta - \\theta_\\mathrm{old})$를 푸는 것입니다. 여기서 $A_{ij} = \\frac{\\partial}{\\partial \\theta_i} \\frac{\\partial}{\\partial \\theta_j} \\overline{D}_\\mathrm{KL}(\\theta_\\mathrm{old}, \\theta)$입니다.</p>\n<p>논문에서 이 부분에 대한 설명이 약간 부족해서 좀 더 내용을 추가하자면, $\\frac{1}{2}x^T A x - x^T g + b = 0$과 같은 quadratic equation의 해는 1차 derivative를 0으로 만드는 $x$를 찾음으로써 알 수 있습니다. 위에서 우리는 목적함수를 1차 근사시키고 제약조건은 2차 근사를 시킴으로써 quadratic equation 형태로 변형했기 때문에 같은 방식으로 풀 수 있습니다. 따라서 $g$는 1차근사시킨 목적함수의 계수가 될 것입니다.</p>\n<p>대규모 문제에서 $A$ 혹은 $A^{-1}$ 을 계산하는 것은 엄청난 계산량이 필요합니다. 하지만 conjugate gradient algorithm을 잘 이용하면 전체 $A$행렬(FIM)을 계산하지 않아도 $Ax=g$ 를 근사적으로 해결할 수 있게 해줍니다.</p>\n<p>탐색 방향 $s \\approx A^{-1}g$과 더불어 최대스텝길이(step size) $\\beta$도 계산할 필요가 있습니다.</p>\n<p>즉, $\\delta = \\overline{D}_\\mathrm{KL} \\approx \\frac{1}{2}(\\beta s)^T A(\\beta s) = \\frac{1}{2}\\beta^2s^TAs$ 이고, </p>\n<p>$\\beta = \\sqrt{2\\delta / s^T As}$가 됩니다. 이때 $\\delta$는 KL divergence의 boundary term 입니다.</p>\n<p>$s^T As$ 항은 Hessian-vector product로 계산할 수 있고, conjugate gradient 과정에서 계산됩니다.</p>\n<p>마지막으로, surrogate objective와 KL divergence 제약식을 위해 다음과 같은 함수를 정의하고 line search를 사용합니다.</p>\n<p>$$<br>L_{\\theta_\\mathrm{old} }(\\theta) - \\chi [\\overline{D}_{KL}(\\theta_\\mathrm{old}, \\theta) \\leq \\delta]<br>$$</p>\n<p>$\\chi[\\cdot]$ 함수는 []안의 조건이 맞으면 0이고 틀리면 무한으로 발산하는 함수입니다.</p>\n<p>위에서 계산한 $\\beta$ 의 최대 값부터 시작해서 목적함수가 개선될때까지 exponential하게 줄여 나갑니다. line search가 없었다면 이 알고리즘은 매우 큰 스텝으로 계산될 것이고 심각한 성능 저하가 발생할 것입니다.</p>\n<p>(여기서 살짝 예고를 하자면, 나중에 이 KL 기반 기법을 개선한 더 단순한 방법이 제안됩니다. PPO라고…)</p>\n<p><br><br></p>\n<h1 id=\"7-Connections-with-Prior-Work\"><a href=\"#7-Connections-with-Prior-Work\" class=\"headerlink\" title=\"7. Connections with Prior Work\"></a>7. Connections with Prior Work</h1><p>Natural Policy Gradient는 $L$의 선형 근사와 $\\overline D_\\mathrm{KL}$ 제약식을 2차근사 하는 아래의 식의 special case입니다.</p>\n<ul>\n<li>Equation (17).</li>\n</ul>\n<p>$L_{\\pi}(\\tilde\\pi) = \\eta(\\pi) + \\underset{s}{\\sum}\\rho_{\\pi}(s) \\underset{a}{\\sum}\\tilde\\pi(a\\vert s)A_\\pi(s,a)$</p>\n<p>$\\underset{\\theta}{\\max}\\quad [\\nabla_{\\theta}L_{\\theta_\\mathrm{old} }(\\theta) \\ \\rvert_{\\theta=\\theta_\\mathrm{old} } \\cdot (\\theta - \\theta_\\mathrm{old})]$</p>\n<p>$\\mathrm{s.t.}\\quad\\frac{1}{2}(\\theta_\\mathrm{old} - \\theta)^{T} A(\\theta_\\mathrm{old})(\\theta_\\mathrm{old}-\\theta) \\leq \\delta$</p>\n<p>$\\mathrm{where\\ }A(\\theta_{old})_{ij} = $</p>\n<p>$\\Large \\frac{\\partial}{\\partial \\theta_{i} } \\frac{\\partial}{\\partial \\theta_{j} }<br>E_{s \\sim \\rho_{\\pi} }[D_\\mathrm{KL}(\\pi(\\cdot\\rvert s, \\theta_\\mathrm{old})  \\rvert\\rvert \\pi(\\cdot \\rvert s, \\theta))] \\rvert_{\\theta=\\theta_\\mathrm{old} }$</p>\n<p>업데이트 식은 다음과 같습니다.</p>\n<p>$\\theta_\\mathrm{new} = \\theta_\\mathrm{old} +  \\color{Red}{\\frac{1}{\\lambda} } A(\\theta_\\mathrm{old})^{-1}\\nabla_{\\theta}L(\\theta) \\rvert_{\\theta=\\theta_\\mathrm{old} }$</p>\n<p>여기서 step size인 $\\frac{1}{\\lambda}$ 일반적으로 알고리즘의 파라미터로 취급되지만 TRPO는 각 업데이트 마다 제약식으로 사용한다는 점이 다릅니다. 사소한 차이처럼 보이지만, 큰 차원을 다루는 실험에서 성능을 크게 향상시켰습니다.</p>\n<p>또한 $L^2$ 제약식(혹은 페널티) 를 사용하면 표준 Policy Gradient 업데이트 식을 얻었습니다.</p>\n<ul>\n<li>Equation (18).</li>\n</ul>\n<p>$\\underset{\\theta}{\\max}\\quad[\\nabla_{\\theta} L_{\\theta_\\mathrm{old} }(\\theta) \\rvert_{\\theta=\\theta_\\mathrm{old} } \\cdot (\\theta - \\theta_\\mathrm{old})]$</p>\n<p>$\\mathrm{s.t.}\\quad \\frac{1}{2} \\vert\\vert \\theta - \\theta_\\mathrm{old} \\vert\\vert^2 \\leq \\delta$</p>\n<p>$L_{\\pi}(\\tilde\\pi) = \\eta(\\pi) + \\underset{s}{\\sum}\\rho_{\\pi}(s) \\underset{a}{\\sum}\\tilde\\pi(a\\vert s)A_\\pi(s,a) $를 이용해서 제약조건없이 $\\underset{\\pi}{\\max}\\quad L_{\\pi_\\mathrm{old} }(\\pi)$를 풀면<br>Policy Iteration update를 하는 것과 같습니다.</p>\n<p><br><br></p>\n<h1 id=\"8-Experiments\"><a href=\"#8-Experiments\" class=\"headerlink\" title=\"8. Experiments\"></a>8. Experiments</h1><ul>\n<li><p>다음과 같은 3 가지 궁금증을 해결하기 위하여 실험을 설계하였습니다.</p>\n<ol>\n<li><p>Single-path 와 vine 의 성능면에서 특징을 알고싶음.</p>\n</li>\n<li><p>Fixed penalty coefficient(NPG) 보다 fixed KL divergence를 사용하는 것(TRPO)으로<br>변경한 것이 얼마나 뚜렷한 차이를 보일지, 성능 면에서 어떤 영향을 미치는지 알고싶음.</p>\n</li>\n<li><p>TRPO가 큰 스케일의 문제를 해결할 수 있는지, 기존에 연구/적용되어왔던 방법들과<br>성능, 계산시간, 샘플 복잡도 면에서 비교하고 싶음.</p>\n</li>\n</ol>\n</li>\n</ul>\n<ul>\n<li><p>1, 2의 궁금증에 대해서 실험 환경에 single path, vine을 비롯한 여러 사전방법들을 함께 실험하였습니다.</p>\n</li>\n<li><p>3에 대해선, robotic locomotion과 Atari game에 TRPO를 실험하였습니다.</p>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"8-1-Simulated-Robotic-Locomotion\"><a href=\"#8-1-Simulated-Robotic-Locomotion\" class=\"headerlink\" title=\"8.1 Simulated Robotic Locomotion\"></a>8.1 Simulated Robotic Locomotion</h2><ul>\n<li>MuJoCo 시뮬레이터를 활용해 로봇 locomotion 실험 진행하였습니다.</li>\n<li>Robot State: positions and velocities, joint torques</li>\n</ul>\n<p><img src=\"https://i.imgur.com/bs5ATS3.png\" alt=\"\"></p>\n<ul>\n<li><p>수영, 점프, 걷기 행동을 학습시키고자 합니다.</p>\n<ul>\n<li><p>수영: 10 dimensional state space , reward: $r(x,u)=v_x - 10^{-5}\\vert \\vert u\\vert \\vert ^2$, quadratic penalty.</p>\n</li>\n<li><p>점프: 12 dimensional state space, episode의 끝은 높이와 각도 threshol로 점프를 판단, non-terminal state에 대해 +1 보상 추가.</p>\n</li>\n<li><p>걷기: 18 dimensional state space, 뜀뛰는 듯한 보법(점프에이전트)보다 부드러운 보법을 유도하기위해 페널티를 추가함.</p>\n</li>\n</ul>\n</li>\n</ul>\n<p><strong>Detailed Experiment Setup and used parameters, used network model</strong><br><img src=\"https://i.imgur.com/zgnsbw6.png\" alt=\"\"></p>\n<p><img src=\"https://i.imgur.com/FqdWC53.png\" alt=\"\"></p>\n<p>equation (12) : $\\max\\quad L_{\\theta_\\mathrm{old}(\\theta)}, \\ \\ \\mathrm{s.t.\\ }\\overline{D}_\\mathrm{KL}^{\\rho_{\\theta_\\mathrm{old} }}(\\theta_\\mathrm{old}, \\theta) \\leq \\delta$</p>\n<p>이 실험에서 $\\delta = 0.01$입니다.</p>\n<ul>\n<li><p>비교에 사용된 모델들</p>\n<ul>\n<li>TRPO Single-path</li>\n<li>TRPO vine</li>\n<li>CEM(cross-entropy method)</li>\n<li>CMA(covariance matrix adaptation)</li>\n<li>NPG (Lagrange Multiplier penalty coefficien를 사용하는 것이 차이점)</li>\n<li>Empirical FIM</li>\n<li>maximum KL(cartpole)</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://i.imgur.com/wSqolvS.png\" alt=\"\"></p>\n<ul>\n<li><p>제안한 single path, vine 모두 task를 잘 학습하는 것을 확인할 수 있습니다.</p>\n</li>\n<li><p>페널티를 고정하는 것보다, KL divergenc를 제약하는 것이 step size를 선택하는 부분에서 Robust합니다.</p>\n</li>\n<li><p>CEM/CMA는 derivative free 알고리즘으로, 샘플 복잡도가 네트워크 파라미터수 만큼 확장되어 high dimension 문제에서는 속도와 성능이 약한 모습을 보입니다.</p>\n</li>\n<li><p>maxKL은 제안방법보다 느린 학습성능을 보입니다.</p>\n</li>\n</ul>\n<p>TRPO video link: <a href=\"https://www.youtube.com/watch?v=jeid0wIrSn4\" target=\"_blank\" rel=\"noopener\">https://www.youtube.com/watch?v=jeid0wIrSn4</a></p>\n<ul>\n<li><p>사전지식이 적은 상태에서 여러 액션을 학습하는 실험으로 TRPO의 성능을 보입니다.</p>\n</li>\n<li><p>지금까지의 균형이나 걸음 같은 개념을 명시적으로 인코딩한 robotic locomotion에서의 사전 연구들과 차이를 보였습니다.</p>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"8-2-Playing-Games-from-Images\"><a href=\"#8-2-Playing-Games-from-Images\" class=\"headerlink\" title=\"8.2 Playing Games from Images\"></a>8.2 Playing Games from Images</h2><ul>\n<li>부분관찰가능하며 복잡한 observation인 환경에서 TRPO를 평가하기위해 raw image를 입력으로 하는 아타리게임 환경에서 실험하였습니다.</li>\n</ul>\n<ul>\n<li>Challenging point :<ul>\n<li>High dimensional space</li>\n<li>delayed reward</li>\n<li>non-stationary image (Enduro는 배경 이미지가 바뀌고, 반전처리가 됨)</li>\n<li>complex action sequence (Q*bert는 21개의 다른 발판에서 점프 해야함)</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li>ALE(Arcade Learning Environment) 에서 테스트</li>\n<li><p>DQN과 전처리 방식은 동일<br>  210 x 160 pixel -&gt; gray-scaling &amp; down-sampling -&gt; 110 x 84  -&gt; crop -&gt; 84 x 84</p>\n</li>\n<li><p>2 convolution layer ( 16 channels) , stride 2, FC layer with 20 units</p>\n</li>\n</ul>\n<p><img src=\"https://i.imgur.com/NJBC69d.png\" alt=\"\"> </p>\n<p><img src=\"https://i.imgur.com/wTe1OEW.png\" alt=\"\"></p>\n<p><img src=\"https://i.imgur.com/j22VtNQ.png\" alt=\"\"></p>\n<ul>\n<li><p>30시간 동안 500 iteration 학습을 하였습니다.</p>\n</li>\n<li><p>DQN과 MCST를 사용한 모델(UCC-I), 사람이 플레이한 것(Human) 과의 비교하였습니다.</p>\n</li>\n<li><p>이전 방법들에 비해 압도적인 점수를 기록하진 못했으나, 다양한 게임에서 적절한 게임 기록하였습니다.</p>\n</li>\n<li><p>이전 방법들과 다른 점은, 특정 task에 초점을 맞춰 설계하지않았는데 robotics나 game playing Task에도 적절한 성능을 내는 점에서 <strong>TRPO의 일반화 성능</strong>을 확인하였습니다.</p>\n</li>\n</ul>\n<p><br></p>\n<h1 id=\"9-Discussion\"><a href=\"#9-Discussion\" class=\"headerlink\" title=\"9. Discussion\"></a>9. Discussion</h1><ul>\n<li><p>Trust Region Policy Optimization 을 제안하였습니다.</p>\n</li>\n<li><p>KL Divergence 페널티로 $\\eta(\\pi)$를 최적화하는 알고리즘이 monotonically improve 함을 증명하였습니다.</p>\n</li>\n<li><p>Locomotion 도메인에서 여러 행동(수영, 걷기, 점프)을 제어하는 controller를 학습하였습니다.</p>\n</li>\n<li><p>Robotics와 게임 실험을 결합해, 시각정보와 센서데이터를 사용하는 로봇제어 정책을 학습시킬 수 있는 가능성을 보았습니다.</p>\n</li>\n<li><p>샘플의 복잡도를 상당히 줄일 수 있어 실제상황에 적용가능성을 보았습니다.</p>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"END\"><a href=\"#END\" class=\"headerlink\" title=\"END\"></a>END</h1>","site":{"data":{}},"excerpt":"","more":"<center> <img src=\"https://www.dropbox.com/s/o7cjcn0e17mpizr/Screen%20Shot%202018-07-18%20at%201.14.22%20AM.png?dl=1\" width=\"700\"> </center>\n\n<p>논문 저자 : John Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, Pieter Abbeel<br>논문 링크 : <a href=\"https://arxiv.org/pdf/1502.05477.pdf\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/pdf/1502.05477.pdf</a><br>Proceeding : International Conference on Machine Learning (ICML) 2015<br>정리 : 공민서, 김동민</p>\n<hr>\n<h1 id=\"1-들어가며…\"><a href=\"#1-들어가며…\" class=\"headerlink\" title=\"1. 들어가며…\"></a>1. 들어가며…</h1><p>Trust region policy optimization (TRPO)는 상당히 우수한 성능을 보여주는 policy gradient 기법으로 알려져 있습니다. 높은 차원의 action space를 가진 robot locomotion부터 action은 적지만 화면을 그대로 처리하여 플레이하기 때문에 control parameter가 매우 많은 Atari game까지 각 application에 세부적으로 hyperparameter들을 특화시키지 않아도 두루두루 좋은 성능을 나타내기 때문에 일반화성능이 매우 좋은 기법입니다. 이 TRPO에 대해서 알아보겠습니다.</p>\n<p>※TRPO를 매우 재밌게 설명한 Crazymuse AI의 <a href=\"https://www.youtube.com/watch?v=CKaN5PgkSBc&amp;t=90s\" target=\"_blank\" rel=\"noopener\">Youtube video</a>에서 일부 그림을 차용했습니다. 이 비디오를 시청하시는 것을 강력하게 추천합니다!</p>\n<p><br></p>\n<h2 id=\"1-1-TRPO-흐름-잡기\"><a href=\"#1-1-TRPO-흐름-잡기\" class=\"headerlink\" title=\"1.1 TRPO 흐름 잡기\"></a>1.1 TRPO 흐름 잡기</h2><p>TRPO 논문은 많은 수식이 등장하여 이 수식들을 따라가다보면 큰 그림을 놓칠 수도 있습니다. 세부적인 내용을 살펴보기 전에 기존 연구에서 출발해서 TRPO로 어떻게 발전해나가는지 간략하게 살펴보겠습니다.</p>\n<h3 id=\"1-1-1-Original-Problem\"><a href=\"#1-1-1-Original-Problem\" class=\"headerlink\" title=\"1.1.1 Original Problem\"></a>1.1.1 Original Problem</h3><p>$$\\max_\\pi \\eta(\\pi)$$</p>\n<p>모든 강화학습이 그렇듯이 expected discounted reward를 최대화하는 policy를 찾는 문제로부터 출발합니다.</p>\n<h3 id=\"1-1-2-Conservative-policy-iteration\"><a href=\"#1-1-2-Conservative-policy-iteration\" class=\"headerlink\" title=\"1.1.2 Conservative policy iteration\"></a>1.1.2 Conservative policy iteration</h3><p>$$\\max L_\\pi(\\tilde\\pi) = \\eta(\\pi) + \\sum_s \\rho_\\pi(s)\\sum_a\\tilde\\pi(a\\vert s)A_\\pi(s,a)$$</p>\n<p>$\\eta(\\pi)$를 바로 최대화하는 것은 많은 경우 어렵습니다. $\\eta(\\pi)$의 성능향상을 보장하면서 policy를 update하는 conservative policy iteration 기법이 <a href=\"http://www.cs.cmu.edu/~./jcl/papers/aoarl/Final.pdf\" target=\"_blank\" rel=\"noopener\">Kakade와 Langford</a>에 의하여 제안되었습니다. 이 기법을 이용하면 policy update가 성능을 향상시키는지는 못하더라도 최소한 성능을 악화시키지는 않는다는 것이 이론적으로 보장됩니다.</p>\n<h3 id=\"1-1-3-Theorem-1-of-TRPO\"><a href=\"#1-1-3-Theorem-1-of-TRPO\" class=\"headerlink\" title=\"1.1.3 Theorem 1 of TRPO\"></a>1.1.3 Theorem 1 of TRPO</h3><p>$$\\max L_{\\pi_\\mathrm{old} }\\left(\\pi_\\mathrm{new}\\right) - \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2,\\quad\\left(\\alpha=D_\\mathrm{TV}^\\max\\left(\\pi_\\mathrm{old},\\pi_\\mathrm{new}\\right)\\right)$$</p>\n<p>기존의 conservative policy iteration은 과거 policy와 새로운 policy를 섞어서 사용해서 실용적이지 않다는 단점이 있었는데 이것을 보완하여 온전히 새로운 policy만으로 update할 수 있는 기법을 제안합니다. </p>\n<h3 id=\"1-1-4-KL-divergence-version-of-Theorem-1\"><a href=\"#1-1-4-KL-divergence-version-of-Theorem-1\" class=\"headerlink\" title=\"1.1.4 KL divergence version of Theorem 1\"></a>1.1.4 KL divergence version of Theorem 1</h3><p>$$\\max L_{\\pi}\\left(\\tilde\\pi\\right) - C\\cdot D_\\mathrm{KL}^\\max\\left(\\pi,\\tilde\\pi\\right),\\quad\\left(C = \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\right)$$</p>\n<p>distance metric을 KL divergence로 바꿀 수 있습니다.</p>\n<h3 id=\"1-1-5-Using-parameterized-policy\"><a href=\"#1-1-5-Using-parameterized-policy\" class=\"headerlink\" title=\"1.1.5 Using parameterized policy\"></a>1.1.5 Using parameterized policy</h3><p>$$\\max_\\theta L_{\\theta_\\mathrm{old} }(\\theta) - C\\cdot D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right)$$</p>\n<p>최적화문제를 더욱 편리하게 풀 수 있도록 낮은 dimension을 가지는 parameter들로 parameterized된 policy를 사용할 수 있습니다.</p>\n<h3 id=\"1-1-6-Trust-region-constraint\"><a href=\"#1-1-6-Trust-region-constraint\" class=\"headerlink\" title=\"1.1.6 Trust region constraint\"></a>1.1.6 Trust region constraint</h3><p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;L_{\\theta_\\mathrm{old} }(\\theta) \\\\<br>\\mathrm{s.t.\\ }&amp;D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right) \\leq \\delta<br>\\end{align}$$</p>\n<p>1.1.5까지는 아직 conservative policy iteration을 약간 변형시킨 것입니다. policy를 update할 때 지나치게 많이 변하는 것을 방지하기 위하여 <a href=\"https://en.wikipedia.org/wiki/Trust_region\" target=\"_blank\" rel=\"noopener\">trust region</a>을 constraint로 설정할 수 있습니다. 이 아이디어로 인해서 TRPO라는 명칭을 가지게 됩니다.</p>\n<h3 id=\"1-1-7-Heuristic-approximation\"><a href=\"#1-1-7-Heuristic-approximation\" class=\"headerlink\" title=\"1.1.7 Heuristic approximation\"></a>1.1.7 Heuristic approximation</h3><p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;L_{\\theta_\\mathrm{old} }(\\theta) \\\\<br>\\mathrm{s.t.\\ }&amp;\\overline{D}_\\mathrm{KL}^{\\rho_\\mathrm{old} }\\left(\\theta_\\mathrm{old}, \\theta\\right) \\leq \\delta<br>\\end{align}$$</p>\n<p>사실 1.1.6의 constraint는 모든 state에 대해서 성립해야 하기 때문에 문제를 매우 어렵게 만듭니다. 이것을 좀 더 다루기 쉽게 state distribution에 대한 평균을 취한 것으로 변형합니다.</p>\n<h3 id=\"1-1-8-Monte-Carlo-simulation\"><a href=\"#1-1-8-Monte-Carlo-simulation\" class=\"headerlink\" title=\"1.1.8 Monte Carlo simulation\"></a>1.1.8 Monte Carlo simulation</h3><p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}Q_{\\theta_\\mathrm{old} }(s,a)\\right] \\\\<br>\\mathrm{s.t.\\ }&amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} }}\\left[D_\\mathrm{KL}\\left(\\pi_{\\theta_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta<br>\\end{align}$$</p>\n<p>Sampling을 통한 계산이 가능하도록 식을 다시 표현할 수 있습니다. </p>\n<h3 id=\"1-1-9-Efficiently-solving-TRPO\"><a href=\"#1-1-9-Efficiently-solving-TRPO\" class=\"headerlink\" title=\"1.1.9 Efficiently solving TRPO\"></a>1.1.9 Efficiently solving TRPO</h3><p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;\\nabla_\\theta \\left. L_{\\theta_\\mathrm{old} }(\\theta)\\right\\vert _{\\theta=\\theta_\\mathrm{old} } \\left(\\theta - \\theta_\\mathrm{old}\\right) \\\\<br>\\mathrm{s.t.\\ }&amp;\\frac{1}{2}\\left(\\theta_\\mathrm{old} - \\theta \\right)^T A\\left(\\theta_\\mathrm{old}\\right)\\left(\\theta_\\mathrm{old} - \\theta \\right) \\leq \\delta \\\\<br>&amp;A_{ij} = \\frac{\\partial}{\\partial \\theta_i}\\frac{\\partial}{\\partial \\theta_j}\\overline{D}_\\mathrm{KL}^{\\rho_\\mathrm{old} }\\left(\\theta_\\mathrm{old}, \\theta\\right)<br>\\end{align}$$</p>\n<p>문제를 효율적으로 풀기 위하여 approximation을 적용할 수 있습니다. objective function은 first order approximation, constraint는 quadratic approximation을 취하면 효율적으로 문제를 풀 수 있는 형태로 바뀌는데 이것을 natural gradient로 풀 수도 있고 conjugate gradient로 풀 수도 있습니다.</p>\n<p>TRPO는 이렇게 다양한 방법으로 문제를 변형한 것입니다! 이제 좀 더 자세히 살펴보겠습니다.</p>\n<p><br><br></p>\n<h1 id=\"2-Preliminaries\"><a href=\"#2-Preliminaries\" class=\"headerlink\" title=\"2. Preliminaries\"></a>2. Preliminaries</h1><p>다음과 같은 파라미터를 가지는 infinite-horizon discounted Markov decision process (MDP)를 고려합니다. </p>\n<ul>\n<li>$\\mathcal{S}$: finite set of states</li>\n<li>$\\mathcal{A}$: finite set of actions</li>\n<li>$P: \\mathcal{S}\\times\\mathcal{A}\\rightarrow\\mathbb{R}$: transition probability distribution</li>\n<li>$r: \\mathcal{S}\\rightarrow\\mathbb{R}$: reward function</li>\n<li>$\\rho_0:\\mathcal{S}\\rightarrow\\mathbb{R}$: distribution of the initial state $s_0$</li>\n<li>$\\gamma\\in(0,1)$: discount factor</li>\n<li>$\\eta(\\pi)=E_{s_0,a_0,\\ldots}\\left[\\sum_{t=0}^\\infty\\gamma^t r\\left(s_t\\right)\\right]$, where $s_0\\sim\\rho_0\\left(s_0\\right), a_t\\sim\\pi\\left(\\left.a_t \\right\\vert s_t\\right), s_{t+1}\\sim P\\left(\\left.s_{t+1}\\right\\vert s_t,a_t\\right)$</li>\n<li>$Q_\\pi\\left(s_t,a_t\\right)=E_{ { \\color{red}{s_{t+1}} },a_{t+1},\\ldots}\\left[\\sum_{l=0}^\\infty \\gamma^l r\\left(s_{t+l}\\right)\\right]$: action value function</li>\n<li>$V_\\pi\\left(s_t\\right)=E_{ { \\color{red}{a_t} }, s_{t+1},a_{t+1},\\ldots}\\left[\\sum_{l=0}^\\infty \\gamma^l r\\left(s_{t+l}\\right)\\right]$: value function (action value function과 expectation의 첨자가 다른 부분을 유의하세요.)</li>\n<li>$A_\\pi(s,a) = Q_\\pi(s,a) - V_\\pi(s)$: advantage function</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-1-Useful-identity-Kakade-amp-Langford-2002\"><a href=\"#2-1-Useful-identity-Kakade-amp-Langford-2002\" class=\"headerlink\" title=\"2.1 Useful identity [Kakade &amp; Langford 2002]\"></a>2.1 Useful identity [Kakade &amp; Langford 2002]</h2><p>우리의 목표는 $\\eta(\\pi)$가 최대화되도록 만드는 것입니다. 하지만 $\\pi$의 변화에 따라 $\\eta$가 어떻게 변하는지 알아내는 것도 쉽지 않습니다. $\\pi$는 기존의 policy이고 $\\tilde\\pi$는 새로운 policy를 나타낸다고 할 때 $\\eta$와 policy update 사이에 다음과 같은 관계가 있다는 것이 밝혀졌습니다.</p>\n<p><strong>Lemma 1.</strong> $\\eta\\left(\\tilde\\pi\\right) = \\eta(\\pi) + E_{s_{0},a_{0},\\ldots\\sim\\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t A_\\pi\\left(s_t,a_t\\right)\\right]$</p>\n<p><em>Proof.</em> $A_\\pi(s,a)=E_{s’\\sim P\\left(s’\\vert s,a\\right)}\\left[r(s)+\\gamma V_\\pi\\left(s’\\right)-V_\\pi(s)\\right]$로 다시 표현할 수 있습니다. 표기의 편의를 위하여 $\\tau:=\\left(s_0,a_0,s_1,a_1,\\ldots\\right)$를 정의하겠습니다. 다음과 같은 수식 전개가 가능합니다.<br>$$<br>\\begin{align}<br>&amp;E_{\\tau\\vert \\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t A_\\pi\\left(s_t,a_t\\right)\\right] \\\\<br> &amp;= E_{\\tau\\vert \\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t \\left(r(s_t)+\\gamma V_\\pi\\left(s_{t+1}\\right)-V_\\pi(s_t)\\right)\\right] \\\\<br>&amp;= E_{\\tau\\vert \\tilde\\pi}\\left[\\left(\\sum_{t=0}^\\infty\\gamma^t r(s_t)\\right)+\\gamma V_\\pi\\left(s_{1}\\right)-V_\\pi(s_0)+\\gamma^2 V_\\pi\\left(s_{2}\\right)-\\gamma V_\\pi(s_1)+\\gamma^3 V_\\pi\\left(s_{3}\\right)-\\gamma^2 V_\\pi(s_2)+\\cdots\\right] \\\\<br>&amp;= E_{\\tau\\vert \\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t r(s_t)+\\color{red}{\\gamma V_\\pi\\left(s_{1}\\right)}-V_\\pi(s_0)+\\color{red}{\\gamma^2 V_\\pi\\left(s_{2}\\right)}-\\color{red}{\\gamma V_\\pi(s_1)}+\\cdots\\right] \\\\<br>&amp;= E_{\\tau\\vert \\tilde\\pi}\\left[-V_\\pi(s_0)+\\sum_{t=0}^\\infty\\gamma^t r(s_t)\\right] \\\\<br>&amp;\\overset{\\underset{\\mathrm{(a)} }{} }{=} -E_{\\color{red}{s_0}}\\left[V_\\pi(s_0)\\right]+E_{\\tau\\vert \\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t r(s_t)\\right] \\\\<br>&amp;=-\\eta(\\pi) + \\eta\\left(\\tilde\\pi\\right)\\\\<br>&amp;\\quad\\therefore \\eta\\left(\\tilde\\pi\\right) = \\eta(\\pi) + E_{s_{0},a_{0},\\ldots\\sim\\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t A_\\pi\\left(s_t,a_t\\right)\\right]<br>\\end{align}<br>$$<br>위의 전개의 (a) 부분은 $V_\\pi\\left(s_0\\right)=E_{a_0,a_1,s_1,\\ldots}\\left[\\sum_{t=0}^\\infty\\gamma^l r\\left(s_l\\right)\\right]$이므로 $\\tau$ 중 많은 부분들이 이미 expectation이 취해진 값이므로 무시됩니다. 오직 $s_0$만 expectation이 취해지지 않았기 때문에 남아있습니다.</p>\n<p><strong>Lemma 1</strong>은 새로운 policy와 기존의 policy 사이의 관계를 규정합니다. 다음과 같은 식을 정의하고 이것을 이용해서 <strong>Lemma 1</strong>을 변형시켜 봅시다.</p>\n<ul>\n<li>$\\rho_\\pi(s) = P\\left(s_0=s\\right) + \\gamma P\\left(s_1=s\\right)\\ + \\gamma^2 P\\left(s_2=s\\right) + \\cdots $: (unnormalized) discounted visitation frequencies</li>\n</ul>\n<p>$$<br>\\begin{align}<br>\\eta\\left(\\tilde\\pi\\right) &amp;= \\eta(\\pi) + \\sum_{t=0}^\\infty\\sum_{s}P\\left(\\left.s_t=s\\right\\vert \\tilde\\pi\\right)\\sum_a\\tilde\\pi(a\\vert s)\\gamma^t A_\\pi(s,a) \\\\<br>&amp;= \\eta(\\pi) + \\sum_{s}{\\color{red}{\\sum_{t=0}^\\infty \\gamma^t P\\left(\\left.s_t=s\\right\\vert \\tilde\\pi\\right)} } \\sum_a\\tilde\\pi(a\\vert s) A_\\pi(s,a) \\\\<br>&amp;= \\eta(\\pi) + \\sum_{s}{\\color{red}{\\rho_\\tilde\\pi(s)} }\\sum_a\\tilde\\pi(a\\vert s) A_\\pi(s,a) \\\\<br>\\end{align}<br>$$</p>\n<p>이 수식의 의미가 무엇일까요? 만약 $\\sum_a\\tilde\\pi(a\\vert s) A_\\pi(s,a) \\geq 0$이라면 $\\eta(\\tilde\\pi)$는 항상 $\\eta(\\pi)$보다 큽니다. 즉, policy를 업데이트함으로써 항상 개선됩니다. 즉, 이 수식을 통해서 항상 더 좋은 성능을 내는 policy update가 가져야 할 특징을 알 수 있습니다. 다음과 같은 deterministic policy가 있다고 합시다.</p>\n<p>$$<br>\\tilde\\pi(s) = \\arg\\max_a A_\\pi(s,a)<br>$$</p>\n<p>이 policy는 적어도 하나의 state-action pair에서 0보다 큰 값을 가지는 advantage가 있고 그 때의 확률이 0이 아니라면 항상 성능을 개선시킵니다. 어려운 점은 policy를 바꾸면 $\\rho$도 바뀐다는 점입니다. </p>\n<p>다음 그림을 봅시다. Starting Point에서 여러가지 경로를 거쳐서 Destination으로 갈 수 있습니다. 다른 policy를 이용하는 것은 다른 경로를 이용한 것입니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=16s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/policy_change.png\" alt=\"policy_change\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>이 때 다른 policy를 이용함에 따라 state visitation frequency도 변하게 됩니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=36s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/state_visitation_change.png\" alt=\"state_visitation_change\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>이로 인해서 policy를 최적화하는 것은 어려워집니다. 그래서 이러한 변화를 무시하는 다음과 같은 approximation을 취합니다.</p>\n<p>$$<br>\\begin{align}<br>L_\\pi\\left(\\tilde\\pi\\right) &amp;= \\eta(\\pi) + \\sum_{s}{\\color{red}{\\rho_\\pi(s)} } \\sum_a\\tilde\\pi(a\\vert s) A_\\pi(s,a)<br>\\end{align}<br>$$</p>\n<p>policy가 바뀌었음에도 이전의 state distribution을 계속 이용하는 것입니다. 이것은 policy의 변화가 크지 않다면 어느 정도 허용될 수 있을 것입니다. 그렇지만 얼마나 많은 변화가 허용될까요? 이것을 정하기 위해서 이용하는 것이 trust region입니다.</p>\n<p><br></p>\n<h2 id=\"2-2-Conservative-Policy-Iteration\"><a href=\"#2-2-Conservative-Policy-Iteration\" class=\"headerlink\" title=\"2.2 Conservative Policy Iteration\"></a>2.2 Conservative Policy Iteration</h2><p>policy의 변화를 다루기 용이하게 하기 위해서 policy를 다음과 같이 파라미터를 이용해서 표현합시다.</p>\n<ul>\n<li>$\\pi_\\theta(a\\vert s)$: parameterized policy</li>\n</ul>\n<p>$\\pi_\\theta$는 $\\theta$에 대하여 미분가능한 함수입니다. $L_\\pi\\left(\\tilde\\pi\\right)$을 $\\theta_0$에서 $\\eta(\\pi)$에 대한 first order approximation이라고 하면 다음 식이 성립합니다.</p>\n<p>$$<br>\\begin{align}<br>L_{\\pi_{\\theta_0} }\\left(\\pi_{\\theta_0}\\right) &amp;= \\eta\\left(\\pi_{\\theta_0}\\right) \\\\<br>\\nabla_\\theta \\left.L_{\\pi_{\\theta_0} }\\left(\\pi_{\\theta_0}\\right)\\right\\vert _{\\theta=\\theta_0} &amp;= \\nabla_\\theta\\left.\\eta(\\pi_{\\theta_0})\\right\\vert _{\\theta=\\theta_0}<br>\\end{align}<br>$$</p>\n<p>이것의 의미는 $\\pi_{\\theta_0}$가 매우 작게 변한다면 $L_{\\pi_{\\theta_0} }$를 개선시키는 것이 $\\eta$를 개선시키는 것이라는 것입니다. 그러나 지금까지의 설명만으로는 $\\pi_{\\theta_0}$를 얼마나 작게 변화시켜야 할지에 대해서는 알 수 없습니다.</p>\n<p>Kakade &amp; Langford가 2002년 발표한 논문에서도 이것에 대해서 고민했습니다. 그 논문에서 <em>conservative policy iteration</em>이라는 기법을 제안합니다. 그 논문의 contribution은 다음과 같습니다.</p>\n<ul>\n<li>$\\eta$ 개선의 lower bound를 제공</li>\n<li>기존의 policy를 $\\pi_\\mathrm{old}$라고 하고 $\\pi’$를 $\\pi’=\\arg\\max_{\\pi’}L_{\\pi_\\mathrm{old} }\\left(\\pi’\\right)$과 같이 정의할 때, 새로운 mixture policy $\\pi_\\mathrm{new}$를 다음과 같이 제안<br>$$<br>\\pi_\\mathrm{new}(a\\vert s) = (1-\\alpha)\\pi_\\mathrm{old}(a\\vert s) + \\alpha \\pi’(a\\vert s)<br>$$</li>\n</ul>\n<p>그림으로 표현하면 다음과 같습니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=2m46s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/mixure_policy.png\" alt=\"mixure_policy\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<ul>\n<li>다음과 같은 lower bound를 정의<br>$$<br>\\eta\\left(\\pi_\\mathrm{new}\\right) \\geq L_{\\pi_{\\theta_\\mathrm{old} }}\\left(\\pi_\\mathrm{new}\\right) - \\frac{2\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2,\\\\<br>\\mathrm{where\\ }\\epsilon = \\max_s\\left\\vert E_{a\\sim\\pi’(a\\vert s)}\\left[A_\\pi(s,a)\\right]\\right\\vert<br>$$</li>\n</ul>\n<p>하지만 mixture된 policy라는 것은 실용적이지 않습니다. </p>\n<p><br><br></p>\n<h1 id=\"3-Monotonic-Improvement-Guarantee-for-General-Stochastic-Policies\"><a href=\"#3-Monotonic-Improvement-Guarantee-for-General-Stochastic-Policies\" class=\"headerlink\" title=\"3. Monotonic Improvement Guarantee for General Stochastic Policies\"></a>3. Monotonic Improvement Guarantee for General Stochastic Policies</h1><p>이전 장에서 설명한 lower bound는 오직 mixture policy에 대해서만 성립하고 더 많이 사용되는 stochastic policy에는 적용되지 않습니다. 따라서 stochastic policy를 이용할 수 있도록 개선할 필요가 있습니다. 아래와 같이 기존 수식에서 두 가지를 바꿈으로써 이것이 가능합니다.</p>\n<ul>\n<li>$\\alpha\\rightarrow$ distance measure between $\\pi$ and $\\tilde\\pi$</li>\n<li>constant $\\epsilon\\rightarrow\\max_{s,a}\\left\\vert A_\\pi(s,a)\\right\\vert $</li>\n</ul>\n<p>여기서 distance measure로 total variation divergence를 이용합니다. discrete porbability distribution $p$와 $q$에 대하여 다음과 같이 정의됩니다.<br>$$<br>D_\\mathrm{TV}(p\\parallel q) = \\frac{1}{2}\\sum_i\\left\\vert p_i - q_i\\right\\vert<br>$$</p>\n<p>이것을 이용하여 $D_\\mathrm{TV}^\\max$를 다음과 같이 정의합니다.<br>$$<br>D_\\mathrm{TV}^\\max(\\pi\\parallel \\tilde\\pi) = \\max_s D_\\mathrm{TV}\\left(\\pi(\\cdot\\vert s)\\parallel\\tilde\\pi(\\cdot\\vert s)\\right)<br>$$</p>\n<p>그림으로 표현하면 다음과 같습니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=3m15s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/tvd.png\" alt=\"tvd\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>이것을 이용하여 다음과 같은 관계식을 얻을 수 있습니다.</p>\n<p><strong>Theorem 1.</strong> Let $\\alpha=D_\\mathrm{TV}^\\max(\\pi\\parallel \\tilde\\pi)$. Then the following bound holds:<br>$$<br>\\begin{align}<br>\\eta\\left(\\pi_\\mathrm{new}\\right) &amp;\\geq L_{\\pi_\\mathrm{old} }\\left(\\pi_\\mathrm{new}\\right) - \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2, \\\\<br>\\mathrm{where\\ } \\epsilon&amp;= \\max_{s,a}\\left\\vert A_\\pi(s,a)\\right\\vert<br>\\end{align}<br>$$<br><!--*Proof.* TBD.--></p>\n<p>또다른 distance metric으로 아래 그림과 같은 KL divergence가 있습니다.<br>(그런데 왜 하필 KL divergence로 바꿀까요? 논문의 뒤쪽에서 계산효율을 위해서 conjugate gradient method를 이용하는데 이를 위해서 바꾼게 아닐까 싶습니다. Wasserstein distance 같은 다른 방향으로 발전시킬 수도 있을 것 같습니다. Schulmann이 아마 해봤겠죠?^^a)</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=3m34s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/kld.png\" alt=\"kld\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>total variation divergence와 KL divergence 사이에는 다음과 같은 관계가 있습니다.<br>$$<br>D_\\mathrm{TV}(p\\parallel q)^2 \\leq D_\\mathrm{KL}(p\\parallel q)<br>$$</p>\n<p>다음 수식을 정의합니다.<br>$$<br>D_\\mathrm{KL}^\\max(\\pi\\parallel \\tilde\\pi) = \\max_s D_\\mathrm{KL}\\left(\\pi(\\cdot\\vert s)\\parallel\\tilde\\pi(\\cdot\\vert s)\\right)<br>$$</p>\n<p><strong>Theorem 1</strong>을 이용하여 다음과 같은 수식이 성립함을 알 수 있습니다.<br>$$<br>\\begin{align}<br>\\eta\\left(\\tilde\\pi\\right) &amp;\\geq L_{\\pi}\\left(\\tilde\\pi\\right) - C\\cdot D_\\mathrm{KL}^\\max(\\pi, \\tilde\\pi), \\\\<br>\\mathrm{where\\ } C&amp;= \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}<br>\\end{align}<br>$$</p>\n<p>이러한 policy imporvement bound를 기반으로 다음과 같은 approximate policy iteration 알고리듬을 고안해낼 수 있습니다.</p>\n<p><strong>Algorithm 1</strong> Policy iteration algorithm guaranteeing non-decreasing expected return $\\eta$</p>\n<blockquote>\n<p>Initialize $\\pi_0$<br><strong>for</strong> $i=0,1,2,\\ldots$ until convergence <strong>do</strong><br>$\\quad$Compute all advantage values $A_{\\pi_i}(s,a)$<br>$\\quad$Solve the contstrained optimization problem<br>$\\quad\\pi_{i+1}=\\arg\\max_\\pi\\left[L_{\\pi_i} - CD_\\mathrm{KL}^\\max\\left(\\pi_i,\\pi\\right)\\right]$<br>$\\quad\\quad$where $C = 4\\epsilon\\gamma / (1-\\gamma)^2$<br>$\\quad\\quad$and $L_{\\pi_i}\\left(\\pi\\right) = \\eta(\\pi_i) + \\sum_{s}\\rho_{\\pi_i}(s)\\sum_a\\pi(a\\vert s) A_{\\pi_i}(s,a)$<br><strong>end for</strong></p>\n</blockquote>\n<p><strong>Algorithm 1</strong>은 advantage를 정확하게 계산할 수 있다고 가정하고 있습니다. 이 알고리듬은  monotonical한 성능 증가($\\eta(\\pi_0)\\leq\\eta(\\pi_1)\\leq\\cdots$)를 한다는 것을 다음과 같이 보일 수 있습니다. $M_i(\\pi)=L_{\\pi_i}(\\pi) - CD_\\mathrm{KL}^\\max\\left(\\pi_i,\\pi\\right)$라고 합시다.</p>\n<p>$$<br>\\begin{align}<br>\\eta \\left(\\pi_{i+1}\\right) &amp;\\geq M_i\\left(\\pi_{i+1}\\right)\\\\<br>\\eta \\left(\\pi_{i}\\right) &amp;= M_i\\left(\\pi_{i}\\right) \\\\<br>\\eta \\left(\\pi_{i+1}\\right) - \\eta \\left(\\pi_{i}\\right) &amp;\\geq M_i\\left(\\pi_{i+1}\\right) - M_i\\left(\\pi_{i}\\right)<br>\\end{align}<br>$$</p>\n<p>위 수식과 같이 매 iteration 마다 $M_i$를 최대화함으로써, $\\eta$가 감소하지 않는다는 것을 보장할 수 있습니다. 이와 같은 타입의 알고리듬을 <a href=\"https://www.jstor.org/stable/pdf/27643496.pdf?casa_token=0qHamcl60WoAAAAA:-fkZ9JcA_nrY3-zbCUpqvPOgcAMgw7Gr96MajCZg2byHf8m5GU1KTSxyJJcBy1lPZbBTZVCjHHUXilh4k-iuwF91Wka4B5qdltC1IR2qMWk8q1FoV6__\" target=\"_blank\" rel=\"noopener\">minorization-maximization (MM) algorithm</a>이라고 합니다. EM 알고리듬도 이 타입의 알고리듬 중 하나입니다.</p>\n<p><strong>Algorithm 1</strong>은 아래 그림과 같이 동작합니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=3m52s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/surrogate.png\" alt=\"surrogate\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>$M_i$는 $\\pi_i$일 때 equality가 되는 $\\eta$에 대한  surrogate function입니다. TRPO는 이 surrogate function을 최대화하고 KL divergence를 penalty가 아닌 constraint로 두는 알고리듬입니다.</p>\n<p><br><br></p>\n<h1 id=\"4-Optimization-of-Parameterized-Policies\"><a href=\"#4-Optimization-of-Parameterized-Policies\" class=\"headerlink\" title=\"4. Optimization of Parameterized Policies\"></a>4. Optimization of Parameterized Policies</h1><p>표기의 편의를 위해 다음과 같이 notation들을 더 간략하게 정의합니다.</p>\n<ul>\n<li>$\\eta(\\theta):=\\eta\\left(\\pi_\\theta\\right)$</li>\n<li>$L_\\theta\\left(\\tilde\\theta\\right):=L_{\\pi_\\theta}\\left(\\pi_{\\tilde\\theta}\\right)$</li>\n<li>$D_\\mathrm{KL}\\left(\\theta\\parallel\\tilde\\theta\\right):=D_\\mathrm{KL}\\left(\\pi_\\theta\\parallel\\pi_{\\tilde\\theta}\\right)$</li>\n<li>$\\theta_\\mathrm{old}$: previous policy parameter</li>\n</ul>\n<h2 id=\"4-1-Trust-Region-Policy-Optimization\"><a href=\"#4-1-Trust-Region-Policy-Optimization\" class=\"headerlink\" title=\"4.1 Trust Region Policy Optimization\"></a>4.1 Trust Region Policy Optimization</h2><p>이전 장의 중요 결과를 위의 notation으로 다시 표기하면 아래와 같습니다.</p>\n<p>$$<br>\\eta\\left(\\theta\\right) \\geq L_{\\theta_\\mathrm{old} }\\left(\\theta\\right) - C\\cdot D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right)<br>$$</p>\n<p>$\\eta$의 성능 향상을 보장하기 위해서 그것의 lower bound를 최대화할 수 있습니다.</p>\n<p>$$<br>\\max_\\theta \\left[L_{\\theta_\\mathrm{old} }\\left(\\theta\\right) - C D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right)\\right]<br>$$</p>\n<p>이 최적화 문제는 step size를 매우 작게 해야 올바른 동작을 합니다. 위에서 살펴봤듯이 first order approximation이기 때문입니다. 좀 더 큰 step size를 가질 수 있도록 이 최적화 문제를 trust region constraint를 도입하여 다음과 같이 바꿉니다.</p>\n<p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;L_{\\theta_\\mathrm{old} }(\\theta) \\\\<br>\\mathrm{s.t.\\ }&amp;D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right) \\leq \\delta<br>\\end{align}$$</p>\n<p>이 최적화문제의 constraint는 모든 state space에 대해서 성립해야 합니다. 또한 maximum값을 매번 찾아야 합니다. state가 많은 경우 constraint의 수가 매우 많아져서 문제를 풀기 어렵게 만듭니다. constraint의 수를 줄이기 위하여 다음과 같은 avergae KL divergence를 이용하는 heuristic approximation을 취합니다. 이것이 최선의 방법은 아닐 수 있지만 실용적인 방법입니다.</p>\n<p>$$<br>\\overline D_\\mathrm{KL}^{\\rho}\\left(\\theta_1, \\theta_2\\right):=E_{s\\sim\\rho}\\left[D_\\mathrm{KL}\\left(\\pi_{\\theta_1}(\\cdot\\vert s)\\parallel\\pi_{\\theta_2}(\\cdot\\vert s)\\right)\\right]<br>$$</p>\n<p>이것을 기반으로 다음과 같은 최적화 문제를 풀 수 있습니다.</p>\n<p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;L_{\\theta_\\mathrm{old} }(\\theta) \\\\<br>\\mathrm{s.t.\\ }&amp;\\overline{D}_\\mathrm{KL}^{\\rho_\\mathrm{old} }\\left(\\theta_\\mathrm{old}, \\theta\\right) \\leq \\delta<br>\\end{align}$$</p>\n<p>아래 그림처럼 step size에 대해서 고려할 수 있습니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=4m35s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/heuristic_approx.png\" alt=\"heuristic_approx\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p><br><br></p>\n<h1 id=\"5-Sample-Based-Estimation-of-the-Objective-and-Constraint\"><a href=\"#5-Sample-Based-Estimation-of-the-Objective-and-Constraint\" class=\"headerlink\" title=\"5. Sample-Based Estimation of the Objective and Constraint\"></a>5. Sample-Based Estimation of the Objective and Constraint</h1><p>실용적인 알고리듬을 만들려고 하는 노력은 아직 끝나지 않았습니다. 이제 앞의 알고리듬을 sample-based estimation 즉, Monte Carlo estimation을 할 수 있도록 바꿔보겠습니다. sampling을 편하게 할 수 있도록 아래와 같이 바꿔줍니다.</p>\n<ul>\n<li>$\\sum_s \\rho_{\\theta_\\mathrm{old} }(s)[\\ldots]\\rightarrow\\frac{1}{1-\\gamma}E_{s\\sim\\rho_{\\theta_\\mathrm{old} }}[\\ldots]$</li>\n<li>$A_{\\theta_\\mathrm{old} }\\rightarrow Q_{\\theta_\\mathrm{old} }$</li>\n<li>$\\sum_a\\pi_{\\theta_\\mathrm{old} }(a\\vert s)A_{\\theta_\\mathrm{old} } \\rightarrow E_{a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}A_{\\theta_\\mathrm{old} }\\right]$</li>\n</ul>\n<p>이러한 변화를 그림처럼 도식화할 수 있습니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=5m31s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/sample-based.png\" alt=\"sample-based\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>한 가지 짚고 넘어가야 할 점은 action sampling을 할 때 importance sampling을 사용한다는 것입니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=5m53s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/importance_sampling.png\" alt=\"importance_sampling\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>바뀐 최적화문제는 아래와 같습니다.</p>\n<ul>\n<li><strong>Equation (14).</strong></li>\n</ul>\n<p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}Q_{\\theta_\\mathrm{old} }(s,a)\\right] \\\\<br>\\mathrm{s.t.\\ }&amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} }}\\left[D_\\mathrm{KL}\\left(\\pi_{\\theta_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta<br>\\end{align}<br>$$</p>\n<p>이 때 sampling하는 두 가지 방법이 있습니다.</p>\n<p><br></p>\n<h2 id=\"5-1-Single-Path\"><a href=\"#5-1-Single-Path\" class=\"headerlink\" title=\"5.1 Single Path\"></a>5.1 Single Path</h2><p><em>single path</em>는 개별 trajectory들을 이용하는 방법입니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=6m15s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/single.png\" alt=\"single\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p><br></p>\n<h2 id=\"5-2-Vine\"><a href=\"#5-2-Vine\" class=\"headerlink\" title=\"5.2 Vine\"></a>5.2 Vine</h2><p><em>vine</em>은 한 state에서 rollout을 이용하여 여러 action을 수행하는 방법입니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=6m32s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/vine1.png\" alt=\"vine1\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=6m49s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/vine2.png\" alt=\"vine2\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>estimation의 variance를 낮출 수 있지만 계산량이 많고 한 state에서 여러 action을 수행할 수 있어야 하기 때문에 현실적인 문제에 적용하기에는 어려움이 있습니다.</p>\n<p><br><br></p>\n<h1 id=\"6-Practical-Algorithm\"><a href=\"#6-Practical-Algorithm\" class=\"headerlink\" title=\"6. Practical Algorithm\"></a>6. Practical Algorithm</h1><p>앞서 single-path, vine 샘플링을 사용하는 두가지 방식의 policy optimization 알고리즘을 살펴봤습니다. 실용적인 알고리듬은 아래의 과정을 반복해서 수행합니다.</p>\n<p>1) Q-values의 몬테카를로 추정을 통해 state-action 쌍 집합을 single path 또는 vine 과정을 통해 수집함.</p>\n<p>2) 샘플 평균으로, (14)식의 목적함수와 제약식을 추정함</p>\n<ul>\n<li><strong>Equation (14).</strong><br>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}Q_{\\theta_\\mathrm{old} }(s,a)\\right] \\\\<br>\\mathrm{s.t.\\ }&amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} }}\\left[D_\\mathrm{KL}\\left(\\pi_{\\theta_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta<br>\\end{align}<br>$$</li>\n</ul>\n<p>3) policy parameter vector인 $\\theta$를 업데이트 하면서 제약조건이 있는 최적화 문제를 근사적으로 풂. 본 논문에서는 gradient를 직접 계산하는 것보다는 약간 더 계산량이 있는 line search와 conjugate gradient algorithm을 사용했습니다.</p>\n<p>3)에 대해서,  gradient의 covariance matrix를 사용하지 않고 KL divergence의 Hessian을 해석적으로 계산하여 Fisher Information Matrix를 구성했습니다.</p>\n<p>다시말해서</p>\n<p>$\\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial}{\\partial \\theta_i} \\log \\pi_{\\theta}(a_n\\vert s_n) \\frac{\\partial}{\\partial \\theta_j} \\log \\pi_{\\theta}(a_n\\vert s_n)$ 대신 $\\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_j} D_{KL}(\\pi_{\\theta_{old} }(\\cdot \\vert  s_n) \\vert \\vert  \\pi_{\\theta}(\\cdot \\vert  s_n))$를 계산한 것입니다.</p>\n<p>이 analytic estimator는 Hessian이나 trajectories의 모든 gradient를 저장하지 않아도 되기 때문에 대규모 환경을 고려할 경우 계산 상 이점이 있습니다. 좀 더 자세하게 설명한 Appendix C 부분을 살펴보겠습니다.</p>\n<p><br></p>\n<h2 id=\"Appendix-C-Efficiently-Solving-the-Trust-Region-Constrained-Optimization-Problem\"><a href=\"#Appendix-C-Efficiently-Solving-the-Trust-Region-Constrained-Optimization-Problem\" class=\"headerlink\" title=\"Appendix C. Efficiently Solving the Trust-Region Constrained Optimization Problem\"></a>Appendix C. Efficiently Solving the Trust-Region Constrained Optimization Problem</h2><p>우리가 풀고자 하는 식은 다음과 같습니다.</p>\n<p>$$<br>\\begin{align}<br>\\max\\quad &amp;L(\\theta) \\\\<br>\\mathrm{s.t.\\ } &amp;\\overline{D}_{KL}(\\theta_\\mathrm{old}, \\theta) \\leq \\delta<br>\\end{align}<br>$$</p>\n<p>이 최적화 문제를 풀기 위하여 우리는 gradient 기법을 이용할 것입니다. gradient 기법은 결국 update할 방향과 크기를 결정하는 문제로 생각할 수 있습니다. 이에 따라 아래와 같은 두가지 과정으로 진행합니다.</p>\n<p>1) 탐색 방향을 계산, 목적함수의 first-order approximation(선형근사)와 제약식의 quadratic approximation(2차 근사).</p>\n<p>2) 이동거리 계산을 위해 해당 방향으로 line search 수행.</p>\n<p>탐색 방향은 $Ax = g$ 수식을 근사적으로 풀어서 구합니다. 여기서 $A$ 는 Fisher Information Matrix이고, 이것은 KL divergence 제약식의 2차 근사 $\\overline{D}_\\mathrm{KL} (\\theta_\\mathrm{old}, \\theta) \\approx \\frac{1}{2}(\\theta - \\theta_\\mathrm{old})^T A(\\theta - \\theta_\\mathrm{old})$를 푸는 것입니다. 여기서 $A_{ij} = \\frac{\\partial}{\\partial \\theta_i} \\frac{\\partial}{\\partial \\theta_j} \\overline{D}_\\mathrm{KL}(\\theta_\\mathrm{old}, \\theta)$입니다.</p>\n<p>논문에서 이 부분에 대한 설명이 약간 부족해서 좀 더 내용을 추가하자면, $\\frac{1}{2}x^T A x - x^T g + b = 0$과 같은 quadratic equation의 해는 1차 derivative를 0으로 만드는 $x$를 찾음으로써 알 수 있습니다. 위에서 우리는 목적함수를 1차 근사시키고 제약조건은 2차 근사를 시킴으로써 quadratic equation 형태로 변형했기 때문에 같은 방식으로 풀 수 있습니다. 따라서 $g$는 1차근사시킨 목적함수의 계수가 될 것입니다.</p>\n<p>대규모 문제에서 $A$ 혹은 $A^{-1}$ 을 계산하는 것은 엄청난 계산량이 필요합니다. 하지만 conjugate gradient algorithm을 잘 이용하면 전체 $A$행렬(FIM)을 계산하지 않아도 $Ax=g$ 를 근사적으로 해결할 수 있게 해줍니다.</p>\n<p>탐색 방향 $s \\approx A^{-1}g$과 더불어 최대스텝길이(step size) $\\beta$도 계산할 필요가 있습니다.</p>\n<p>즉, $\\delta = \\overline{D}_\\mathrm{KL} \\approx \\frac{1}{2}(\\beta s)^T A(\\beta s) = \\frac{1}{2}\\beta^2s^TAs$ 이고, </p>\n<p>$\\beta = \\sqrt{2\\delta / s^T As}$가 됩니다. 이때 $\\delta$는 KL divergence의 boundary term 입니다.</p>\n<p>$s^T As$ 항은 Hessian-vector product로 계산할 수 있고, conjugate gradient 과정에서 계산됩니다.</p>\n<p>마지막으로, surrogate objective와 KL divergence 제약식을 위해 다음과 같은 함수를 정의하고 line search를 사용합니다.</p>\n<p>$$<br>L_{\\theta_\\mathrm{old} }(\\theta) - \\chi [\\overline{D}_{KL}(\\theta_\\mathrm{old}, \\theta) \\leq \\delta]<br>$$</p>\n<p>$\\chi[\\cdot]$ 함수는 []안의 조건이 맞으면 0이고 틀리면 무한으로 발산하는 함수입니다.</p>\n<p>위에서 계산한 $\\beta$ 의 최대 값부터 시작해서 목적함수가 개선될때까지 exponential하게 줄여 나갑니다. line search가 없었다면 이 알고리즘은 매우 큰 스텝으로 계산될 것이고 심각한 성능 저하가 발생할 것입니다.</p>\n<p>(여기서 살짝 예고를 하자면, 나중에 이 KL 기반 기법을 개선한 더 단순한 방법이 제안됩니다. PPO라고…)</p>\n<p><br><br></p>\n<h1 id=\"7-Connections-with-Prior-Work\"><a href=\"#7-Connections-with-Prior-Work\" class=\"headerlink\" title=\"7. Connections with Prior Work\"></a>7. Connections with Prior Work</h1><p>Natural Policy Gradient는 $L$의 선형 근사와 $\\overline D_\\mathrm{KL}$ 제약식을 2차근사 하는 아래의 식의 special case입니다.</p>\n<ul>\n<li>Equation (17).</li>\n</ul>\n<p>$L_{\\pi}(\\tilde\\pi) = \\eta(\\pi) + \\underset{s}{\\sum}\\rho_{\\pi}(s) \\underset{a}{\\sum}\\tilde\\pi(a\\vert s)A_\\pi(s,a)$</p>\n<p>$\\underset{\\theta}{\\max}\\quad [\\nabla_{\\theta}L_{\\theta_\\mathrm{old} }(\\theta) \\ \\rvert_{\\theta=\\theta_\\mathrm{old} } \\cdot (\\theta - \\theta_\\mathrm{old})]$</p>\n<p>$\\mathrm{s.t.}\\quad\\frac{1}{2}(\\theta_\\mathrm{old} - \\theta)^{T} A(\\theta_\\mathrm{old})(\\theta_\\mathrm{old}-\\theta) \\leq \\delta$</p>\n<p>$\\mathrm{where\\ }A(\\theta_{old})_{ij} = $</p>\n<p>$\\Large \\frac{\\partial}{\\partial \\theta_{i} } \\frac{\\partial}{\\partial \\theta_{j} }<br>E_{s \\sim \\rho_{\\pi} }[D_\\mathrm{KL}(\\pi(\\cdot\\rvert s, \\theta_\\mathrm{old})  \\rvert\\rvert \\pi(\\cdot \\rvert s, \\theta))] \\rvert_{\\theta=\\theta_\\mathrm{old} }$</p>\n<p>업데이트 식은 다음과 같습니다.</p>\n<p>$\\theta_\\mathrm{new} = \\theta_\\mathrm{old} +  \\color{Red}{\\frac{1}{\\lambda} } A(\\theta_\\mathrm{old})^{-1}\\nabla_{\\theta}L(\\theta) \\rvert_{\\theta=\\theta_\\mathrm{old} }$</p>\n<p>여기서 step size인 $\\frac{1}{\\lambda}$ 일반적으로 알고리즘의 파라미터로 취급되지만 TRPO는 각 업데이트 마다 제약식으로 사용한다는 점이 다릅니다. 사소한 차이처럼 보이지만, 큰 차원을 다루는 실험에서 성능을 크게 향상시켰습니다.</p>\n<p>또한 $L^2$ 제약식(혹은 페널티) 를 사용하면 표준 Policy Gradient 업데이트 식을 얻었습니다.</p>\n<ul>\n<li>Equation (18).</li>\n</ul>\n<p>$\\underset{\\theta}{\\max}\\quad[\\nabla_{\\theta} L_{\\theta_\\mathrm{old} }(\\theta) \\rvert_{\\theta=\\theta_\\mathrm{old} } \\cdot (\\theta - \\theta_\\mathrm{old})]$</p>\n<p>$\\mathrm{s.t.}\\quad \\frac{1}{2} \\vert\\vert \\theta - \\theta_\\mathrm{old} \\vert\\vert^2 \\leq \\delta$</p>\n<p>$L_{\\pi}(\\tilde\\pi) = \\eta(\\pi) + \\underset{s}{\\sum}\\rho_{\\pi}(s) \\underset{a}{\\sum}\\tilde\\pi(a\\vert s)A_\\pi(s,a) $를 이용해서 제약조건없이 $\\underset{\\pi}{\\max}\\quad L_{\\pi_\\mathrm{old} }(\\pi)$를 풀면<br>Policy Iteration update를 하는 것과 같습니다.</p>\n<p><br><br></p>\n<h1 id=\"8-Experiments\"><a href=\"#8-Experiments\" class=\"headerlink\" title=\"8. Experiments\"></a>8. Experiments</h1><ul>\n<li><p>다음과 같은 3 가지 궁금증을 해결하기 위하여 실험을 설계하였습니다.</p>\n<ol>\n<li><p>Single-path 와 vine 의 성능면에서 특징을 알고싶음.</p>\n</li>\n<li><p>Fixed penalty coefficient(NPG) 보다 fixed KL divergence를 사용하는 것(TRPO)으로<br>변경한 것이 얼마나 뚜렷한 차이를 보일지, 성능 면에서 어떤 영향을 미치는지 알고싶음.</p>\n</li>\n<li><p>TRPO가 큰 스케일의 문제를 해결할 수 있는지, 기존에 연구/적용되어왔던 방법들과<br>성능, 계산시간, 샘플 복잡도 면에서 비교하고 싶음.</p>\n</li>\n</ol>\n</li>\n</ul>\n<ul>\n<li><p>1, 2의 궁금증에 대해서 실험 환경에 single path, vine을 비롯한 여러 사전방법들을 함께 실험하였습니다.</p>\n</li>\n<li><p>3에 대해선, robotic locomotion과 Atari game에 TRPO를 실험하였습니다.</p>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"8-1-Simulated-Robotic-Locomotion\"><a href=\"#8-1-Simulated-Robotic-Locomotion\" class=\"headerlink\" title=\"8.1 Simulated Robotic Locomotion\"></a>8.1 Simulated Robotic Locomotion</h2><ul>\n<li>MuJoCo 시뮬레이터를 활용해 로봇 locomotion 실험 진행하였습니다.</li>\n<li>Robot State: positions and velocities, joint torques</li>\n</ul>\n<p><img src=\"https://i.imgur.com/bs5ATS3.png\" alt=\"\"></p>\n<ul>\n<li><p>수영, 점프, 걷기 행동을 학습시키고자 합니다.</p>\n<ul>\n<li><p>수영: 10 dimensional state space , reward: $r(x,u)=v_x - 10^{-5}\\vert \\vert u\\vert \\vert ^2$, quadratic penalty.</p>\n</li>\n<li><p>점프: 12 dimensional state space, episode의 끝은 높이와 각도 threshol로 점프를 판단, non-terminal state에 대해 +1 보상 추가.</p>\n</li>\n<li><p>걷기: 18 dimensional state space, 뜀뛰는 듯한 보법(점프에이전트)보다 부드러운 보법을 유도하기위해 페널티를 추가함.</p>\n</li>\n</ul>\n</li>\n</ul>\n<p><strong>Detailed Experiment Setup and used parameters, used network model</strong><br><img src=\"https://i.imgur.com/zgnsbw6.png\" alt=\"\"></p>\n<p><img src=\"https://i.imgur.com/FqdWC53.png\" alt=\"\"></p>\n<p>equation (12) : $\\max\\quad L_{\\theta_\\mathrm{old}(\\theta)}, \\ \\ \\mathrm{s.t.\\ }\\overline{D}_\\mathrm{KL}^{\\rho_{\\theta_\\mathrm{old} }}(\\theta_\\mathrm{old}, \\theta) \\leq \\delta$</p>\n<p>이 실험에서 $\\delta = 0.01$입니다.</p>\n<ul>\n<li><p>비교에 사용된 모델들</p>\n<ul>\n<li>TRPO Single-path</li>\n<li>TRPO vine</li>\n<li>CEM(cross-entropy method)</li>\n<li>CMA(covariance matrix adaptation)</li>\n<li>NPG (Lagrange Multiplier penalty coefficien를 사용하는 것이 차이점)</li>\n<li>Empirical FIM</li>\n<li>maximum KL(cartpole)</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://i.imgur.com/wSqolvS.png\" alt=\"\"></p>\n<ul>\n<li><p>제안한 single path, vine 모두 task를 잘 학습하는 것을 확인할 수 있습니다.</p>\n</li>\n<li><p>페널티를 고정하는 것보다, KL divergenc를 제약하는 것이 step size를 선택하는 부분에서 Robust합니다.</p>\n</li>\n<li><p>CEM/CMA는 derivative free 알고리즘으로, 샘플 복잡도가 네트워크 파라미터수 만큼 확장되어 high dimension 문제에서는 속도와 성능이 약한 모습을 보입니다.</p>\n</li>\n<li><p>maxKL은 제안방법보다 느린 학습성능을 보입니다.</p>\n</li>\n</ul>\n<p>TRPO video link: <a href=\"https://www.youtube.com/watch?v=jeid0wIrSn4\" target=\"_blank\" rel=\"noopener\">https://www.youtube.com/watch?v=jeid0wIrSn4</a></p>\n<ul>\n<li><p>사전지식이 적은 상태에서 여러 액션을 학습하는 실험으로 TRPO의 성능을 보입니다.</p>\n</li>\n<li><p>지금까지의 균형이나 걸음 같은 개념을 명시적으로 인코딩한 robotic locomotion에서의 사전 연구들과 차이를 보였습니다.</p>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"8-2-Playing-Games-from-Images\"><a href=\"#8-2-Playing-Games-from-Images\" class=\"headerlink\" title=\"8.2 Playing Games from Images\"></a>8.2 Playing Games from Images</h2><ul>\n<li>부분관찰가능하며 복잡한 observation인 환경에서 TRPO를 평가하기위해 raw image를 입력으로 하는 아타리게임 환경에서 실험하였습니다.</li>\n</ul>\n<ul>\n<li>Challenging point :<ul>\n<li>High dimensional space</li>\n<li>delayed reward</li>\n<li>non-stationary image (Enduro는 배경 이미지가 바뀌고, 반전처리가 됨)</li>\n<li>complex action sequence (Q*bert는 21개의 다른 발판에서 점프 해야함)</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li>ALE(Arcade Learning Environment) 에서 테스트</li>\n<li><p>DQN과 전처리 방식은 동일<br>  210 x 160 pixel -&gt; gray-scaling &amp; down-sampling -&gt; 110 x 84  -&gt; crop -&gt; 84 x 84</p>\n</li>\n<li><p>2 convolution layer ( 16 channels) , stride 2, FC layer with 20 units</p>\n</li>\n</ul>\n<p><img src=\"https://i.imgur.com/NJBC69d.png\" alt=\"\"> </p>\n<p><img src=\"https://i.imgur.com/wTe1OEW.png\" alt=\"\"></p>\n<p><img src=\"https://i.imgur.com/j22VtNQ.png\" alt=\"\"></p>\n<ul>\n<li><p>30시간 동안 500 iteration 학습을 하였습니다.</p>\n</li>\n<li><p>DQN과 MCST를 사용한 모델(UCC-I), 사람이 플레이한 것(Human) 과의 비교하였습니다.</p>\n</li>\n<li><p>이전 방법들에 비해 압도적인 점수를 기록하진 못했으나, 다양한 게임에서 적절한 게임 기록하였습니다.</p>\n</li>\n<li><p>이전 방법들과 다른 점은, 특정 task에 초점을 맞춰 설계하지않았는데 robotics나 game playing Task에도 적절한 성능을 내는 점에서 <strong>TRPO의 일반화 성능</strong>을 확인하였습니다.</p>\n</li>\n</ul>\n<p><br></p>\n<h1 id=\"9-Discussion\"><a href=\"#9-Discussion\" class=\"headerlink\" title=\"9. Discussion\"></a>9. Discussion</h1><ul>\n<li><p>Trust Region Policy Optimization 을 제안하였습니다.</p>\n</li>\n<li><p>KL Divergence 페널티로 $\\eta(\\pi)$를 최적화하는 알고리즘이 monotonically improve 함을 증명하였습니다.</p>\n</li>\n<li><p>Locomotion 도메인에서 여러 행동(수영, 걷기, 점프)을 제어하는 controller를 학습하였습니다.</p>\n</li>\n<li><p>Robotics와 게임 실험을 결합해, 시각정보와 센서데이터를 사용하는 로봇제어 정책을 학습시킬 수 있는 가능성을 보았습니다.</p>\n</li>\n<li><p>샘플의 복잡도를 상당히 줄일 수 있어 실제상황에 적용가능성을 보았습니다.</p>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"END\"><a href=\"#END\" class=\"headerlink\" title=\"END\"></a>END</h1>"}],"PostAsset":[],"PostCategory":[{"post_id":"cjiinnj6n0002j28aigqpmv52","category_id":"cjiinnj6o0003j28aoi038xjq","_id":"cjiinnj6v000cj28a4b9xquku"},{"post_id":"cjiinnj6q0005j28a4kpaxi6x","category_id":"cjiinnj6o0003j28aoi038xjq","_id":"cjiinnj6x000fj28ahxrt77uz"},{"post_id":"cjjgzyg440000xo155e3g8dvy","category_id":"cjiinnj6o0003j28aoi038xjq","_id":"cjjgzyg4d0004xo15qhfekcw3"},{"post_id":"cjjgzyg4a0001xo15qjhkwruc","category_id":"cjiinnj6o0003j28aoi038xjq","_id":"cjjgzyg4e0006xo15arbwx7bl"},{"post_id":"cjjgzyg4h0008xo15m13f942j","category_id":"cjiinnj6o0003j28aoi038xjq","_id":"cjjgzyg4k000bxo155hmgt28u"},{"post_id":"cjjgzyg4q000cxo15a2yfnmeh","category_id":"cjiinnj6o0003j28aoi038xjq","_id":"cjjgzyg4r000fxo15rmjgvhsd"},{"post_id":"cjjptjn050000ue153nep5uvv","category_id":"cjiinnj6o0003j28aoi038xjq","_id":"cjjpwko5p0003ue15qna2hyfh"},{"post_id":"cjjr006cr0001ie15li5wd6lu","category_id":"cjiinnj6o0003j28aoi038xjq","_id":"cjjr0l7340002xt15w148elij"}],"PostTag":[{"post_id":"cjiinnj6n0002j28aigqpmv52","tag_id":"cjiinnj6q0004j28ajv2qofj6","_id":"cjiinnj6x000hj28a6oe113d1"},{"post_id":"cjiinnj6n0002j28aigqpmv52","tag_id":"cjiinnj6r0007j28a9ifjaq6c","_id":"cjiinnj6x000ij28ae2s24zk6"},{"post_id":"cjiinnj6q0005j28a4kpaxi6x","tag_id":"cjiinnj6q0004j28ajv2qofj6","_id":"cjiinnj6y000kj28a6c947dl2"},{"post_id":"cjiinnj6q0005j28a4kpaxi6x","tag_id":"cjiinnj6r0007j28a9ifjaq6c","_id":"cjiinnj6y000lj28adzfrq45d"},{"post_id":"cjjgzyg440000xo155e3g8dvy","tag_id":"cjiinnj6q0004j28ajv2qofj6","_id":"cjjgzyg4d0002xo159z9739xo"},{"post_id":"cjjgzyg440000xo155e3g8dvy","tag_id":"cjiinnj6r0007j28a9ifjaq6c","_id":"cjjgzyg4d0003xo15dky9nobj"},{"post_id":"cjjgzyg4a0001xo15qjhkwruc","tag_id":"cjiinnj6q0004j28ajv2qofj6","_id":"cjjgzyg4d0005xo15lberci4o"},{"post_id":"cjjgzyg4a0001xo15qjhkwruc","tag_id":"cjiinnj6r0007j28a9ifjaq6c","_id":"cjjgzyg4f0007xo15gtfyxdmt"},{"post_id":"cjjgzyg4h0008xo15m13f942j","tag_id":"cjiinnj6q0004j28ajv2qofj6","_id":"cjjgzyg4k0009xo15c87l0wgp"},{"post_id":"cjjgzyg4h0008xo15m13f942j","tag_id":"cjiinnj6r0007j28a9ifjaq6c","_id":"cjjgzyg4k000axo15zbyevg11"},{"post_id":"cjjgzyg4q000cxo15a2yfnmeh","tag_id":"cjiinnj6q0004j28ajv2qofj6","_id":"cjjgzyg4r000dxo15tk85qwq2"},{"post_id":"cjjgzyg4q000cxo15a2yfnmeh","tag_id":"cjiinnj6r0007j28a9ifjaq6c","_id":"cjjgzyg4r000exo15ah7nu7a8"},{"post_id":"cjjptjn050000ue153nep5uvv","tag_id":"cjiinnj6q0004j28ajv2qofj6","_id":"cjjpwko5p0001ue157qlfenx7"},{"post_id":"cjjptjn050000ue153nep5uvv","tag_id":"cjiinnj6r0007j28a9ifjaq6c","_id":"cjjpwko5p0002ue15avzv023y"},{"post_id":"cjjr006cr0001ie15li5wd6lu","tag_id":"cjiinnj6q0004j28ajv2qofj6","_id":"cjjr0l7330000xt15rdzre7jl"},{"post_id":"cjjr006cr0001ie15li5wd6lu","tag_id":"cjiinnj6r0007j28a9ifjaq6c","_id":"cjjr0l7340001xt15h1cxp7a3"}],"Tag":[{"name":"프로젝트","_id":"cjiinnj6q0004j28ajv2qofj6"},{"name":"피지여행","_id":"cjiinnj6r0007j28a9ifjaq6c"}]}}