{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/img/Exp_OctopusArm.png","path":"img/Exp_OctopusArm.png","modified":0,"renderable":0},{"_id":"source/img/Exp_ContinuousBandit.png","path":"img/Exp_ContinuousBandit.png","modified":0,"renderable":0},{"_id":"source/img/Ref_Degris2012b_Theorem1.png","path":"img/Ref_Degris2012b_Theorem1.png","modified":0,"renderable":0},{"_id":"source/img/Exp_OctopusArm_Ref.png","path":"img/Exp_OctopusArm_Ref.png","modified":0,"renderable":0},{"_id":"source/img/Exp_ContinuousRL.png","path":"img/Exp_ContinuousRL.png","modified":0,"renderable":0},{"_id":"source/img/Ref_Degris2012b_offpolicygradient.png","path":"img/Ref_Degris2012b_offpolicygradient.png","modified":0,"renderable":0},{"_id":"themes/clean-blog/source/css/article.styl","path":"css/article.styl","modified":0,"renderable":1},{"_id":"themes/clean-blog/source/css/base.styl","path":"css/base.styl","modified":0,"renderable":1},{"_id":"themes/clean-blog/source/css/mixins.styl","path":"css/mixins.styl","modified":0,"renderable":1},{"_id":"themes/clean-blog/source/css/style.styl","path":"css/style.styl","modified":0,"renderable":1},{"_id":"themes/clean-blog/source/css/variables.styl","path":"css/variables.styl","modified":0,"renderable":1},{"_id":"themes/clean-blog/source/img/about-bg.jpg","path":"img/about-bg.jpg","modified":0,"renderable":1},{"_id":"themes/clean-blog/source/img/favicon.ico","path":"img/favicon.ico","modified":0,"renderable":1},{"_id":"themes/clean-blog/source/img/home-bg.jpg","path":"img/home-bg.jpg","modified":0,"renderable":1},{"_id":"themes/clean-blog/source/img/contact-bg.jpg","path":"img/contact-bg.jpg","modified":0,"renderable":1},{"_id":"source/img/1.jpg","path":"img/1.jpg","modified":0,"renderable":0},{"_id":"source/img/mixure_policy.png","path":"img/mixure_policy.png","modified":0,"renderable":0},{"_id":"source/img/tvd.png","path":"img/tvd.png","modified":0,"renderable":0},{"_id":"source/img/importance_sampling.png","path":"img/importance_sampling.png","modified":0,"renderable":0},{"_id":"source/img/heuristic_approx.png","path":"img/heuristic_approx.png","modified":0,"renderable":0},{"_id":"source/img/kld.png","path":"img/kld.png","modified":0,"renderable":0},{"_id":"source/img/sample-based.png","path":"img/sample-based.png","modified":0,"renderable":0},{"_id":"source/img/single.png","path":"img/single.png","modified":0,"renderable":0},{"_id":"source/img/surrogate.png","path":"img/surrogate.png","modified":0,"renderable":0},{"_id":"source/img/vine1.png","path":"img/vine1.png","modified":0,"renderable":0},{"_id":"source/img/vine2.png","path":"img/vine2.png","modified":0,"renderable":0},{"_id":"source/img/state_visitation_change.png","path":"img/state_visitation_change.png","modified":0,"renderable":0},{"_id":"source/img/policy_change.png","path":"img/policy_change.png","modified":0,"renderable":0}],"Cache":[{"_id":"source/.DS_Store","hash":"b48c4f7d61a5928be717d4bd654481ff1eab36ee","modified":1531486622126},{"_id":"themes/clean-blog/.DS_Store","hash":"18b40c0a074fb756ef76fd437c21c2205a08a585","modified":1529045736390},{"_id":"themes/clean-blog/LICENSE","hash":"8726b416df4f067cff579e859f05c4b594b8be09","modified":1529043522273},{"_id":"themes/clean-blog/README.md","hash":"861dd2f959ab75d121226f4f3e2f61f4bc95fddb","modified":1529043522273},{"_id":"themes/clean-blog/_config.yml","hash":"c0ee3fcf1841410d07402e8d6e50e8847f31ae4b","modified":1529047469272},{"_id":"source/_posts/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1529228886819},{"_id":"source/_posts/sutton-pg.md","hash":"635caa593e001a1a2150fbd7a2a50de6d28d8175","modified":1531628245398},{"_id":"source/_posts/피지여행-소개.md","hash":"a3b89bda4864cfb84bdb759847d6531e9a39c491","modified":1529229006186},{"_id":"source/_posts/2018-06-15-npg.md","hash":"ad0e79383db0552d94078ca181c5e7c764b81f57","modified":1529150062936},{"_id":"source/_posts/dpg.md","hash":"9513ae8995eda0086e49dce88cac1669b3adc636","modified":1529158194593},{"_id":"source/img/Exp_OctopusArm.png","hash":"f0ae59d0c2a6600ef961fd0b2ea8903dcfdbd4d9","modified":1529117349098},{"_id":"source/img/Exp_ContinuousBandit.png","hash":"72eddde3b296070075706688038f1267bfd04d31","modified":1529113799292},{"_id":"source/img/Ref_Degris2012b_Theorem1.png","hash":"154f42239538c3dd0508e815014d84685e18f89b","modified":1529106775328},{"_id":"themes/clean-blog/languages/default.yml","hash":"97326c9e6518d9f379778178b3b8f9a58434725d","modified":1529043522274},{"_id":"themes/clean-blog/languages/en.yml","hash":"97326c9e6518d9f379778178b3b8f9a58434725d","modified":1529043522274},{"_id":"themes/clean-blog/languages/de.yml","hash":"424a9c1e6ab69334d7873f6574da02ca960aa572","modified":1529043522273},{"_id":"themes/clean-blog/languages/no.yml","hash":"8ca475a3b4f8efe6603030f0013aae39668230e1","modified":1529043522274},{"_id":"themes/clean-blog/languages/es.yml","hash":"cb4eeca0ed3768a77e0cd216300f2b2549628b1b","modified":1529043522274},{"_id":"themes/clean-blog/languages/fr.yml","hash":"e9e6f7cb362ebb7997f11027498a2748fe3bac95","modified":1529043522274},{"_id":"themes/clean-blog/languages/pt.yml","hash":"1d0c3689eb32fe13f37f8f6f303af7624ebfbaf0","modified":1529043522275},{"_id":"themes/clean-blog/languages/pl.yml","hash":"de7eb5850ae65ba7638e907c805fea90617a988c","modified":1529043522274},{"_id":"themes/clean-blog/languages/ru.yml","hash":"42df7afeb7a35dc46d272b7f4fb880a9d9ebcaa5","modified":1529043522275},{"_id":"themes/clean-blog/languages/zh-TW.yml","hash":"9acac6cc4f8002c3fa53ff69fb8cf66c915bd016","modified":1529043522275},{"_id":"themes/clean-blog/languages/zh-CN.yml","hash":"7bfcb0b8e97d7e5edcfca8ab26d55d9da2573c1c","modified":1529043522275},{"_id":"themes/clean-blog/layout/index.ejs","hash":"425da730d537805046040c5070571d9e739d4b19","modified":1529229786757},{"_id":"themes/clean-blog/layout/archive.ejs","hash":"f2ef73afc3d275333329bb30b9369b82e119da76","modified":1529043522279},{"_id":"themes/clean-blog/layout/layout.ejs","hash":"da2f9018047924ddaf376aee5996c7ddc06cebc1","modified":1529043522279},{"_id":"themes/clean-blog/layout/page.ejs","hash":"591af587e1aae962950de7e79bd25c1f060c69ac","modified":1529043522279},{"_id":"themes/clean-blog/source/.DS_Store","hash":"84f35e390633eadc3c78584a28f5f5f8ce7f43a5","modified":1529045731387},{"_id":"themes/clean-blog/layout/post.ejs","hash":"38382e9bbeb6b8d2eafbd53fff2984111f524c1a","modified":1529043522279},{"_id":"source/img/Exp_OctopusArm_Ref.png","hash":"ebf69a16fc09e447442808f8cc46d770bed0cf10","modified":1529116470075},{"_id":"source/img/Exp_ContinuousRL.png","hash":"85b9ebfb164631804d2fb6be0164e49cd1db746b","modified":1529115804602},{"_id":"source/img/Ref_Degris2012b_offpolicygradient.png","hash":"46f0f17392ff40927579899fd10bdaf6bf562ebd","modified":1529106760142},{"_id":"themes/clean-blog/layout/_partial/article-categories.ejs","hash":"5a0bf5a20f670621d8013c9b9d7976b45c8aa80f","modified":1529043522276},{"_id":"themes/clean-blog/layout/_partial/article-archive.ejs","hash":"3d8d98c6545b8332a6d6ed4f8b00327df03ea945","modified":1529043522276},{"_id":"themes/clean-blog/layout/_partial/after-footer.ejs","hash":"a6ad079ded70024d35264fae798ae73bdbcb0ae6","modified":1529149982621},{"_id":"themes/clean-blog/layout/_partial/article-full.ejs","hash":"6cf24bd7785d57cb7198b3f1ed4fa6a86c84a502","modified":1529043522276},{"_id":"themes/clean-blog/layout/_partial/article-index.ejs","hash":"e433df4e245e2d4c628052c6e59966563542d94d","modified":1529043522276},{"_id":"themes/clean-blog/layout/_partial/article-tags.ejs","hash":"6136434be09056c1466149cecb3cc2e80d107999","modified":1529043522277},{"_id":"themes/clean-blog/layout/_partial/comments.ejs","hash":"3fedb75436439d1d6979b7e4d20d48a593e12be4","modified":1529043522277},{"_id":"themes/clean-blog/layout/_partial/footer.ejs","hash":"a92f5168c006193c3d964fd293ad3c38aae69419","modified":1529043522277},{"_id":"themes/clean-blog/layout/_partial/gallery.ejs","hash":"21e4f28909f4a79ff7d9f10bdfef6a8cb11632bf","modified":1529043522277},{"_id":"themes/clean-blog/layout/_partial/google-analytics.ejs","hash":"4e6e8de9becea5a1636a4dcadcf7a10c06e2426e","modified":1529043522277},{"_id":"themes/clean-blog/layout/_partial/head.ejs","hash":"f8ddbced1627704ab35993e8fc6d6e34cc6f2ba9","modified":1529043522278},{"_id":"themes/clean-blog/layout/_partial/menu_origin.ejs","hash":"cfc30e6b1ef9487cff3ce594d403d1e7c4d9cdf4","modified":1529043522278},{"_id":"themes/clean-blog/layout/_partial/pagination.ejs","hash":"557d6bb069a1d48af49ae912994653f44b32a570","modified":1529043522278},{"_id":"themes/clean-blog/layout/_partial/menu.ejs","hash":"bf186aa55623a77e18c6b1748e6af429ca7e7d67","modified":1529230461987},{"_id":"themes/clean-blog/layout/_partial/tag-category-index.ejs","hash":"10cdc1b7866999c714a666557c150d2c79c1fba9","modified":1529043522278},{"_id":"themes/clean-blog/source/css/article.styl","hash":"f5294d7a3d6127fcb287de3ff0c12aebb1766c7b","modified":1529043522280},{"_id":"themes/clean-blog/source/css/base.styl","hash":"29b54c63060bd2d7f5c501d403d9db5a552ad10c","modified":1529043522280},{"_id":"themes/clean-blog/source/css/mixins.styl","hash":"14264bf86b4e3194a3156447f7b7bce2fd0db5bd","modified":1529043522280},{"_id":"themes/clean-blog/source/css/style.styl","hash":"c40dc495a41007d21c59f342ee42b2d31d7b5ff4","modified":1529043522280},{"_id":"themes/clean-blog/source/css/variables.styl","hash":"cd82df5ca8dfbcfec12d833f01adfac00878e835","modified":1529043522280},{"_id":"themes/clean-blog/source/img/about-bg.jpg","hash":"d39126a6456f2bac0169d1779304725f179c9900","modified":1529043522281},{"_id":"themes/clean-blog/source/img/favicon.ico","hash":"3412e0d657aa5a6cfbbfcf4ef398572c24035565","modified":1529045668405},{"_id":"themes/clean-blog/source/img/home-bg.jpg","hash":"990f6f9dd0ecb5348bfcc47305553d58c0d8f326","modified":1529043522283},{"_id":"themes/clean-blog/source/img/contact-bg.jpg","hash":"6af63305c923899017e727b5ca968a2703bc08cf","modified":1529043522282},{"_id":"public/index.html","hash":"7ec571191d829b2fae80a2b201d50cec50d4236b","modified":1531306829477},{"_id":"public/archives/index.html","hash":"84f1f075bb5ca773dad2dfce11a2e38cf63335af","modified":1531306829469},{"_id":"public/archives/2018/index.html","hash":"03977bb2ca137aff0121b32db030332fe4dac5ea","modified":1531306829469},{"_id":"public/archives/2018/06/index.html","hash":"229f2f899c2eec800479d40d719cd69f660bfdb8","modified":1531306829477},{"_id":"public/categories/프로젝트/index.html","hash":"9189730fe1800b3a637ad2e3a30b07857c7a19ac","modified":1531306829478},{"_id":"public/tags/프로젝트/index.html","hash":"5ab827553d9304271cecd7fda7f5b9ec31f2e385","modified":1531306829478},{"_id":"public/tags/피지여행/index.html","hash":"067bbb31fe0bbd11a30ea5db37bb8bce5c217f41","modified":1531306829478},{"_id":"public/2018/06/16/dpg/index.html","hash":"7bc394c8fcb2dca3d19acc45af754700fb5389e7","modified":1529230754824},{"_id":"public/2018/06/15/sutton-pg/index.html","hash":"2a84a7e4ea8168f22bebf2b1150355e14e9efea1","modified":1529230754824},{"_id":"public/2018/06/14/2018-06-15-npg/index.html","hash":"63865f511c586719f341629a55a4354cf9e39371","modified":1529230754824},{"_id":"public/2018/06/17/피지여행-소개/index.html","hash":"4444d94a395bd894dc96f4c1968ca4d457493924","modified":1529230754823},{"_id":"source/img/Screen Shot 2018-07-10 at 3.41.06 PM.png","hash":"c20c1e539d1b1fe5801de009e693492158ac961f","modified":1531204874000},{"_id":"source/_posts/ddpg.md","hash":"9da89762e7f5544df904517994a4b70493374a64","modified":1531290478253},{"_id":"source/_posts/npg.md","hash":"ad0e79383db0552d94078ca181c5e7c764b81f57","modified":1531290478252},{"_id":"source/_posts/gae.md","hash":"03665a07685a4ec7cd9a38237ee404e071fd6a04","modified":1531630364205},{"_id":"source/_posts/pg-travel-guide.md","hash":"52d82697a3caa675b877d31db280b11f5cd2359f","modified":1531290852959},{"_id":"source/images/Exp_ContinuousBandit.png","hash":"72eddde3b296070075706688038f1267bfd04d31","modified":1531290478254},{"_id":"source/images/Exp_OctopusArm.png","hash":"f0ae59d0c2a6600ef961fd0b2ea8903dcfdbd4d9","modified":1531290478256},{"_id":"source/images/Ref_Degris2012b_Theorem1.png","hash":"154f42239538c3dd0508e815014d84685e18f89b","modified":1531290478257},{"_id":"source/images/figure.png","hash":"c20c1e539d1b1fe5801de009e693492158ac961f","modified":1531204874000},{"_id":"source/images/Exp_ContinuousRL.png","hash":"85b9ebfb164631804d2fb6be0164e49cd1db746b","modified":1531290478255},{"_id":"source/images/Exp_OctopusArm_Ref.png","hash":"ebf69a16fc09e447442808f8cc46d770bed0cf10","modified":1531290478256},{"_id":"source/images/Ref_Degris2012b_offpolicygradient.png","hash":"46f0f17392ff40927579899fd10bdaf6bf562ebd","modified":1531290478257},{"_id":"source/img/figure.png","hash":"c20c1e539d1b1fe5801de009e693492158ac961f","modified":1531204874000},{"_id":"source/img/1.jpg","hash":"179e50805da05b9a2417192bff85de448f5de998","modified":1531306513731},{"_id":"public/2018/07/11/gae/index.html","hash":"27ec6523b1572a0fe8e9bd806926005578ec9067","modified":1531306829478},{"_id":"public/2018/07/11/pg-travel-guide/index.html","hash":"5a90be92767db785e6e33ef1b213cceb48882f71","modified":1531306829479},{"_id":"public/2018/07/10/sutton-pg/index.html","hash":"709c482d1075fdee66a7cc080add172ff3892017","modified":1531306829479},{"_id":"public/archives/2018/07/index.html","hash":"9713c99afe7263b1f69f4bc88c757bda9ea22a4b","modified":1531306829480},{"_id":"public/2018/06/23/ddpg/index.html","hash":"c8278826e2c5a127debc6847b215143da26c7a0d","modified":1531306829481},{"_id":"public/2018/06/14/npg/index.html","hash":"63865f511c586719f341629a55a4354cf9e39371","modified":1531306829481},{"_id":"public/img/figure.png","hash":"c20c1e539d1b1fe5801de009e693492158ac961f","modified":1531306829481},{"_id":"public/img/1.jpg","hash":"179e50805da05b9a2417192bff85de448f5de998","modified":1531306829481},{"_id":"source/img/figure10.jpg","hash":"5ac3e77e51f235f1ff01913f60b39b0d4c15c702","modified":1531380515987},{"_id":"source/img/figure2.jpg","hash":"be090f33c5c52aac5affb1a5141b7bb59df95feb","modified":1531378668469},{"_id":"source/img/figure4.jpg","hash":"4f3a5da99d582cfaeef680bcad5ffe04b9f77b2e","modified":1531378999629},{"_id":"source/img/figure6.jpg","hash":"dc10abad67ca9b7dc1e1f2f5a50979b597cff957","modified":1531379789001},{"_id":"source/img/figure9.jpg","hash":"a5b762eedea6af2e0f842f6c8171cf8600d0f066","modified":1531380074106},{"_id":"source/img/figure5.jpg","hash":"6ff807873a00811aebdf9dbd3d7a2780ddac3719","modified":1531379639148},{"_id":"source/img/figure3.jpg","hash":"b48ff63551254da2f6ef90124ca63435e3768ed7","modified":1531378674752},{"_id":"source/img/figure1.jpg","hash":"9e0b12cbbedcc3e1d4ac20552fcbad8bb3e8071a","modified":1531378413225},{"_id":"source/img/figure8.jpg","hash":"74a82b06007bcbe9ea602275785247bd250c4341","modified":1531380066337},{"_id":"source/img/figure7.jpg","hash":"a72367c5b094841966473e3d3be983413b486933","modified":1531380059406},{"_id":"source/_posts/trpo.md","hash":"46826d8051593bf072601e85838d07ff18c800ad","modified":1531555460384},{"_id":"source/img/mixure_policy.png","hash":"a29de731fa31bb01bfe2d32714d2c330b62ebf1c","modified":1531555460393},{"_id":"source/img/tvd.png","hash":"48832e9dfdc030c0eec69f4eb3a6df6f79e443b2","modified":1531555460408},{"_id":"source/img/importance_sampling.png","hash":"557cb910e6cbaa52570749c971f30a9ac99f5b90","modified":1531555460389},{"_id":"source/img/heuristic_approx.png","hash":"5f2474685413fe5220cabe92f8efd55541c19654","modified":1531555460387},{"_id":"source/img/kld.png","hash":"ec5b1eebc409fbe8b5a2a1d6ab8cdbb921b39d42","modified":1531555460392},{"_id":"source/img/sample-based.png","hash":"c4b79bd8d64eaa2ce36fb0a3ee0981b5455cce30","modified":1531555460399},{"_id":"source/img/single.png","hash":"52a63dd842c7448ebf20fa53d5485bd0364250b9","modified":1531555460401},{"_id":"source/img/surrogate.png","hash":"a0f501e64f45b6edde6282688416c513a27b6dff","modified":1531555460407},{"_id":"source/img/vine1.png","hash":"acea47e2739665b82e467660d2f1e382ef0307b6","modified":1531555460410},{"_id":"source/img/vine2.png","hash":"80103e41a869da947827726b5719bb30a33de414","modified":1531555460413},{"_id":"source/img/state_visitation_change.png","hash":"81c9b7879e5cb69e65519415b5ba72dbf09c19ee","modified":1531555460404},{"_id":"source/img/policy_change.png","hash":"e1fd4af3c5e8236d14eed042b9eec7358479626d","modified":1531555460397},{"_id":"source/img/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1531628462724}],"Category":[{"name":"프로젝트","_id":"cjiinnj6o0003j28aoi038xjq"}],"Data":[],"Page":[],"Post":[{"title":"피지여행 소개","date":"2018-06-17T09:50:06.000Z","_content":"","source":"_posts/피지여행-소개.md","raw":"---\ntitle: 피지여행 소개\ndate: 2018-06-17 18:50:06\ntags:\n---\n","slug":"피지여행-소개","published":1,"updated":"2018-06-17T09:50:06.186Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjiinnj690000j28ai14rugme","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"Policy Gradient Methods for Reinforcement Learning with Function Approximation","date":"2018-07-10T05:18:32.000Z","author":"김동민, 이동민","subtitle":"피지여행 1번째 논문","_content":"\n---\n# 1. Intro to Policy Gradient\n\n이 논문은 policy gradient (PG) 기법의 효시와도 같으며 향후 많은 파생연구를 낳은 중요한 논문입니다. 이 논문을 이해하기 위해 필요한 배경지식을 먼저 설명하고 논문을 차근차근 살펴보도록 하겠습니다.\n<br>\n\n## 1.1 Value Function Approach\n전통적으로 강화학습 기법은 value function을 기반으로 동작하였습니다. 특정 state에서 vaue function 또는 value function을 근사하는 함수(function approximation)를 최대화하는 action을 찾는 greedy action-selection policy가  대표적입니다.\n\n논문에서는 이러한 방법은 deterministic한 policy를 찾는 쪽으로 나아가게 되지만 종종 최적의 policy는 stochastic한 성질을 가지기 때문에 이 방법으로는 최적의 policy를 찾을 수 없다고 언급하고 있습니다. (이 논문이 나오고 나서 한참 후 개발된 deterministic policy gradient 기법과는 반대되는 서술일 수 있습니다.) 아마도 이러한 부분은 $\\epsilon$-greedy action-selection policy로 개선될 수 있을 것입니다. 또 하나의 문제는 value function의 작은 변화로 인해서 action이 크게 변할 수도 있다는 것입니다. 이것은 알고리듬의 수렴성에 문제를 야기할 수 있습니다. 이러한 문제점을 해결하기 위하여 policy search라는 새로운 기법이 고안됩니다.\n<br>\n\n## 1.2 Policy Search\npolicy search는 최적의 policy $\\pi^*$를 직접 찾기 때문에 value function 모델링이 필요가 없습니다. (뒤에서보겠지만 사실 꼭 그렇지만도 않습니다.) policy는 좀 더 다루기 쉽도록 parameter $\\theta$를 이용하여 $\\pi_{\\theta}$로 표현할 수 있습니다. 최적의 policy를 찾기 위하여 expected return $E\\left[R|\\theta\\right]$이 최대화되도록 parameter를 조정합니다. 이를 간단한 수식으로 정리하면 다음과 같습니다.\n\n$$\n\\pi^* = \\arg \\max\\limits_\\pi  E\\left[ {\\left. R \\right|\\pi } \\right] \\to {\\text{original problem} } \n\\\\\\\\  {\\Downarrow} \\\\\\\\\n{\\text{policy parameterization by } } \\pi_{\\theta}: \\Theta  \\to \\Pi \\\\\\\\ {\\Downarrow} $$\n\n$$\\pi^* = \\arg \\max\\limits_{\\theta}  E\\left[ {\\left. R \\right|\\theta } \\right] \\to {\\text{policy search problem} }\n$$\n\npolicy search 방법은 크게 두 가지로 분류할 수 있습니다. 첫 번째는 gradient-based optimization으로 policy gradient method가 이에 속합니다. 두 번째는 gradient-free optimization으로 진화(evolutionary) 연산을 이용하는 것이 이에 속합니다.\n<br>\n\n## 1.3 How to Obtain the Expected Return\nexpected return을 최대화하는 방향으로 policy를 update한다고 하였습니다. 그렇다면 expected return은 어떻게 구할 수 있을까요? 여기에도 크게 두 가지 방법이 있습니다. 첫 번째는 deterministic approximation으로 Markov decision process의 dynamics를 모델링한 후 수식적으로 구하는 것입니다. 두 번째 방법은 monte carlo estimation으로 dynamics에 대한 모델을 하지 않고 많은 sample들을 얻은 후 empirical하게 expected return을 계산하는 방법입니다. 두 번째 방법이 좀 더 현실적이지만 gradient를 구하는 것은 더 어렵습니다. 결국 gradient를 esimate하는방법을 고안해야 합니다. 가장 유명한 gradient estimation 방법이 1992년 R. J. Williams에 의해서 제안된 REINFORCE 기법입니다. REINFORCE는 Monte Carlo estimate 또는 likelihood-ratio estimate라고 부르는 방법을 이용해서 gradient와 expectation의 위치를 서로 바꿔줍니다.\n\n### Monte Carlo Gradient Estimation\n다음과 같은 parameter $\\theta$를 가지는 random variable $X$가 있습니다: $X:\\Omega\\mapsto\\mathcal{X}$. 그리고 이 $x$에 대한 함수 $f$가 있습니다: $f:\\mathcal{X}\\mapsto\\mathbb{R}$. expected return처럼 $E[f(x)]$를 최대화하고자 합니다. 이를 위해서는 $\\nabla_{\\theta}E[f(x)]$를 구해야 합니다. 이 때 다음과 같은 log derivate trick을 이용하여 다음과 같이 수식을 변형시킬 수 있습니다.\n\n$$\n\\begin{align}\n\\nabla_{\\theta}E_{p(x;\\theta)}\\left[ {f(x)} \\right] \n&= \\nabla_\\theta\\int{f\\left( x \\right)p\\left( {x;\\theta} \\right) dx}\n\\\\\\\\\n&= \\int { {\\nabla_\\theta }p\\left( {x;\\theta } \\right)f\\left( x \\right)dx} \n\\\\\\\\\n&= \\int {\\frac{ {p\\left( {x;\\theta } \\right)} }{ {p\\left( {x;\\theta } \\right)} }{\\nabla_\\theta }p\\left( {x;\\theta } \\right)f\\left( x \\right)dx} \n\\\\\\\\\n&= \\int {p\\left( {x;\\theta } \\right){\\nabla_\\theta }\\log p\\left( {x;\\theta } \\right)f\\left( x \\right)dx} \n\\\\\\\\\n&= E_{p(x;\\theta)}[f(x)\\nabla_\\theta \\log p({x;\\theta})]\n\\end{align}\n$$\n\n이제 expectation은 많은 샘플들을 모아서 평균을 취하는 Monte Carlo 기법으로 근사화할 수 있습니다.\n\n$${\\nabla_{\\theta} }{E_{p\\left( {x;\\theta } \\right)} }\\left[ {f\\left( x \\right)} \\right] = \\frac{1}{N}\\sum_{n=1}^N f\\left(X_n\\right)\\nabla_\\theta\\log p\\left(X_n;\\theta\\right)$$\n\n이 방법을 사용하기 위해서 필요한 조건은 $\\log p\\left(X_n;\\theta\\right)$가 미분가능해야 한다는 것 뿐입니다. 그러나 이 방법은 얻은 샘플들에 의존하기 때문에 경우에 따라서 큰 variance를 가질 수 있습니다.\n\n<br><br>\n\n---\n# 2. Policy Gradient Methods\n이제 본격적으로 policy gradient 기법에 대해서 알아보겠습니다.\n\n## 2.1 Policy Gradent Approach\npolicy gradient는 stochastic policy를 자체적인 파리미터를 가진 function approximator를 이용해서 근사화시킵니다. 몇 가지 notation들을 정의하겠습니다.\n\n* $\\theta$: policy parameter들의 vector  \n* $\\rho$: 해당 policy들의 성능을 나타내는 척도 (예. average reward per step)  \n* $\\alpha$: positive-definite한 step size \n\npolicy parameter는 gradient에 비례하여 업데이트됩니다.\n\n$$\\Delta\\theta \\approx \\alpha\\frac{\\partial\\rho}{\\partial\\theta}$$\n\nlocal optimal policy로 수렴합니다. 논문의 가장 중요한 contribution은 특정 조건을 만족하는 function approximator를 이용하여 경험(experience, sample)을 축적하고 이것들을 이용하여 위의 gradient를 unbiased estimate할 수 있다는 것을 증명한 것입니다.\n<br>\n\n## 2.2 System Model\n시스템모델은 다음과 같습니다.\n\n* Markov decision process (MDP) at each time $t\\in{0,1,2,...}$\n* $S_t\\in\\mathcal{S}$: state\n* $A_t\\in\\mathcal{A}$: action\n* $r_t\\in E$: reward\n* $\\theta\\in\\mathbb{R}^l$: vector of policy parameters, for $l\\ll\\left|\\mathcal{S}\\right|$\n* $\\mathcal{P}_{s s'}^a = \\Pr[S_{t+1}=s'|S_t=s,A_t=a]$: state transition probabilities\n* $\\mathcal{R}_{s s'}^a = E[R_{t+1}|S_t=s,A_t=a]$: expected reward\n* $\\pi(s,a,\\theta)=\\Pr[A_t=a|S_t=s,\\theta]$: policy, shortened as $\\pi(s,a)$ or $\\pi(a|s)$\n\n다음으로는 reward를 표현하는 두 가지 방법에 대해서 알아보겠습니다.\n<br>\n\n## 2.3 Average Reward Formulation\nAverage rewward formulation은 시간의 흐름에 따른 reward를 표현한다기보다는 모든 시간의 reward를 평균을 내서 표현하는 방법입니다.\n\n* Long‐term expected reward per step\n$$\\rho(\\pi)=\\lim_{n\\to\\infty}\\frac{1}{n}E[r_1+r_2+\\cdots+r_n|\\pi] = \\sum_s d^\\pi(s)\\sum_a\\pi(s,a)\\mathcal{R}_s^a$$\n\n    - $d^\\pi(s): \\lim_{t\\to\\infty}\\Pr[S_t=s|s_0,\\pi]$: stationary distribution of states under $\\pi$\n    - $d^\\pi(s)$가 존재한다고 가정합니다. 그리고 모든 policy에 대해서 $d^\\pi(s)$는 $s_0$와 independent합니다.\n    - 위의 두 표현식은 왜 같을까요? Time average와 Ensemble average가 같다는 뜻으로 ergodic한 system에서 성립합니다.\n\n* Value of a state-action pair given a policy\n$$Q^{\\pi(s,a)} = \\sum_{t=1}^\\infty E[r_t - \\rho(\\pi)|S_0=s, A_0=a, \\pi]$$\n* State-valu function\n$$V^\\pi(s) = \\sum_{a} \\pi(s,a)Q^\\pi(s,a)$$\n<br>\n\n## 2.4 Start-State Formulation\nstart-state formulation은 시간의 흐름에 따라 감소하는 reward들을 표현합니다.\n\n* Long‐term expected reward per step with a designated start state $S_0$\n$$\\rho(\\pi) = E\\left[\\left.\\sum_{t=1}^\\infty \\gamma^{t-1}r_t\\right|S_0,\\pi\\right]$$\n    - $\\gamma\\in\\left[0,1\\right]$: discount rate\n\n* Value of a state‐action pair given a policy\n$$Q^\\pi(s,a) = E\\left[\\left.\\sum_{k=1}^\\infty \\gamma^{k-1} r_{t+k}\\right|S_t=s, A_t=a, \\pi\\right]$$\n\n* Alternative form of $Q^\\pi(s,a)$\n$$\nQ^\\pi(s,a) =R_s^a + \\gamma\\sum_{s'}P_{s s'}^a V^{\\pi}(s')\n$$\n<br>\n\n## 2.5 Policy Gradient Theorem\n논문의 중요한 결과인 Theorem 1은 다음과 같습니다.\n\n*For any MDP, in either the average‐reward or start‐state formulations,*\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)$$\n\n놀라운 부분은 expected return의 gradient를 취할 때 $\\frac{\\partial d^\\pi(s,a)}{\\partial\\theta}$를 구하지 않아도 된다는 점입니다. 즉, policy의 변화가 state distribution에 영향을 주지 않는다는 것입니다. 이것은 다시 말하면 아래 수식과 같이 우리는 $\\frac{\\partial\\rho}{\\partial\\theta}$에 대한 unbiased estimator를 구할 수 있다는 뜻입니다.\n\n$$E_s\\left[\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)\\right] = \\frac{\\partial\\rho}{\\partial\\theta}$$\n\n이로인해서 우리는 gradient를 sampling을 통해서 추정할 수 있습니다. 그렇지만 이것은 sample이 아주 많을 때만 성립합니다!\n\n또 한 가지 문제는 우리는 $Q^\\pi(s,a)$의 정확한 값을 알 수 없다는 것 입니다. 이것을 estimate하기 위해서 현재의 return값 $R_t$를 이용할 수 있습니다.\n<br>\n\n## 2.6 Proof of Policy Gradient Theorem\n증명을 살펴보겠습니다.\n\n* average-reward formulation을 이용할 때의 증명은 아래와 같습니다.\n\n$$\n\\begin{align}\n\\frac{\\partial V^\\pi(s)}{\\partial\\theta} &\\overset{\\underset{\\mathrm{def} }{} }{=}\\frac{\\partial}{\\partial\\theta}\\sum_a \\pi(s,a)Q^\\pi(s,a)\n\\\\\\\\\n&=\\sum_a \\frac{\\partial}{\\partial\\theta}\\left[\\pi(s,a)Q^\\pi(s,a)\\right]\\\\\\\\\n&=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\frac{\\partial Q^\\pi(s,a)}{\\partial\\theta}\\right]\\\\\\\\\n&=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\frac{\\partial}{\\partial\\theta}\\left[R_s^a -\\rho(\\pi) + \\sum_{s'}P_{s s'}^a V^{\\pi}(s')\\right]\\right]\\\\\\\\\n&=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\left[-\\frac{\\partial\\rho}{\\partial\\theta} + \\sum_{s'}\\mathcal{P}_{s s'}^a \\frac{\\partial V^{\\pi}(s')}{\\partial\\theta}\\right]\\right]\\\\\\\\\n\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\sum_{s'}\\mathcal{P}_{s s'}^a \\frac{\\partial V^{\\pi}(s')}{\\partial\\theta}\\right] - \\frac{\\partial V^\\pi(s)}{\\partial\\theta}\n\\end{align}\n$$\n\n양변에 stationary distribution에 대한 평균을 취합니다.\n$$\n\\begin{align}\n\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+ \\sum_s d^\\pi(s)\\sum_a\\pi(s,a)\\sum_{s'}\\mathcal{P}_{s s'}^a \\frac{\\partial V^{\\pi}(s')}{\\partial\\theta} \n\\\\\\\\\n&- \\sum_s d^\\pi(s)\\frac{\\partial V^\\pi(s)}{\\partial\\theta}\n\\end{align}\n$$\n\n모든 state $s$에서 모든 state $s'$으로 이동하는 것은 state $s'$에 존재할 확률이므로 다음과 같이 표현할 수 있습니다.\n\n$$\n\\begin{align}\n\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+ \\sum_s d^\\pi(s') \\frac{\\partial V^{\\pi}(s')}{\\partial\\theta} - \\sum_s d^\\pi(s)\\frac{\\partial V^\\pi(s)}{\\partial\\theta}\n\\end{align}\n$$\n\n뒤의 두 항은 같은 식이므로 이것은 다시 다음과 같이 표현됩니다.\n$$\n\\begin{align}\n\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)\n\\end{align}\n$$\n\n$\\frac{\\partial\\rho}{\\partial\\theta}$는 $s$에 대한 함수가 아니므로 $\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}=\\frac{\\partial\\rho}{\\partial\\theta}$입니다.\n이를 이용해서 위의 식을 다시 표현하면 아래와 같고 이를 통해 증명이 완성됩니다.\n\n$$\n\\begin{align}\n\\therefore\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)\n\\end{align}\n$$\n\n다음으로 start-state formulation에 대한 증명입니다.\n\n* start-state formulation은 이 논문에서도 나오고 서튼책에서도 나오는데, 기존의 논문과 다르게 좀 더 진행되는 부분이 있습니다. 그래서 논문에 있는 증명이 아닌 서튼책에 있는 증명은 아래와 같습니다. (서튼책 [Link](https://drive.google.com/file/d/1xeUDVGWGUUv1-ccUMAZHJLej2C7aAFWY/view))\n\n$$\\nabla{v_\\pi}(s)=\\nabla\\big[\\sum_a\\pi(a|s)q_\\pi(s,a)\\big]$$ \n\n다음으로 $(f\\cdot g)'=f'\\cdot g+f\\cdot g'$ (product rule of calculus)에 의해 다음과 같이 나타낼 수 있습니다.\n$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\nabla q_\\pi(s,a)\\big]$$  \n\n이어서 $p(s',r|s,a)\\cong Pr[S_t=s', R_t=r|S_{t-1}=s,A_{t-1}=a ]$ (서튼책 3.2 공식) 에 의해 다음과 같이 나타낼 수 있습니다.\n\n$$=\\sum_a\\big[\\nabla\\pi(a|s)q_{\\pi(s,a)}+\\pi(a|s)\\nabla\\big[\\sum_{s', r}p(s',r|s,a)(r+v_{\\pi}(s'))\\big]\\big]$$  \n\n계속해서 $p(s'|s,a)\\cong Pr[S_t=s', R_t=r|S_{t-1}=s,A_{t-1}=a ]=\\sum_{r\\in R}p(s',r|s,a)$ (서튼책 3.4 공식) 에 의해 다음과 같이 나타낼 수 있습니다.\n$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\sum_{s'}p(s'|s,a)\\nabla v_\\pi(s') \\big]$$ \n\n여기서 $\\nabla v_\\pi(s')$ 부분을 unrolling을 하면 다음과 같이 나타낼 수 있습니다.\n$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\sum_{s'}p(s'|s,a)\\sum_{a'}\\big[\\nabla\\pi(a'|s')q_\\pi(s',a')\\\\\\\\+\\pi(a'|s')\\sum_{s''}p(s''|s',a')\\nabla v_\\pi(s''))\\big] \\big]$$ \n\nunrolling을 반복하다보면 다음과 같이 나타낼 수 있습니다.\n$$=\\sum_{x\\in S}\\sum_{k=0}^{\\infty}Pr(s\\rightarrow x,k,\\pi)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$\n\n여기에서 $Pr(s\\rightarrow x,k,\\pi)$ 는 policy $\\pi$ 에 대해 state s에서 state x까지 k step만큼 움직일 때의 변환 확률입니다. 따라서 위의 수식을 아래처럼 다시 나타낼 수 있습니다.\n$$ \\begin{align} \\nabla J(\\theta)= &\\nabla v_\\pi (s_0) \\\\ =&\\sum_s(\\sum_{k=0}^{\\infty}Pr(s_0\\rightarrow s,k,\\pi))\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a) \\\\ =&\\sum_s \\eta(s) \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a) \\end{align}$$\n\n- 서튼책에서 $\\eta(s)$는 다음과 같이 표현할 수 있습니다.\n    - $\\eta(s)$ = 평균적으로 하나의 episode마다 state에서 머무른 time step의 수 (page 199)\n- 논문에서는 $\\eta(s)$를 다음과 같이 표현합니다.\n    - $d^\\pi(s)\\\\\\\\=\\sum_{t=0}^{\\infty}\\gamma^t Pr(s_t=s|s_0,\\pi)$ ($\\gamma=1$ is allowed only in episodic tasks) = discounted weighting of states\n\n여기서부터가 논문에는 없는 내용입니다. 위의 최종 수식을 다시 한 번 적으면 아래와 같습니다.\n$$\\sum_s \\eta(s) \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$\n\n위의 수식을 아래의 수식처럼 바꿀 수 있습니다.\n$$\\sum_{s'} \\eta(s') \\cdot\\sum_s \\frac{\\eta(s)}{\\sum_{s'}\\eta(s')} \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$\n\n그러면 이것을 새로운 기호로 다시 나타낼 수 있습니다.\n$$\\sum_{s'} \\eta(s') \\cdot\\sum_s \\mu(s)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$ \n- 여기서 $\\mu(s)$ 는 state distribution이라고 하며, weight를 state distribution으로 바꿔주는 과정이라고 생각할 수 있습니다. 쉽게 말해 어떠한 state에 agent가 머무르는 확률입니다.\n\n위의 수식은 아래의 수식과 비례합니다. 따라서 최종 형태는 다음과 같습니다.\n$$\\propto \\sum_s \\mu(s)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$\n<br><br>\n\n---\n# 3. Policy Gradient with Approximation\n이번 section은 앞서 다뤘던 Theorem 1 중에 $Q^\\pi$에 대해서 중점적으로 다룹니다. 어떠한 $Q^\\pi$가 학습된 function approximator로 근사된다고 합시다. \n\n$f_w$ ($f_w$는 $S × A \\rightarrow \\Re$)를 parameter $w$를 가지는 어떠한 $Q^\\pi$ 에 대한 근사치입니다. 학습된 function approximator에 대해서 생각하기 때문에, 학습된 $f_w$는 error를 최소화하는 방향으로 parameter $w$를 업데이트된 것을 의미합니다. 그러면 하나의 rule에 의해서 $\\pi$를 따르고 $w$를 업데이트하는 $f_w$에 대해서 다음과 같이 생각할 수 있습니다.\n$$\\Delta w_t \\propto \\frac{\\partial}{\\partial w}\\big[ {\\hat Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]^2 \\propto \\big[{\\hat Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s_t,a_t)}{\\partial w}$$ \n(여기서 ${\\hat Q}^\\pi(s_t,a_t)$ 는 $Q^\\pi(s_t,a_t)$의 unbiased estimator $R_t$입니다.)\n\n그러면 위와 같은 수식이 local optimum에 수렴을 했을 때, 다음과 같이 나타낼 수 있습니다.\n$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$\n- 위의 수식에 대한 추가 설명\n    - local optimum으로 수렴을 하기 때문에 당연히 위의 수식은 0이 됩니다.\n    - 또한 stochastic policy이기 때문에 local optimum으로 수렴하려면 모든 state, action에 대해 expectation을 해야합니다. 따라서 $\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)$이 붙게 됩니다.\n<br>\n\n## 3.1 Theorem 2: Policy Gradient with Function Approximation\n만약 $f_w$가 아래의 등식을 만족한다고 합시다.\n$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$\n\n또한 $f_w$가 policy parameterization과 양립할 수 있다고 합시다. policy parameterization에 대한 여러가지 의미가 있는데 그 중에서 이 논문에서는 다음을 의미합니다.\n$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}$$\n- 위의 수식에 대한 추가 설명\n    - 이 수식은 Monte-Calro Estimation을 이용하기 위한 조건입니다.\n    - Compatibility Condition이라고 부릅니다.\n    - 우변(= $\\nabla\\log\\pi(s,a)$)으로 유도함으로써 아래의 수식과 같이 sampling을 이용한 추정이 가능해집니다.\n\n따라서 위의 두 수식을 이용하여 아래와 같이 나타낼 수 있다.\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}f_w(s,a)$$\n<br>\n\n## 3.2 Proof of Theorem 2\n$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$\n\n$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}$$\n\n위의 두 수식을 합치면 다음과 같이 나타낼 수 있습니다.\n$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}=0$$\n\n분자와 분모에 있는 $\\pi(s,a)$를 지우면 아래와 같습니다.\n$$\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]=0$$\n\n이어서 위의 수식이 0이기 때문에 policy gradient theorem(앞서 다뤘던 Theorem 1)에서 위의 수식을 뺄 수 있습니다.\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)-\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ Q^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]$$\n\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ Q^\\pi(s_t,a_t)-Q^\\pi(s_t,a_t)+f_w(s_t,a_t) \\big]$$\n\n$$\\therefore\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}f_w(s_t,a_t)$$\n<br><br>\n\n---\n# 4. Application to Deriving Algorithms and Advantages\n\n## 4.1 Application to Deriving Algorithms\nfeature의 linear combination에서 Gibbs distribution (softmax)를 하나의 policy로 생각해볼 수 있습니다.\n$$\\pi(s,a)=\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_b e^{\\theta^{T}\\phi_{sb} }}$$\n- ($\\phi_{sa}$: state-action pair s,a을 나타내는 l-dimensional feature vector)\n\ncompatible condition을 추가하면 다음과 같습니다. (4.2 Proof of application of compatible condition 참고)\n$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\frac{1}{\\pi(s,a)}=\\phi_{sa}-\\sum_b \\pi(s,b)\\phi_{sb}$$ \n\n이어서 $f_w$을 적분한 natural parameterization은 다음과 같습니다.\n$$f_w(s,a)=w^T\\big[ \\phi_{sa}-\\sum_b \\pi(s,b)\\phi_{sb}\\big]$$\n- 위의 수식에 대한 추가 설명\n    - $f_w$는 policy로서 같은 feature들에 대해 linear합니다.\n    - $f_w$는 각각의 state에 대해서 평균이 0입니다. ($\\sum_a\\pi(s,a)f_w(s,a)=0$)\n    - advantage function $A^\\pi(s,a)=Q^\\pi(s,a)-V^\\pi(s)$의 하나의 근사치로서 $f_w$를 생각하는 것 좋습니다.\n<br>\n\n## 4.2 Proof of application of compatible condition\n증명을 하기 전에 다음을 가정합니다.\n1. $(\\frac{f(x)}{g(x)})'=f'(x)\\cdot \\frac{1}{g(x)}-f(x)\\cdot \\frac{g'(x)}{g(x)^{2} }=\\frac{f'(x)\\cdot g(x)-f(x)\\cdot g'(x)}{g(x)^{2} }$\n2. $f(\\theta)=e^{\\theta^{T}\\phi_{sa} }$, $g(\\theta)=\\sum_be^{\\theta^{T}\\phi_{sb} }$\n\n$$\n\\begin{align}\n\\frac{\\partial f_w(s,a)}{\\partial w}&=\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\frac{1}{\\pi(s,a)}\n\\\\\\\\\n&=\\big(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }}\\big)'\\frac{1}{\\pi(s,a)}\n\\\\\\\\\n&=\\big[ \\phi_{sa}(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }})-(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }})\\frac{\\sum_b\\phi_{sb}e^{ {\\theta^T}\\phi_{sb} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }}\\big]\\frac{1}{\\pi(s,a)}\\\\\\\\\n&=\\big[ \\phi_{sa}\\pi(s,a)-\\pi(s,a)\\sum_b\\phi_{sb}\\pi(s,b)\\big]\\frac{1}{\\pi(s,a)}\n\\\\\\\\\n&=\\pi(s,a)\\big[ \\phi_{sa}-\\sum_b\\pi(s,b)\\phi_{sb}\\big]\\frac{1}{\\pi(s,a)}\n\\\\\\\\\n&\\therefore\\phi_{sa}-\\sum_b\\pi(s,b)\\phi_{sb}\n\\end{align}\n$$\n\n<br>\n\n## 4.3 Application to Advantages\nPolicy Gradient with Function Approximation Theorem (Theorem 2)는 advantage function으로 확장될 수 있습니다.\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_sd^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ f_w(s,a)+v(s) \\big]$$\n- 위의 수식에 대한 추가설명\n    - $v$ ($v$는 $S\\rightarrow\\Re$) 는 arbitrary function입니다.\n    - 이 수식은 $\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}=0$이기 때문에 가능해집니다.\n        - 즉, 이 수식은 $\\pi(s,a)$의 gradient에만 dependent하기 때문에 advantage 역할을 하는 함수들을 넣어도 아무런 상관이 없습니다.\n    - $v$의 선택은 Theorem들에 영향을 미치지 못하지만, 실질적으로 gradient estimator의 variance에 영향을 미칩니다.\n    - 이러한 문제는 전체적으로 이전의 연구에 reinforcement baseline의 사용에 있어서 유사합니다.\n    - (comment) 위의 수식에서 $f_w(s,a)+v(s)$와 Application to Deriving Algorithms의 $f_w(s,a)$와는 다른 것입니다. Application to Deriving Algorithms의 $f_w(s,a)$은 softmax에 의해 스스로 advantage function의 역할을 할 수 있습니다. 하지만 보통의 경우에는 그러지 못할 수도 있기 때문에 위의 수식처럼 $f_w(s,a)+v(s)$을 추가하여 zero mean만들어서 variance를 줄일 수 있습니다.\n<br><br>\n\n---\n# 5. Convergence of Policy Iteration with Function Approximation\n\n## 5.1 Theorem 3: Convergence of Policy Iteration with Function Approximation\n\npolicy iteration with function approximation의 형태는 locally optimal policy에 수렴합니다. $\\pi$와 $f_w$를 \n\ncompatibility condition을 만족하는 policy와 value function에 대해 $\\max_{\\theta, s, a, i, j}\\big| \\frac{\\partial^2\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}\\big|<B<\\infty$ 을\n만족하는 어떠한 미분가능한 function approximator라고 합시다.\n\n(comment) $\\pi(s,a)$라는 function은 이계미분값이 존재하고, 임의의 상수인 (Bound) B에 유계하기 때문에 function의 그래프는 smooth하다고 볼 수 있습니다. (아래의 그림 중 빨간색 그래프 참고)\n\n<img src=\"https://www.dropbox.com/s/xx02ejfg5ao19ps/Screen%20Shot%202018-07-10%20at%203.41.06%20PM.png?dl=1\">\n\n${\\alpha_k}$는 $\\lim_{k\\rightarrow\\infty}\\alpha_k=0$ 이며 $\\sum_k\\alpha_k=\\infty$를 만족하는 step-size sequence라고 합시다.\n\n그 때, bounded reward를 가진 MDP에 대해\n- 1. 어떠한 $\\theta_0, \\pi_k=\\pi(\\cdot,\\cdot,\\theta_k)$ \n- 2. $\\sum_sd^{\\pi_{k} }(s)\\sum_a\\pi_k(s,a)\\big[ Q^{\\pi_{k} }(s,a)-f_w(s,a)\\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$로 인하여 $w_k=w$, $\\theta_{k+1}=\\theta_k+\\alpha_k\\sum_sd^{\\pi_{k} }(s)\\sum_a\\frac{\\partial\\pi_k(s,a)}{\\partial\\theta}f_{w_{k} }(s,a)$\n\n으로 정의된 sequence $\\rho(\\pi_k)$는 $\\lim_{k\\rightarrow\\infty}\\frac{\\partial\\rho(\\pi_k)}{\\partial\\theta}=0$이기 때문에 수렴합니다.\n- sequence $\\rho(\\pi_k)_{k=0}^\\infty$에 대한 추가 설명\n    - $\\theta_{k+1}=\\theta_k+\\alpha_k\\sum_sd^{\\pi_{k} }(s)\\sum_a\\frac{\\partial\\pi_k(s,a)}{\\partial\\theta}f_{w_{k} }(s,a)$ 에 따라 $\\theta$가 1, 2, ..., $\\infty$로 갈텐데, 거기에 따른 objective function or performance의 sequence입니다.\n    - (comment) 굳이 sequence라는 표현이 없어도 될 것 같다. 어짜피 k가 $\\infty$로 가면 $\\rho(\\pi_k)$가 수렴한다는 의미이기 때문에 불필요해보입니다.\n\n## 5.2 Proof of Theorem 3\n- Theorem 2는 $\\theta_k$ update가 gradient의 error를 최소화한다는 것을 증명했습니다.\n- $\\frac{\\partial^{2}\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}$와 MDP의 reward에서의 bound는, $\\frac{\\partial^{2}\\rho}{\\partial\\theta_i\\partial\\theta_j}$ 또한 bound된다는 것을 증명합니다.\n- step-size 필요조건 때문에 이러한 bound된 것들은 Proposition 3.5 from page 94 of Bertsekas and Tsitsiklis(1996)에 적용하기 위해 필요한 조건입니다.\n- Proposition 3.5 from page 94 of Bertsekas and Tsitsiklis(1996)은 local optimum으로 수렴한다는 것을 증명했습니다.\n<br><br>\n\n---\n# 6. Summary \n\n논문에서 설명한 policy gradient 기법을 요약하면 다음과 같습니다.\n\n* Original maximization problem: $\\max\\rho_{\\theta}=E\\left[R\\right]$\n* gradient ascent method를 이용한 parameter update: $\\theta_{t+1} = \\theta_{t} + \\alpha\\left.\\frac{\\partial E[R]}{\\partial\\theta}\\right|_{\\theta_t}$\n    - 그러나 우리는 gradient를 모르고, 추정하기도 어렵습니다. 왜냐하면 expectation이 안에 있기 때문입니다.\n    - 그렇다면 이것을 추정 가능한 형태로 바꿔야합니다. 다시 말해 expectation이 밖에 있는 형태로 바꾸는 것입니다.\n    - Expectation이 밖에 있으면 왜 추정이 유리할까요? 바로 Sample mean을 취하면 되기 때문입니다.\n* Approximate gradient\n    - 방법1: 논문의 Theorem 1을 이용하면 gradient와 expectation의 위치를 바꿀 수 있습니다.\n    $$\\frac{\\partial E[R]}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)$$\n    - 방법2: Log derivative trick을 이용하면 gradient와 expectation의 위치를 바꿀 수 있습니다. (Theorem 2)\n    $$\\theta_{t+1} = \\theta_{t} + \\alpha E[R\\frac{\\partial}{\\partial\\theta}\\log p_{\\theta}|\\theta_t]$$\n    - 그리고 특정 trajectory를 따라가면서 return값을 구하고 이것을 여러 번 수행하여 sample mean을 취하면 gradient를 추정하는 것이 가능합니다.\n    - 이러한 기법을 제시한 기존의 연구가 REINFORCE입니다.\n    - 여러 Trajectory를 이용하므로 variance가 높을 수 밖에 없습니다.\n    - Advantage를 활용하거나 하는 방식으로 이후 여러 연구가 진행되었습니다.\n<br>\n","source":"_posts/sutton-pg.md","raw":"---\ntitle: Policy Gradient Methods for Reinforcement Learning with Function Approximation\ndate: 2018-07-10 14:18:32\ntags: [\"프로젝트\", \"피지여행\"]\ncategories: 프로젝트\nauthor: 김동민, 이동민\nsubtitle: 피지여행 1번째 논문\n---\n\n---\n# 1. Intro to Policy Gradient\n\n이 논문은 policy gradient (PG) 기법의 효시와도 같으며 향후 많은 파생연구를 낳은 중요한 논문입니다. 이 논문을 이해하기 위해 필요한 배경지식을 먼저 설명하고 논문을 차근차근 살펴보도록 하겠습니다.\n<br>\n\n## 1.1 Value Function Approach\n전통적으로 강화학습 기법은 value function을 기반으로 동작하였습니다. 특정 state에서 vaue function 또는 value function을 근사하는 함수(function approximation)를 최대화하는 action을 찾는 greedy action-selection policy가  대표적입니다.\n\n논문에서는 이러한 방법은 deterministic한 policy를 찾는 쪽으로 나아가게 되지만 종종 최적의 policy는 stochastic한 성질을 가지기 때문에 이 방법으로는 최적의 policy를 찾을 수 없다고 언급하고 있습니다. (이 논문이 나오고 나서 한참 후 개발된 deterministic policy gradient 기법과는 반대되는 서술일 수 있습니다.) 아마도 이러한 부분은 $\\epsilon$-greedy action-selection policy로 개선될 수 있을 것입니다. 또 하나의 문제는 value function의 작은 변화로 인해서 action이 크게 변할 수도 있다는 것입니다. 이것은 알고리듬의 수렴성에 문제를 야기할 수 있습니다. 이러한 문제점을 해결하기 위하여 policy search라는 새로운 기법이 고안됩니다.\n<br>\n\n## 1.2 Policy Search\npolicy search는 최적의 policy $\\pi^*$를 직접 찾기 때문에 value function 모델링이 필요가 없습니다. (뒤에서보겠지만 사실 꼭 그렇지만도 않습니다.) policy는 좀 더 다루기 쉽도록 parameter $\\theta$를 이용하여 $\\pi_{\\theta}$로 표현할 수 있습니다. 최적의 policy를 찾기 위하여 expected return $E\\left[R|\\theta\\right]$이 최대화되도록 parameter를 조정합니다. 이를 간단한 수식으로 정리하면 다음과 같습니다.\n\n$$\n\\pi^* = \\arg \\max\\limits_\\pi  E\\left[ {\\left. R \\right|\\pi } \\right] \\to {\\text{original problem} } \n\\\\\\\\  {\\Downarrow} \\\\\\\\\n{\\text{policy parameterization by } } \\pi_{\\theta}: \\Theta  \\to \\Pi \\\\\\\\ {\\Downarrow} $$\n\n$$\\pi^* = \\arg \\max\\limits_{\\theta}  E\\left[ {\\left. R \\right|\\theta } \\right] \\to {\\text{policy search problem} }\n$$\n\npolicy search 방법은 크게 두 가지로 분류할 수 있습니다. 첫 번째는 gradient-based optimization으로 policy gradient method가 이에 속합니다. 두 번째는 gradient-free optimization으로 진화(evolutionary) 연산을 이용하는 것이 이에 속합니다.\n<br>\n\n## 1.3 How to Obtain the Expected Return\nexpected return을 최대화하는 방향으로 policy를 update한다고 하였습니다. 그렇다면 expected return은 어떻게 구할 수 있을까요? 여기에도 크게 두 가지 방법이 있습니다. 첫 번째는 deterministic approximation으로 Markov decision process의 dynamics를 모델링한 후 수식적으로 구하는 것입니다. 두 번째 방법은 monte carlo estimation으로 dynamics에 대한 모델을 하지 않고 많은 sample들을 얻은 후 empirical하게 expected return을 계산하는 방법입니다. 두 번째 방법이 좀 더 현실적이지만 gradient를 구하는 것은 더 어렵습니다. 결국 gradient를 esimate하는방법을 고안해야 합니다. 가장 유명한 gradient estimation 방법이 1992년 R. J. Williams에 의해서 제안된 REINFORCE 기법입니다. REINFORCE는 Monte Carlo estimate 또는 likelihood-ratio estimate라고 부르는 방법을 이용해서 gradient와 expectation의 위치를 서로 바꿔줍니다.\n\n### Monte Carlo Gradient Estimation\n다음과 같은 parameter $\\theta$를 가지는 random variable $X$가 있습니다: $X:\\Omega\\mapsto\\mathcal{X}$. 그리고 이 $x$에 대한 함수 $f$가 있습니다: $f:\\mathcal{X}\\mapsto\\mathbb{R}$. expected return처럼 $E[f(x)]$를 최대화하고자 합니다. 이를 위해서는 $\\nabla_{\\theta}E[f(x)]$를 구해야 합니다. 이 때 다음과 같은 log derivate trick을 이용하여 다음과 같이 수식을 변형시킬 수 있습니다.\n\n$$\n\\begin{align}\n\\nabla_{\\theta}E_{p(x;\\theta)}\\left[ {f(x)} \\right] \n&= \\nabla_\\theta\\int{f\\left( x \\right)p\\left( {x;\\theta} \\right) dx}\n\\\\\\\\\n&= \\int { {\\nabla_\\theta }p\\left( {x;\\theta } \\right)f\\left( x \\right)dx} \n\\\\\\\\\n&= \\int {\\frac{ {p\\left( {x;\\theta } \\right)} }{ {p\\left( {x;\\theta } \\right)} }{\\nabla_\\theta }p\\left( {x;\\theta } \\right)f\\left( x \\right)dx} \n\\\\\\\\\n&= \\int {p\\left( {x;\\theta } \\right){\\nabla_\\theta }\\log p\\left( {x;\\theta } \\right)f\\left( x \\right)dx} \n\\\\\\\\\n&= E_{p(x;\\theta)}[f(x)\\nabla_\\theta \\log p({x;\\theta})]\n\\end{align}\n$$\n\n이제 expectation은 많은 샘플들을 모아서 평균을 취하는 Monte Carlo 기법으로 근사화할 수 있습니다.\n\n$${\\nabla_{\\theta} }{E_{p\\left( {x;\\theta } \\right)} }\\left[ {f\\left( x \\right)} \\right] = \\frac{1}{N}\\sum_{n=1}^N f\\left(X_n\\right)\\nabla_\\theta\\log p\\left(X_n;\\theta\\right)$$\n\n이 방법을 사용하기 위해서 필요한 조건은 $\\log p\\left(X_n;\\theta\\right)$가 미분가능해야 한다는 것 뿐입니다. 그러나 이 방법은 얻은 샘플들에 의존하기 때문에 경우에 따라서 큰 variance를 가질 수 있습니다.\n\n<br><br>\n\n---\n# 2. Policy Gradient Methods\n이제 본격적으로 policy gradient 기법에 대해서 알아보겠습니다.\n\n## 2.1 Policy Gradent Approach\npolicy gradient는 stochastic policy를 자체적인 파리미터를 가진 function approximator를 이용해서 근사화시킵니다. 몇 가지 notation들을 정의하겠습니다.\n\n* $\\theta$: policy parameter들의 vector  \n* $\\rho$: 해당 policy들의 성능을 나타내는 척도 (예. average reward per step)  \n* $\\alpha$: positive-definite한 step size \n\npolicy parameter는 gradient에 비례하여 업데이트됩니다.\n\n$$\\Delta\\theta \\approx \\alpha\\frac{\\partial\\rho}{\\partial\\theta}$$\n\nlocal optimal policy로 수렴합니다. 논문의 가장 중요한 contribution은 특정 조건을 만족하는 function approximator를 이용하여 경험(experience, sample)을 축적하고 이것들을 이용하여 위의 gradient를 unbiased estimate할 수 있다는 것을 증명한 것입니다.\n<br>\n\n## 2.2 System Model\n시스템모델은 다음과 같습니다.\n\n* Markov decision process (MDP) at each time $t\\in{0,1,2,...}$\n* $S_t\\in\\mathcal{S}$: state\n* $A_t\\in\\mathcal{A}$: action\n* $r_t\\in E$: reward\n* $\\theta\\in\\mathbb{R}^l$: vector of policy parameters, for $l\\ll\\left|\\mathcal{S}\\right|$\n* $\\mathcal{P}_{s s'}^a = \\Pr[S_{t+1}=s'|S_t=s,A_t=a]$: state transition probabilities\n* $\\mathcal{R}_{s s'}^a = E[R_{t+1}|S_t=s,A_t=a]$: expected reward\n* $\\pi(s,a,\\theta)=\\Pr[A_t=a|S_t=s,\\theta]$: policy, shortened as $\\pi(s,a)$ or $\\pi(a|s)$\n\n다음으로는 reward를 표현하는 두 가지 방법에 대해서 알아보겠습니다.\n<br>\n\n## 2.3 Average Reward Formulation\nAverage rewward formulation은 시간의 흐름에 따른 reward를 표현한다기보다는 모든 시간의 reward를 평균을 내서 표현하는 방법입니다.\n\n* Long‐term expected reward per step\n$$\\rho(\\pi)=\\lim_{n\\to\\infty}\\frac{1}{n}E[r_1+r_2+\\cdots+r_n|\\pi] = \\sum_s d^\\pi(s)\\sum_a\\pi(s,a)\\mathcal{R}_s^a$$\n\n    - $d^\\pi(s): \\lim_{t\\to\\infty}\\Pr[S_t=s|s_0,\\pi]$: stationary distribution of states under $\\pi$\n    - $d^\\pi(s)$가 존재한다고 가정합니다. 그리고 모든 policy에 대해서 $d^\\pi(s)$는 $s_0$와 independent합니다.\n    - 위의 두 표현식은 왜 같을까요? Time average와 Ensemble average가 같다는 뜻으로 ergodic한 system에서 성립합니다.\n\n* Value of a state-action pair given a policy\n$$Q^{\\pi(s,a)} = \\sum_{t=1}^\\infty E[r_t - \\rho(\\pi)|S_0=s, A_0=a, \\pi]$$\n* State-valu function\n$$V^\\pi(s) = \\sum_{a} \\pi(s,a)Q^\\pi(s,a)$$\n<br>\n\n## 2.4 Start-State Formulation\nstart-state formulation은 시간의 흐름에 따라 감소하는 reward들을 표현합니다.\n\n* Long‐term expected reward per step with a designated start state $S_0$\n$$\\rho(\\pi) = E\\left[\\left.\\sum_{t=1}^\\infty \\gamma^{t-1}r_t\\right|S_0,\\pi\\right]$$\n    - $\\gamma\\in\\left[0,1\\right]$: discount rate\n\n* Value of a state‐action pair given a policy\n$$Q^\\pi(s,a) = E\\left[\\left.\\sum_{k=1}^\\infty \\gamma^{k-1} r_{t+k}\\right|S_t=s, A_t=a, \\pi\\right]$$\n\n* Alternative form of $Q^\\pi(s,a)$\n$$\nQ^\\pi(s,a) =R_s^a + \\gamma\\sum_{s'}P_{s s'}^a V^{\\pi}(s')\n$$\n<br>\n\n## 2.5 Policy Gradient Theorem\n논문의 중요한 결과인 Theorem 1은 다음과 같습니다.\n\n*For any MDP, in either the average‐reward or start‐state formulations,*\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)$$\n\n놀라운 부분은 expected return의 gradient를 취할 때 $\\frac{\\partial d^\\pi(s,a)}{\\partial\\theta}$를 구하지 않아도 된다는 점입니다. 즉, policy의 변화가 state distribution에 영향을 주지 않는다는 것입니다. 이것은 다시 말하면 아래 수식과 같이 우리는 $\\frac{\\partial\\rho}{\\partial\\theta}$에 대한 unbiased estimator를 구할 수 있다는 뜻입니다.\n\n$$E_s\\left[\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)\\right] = \\frac{\\partial\\rho}{\\partial\\theta}$$\n\n이로인해서 우리는 gradient를 sampling을 통해서 추정할 수 있습니다. 그렇지만 이것은 sample이 아주 많을 때만 성립합니다!\n\n또 한 가지 문제는 우리는 $Q^\\pi(s,a)$의 정확한 값을 알 수 없다는 것 입니다. 이것을 estimate하기 위해서 현재의 return값 $R_t$를 이용할 수 있습니다.\n<br>\n\n## 2.6 Proof of Policy Gradient Theorem\n증명을 살펴보겠습니다.\n\n* average-reward formulation을 이용할 때의 증명은 아래와 같습니다.\n\n$$\n\\begin{align}\n\\frac{\\partial V^\\pi(s)}{\\partial\\theta} &\\overset{\\underset{\\mathrm{def} }{} }{=}\\frac{\\partial}{\\partial\\theta}\\sum_a \\pi(s,a)Q^\\pi(s,a)\n\\\\\\\\\n&=\\sum_a \\frac{\\partial}{\\partial\\theta}\\left[\\pi(s,a)Q^\\pi(s,a)\\right]\\\\\\\\\n&=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\frac{\\partial Q^\\pi(s,a)}{\\partial\\theta}\\right]\\\\\\\\\n&=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\frac{\\partial}{\\partial\\theta}\\left[R_s^a -\\rho(\\pi) + \\sum_{s'}P_{s s'}^a V^{\\pi}(s')\\right]\\right]\\\\\\\\\n&=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\left[-\\frac{\\partial\\rho}{\\partial\\theta} + \\sum_{s'}\\mathcal{P}_{s s'}^a \\frac{\\partial V^{\\pi}(s')}{\\partial\\theta}\\right]\\right]\\\\\\\\\n\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\sum_{s'}\\mathcal{P}_{s s'}^a \\frac{\\partial V^{\\pi}(s')}{\\partial\\theta}\\right] - \\frac{\\partial V^\\pi(s)}{\\partial\\theta}\n\\end{align}\n$$\n\n양변에 stationary distribution에 대한 평균을 취합니다.\n$$\n\\begin{align}\n\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+ \\sum_s d^\\pi(s)\\sum_a\\pi(s,a)\\sum_{s'}\\mathcal{P}_{s s'}^a \\frac{\\partial V^{\\pi}(s')}{\\partial\\theta} \n\\\\\\\\\n&- \\sum_s d^\\pi(s)\\frac{\\partial V^\\pi(s)}{\\partial\\theta}\n\\end{align}\n$$\n\n모든 state $s$에서 모든 state $s'$으로 이동하는 것은 state $s'$에 존재할 확률이므로 다음과 같이 표현할 수 있습니다.\n\n$$\n\\begin{align}\n\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+ \\sum_s d^\\pi(s') \\frac{\\partial V^{\\pi}(s')}{\\partial\\theta} - \\sum_s d^\\pi(s)\\frac{\\partial V^\\pi(s)}{\\partial\\theta}\n\\end{align}\n$$\n\n뒤의 두 항은 같은 식이므로 이것은 다시 다음과 같이 표현됩니다.\n$$\n\\begin{align}\n\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)\n\\end{align}\n$$\n\n$\\frac{\\partial\\rho}{\\partial\\theta}$는 $s$에 대한 함수가 아니므로 $\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}=\\frac{\\partial\\rho}{\\partial\\theta}$입니다.\n이를 이용해서 위의 식을 다시 표현하면 아래와 같고 이를 통해 증명이 완성됩니다.\n\n$$\n\\begin{align}\n\\therefore\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)\n\\end{align}\n$$\n\n다음으로 start-state formulation에 대한 증명입니다.\n\n* start-state formulation은 이 논문에서도 나오고 서튼책에서도 나오는데, 기존의 논문과 다르게 좀 더 진행되는 부분이 있습니다. 그래서 논문에 있는 증명이 아닌 서튼책에 있는 증명은 아래와 같습니다. (서튼책 [Link](https://drive.google.com/file/d/1xeUDVGWGUUv1-ccUMAZHJLej2C7aAFWY/view))\n\n$$\\nabla{v_\\pi}(s)=\\nabla\\big[\\sum_a\\pi(a|s)q_\\pi(s,a)\\big]$$ \n\n다음으로 $(f\\cdot g)'=f'\\cdot g+f\\cdot g'$ (product rule of calculus)에 의해 다음과 같이 나타낼 수 있습니다.\n$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\nabla q_\\pi(s,a)\\big]$$  \n\n이어서 $p(s',r|s,a)\\cong Pr[S_t=s', R_t=r|S_{t-1}=s,A_{t-1}=a ]$ (서튼책 3.2 공식) 에 의해 다음과 같이 나타낼 수 있습니다.\n\n$$=\\sum_a\\big[\\nabla\\pi(a|s)q_{\\pi(s,a)}+\\pi(a|s)\\nabla\\big[\\sum_{s', r}p(s',r|s,a)(r+v_{\\pi}(s'))\\big]\\big]$$  \n\n계속해서 $p(s'|s,a)\\cong Pr[S_t=s', R_t=r|S_{t-1}=s,A_{t-1}=a ]=\\sum_{r\\in R}p(s',r|s,a)$ (서튼책 3.4 공식) 에 의해 다음과 같이 나타낼 수 있습니다.\n$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\sum_{s'}p(s'|s,a)\\nabla v_\\pi(s') \\big]$$ \n\n여기서 $\\nabla v_\\pi(s')$ 부분을 unrolling을 하면 다음과 같이 나타낼 수 있습니다.\n$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\sum_{s'}p(s'|s,a)\\sum_{a'}\\big[\\nabla\\pi(a'|s')q_\\pi(s',a')\\\\\\\\+\\pi(a'|s')\\sum_{s''}p(s''|s',a')\\nabla v_\\pi(s''))\\big] \\big]$$ \n\nunrolling을 반복하다보면 다음과 같이 나타낼 수 있습니다.\n$$=\\sum_{x\\in S}\\sum_{k=0}^{\\infty}Pr(s\\rightarrow x,k,\\pi)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$\n\n여기에서 $Pr(s\\rightarrow x,k,\\pi)$ 는 policy $\\pi$ 에 대해 state s에서 state x까지 k step만큼 움직일 때의 변환 확률입니다. 따라서 위의 수식을 아래처럼 다시 나타낼 수 있습니다.\n$$ \\begin{align} \\nabla J(\\theta)= &\\nabla v_\\pi (s_0) \\\\ =&\\sum_s(\\sum_{k=0}^{\\infty}Pr(s_0\\rightarrow s,k,\\pi))\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a) \\\\ =&\\sum_s \\eta(s) \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a) \\end{align}$$\n\n- 서튼책에서 $\\eta(s)$는 다음과 같이 표현할 수 있습니다.\n    - $\\eta(s)$ = 평균적으로 하나의 episode마다 state에서 머무른 time step의 수 (page 199)\n- 논문에서는 $\\eta(s)$를 다음과 같이 표현합니다.\n    - $d^\\pi(s)\\\\\\\\=\\sum_{t=0}^{\\infty}\\gamma^t Pr(s_t=s|s_0,\\pi)$ ($\\gamma=1$ is allowed only in episodic tasks) = discounted weighting of states\n\n여기서부터가 논문에는 없는 내용입니다. 위의 최종 수식을 다시 한 번 적으면 아래와 같습니다.\n$$\\sum_s \\eta(s) \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$\n\n위의 수식을 아래의 수식처럼 바꿀 수 있습니다.\n$$\\sum_{s'} \\eta(s') \\cdot\\sum_s \\frac{\\eta(s)}{\\sum_{s'}\\eta(s')} \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$\n\n그러면 이것을 새로운 기호로 다시 나타낼 수 있습니다.\n$$\\sum_{s'} \\eta(s') \\cdot\\sum_s \\mu(s)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$ \n- 여기서 $\\mu(s)$ 는 state distribution이라고 하며, weight를 state distribution으로 바꿔주는 과정이라고 생각할 수 있습니다. 쉽게 말해 어떠한 state에 agent가 머무르는 확률입니다.\n\n위의 수식은 아래의 수식과 비례합니다. 따라서 최종 형태는 다음과 같습니다.\n$$\\propto \\sum_s \\mu(s)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$\n<br><br>\n\n---\n# 3. Policy Gradient with Approximation\n이번 section은 앞서 다뤘던 Theorem 1 중에 $Q^\\pi$에 대해서 중점적으로 다룹니다. 어떠한 $Q^\\pi$가 학습된 function approximator로 근사된다고 합시다. \n\n$f_w$ ($f_w$는 $S × A \\rightarrow \\Re$)를 parameter $w$를 가지는 어떠한 $Q^\\pi$ 에 대한 근사치입니다. 학습된 function approximator에 대해서 생각하기 때문에, 학습된 $f_w$는 error를 최소화하는 방향으로 parameter $w$를 업데이트된 것을 의미합니다. 그러면 하나의 rule에 의해서 $\\pi$를 따르고 $w$를 업데이트하는 $f_w$에 대해서 다음과 같이 생각할 수 있습니다.\n$$\\Delta w_t \\propto \\frac{\\partial}{\\partial w}\\big[ {\\hat Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]^2 \\propto \\big[{\\hat Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s_t,a_t)}{\\partial w}$$ \n(여기서 ${\\hat Q}^\\pi(s_t,a_t)$ 는 $Q^\\pi(s_t,a_t)$의 unbiased estimator $R_t$입니다.)\n\n그러면 위와 같은 수식이 local optimum에 수렴을 했을 때, 다음과 같이 나타낼 수 있습니다.\n$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$\n- 위의 수식에 대한 추가 설명\n    - local optimum으로 수렴을 하기 때문에 당연히 위의 수식은 0이 됩니다.\n    - 또한 stochastic policy이기 때문에 local optimum으로 수렴하려면 모든 state, action에 대해 expectation을 해야합니다. 따라서 $\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)$이 붙게 됩니다.\n<br>\n\n## 3.1 Theorem 2: Policy Gradient with Function Approximation\n만약 $f_w$가 아래의 등식을 만족한다고 합시다.\n$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$\n\n또한 $f_w$가 policy parameterization과 양립할 수 있다고 합시다. policy parameterization에 대한 여러가지 의미가 있는데 그 중에서 이 논문에서는 다음을 의미합니다.\n$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}$$\n- 위의 수식에 대한 추가 설명\n    - 이 수식은 Monte-Calro Estimation을 이용하기 위한 조건입니다.\n    - Compatibility Condition이라고 부릅니다.\n    - 우변(= $\\nabla\\log\\pi(s,a)$)으로 유도함으로써 아래의 수식과 같이 sampling을 이용한 추정이 가능해집니다.\n\n따라서 위의 두 수식을 이용하여 아래와 같이 나타낼 수 있다.\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}f_w(s,a)$$\n<br>\n\n## 3.2 Proof of Theorem 2\n$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$\n\n$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}$$\n\n위의 두 수식을 합치면 다음과 같이 나타낼 수 있습니다.\n$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}=0$$\n\n분자와 분모에 있는 $\\pi(s,a)$를 지우면 아래와 같습니다.\n$$\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]=0$$\n\n이어서 위의 수식이 0이기 때문에 policy gradient theorem(앞서 다뤘던 Theorem 1)에서 위의 수식을 뺄 수 있습니다.\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)-\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ Q^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]$$\n\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ Q^\\pi(s_t,a_t)-Q^\\pi(s_t,a_t)+f_w(s_t,a_t) \\big]$$\n\n$$\\therefore\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}f_w(s_t,a_t)$$\n<br><br>\n\n---\n# 4. Application to Deriving Algorithms and Advantages\n\n## 4.1 Application to Deriving Algorithms\nfeature의 linear combination에서 Gibbs distribution (softmax)를 하나의 policy로 생각해볼 수 있습니다.\n$$\\pi(s,a)=\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_b e^{\\theta^{T}\\phi_{sb} }}$$\n- ($\\phi_{sa}$: state-action pair s,a을 나타내는 l-dimensional feature vector)\n\ncompatible condition을 추가하면 다음과 같습니다. (4.2 Proof of application of compatible condition 참고)\n$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\frac{1}{\\pi(s,a)}=\\phi_{sa}-\\sum_b \\pi(s,b)\\phi_{sb}$$ \n\n이어서 $f_w$을 적분한 natural parameterization은 다음과 같습니다.\n$$f_w(s,a)=w^T\\big[ \\phi_{sa}-\\sum_b \\pi(s,b)\\phi_{sb}\\big]$$\n- 위의 수식에 대한 추가 설명\n    - $f_w$는 policy로서 같은 feature들에 대해 linear합니다.\n    - $f_w$는 각각의 state에 대해서 평균이 0입니다. ($\\sum_a\\pi(s,a)f_w(s,a)=0$)\n    - advantage function $A^\\pi(s,a)=Q^\\pi(s,a)-V^\\pi(s)$의 하나의 근사치로서 $f_w$를 생각하는 것 좋습니다.\n<br>\n\n## 4.2 Proof of application of compatible condition\n증명을 하기 전에 다음을 가정합니다.\n1. $(\\frac{f(x)}{g(x)})'=f'(x)\\cdot \\frac{1}{g(x)}-f(x)\\cdot \\frac{g'(x)}{g(x)^{2} }=\\frac{f'(x)\\cdot g(x)-f(x)\\cdot g'(x)}{g(x)^{2} }$\n2. $f(\\theta)=e^{\\theta^{T}\\phi_{sa} }$, $g(\\theta)=\\sum_be^{\\theta^{T}\\phi_{sb} }$\n\n$$\n\\begin{align}\n\\frac{\\partial f_w(s,a)}{\\partial w}&=\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\frac{1}{\\pi(s,a)}\n\\\\\\\\\n&=\\big(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }}\\big)'\\frac{1}{\\pi(s,a)}\n\\\\\\\\\n&=\\big[ \\phi_{sa}(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }})-(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }})\\frac{\\sum_b\\phi_{sb}e^{ {\\theta^T}\\phi_{sb} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }}\\big]\\frac{1}{\\pi(s,a)}\\\\\\\\\n&=\\big[ \\phi_{sa}\\pi(s,a)-\\pi(s,a)\\sum_b\\phi_{sb}\\pi(s,b)\\big]\\frac{1}{\\pi(s,a)}\n\\\\\\\\\n&=\\pi(s,a)\\big[ \\phi_{sa}-\\sum_b\\pi(s,b)\\phi_{sb}\\big]\\frac{1}{\\pi(s,a)}\n\\\\\\\\\n&\\therefore\\phi_{sa}-\\sum_b\\pi(s,b)\\phi_{sb}\n\\end{align}\n$$\n\n<br>\n\n## 4.3 Application to Advantages\nPolicy Gradient with Function Approximation Theorem (Theorem 2)는 advantage function으로 확장될 수 있습니다.\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_sd^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ f_w(s,a)+v(s) \\big]$$\n- 위의 수식에 대한 추가설명\n    - $v$ ($v$는 $S\\rightarrow\\Re$) 는 arbitrary function입니다.\n    - 이 수식은 $\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}=0$이기 때문에 가능해집니다.\n        - 즉, 이 수식은 $\\pi(s,a)$의 gradient에만 dependent하기 때문에 advantage 역할을 하는 함수들을 넣어도 아무런 상관이 없습니다.\n    - $v$의 선택은 Theorem들에 영향을 미치지 못하지만, 실질적으로 gradient estimator의 variance에 영향을 미칩니다.\n    - 이러한 문제는 전체적으로 이전의 연구에 reinforcement baseline의 사용에 있어서 유사합니다.\n    - (comment) 위의 수식에서 $f_w(s,a)+v(s)$와 Application to Deriving Algorithms의 $f_w(s,a)$와는 다른 것입니다. Application to Deriving Algorithms의 $f_w(s,a)$은 softmax에 의해 스스로 advantage function의 역할을 할 수 있습니다. 하지만 보통의 경우에는 그러지 못할 수도 있기 때문에 위의 수식처럼 $f_w(s,a)+v(s)$을 추가하여 zero mean만들어서 variance를 줄일 수 있습니다.\n<br><br>\n\n---\n# 5. Convergence of Policy Iteration with Function Approximation\n\n## 5.1 Theorem 3: Convergence of Policy Iteration with Function Approximation\n\npolicy iteration with function approximation의 형태는 locally optimal policy에 수렴합니다. $\\pi$와 $f_w$를 \n\ncompatibility condition을 만족하는 policy와 value function에 대해 $\\max_{\\theta, s, a, i, j}\\big| \\frac{\\partial^2\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}\\big|<B<\\infty$ 을\n만족하는 어떠한 미분가능한 function approximator라고 합시다.\n\n(comment) $\\pi(s,a)$라는 function은 이계미분값이 존재하고, 임의의 상수인 (Bound) B에 유계하기 때문에 function의 그래프는 smooth하다고 볼 수 있습니다. (아래의 그림 중 빨간색 그래프 참고)\n\n<img src=\"https://www.dropbox.com/s/xx02ejfg5ao19ps/Screen%20Shot%202018-07-10%20at%203.41.06%20PM.png?dl=1\">\n\n${\\alpha_k}$는 $\\lim_{k\\rightarrow\\infty}\\alpha_k=0$ 이며 $\\sum_k\\alpha_k=\\infty$를 만족하는 step-size sequence라고 합시다.\n\n그 때, bounded reward를 가진 MDP에 대해\n- 1. 어떠한 $\\theta_0, \\pi_k=\\pi(\\cdot,\\cdot,\\theta_k)$ \n- 2. $\\sum_sd^{\\pi_{k} }(s)\\sum_a\\pi_k(s,a)\\big[ Q^{\\pi_{k} }(s,a)-f_w(s,a)\\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$로 인하여 $w_k=w$, $\\theta_{k+1}=\\theta_k+\\alpha_k\\sum_sd^{\\pi_{k} }(s)\\sum_a\\frac{\\partial\\pi_k(s,a)}{\\partial\\theta}f_{w_{k} }(s,a)$\n\n으로 정의된 sequence $\\rho(\\pi_k)$는 $\\lim_{k\\rightarrow\\infty}\\frac{\\partial\\rho(\\pi_k)}{\\partial\\theta}=0$이기 때문에 수렴합니다.\n- sequence $\\rho(\\pi_k)_{k=0}^\\infty$에 대한 추가 설명\n    - $\\theta_{k+1}=\\theta_k+\\alpha_k\\sum_sd^{\\pi_{k} }(s)\\sum_a\\frac{\\partial\\pi_k(s,a)}{\\partial\\theta}f_{w_{k} }(s,a)$ 에 따라 $\\theta$가 1, 2, ..., $\\infty$로 갈텐데, 거기에 따른 objective function or performance의 sequence입니다.\n    - (comment) 굳이 sequence라는 표현이 없어도 될 것 같다. 어짜피 k가 $\\infty$로 가면 $\\rho(\\pi_k)$가 수렴한다는 의미이기 때문에 불필요해보입니다.\n\n## 5.2 Proof of Theorem 3\n- Theorem 2는 $\\theta_k$ update가 gradient의 error를 최소화한다는 것을 증명했습니다.\n- $\\frac{\\partial^{2}\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}$와 MDP의 reward에서의 bound는, $\\frac{\\partial^{2}\\rho}{\\partial\\theta_i\\partial\\theta_j}$ 또한 bound된다는 것을 증명합니다.\n- step-size 필요조건 때문에 이러한 bound된 것들은 Proposition 3.5 from page 94 of Bertsekas and Tsitsiklis(1996)에 적용하기 위해 필요한 조건입니다.\n- Proposition 3.5 from page 94 of Bertsekas and Tsitsiklis(1996)은 local optimum으로 수렴한다는 것을 증명했습니다.\n<br><br>\n\n---\n# 6. Summary \n\n논문에서 설명한 policy gradient 기법을 요약하면 다음과 같습니다.\n\n* Original maximization problem: $\\max\\rho_{\\theta}=E\\left[R\\right]$\n* gradient ascent method를 이용한 parameter update: $\\theta_{t+1} = \\theta_{t} + \\alpha\\left.\\frac{\\partial E[R]}{\\partial\\theta}\\right|_{\\theta_t}$\n    - 그러나 우리는 gradient를 모르고, 추정하기도 어렵습니다. 왜냐하면 expectation이 안에 있기 때문입니다.\n    - 그렇다면 이것을 추정 가능한 형태로 바꿔야합니다. 다시 말해 expectation이 밖에 있는 형태로 바꾸는 것입니다.\n    - Expectation이 밖에 있으면 왜 추정이 유리할까요? 바로 Sample mean을 취하면 되기 때문입니다.\n* Approximate gradient\n    - 방법1: 논문의 Theorem 1을 이용하면 gradient와 expectation의 위치를 바꿀 수 있습니다.\n    $$\\frac{\\partial E[R]}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)$$\n    - 방법2: Log derivative trick을 이용하면 gradient와 expectation의 위치를 바꿀 수 있습니다. (Theorem 2)\n    $$\\theta_{t+1} = \\theta_{t} + \\alpha E[R\\frac{\\partial}{\\partial\\theta}\\log p_{\\theta}|\\theta_t]$$\n    - 그리고 특정 trajectory를 따라가면서 return값을 구하고 이것을 여러 번 수행하여 sample mean을 취하면 gradient를 추정하는 것이 가능합니다.\n    - 이러한 기법을 제시한 기존의 연구가 REINFORCE입니다.\n    - 여러 Trajectory를 이용하므로 variance가 높을 수 밖에 없습니다.\n    - Advantage를 활용하거나 하는 방식으로 이후 여러 연구가 진행되었습니다.\n<br>\n","slug":"sutton-pg","published":1,"updated":"2018-07-15T04:17:25.398Z","_id":"cjiinnj6n0002j28aigqpmv52","comments":1,"layout":"post","photos":[],"link":"","content":"<hr>\n<h1 id=\"1-Intro-to-Policy-Gradient\"><a href=\"#1-Intro-to-Policy-Gradient\" class=\"headerlink\" title=\"1. Intro to Policy Gradient\"></a>1. Intro to Policy Gradient</h1><p>이 논문은 policy gradient (PG) 기법의 효시와도 같으며 향후 많은 파생연구를 낳은 중요한 논문입니다. 이 논문을 이해하기 위해 필요한 배경지식을 먼저 설명하고 논문을 차근차근 살펴보도록 하겠습니다.<br><br></p>\n<h2 id=\"1-1-Value-Function-Approach\"><a href=\"#1-1-Value-Function-Approach\" class=\"headerlink\" title=\"1.1 Value Function Approach\"></a>1.1 Value Function Approach</h2><p>전통적으로 강화학습 기법은 value function을 기반으로 동작하였습니다. 특정 state에서 vaue function 또는 value function을 근사하는 함수(function approximation)를 최대화하는 action을 찾는 greedy action-selection policy가  대표적입니다.</p>\n<p>논문에서는 이러한 방법은 deterministic한 policy를 찾는 쪽으로 나아가게 되지만 종종 최적의 policy는 stochastic한 성질을 가지기 때문에 이 방법으로는 최적의 policy를 찾을 수 없다고 언급하고 있습니다. (이 논문이 나오고 나서 한참 후 개발된 deterministic policy gradient 기법과는 반대되는 서술일 수 있습니다.) 아마도 이러한 부분은 $\\epsilon$-greedy action-selection policy로 개선될 수 있을 것입니다. 또 하나의 문제는 value function의 작은 변화로 인해서 action이 크게 변할 수도 있다는 것입니다. 이것은 알고리듬의 수렴성에 문제를 야기할 수 있습니다. 이러한 문제점을 해결하기 위하여 policy search라는 새로운 기법이 고안됩니다.<br><br></p>\n<h2 id=\"1-2-Policy-Search\"><a href=\"#1-2-Policy-Search\" class=\"headerlink\" title=\"1.2 Policy Search\"></a>1.2 Policy Search</h2><p>policy search는 최적의 policy $\\pi^*$를 직접 찾기 때문에 value function 모델링이 필요가 없습니다. (뒤에서보겠지만 사실 꼭 그렇지만도 않습니다.) policy는 좀 더 다루기 쉽도록 parameter $\\theta$를 이용하여 $\\pi_{\\theta}$로 표현할 수 있습니다. 최적의 policy를 찾기 위하여 expected return $E\\left[R|\\theta\\right]$이 최대화되도록 parameter를 조정합니다. 이를 간단한 수식으로 정리하면 다음과 같습니다.</p>\n<p>$$<br>\\pi^* = \\arg \\max\\limits_\\pi  E\\left[ {\\left. R \\right|\\pi } \\right] \\to {\\text{original problem} }<br>\\\\  {\\Downarrow} \\\\<br>{\\text{policy parameterization by } } \\pi_{\\theta}: \\Theta  \\to \\Pi \\\\ {\\Downarrow} $$</p>\n<p>$$\\pi^* = \\arg \\max\\limits_{\\theta}  E\\left[ {\\left. R \\right|\\theta } \\right] \\to {\\text{policy search problem} }<br>$$</p>\n<p>policy search 방법은 크게 두 가지로 분류할 수 있습니다. 첫 번째는 gradient-based optimization으로 policy gradient method가 이에 속합니다. 두 번째는 gradient-free optimization으로 진화(evolutionary) 연산을 이용하는 것이 이에 속합니다.<br><br></p>\n<h2 id=\"1-3-How-to-Obtain-the-Expected-Return\"><a href=\"#1-3-How-to-Obtain-the-Expected-Return\" class=\"headerlink\" title=\"1.3 How to Obtain the Expected Return\"></a>1.3 How to Obtain the Expected Return</h2><p>expected return을 최대화하는 방향으로 policy를 update한다고 하였습니다. 그렇다면 expected return은 어떻게 구할 수 있을까요? 여기에도 크게 두 가지 방법이 있습니다. 첫 번째는 deterministic approximation으로 Markov decision process의 dynamics를 모델링한 후 수식적으로 구하는 것입니다. 두 번째 방법은 monte carlo estimation으로 dynamics에 대한 모델을 하지 않고 많은 sample들을 얻은 후 empirical하게 expected return을 계산하는 방법입니다. 두 번째 방법이 좀 더 현실적이지만 gradient를 구하는 것은 더 어렵습니다. 결국 gradient를 esimate하는방법을 고안해야 합니다. 가장 유명한 gradient estimation 방법이 1992년 R. J. Williams에 의해서 제안된 REINFORCE 기법입니다. REINFORCE는 Monte Carlo estimate 또는 likelihood-ratio estimate라고 부르는 방법을 이용해서 gradient와 expectation의 위치를 서로 바꿔줍니다.</p>\n<h3 id=\"Monte-Carlo-Gradient-Estimation\"><a href=\"#Monte-Carlo-Gradient-Estimation\" class=\"headerlink\" title=\"Monte Carlo Gradient Estimation\"></a>Monte Carlo Gradient Estimation</h3><p>다음과 같은 parameter $\\theta$를 가지는 random variable $X$가 있습니다: $X:\\Omega\\mapsto\\mathcal{X}$. 그리고 이 $x$에 대한 함수 $f$가 있습니다: $f:\\mathcal{X}\\mapsto\\mathbb{R}$. expected return처럼 $E[f(x)]$를 최대화하고자 합니다. 이를 위해서는 $\\nabla_{\\theta}E[f(x)]$를 구해야 합니다. 이 때 다음과 같은 log derivate trick을 이용하여 다음과 같이 수식을 변형시킬 수 있습니다.</p>\n<p>$$<br>\\begin{align}<br>\\nabla_{\\theta}E_{p(x;\\theta)}\\left[ {f(x)} \\right]<br>&amp;= \\nabla_\\theta\\int{f\\left( x \\right)p\\left( {x;\\theta} \\right) dx}<br>\\\\<br>&amp;= \\int { {\\nabla_\\theta }p\\left( {x;\\theta } \\right)f\\left( x \\right)dx}<br>\\\\<br>&amp;= \\int {\\frac{ {p\\left( {x;\\theta } \\right)} }{ {p\\left( {x;\\theta } \\right)} }{\\nabla_\\theta }p\\left( {x;\\theta } \\right)f\\left( x \\right)dx}<br>\\\\<br>&amp;= \\int {p\\left( {x;\\theta } \\right){\\nabla_\\theta }\\log p\\left( {x;\\theta } \\right)f\\left( x \\right)dx}<br>\\\\<br>&amp;= E_{p(x;\\theta)}[f(x)\\nabla_\\theta \\log p({x;\\theta})]<br>\\end{align}<br>$$</p>\n<p>이제 expectation은 많은 샘플들을 모아서 평균을 취하는 Monte Carlo 기법으로 근사화할 수 있습니다.</p>\n<p>$${\\nabla_{\\theta} }{E_{p\\left( {x;\\theta } \\right)} }\\left[ {f\\left( x \\right)} \\right] = \\frac{1}{N}\\sum_{n=1}^N f\\left(X_n\\right)\\nabla_\\theta\\log p\\left(X_n;\\theta\\right)$$</p>\n<p>이 방법을 사용하기 위해서 필요한 조건은 $\\log p\\left(X_n;\\theta\\right)$가 미분가능해야 한다는 것 뿐입니다. 그러나 이 방법은 얻은 샘플들에 의존하기 때문에 경우에 따라서 큰 variance를 가질 수 있습니다.</p>\n<p><br><br></p>\n<hr>\n<h1 id=\"2-Policy-Gradient-Methods\"><a href=\"#2-Policy-Gradient-Methods\" class=\"headerlink\" title=\"2. Policy Gradient Methods\"></a>2. Policy Gradient Methods</h1><p>이제 본격적으로 policy gradient 기법에 대해서 알아보겠습니다.</p>\n<h2 id=\"2-1-Policy-Gradent-Approach\"><a href=\"#2-1-Policy-Gradent-Approach\" class=\"headerlink\" title=\"2.1 Policy Gradent Approach\"></a>2.1 Policy Gradent Approach</h2><p>policy gradient는 stochastic policy를 자체적인 파리미터를 가진 function approximator를 이용해서 근사화시킵니다. 몇 가지 notation들을 정의하겠습니다.</p>\n<ul>\n<li>$\\theta$: policy parameter들의 vector  </li>\n<li>$\\rho$: 해당 policy들의 성능을 나타내는 척도 (예. average reward per step)  </li>\n<li>$\\alpha$: positive-definite한 step size </li>\n</ul>\n<p>policy parameter는 gradient에 비례하여 업데이트됩니다.</p>\n<p>$$\\Delta\\theta \\approx \\alpha\\frac{\\partial\\rho}{\\partial\\theta}$$</p>\n<p>local optimal policy로 수렴합니다. 논문의 가장 중요한 contribution은 특정 조건을 만족하는 function approximator를 이용하여 경험(experience, sample)을 축적하고 이것들을 이용하여 위의 gradient를 unbiased estimate할 수 있다는 것을 증명한 것입니다.<br><br></p>\n<h2 id=\"2-2-System-Model\"><a href=\"#2-2-System-Model\" class=\"headerlink\" title=\"2.2 System Model\"></a>2.2 System Model</h2><p>시스템모델은 다음과 같습니다.</p>\n<ul>\n<li>Markov decision process (MDP) at each time $t\\in{0,1,2,…}$</li>\n<li>$S_t\\in\\mathcal{S}$: state</li>\n<li>$A_t\\in\\mathcal{A}$: action</li>\n<li>$r_t\\in E$: reward</li>\n<li>$\\theta\\in\\mathbb{R}^l$: vector of policy parameters, for $l\\ll\\left|\\mathcal{S}\\right|$</li>\n<li>$\\mathcal{P}<em>{s s’}^a = \\Pr[S</em>{t+1}=s’|S_t=s,A_t=a]$: state transition probabilities</li>\n<li>$\\mathcal{R}<em>{s s’}^a = E[R</em>{t+1}|S_t=s,A_t=a]$: expected reward</li>\n<li>$\\pi(s,a,\\theta)=\\Pr[A_t=a|S_t=s,\\theta]$: policy, shortened as $\\pi(s,a)$ or $\\pi(a|s)$</li>\n</ul>\n<p>다음으로는 reward를 표현하는 두 가지 방법에 대해서 알아보겠습니다.<br><br></p>\n<h2 id=\"2-3-Average-Reward-Formulation\"><a href=\"#2-3-Average-Reward-Formulation\" class=\"headerlink\" title=\"2.3 Average Reward Formulation\"></a>2.3 Average Reward Formulation</h2><p>Average rewward formulation은 시간의 흐름에 따른 reward를 표현한다기보다는 모든 시간의 reward를 평균을 내서 표현하는 방법입니다.</p>\n<ul>\n<li><p>Long‐term expected reward per step<br>$$\\rho(\\pi)=\\lim_{n\\to\\infty}\\frac{1}{n}E[r_1+r_2+\\cdots+r_n|\\pi] = \\sum_s d^\\pi(s)\\sum_a\\pi(s,a)\\mathcal{R}_s^a$$</p>\n<ul>\n<li>$d^\\pi(s): \\lim_{t\\to\\infty}\\Pr[S_t=s|s_0,\\pi]$: stationary distribution of states under $\\pi$</li>\n<li>$d^\\pi(s)$가 존재한다고 가정합니다. 그리고 모든 policy에 대해서 $d^\\pi(s)$는 $s_0$와 independent합니다.</li>\n<li>위의 두 표현식은 왜 같을까요? Time average와 Ensemble average가 같다는 뜻으로 ergodic한 system에서 성립합니다.</li>\n</ul>\n</li>\n<li><p>Value of a state-action pair given a policy<br>$$Q^{\\pi(s,a)} = \\sum_{t=1}^\\infty E[r_t - \\rho(\\pi)|S_0=s, A_0=a, \\pi]$$</p>\n</li>\n<li>State-valu function<br>$$V^\\pi(s) = \\sum_{a} \\pi(s,a)Q^\\pi(s,a)$$<br><br></li>\n</ul>\n<h2 id=\"2-4-Start-State-Formulation\"><a href=\"#2-4-Start-State-Formulation\" class=\"headerlink\" title=\"2.4 Start-State Formulation\"></a>2.4 Start-State Formulation</h2><p>start-state formulation은 시간의 흐름에 따라 감소하는 reward들을 표현합니다.</p>\n<ul>\n<li><p>Long‐term expected reward per step with a designated start state $S_0$<br>$$\\rho(\\pi) = E\\left[\\left.\\sum_{t=1}^\\infty \\gamma^{t-1}r_t\\right|S_0,\\pi\\right]$$</p>\n<ul>\n<li>$\\gamma\\in\\left[0,1\\right]$: discount rate</li>\n</ul>\n</li>\n<li><p>Value of a state‐action pair given a policy<br>$$Q^\\pi(s,a) = E\\left[\\left.\\sum_{k=1}^\\infty \\gamma^{k-1} r_{t+k}\\right|S_t=s, A_t=a, \\pi\\right]$$</p>\n</li>\n<li><p>Alternative form of $Q^\\pi(s,a)$<br>$$<br>Q^\\pi(s,a) =R_s^a + \\gamma\\sum_{s’}P_{s s’}^a V^{\\pi}(s’)<br>$$<br><br></p>\n</li>\n</ul>\n<h2 id=\"2-5-Policy-Gradient-Theorem\"><a href=\"#2-5-Policy-Gradient-Theorem\" class=\"headerlink\" title=\"2.5 Policy Gradient Theorem\"></a>2.5 Policy Gradient Theorem</h2><p>논문의 중요한 결과인 Theorem 1은 다음과 같습니다.</p>\n<p><em>For any MDP, in either the average‐reward or start‐state formulations,</em><br>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)$$</p>\n<p>놀라운 부분은 expected return의 gradient를 취할 때 $\\frac{\\partial d^\\pi(s,a)}{\\partial\\theta}$를 구하지 않아도 된다는 점입니다. 즉, policy의 변화가 state distribution에 영향을 주지 않는다는 것입니다. 이것은 다시 말하면 아래 수식과 같이 우리는 $\\frac{\\partial\\rho}{\\partial\\theta}$에 대한 unbiased estimator를 구할 수 있다는 뜻입니다.</p>\n<p>$$E_s\\left[\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)\\right] = \\frac{\\partial\\rho}{\\partial\\theta}$$</p>\n<p>이로인해서 우리는 gradient를 sampling을 통해서 추정할 수 있습니다. 그렇지만 이것은 sample이 아주 많을 때만 성립합니다!</p>\n<p>또 한 가지 문제는 우리는 $Q^\\pi(s,a)$의 정확한 값을 알 수 없다는 것 입니다. 이것을 estimate하기 위해서 현재의 return값 $R_t$를 이용할 수 있습니다.<br><br></p>\n<h2 id=\"2-6-Proof-of-Policy-Gradient-Theorem\"><a href=\"#2-6-Proof-of-Policy-Gradient-Theorem\" class=\"headerlink\" title=\"2.6 Proof of Policy Gradient Theorem\"></a>2.6 Proof of Policy Gradient Theorem</h2><p>증명을 살펴보겠습니다.</p>\n<ul>\n<li>average-reward formulation을 이용할 때의 증명은 아래와 같습니다.</li>\n</ul>\n<p>$$<br>\\begin{align}<br>\\frac{\\partial V^\\pi(s)}{\\partial\\theta} &amp;\\overset{\\underset{\\mathrm{def} }{} }{=}\\frac{\\partial}{\\partial\\theta}\\sum_a \\pi(s,a)Q^\\pi(s,a)<br>\\\\<br>&amp;=\\sum_a \\frac{\\partial}{\\partial\\theta}\\left[\\pi(s,a)Q^\\pi(s,a)\\right]\\\\<br>&amp;=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\frac{\\partial Q^\\pi(s,a)}{\\partial\\theta}\\right]\\\\<br>&amp;=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\frac{\\partial}{\\partial\\theta}\\left[R_s^a -\\rho(\\pi) + \\sum_{s’}P_{s s’}^a V^{\\pi}(s’)\\right]\\right]\\\\<br>&amp;=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\left[-\\frac{\\partial\\rho}{\\partial\\theta} + \\sum_{s’}\\mathcal{P}_{s s’}^a \\frac{\\partial V^{\\pi}(s’)}{\\partial\\theta}\\right]\\right]\\\\<br>\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\sum_{s’}\\mathcal{P}_{s s’}^a \\frac{\\partial V^{\\pi}(s’)}{\\partial\\theta}\\right] - \\frac{\\partial V^\\pi(s)}{\\partial\\theta}<br>\\end{align}<br>$$</p>\n<p>양변에 stationary distribution에 대한 평균을 취합니다.<br>$$<br>\\begin{align}<br>\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+ \\sum_s d^\\pi(s)\\sum_a\\pi(s,a)\\sum_{s’}\\mathcal{P}_{s s’}^a \\frac{\\partial V^{\\pi}(s’)}{\\partial\\theta}<br>\\\\<br>&amp;- \\sum_s d^\\pi(s)\\frac{\\partial V^\\pi(s)}{\\partial\\theta}<br>\\end{align}<br>$$</p>\n<p>모든 state $s$에서 모든 state $s’$으로 이동하는 것은 state $s’$에 존재할 확률이므로 다음과 같이 표현할 수 있습니다.</p>\n<p>$$<br>\\begin{align}<br>\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+ \\sum_s d^\\pi(s’) \\frac{\\partial V^{\\pi}(s’)}{\\partial\\theta} - \\sum_s d^\\pi(s)\\frac{\\partial V^\\pi(s)}{\\partial\\theta}<br>\\end{align}<br>$$</p>\n<p>뒤의 두 항은 같은 식이므로 이것은 다시 다음과 같이 표현됩니다.<br>$$<br>\\begin{align}<br>\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)<br>\\end{align}<br>$$</p>\n<p>$\\frac{\\partial\\rho}{\\partial\\theta}$는 $s$에 대한 함수가 아니므로 $\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}=\\frac{\\partial\\rho}{\\partial\\theta}$입니다.<br>이를 이용해서 위의 식을 다시 표현하면 아래와 같고 이를 통해 증명이 완성됩니다.</p>\n<p>$$<br>\\begin{align}<br>\\therefore\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)<br>\\end{align}<br>$$</p>\n<p>다음으로 start-state formulation에 대한 증명입니다.</p>\n<ul>\n<li>start-state formulation은 이 논문에서도 나오고 서튼책에서도 나오는데, 기존의 논문과 다르게 좀 더 진행되는 부분이 있습니다. 그래서 논문에 있는 증명이 아닌 서튼책에 있는 증명은 아래와 같습니다. (서튼책 <a href=\"https://drive.google.com/file/d/1xeUDVGWGUUv1-ccUMAZHJLej2C7aAFWY/view\" target=\"_blank\" rel=\"noopener\">Link</a>)</li>\n</ul>\n<p>$$\\nabla{v_\\pi}(s)=\\nabla\\big[\\sum_a\\pi(a|s)q_\\pi(s,a)\\big]$$ </p>\n<p>다음으로 $(f\\cdot g)’=f’\\cdot g+f\\cdot g’$ (product rule of calculus)에 의해 다음과 같이 나타낼 수 있습니다.<br>$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\nabla q_\\pi(s,a)\\big]$$  </p>\n<p>이어서 $p(s’,r|s,a)\\cong Pr[S_t=s’, R_t=r|S_{t-1}=s,A_{t-1}=a ]$ (서튼책 3.2 공식) 에 의해 다음과 같이 나타낼 수 있습니다.</p>\n<p>$$=\\sum_a\\big[\\nabla\\pi(a|s)q_{\\pi(s,a)}+\\pi(a|s)\\nabla\\big[\\sum_{s’, r}p(s’,r|s,a)(r+v_{\\pi}(s’))\\big]\\big]$$  </p>\n<p>계속해서 $p(s’|s,a)\\cong Pr[S_t=s’, R_t=r|S_{t-1}=s,A_{t-1}=a ]=\\sum_{r\\in R}p(s’,r|s,a)$ (서튼책 3.4 공식) 에 의해 다음과 같이 나타낼 수 있습니다.<br>$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\sum_{s’}p(s’|s,a)\\nabla v_\\pi(s’) \\big]$$ </p>\n<p>여기서 $\\nabla v_\\pi(s’)$ 부분을 unrolling을 하면 다음과 같이 나타낼 수 있습니다.<br>$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\sum_{s’}p(s’|s,a)\\sum_{a’}\\big[\\nabla\\pi(a’|s’)q_\\pi(s’,a’)\\\\+\\pi(a’|s’)\\sum_{s’’}p(s’’|s’,a’)\\nabla v_\\pi(s’’))\\big] \\big]$$ </p>\n<p>unrolling을 반복하다보면 다음과 같이 나타낼 수 있습니다.<br>$$=\\sum_{x\\in S}\\sum_{k=0}^{\\infty}Pr(s\\rightarrow x,k,\\pi)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$</p>\n<p>여기에서 $Pr(s\\rightarrow x,k,\\pi)$ 는 policy $\\pi$ 에 대해 state s에서 state x까지 k step만큼 움직일 때의 변환 확률입니다. 따라서 위의 수식을 아래처럼 다시 나타낼 수 있습니다.<br>$$ \\begin{align} \\nabla J(\\theta)= &amp;\\nabla v_\\pi (s_0) \\ =&amp;\\sum_s(\\sum_{k=0}^{\\infty}Pr(s_0\\rightarrow s,k,\\pi))\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a) \\ =&amp;\\sum_s \\eta(s) \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a) \\end{align}$$</p>\n<ul>\n<li>서튼책에서 $\\eta(s)$는 다음과 같이 표현할 수 있습니다.<ul>\n<li>$\\eta(s)$ = 평균적으로 하나의 episode마다 state에서 머무른 time step의 수 (page 199)</li>\n</ul>\n</li>\n<li>논문에서는 $\\eta(s)$를 다음과 같이 표현합니다.<ul>\n<li>$d^\\pi(s)\\\\=\\sum_{t=0}^{\\infty}\\gamma^t Pr(s_t=s|s_0,\\pi)$ ($\\gamma=1$ is allowed only in episodic tasks) = discounted weighting of states</li>\n</ul>\n</li>\n</ul>\n<p>여기서부터가 논문에는 없는 내용입니다. 위의 최종 수식을 다시 한 번 적으면 아래와 같습니다.<br>$$\\sum_s \\eta(s) \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$</p>\n<p>위의 수식을 아래의 수식처럼 바꿀 수 있습니다.<br>$$\\sum_{s’} \\eta(s’) \\cdot\\sum_s \\frac{\\eta(s)}{\\sum_{s’}\\eta(s’)} \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$</p>\n<p>그러면 이것을 새로운 기호로 다시 나타낼 수 있습니다.<br>$$\\sum_{s’} \\eta(s’) \\cdot\\sum_s \\mu(s)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$ </p>\n<ul>\n<li>여기서 $\\mu(s)$ 는 state distribution이라고 하며, weight를 state distribution으로 바꿔주는 과정이라고 생각할 수 있습니다. 쉽게 말해 어떠한 state에 agent가 머무르는 확률입니다.</li>\n</ul>\n<p>위의 수식은 아래의 수식과 비례합니다. 따라서 최종 형태는 다음과 같습니다.<br>$$\\propto \\sum_s \\mu(s)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$<br><br><br></p>\n<hr>\n<h1 id=\"3-Policy-Gradient-with-Approximation\"><a href=\"#3-Policy-Gradient-with-Approximation\" class=\"headerlink\" title=\"3. Policy Gradient with Approximation\"></a>3. Policy Gradient with Approximation</h1><p>이번 section은 앞서 다뤘던 Theorem 1 중에 $Q^\\pi$에 대해서 중점적으로 다룹니다. 어떠한 $Q^\\pi$가 학습된 function approximator로 근사된다고 합시다. </p>\n<p>$f_w$ ($f_w$는 $S × A \\rightarrow \\Re$)를 parameter $w$를 가지는 어떠한 $Q^\\pi$ 에 대한 근사치입니다. 학습된 function approximator에 대해서 생각하기 때문에, 학습된 $f_w$는 error를 최소화하는 방향으로 parameter $w$를 업데이트된 것을 의미합니다. 그러면 하나의 rule에 의해서 $\\pi$를 따르고 $w$를 업데이트하는 $f_w$에 대해서 다음과 같이 생각할 수 있습니다.<br>$$\\Delta w_t \\propto \\frac{\\partial}{\\partial w}\\big[ {\\hat Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]^2 \\propto \\big[{\\hat Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s_t,a_t)}{\\partial w}$$<br>(여기서 ${\\hat Q}^\\pi(s_t,a_t)$ 는 $Q^\\pi(s_t,a_t)$의 unbiased estimator $R_t$입니다.)</p>\n<p>그러면 위와 같은 수식이 local optimum에 수렴을 했을 때, 다음과 같이 나타낼 수 있습니다.<br>$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$</p>\n<ul>\n<li>위의 수식에 대한 추가 설명<ul>\n<li>local optimum으로 수렴을 하기 때문에 당연히 위의 수식은 0이 됩니다.</li>\n<li>또한 stochastic policy이기 때문에 local optimum으로 수렴하려면 모든 state, action에 대해 expectation을 해야합니다. 따라서 $\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)$이 붙게 됩니다.<br><br></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"3-1-Theorem-2-Policy-Gradient-with-Function-Approximation\"><a href=\"#3-1-Theorem-2-Policy-Gradient-with-Function-Approximation\" class=\"headerlink\" title=\"3.1 Theorem 2: Policy Gradient with Function Approximation\"></a>3.1 Theorem 2: Policy Gradient with Function Approximation</h2><p>만약 $f_w$가 아래의 등식을 만족한다고 합시다.<br>$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$</p>\n<p>또한 $f_w$가 policy parameterization과 양립할 수 있다고 합시다. policy parameterization에 대한 여러가지 의미가 있는데 그 중에서 이 논문에서는 다음을 의미합니다.<br>$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}$$</p>\n<ul>\n<li>위의 수식에 대한 추가 설명<ul>\n<li>이 수식은 Monte-Calro Estimation을 이용하기 위한 조건입니다.</li>\n<li>Compatibility Condition이라고 부릅니다.</li>\n<li>우변(= $\\nabla\\log\\pi(s,a)$)으로 유도함으로써 아래의 수식과 같이 sampling을 이용한 추정이 가능해집니다.</li>\n</ul>\n</li>\n</ul>\n<p>따라서 위의 두 수식을 이용하여 아래와 같이 나타낼 수 있다.<br>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}f_w(s,a)$$<br><br></p>\n<h2 id=\"3-2-Proof-of-Theorem-2\"><a href=\"#3-2-Proof-of-Theorem-2\" class=\"headerlink\" title=\"3.2 Proof of Theorem 2\"></a>3.2 Proof of Theorem 2</h2><p>$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$</p>\n<p>$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}$$</p>\n<p>위의 두 수식을 합치면 다음과 같이 나타낼 수 있습니다.<br>$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}=0$$</p>\n<p>분자와 분모에 있는 $\\pi(s,a)$를 지우면 아래와 같습니다.<br>$$\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]=0$$</p>\n<p>이어서 위의 수식이 0이기 때문에 policy gradient theorem(앞서 다뤘던 Theorem 1)에서 위의 수식을 뺄 수 있습니다.<br>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)-\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ Q^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]$$</p>\n<p>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ Q^\\pi(s_t,a_t)-Q^\\pi(s_t,a_t)+f_w(s_t,a_t) \\big]$$</p>\n<p>$$\\therefore\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}f_w(s_t,a_t)$$<br><br><br></p>\n<hr>\n<h1 id=\"4-Application-to-Deriving-Algorithms-and-Advantages\"><a href=\"#4-Application-to-Deriving-Algorithms-and-Advantages\" class=\"headerlink\" title=\"4. Application to Deriving Algorithms and Advantages\"></a>4. Application to Deriving Algorithms and Advantages</h1><h2 id=\"4-1-Application-to-Deriving-Algorithms\"><a href=\"#4-1-Application-to-Deriving-Algorithms\" class=\"headerlink\" title=\"4.1 Application to Deriving Algorithms\"></a>4.1 Application to Deriving Algorithms</h2><p>feature의 linear combination에서 Gibbs distribution (softmax)를 하나의 policy로 생각해볼 수 있습니다.<br>$$\\pi(s,a)=\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_b e^{\\theta^{T}\\phi_{sb} }}$$</p>\n<ul>\n<li>($\\phi_{sa}$: state-action pair s,a을 나타내는 l-dimensional feature vector)</li>\n</ul>\n<p>compatible condition을 추가하면 다음과 같습니다. (4.2 Proof of application of compatible condition 참고)<br>$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\frac{1}{\\pi(s,a)}=\\phi_{sa}-\\sum_b \\pi(s,b)\\phi_{sb}$$ </p>\n<p>이어서 $f_w$을 적분한 natural parameterization은 다음과 같습니다.<br>$$f_w(s,a)=w^T\\big[ \\phi_{sa}-\\sum_b \\pi(s,b)\\phi_{sb}\\big]$$</p>\n<ul>\n<li>위의 수식에 대한 추가 설명<ul>\n<li>$f_w$는 policy로서 같은 feature들에 대해 linear합니다.</li>\n<li>$f_w$는 각각의 state에 대해서 평균이 0입니다. ($\\sum_a\\pi(s,a)f_w(s,a)=0$)</li>\n<li>advantage function $A^\\pi(s,a)=Q^\\pi(s,a)-V^\\pi(s)$의 하나의 근사치로서 $f_w$를 생각하는 것 좋습니다.<br><br></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"4-2-Proof-of-application-of-compatible-condition\"><a href=\"#4-2-Proof-of-application-of-compatible-condition\" class=\"headerlink\" title=\"4.2 Proof of application of compatible condition\"></a>4.2 Proof of application of compatible condition</h2><p>증명을 하기 전에 다음을 가정합니다.</p>\n<ol>\n<li>$(\\frac{f(x)}{g(x)})’=f’(x)\\cdot \\frac{1}{g(x)}-f(x)\\cdot \\frac{g’(x)}{g(x)^{2} }=\\frac{f’(x)\\cdot g(x)-f(x)\\cdot g’(x)}{g(x)^{2} }$</li>\n<li>$f(\\theta)=e^{\\theta^{T}\\phi_{sa} }$, $g(\\theta)=\\sum_be^{\\theta^{T}\\phi_{sb} }$</li>\n</ol>\n<p>$$<br>\\begin{align}<br>\\frac{\\partial f_w(s,a)}{\\partial w}&amp;=\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\frac{1}{\\pi(s,a)}<br>\\\\<br>&amp;=\\big(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }}\\big)’\\frac{1}{\\pi(s,a)}<br>\\\\<br>&amp;=\\big[ \\phi_{sa}(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }})-(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }})\\frac{\\sum_b\\phi_{sb}e^{ {\\theta^T}\\phi_{sb} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }}\\big]\\frac{1}{\\pi(s,a)}\\\\<br>&amp;=\\big[ \\phi_{sa}\\pi(s,a)-\\pi(s,a)\\sum_b\\phi_{sb}\\pi(s,b)\\big]\\frac{1}{\\pi(s,a)}<br>\\\\<br>&amp;=\\pi(s,a)\\big[ \\phi_{sa}-\\sum_b\\pi(s,b)\\phi_{sb}\\big]\\frac{1}{\\pi(s,a)}<br>\\\\<br>&amp;\\therefore\\phi_{sa}-\\sum_b\\pi(s,b)\\phi_{sb}<br>\\end{align}<br>$$</p>\n<p><br></p>\n<h2 id=\"4-3-Application-to-Advantages\"><a href=\"#4-3-Application-to-Advantages\" class=\"headerlink\" title=\"4.3 Application to Advantages\"></a>4.3 Application to Advantages</h2><p>Policy Gradient with Function Approximation Theorem (Theorem 2)는 advantage function으로 확장될 수 있습니다.<br>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_sd^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ f_w(s,a)+v(s) \\big]$$</p>\n<ul>\n<li>위의 수식에 대한 추가설명<ul>\n<li>$v$ ($v$는 $S\\rightarrow\\Re$) 는 arbitrary function입니다.</li>\n<li>이 수식은 $\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}=0$이기 때문에 가능해집니다.<ul>\n<li>즉, 이 수식은 $\\pi(s,a)$의 gradient에만 dependent하기 때문에 advantage 역할을 하는 함수들을 넣어도 아무런 상관이 없습니다.</li>\n</ul>\n</li>\n<li>$v$의 선택은 Theorem들에 영향을 미치지 못하지만, 실질적으로 gradient estimator의 variance에 영향을 미칩니다.</li>\n<li>이러한 문제는 전체적으로 이전의 연구에 reinforcement baseline의 사용에 있어서 유사합니다.</li>\n<li>(comment) 위의 수식에서 $f_w(s,a)+v(s)$와 Application to Deriving Algorithms의 $f_w(s,a)$와는 다른 것입니다. Application to Deriving Algorithms의 $f_w(s,a)$은 softmax에 의해 스스로 advantage function의 역할을 할 수 있습니다. 하지만 보통의 경우에는 그러지 못할 수도 있기 때문에 위의 수식처럼 $f_w(s,a)+v(s)$을 추가하여 zero mean만들어서 variance를 줄일 수 있습니다.<br><br><br></li>\n</ul>\n</li>\n</ul>\n<hr>\n<h1 id=\"5-Convergence-of-Policy-Iteration-with-Function-Approximation\"><a href=\"#5-Convergence-of-Policy-Iteration-with-Function-Approximation\" class=\"headerlink\" title=\"5. Convergence of Policy Iteration with Function Approximation\"></a>5. Convergence of Policy Iteration with Function Approximation</h1><h2 id=\"5-1-Theorem-3-Convergence-of-Policy-Iteration-with-Function-Approximation\"><a href=\"#5-1-Theorem-3-Convergence-of-Policy-Iteration-with-Function-Approximation\" class=\"headerlink\" title=\"5.1 Theorem 3: Convergence of Policy Iteration with Function Approximation\"></a>5.1 Theorem 3: Convergence of Policy Iteration with Function Approximation</h2><p>policy iteration with function approximation의 형태는 locally optimal policy에 수렴합니다. $\\pi$와 $f_w$를 </p>\n<p>compatibility condition을 만족하는 policy와 value function에 대해 $\\max_{\\theta, s, a, i, j}\\big| \\frac{\\partial^2\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}\\big|&lt;B&lt;\\infty$ 을<br>만족하는 어떠한 미분가능한 function approximator라고 합시다.</p>\n<p>(comment) $\\pi(s,a)$라는 function은 이계미분값이 존재하고, 임의의 상수인 (Bound) B에 유계하기 때문에 function의 그래프는 smooth하다고 볼 수 있습니다. (아래의 그림 중 빨간색 그래프 참고)</p>\n<p><img src=\"https://www.dropbox.com/s/xx02ejfg5ao19ps/Screen%20Shot%202018-07-10%20at%203.41.06%20PM.png?dl=1\"></p>\n<p>${\\alpha_k}$는 $\\lim_{k\\rightarrow\\infty}\\alpha_k=0$ 이며 $\\sum_k\\alpha_k=\\infty$를 만족하는 step-size sequence라고 합시다.</p>\n<p>그 때, bounded reward를 가진 MDP에 대해</p>\n<ul>\n<li><ol>\n<li>어떠한 $\\theta_0, \\pi_k=\\pi(\\cdot,\\cdot,\\theta_k)$ </li>\n</ol>\n</li>\n<li><ol start=\"2\">\n<li>$\\sum_sd^{\\pi_{k} }(s)\\sum_a\\pi_k(s,a)\\big[ Q^{\\pi_{k} }(s,a)-f_w(s,a)\\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$로 인하여 $w_k=w$, $\\theta_{k+1}=\\theta_k+\\alpha_k\\sum_sd^{\\pi_{k} }(s)\\sum_a\\frac{\\partial\\pi_k(s,a)}{\\partial\\theta}f_{w_{k} }(s,a)$</li>\n</ol>\n</li>\n</ul>\n<p>으로 정의된 sequence $\\rho(\\pi_k)$는 $\\lim_{k\\rightarrow\\infty}\\frac{\\partial\\rho(\\pi_k)}{\\partial\\theta}=0$이기 때문에 수렴합니다.</p>\n<ul>\n<li>sequence $\\rho(\\pi_k)_{k=0}^\\infty$에 대한 추가 설명<ul>\n<li>$\\theta_{k+1}=\\theta_k+\\alpha_k\\sum_sd^{\\pi_{k} }(s)\\sum_a\\frac{\\partial\\pi_k(s,a)}{\\partial\\theta}f_{w_{k} }(s,a)$ 에 따라 $\\theta$가 1, 2, …, $\\infty$로 갈텐데, 거기에 따른 objective function or performance의 sequence입니다.</li>\n<li>(comment) 굳이 sequence라는 표현이 없어도 될 것 같다. 어짜피 k가 $\\infty$로 가면 $\\rho(\\pi_k)$가 수렴한다는 의미이기 때문에 불필요해보입니다.</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"5-2-Proof-of-Theorem-3\"><a href=\"#5-2-Proof-of-Theorem-3\" class=\"headerlink\" title=\"5.2 Proof of Theorem 3\"></a>5.2 Proof of Theorem 3</h2><ul>\n<li>Theorem 2는 $\\theta_k$ update가 gradient의 error를 최소화한다는 것을 증명했습니다.</li>\n<li>$\\frac{\\partial^{2}\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}$와 MDP의 reward에서의 bound는, $\\frac{\\partial^{2}\\rho}{\\partial\\theta_i\\partial\\theta_j}$ 또한 bound된다는 것을 증명합니다.</li>\n<li>step-size 필요조건 때문에 이러한 bound된 것들은 Proposition 3.5 from page 94 of Bertsekas and Tsitsiklis(1996)에 적용하기 위해 필요한 조건입니다.</li>\n<li>Proposition 3.5 from page 94 of Bertsekas and Tsitsiklis(1996)은 local optimum으로 수렴한다는 것을 증명했습니다.<br><br><br></li>\n</ul>\n<hr>\n<h1 id=\"6-Summary\"><a href=\"#6-Summary\" class=\"headerlink\" title=\"6. Summary\"></a>6. Summary</h1><p>논문에서 설명한 policy gradient 기법을 요약하면 다음과 같습니다.</p>\n<ul>\n<li>Original maximization problem: $\\max\\rho_{\\theta}=E\\left[R\\right]$</li>\n<li>gradient ascent method를 이용한 parameter update: $\\theta_{t+1} = \\theta_{t} + \\alpha\\left.\\frac{\\partial E[R]}{\\partial\\theta}\\right|_{\\theta_t}$<ul>\n<li>그러나 우리는 gradient를 모르고, 추정하기도 어렵습니다. 왜냐하면 expectation이 안에 있기 때문입니다.</li>\n<li>그렇다면 이것을 추정 가능한 형태로 바꿔야합니다. 다시 말해 expectation이 밖에 있는 형태로 바꾸는 것입니다.</li>\n<li>Expectation이 밖에 있으면 왜 추정이 유리할까요? 바로 Sample mean을 취하면 되기 때문입니다.</li>\n</ul>\n</li>\n<li>Approximate gradient<ul>\n<li>방법1: 논문의 Theorem 1을 이용하면 gradient와 expectation의 위치를 바꿀 수 있습니다.<br>$$\\frac{\\partial E[R]}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)$$</li>\n<li>방법2: Log derivative trick을 이용하면 gradient와 expectation의 위치를 바꿀 수 있습니다. (Theorem 2)<br>$$\\theta_{t+1} = \\theta_{t} + \\alpha E[R\\frac{\\partial}{\\partial\\theta}\\log p_{\\theta}|\\theta_t]$$</li>\n<li>그리고 특정 trajectory를 따라가면서 return값을 구하고 이것을 여러 번 수행하여 sample mean을 취하면 gradient를 추정하는 것이 가능합니다.</li>\n<li>이러한 기법을 제시한 기존의 연구가 REINFORCE입니다.</li>\n<li>여러 Trajectory를 이용하므로 variance가 높을 수 밖에 없습니다.</li>\n<li>Advantage를 활용하거나 하는 방식으로 이후 여러 연구가 진행되었습니다.<br><br></li>\n</ul>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<hr>\n<h1 id=\"1-Intro-to-Policy-Gradient\"><a href=\"#1-Intro-to-Policy-Gradient\" class=\"headerlink\" title=\"1. Intro to Policy Gradient\"></a>1. Intro to Policy Gradient</h1><p>이 논문은 policy gradient (PG) 기법의 효시와도 같으며 향후 많은 파생연구를 낳은 중요한 논문입니다. 이 논문을 이해하기 위해 필요한 배경지식을 먼저 설명하고 논문을 차근차근 살펴보도록 하겠습니다.<br><br></p>\n<h2 id=\"1-1-Value-Function-Approach\"><a href=\"#1-1-Value-Function-Approach\" class=\"headerlink\" title=\"1.1 Value Function Approach\"></a>1.1 Value Function Approach</h2><p>전통적으로 강화학습 기법은 value function을 기반으로 동작하였습니다. 특정 state에서 vaue function 또는 value function을 근사하는 함수(function approximation)를 최대화하는 action을 찾는 greedy action-selection policy가  대표적입니다.</p>\n<p>논문에서는 이러한 방법은 deterministic한 policy를 찾는 쪽으로 나아가게 되지만 종종 최적의 policy는 stochastic한 성질을 가지기 때문에 이 방법으로는 최적의 policy를 찾을 수 없다고 언급하고 있습니다. (이 논문이 나오고 나서 한참 후 개발된 deterministic policy gradient 기법과는 반대되는 서술일 수 있습니다.) 아마도 이러한 부분은 $\\epsilon$-greedy action-selection policy로 개선될 수 있을 것입니다. 또 하나의 문제는 value function의 작은 변화로 인해서 action이 크게 변할 수도 있다는 것입니다. 이것은 알고리듬의 수렴성에 문제를 야기할 수 있습니다. 이러한 문제점을 해결하기 위하여 policy search라는 새로운 기법이 고안됩니다.<br><br></p>\n<h2 id=\"1-2-Policy-Search\"><a href=\"#1-2-Policy-Search\" class=\"headerlink\" title=\"1.2 Policy Search\"></a>1.2 Policy Search</h2><p>policy search는 최적의 policy $\\pi^*$를 직접 찾기 때문에 value function 모델링이 필요가 없습니다. (뒤에서보겠지만 사실 꼭 그렇지만도 않습니다.) policy는 좀 더 다루기 쉽도록 parameter $\\theta$를 이용하여 $\\pi_{\\theta}$로 표현할 수 있습니다. 최적의 policy를 찾기 위하여 expected return $E\\left[R|\\theta\\right]$이 최대화되도록 parameter를 조정합니다. 이를 간단한 수식으로 정리하면 다음과 같습니다.</p>\n<p>$$<br>\\pi^* = \\arg \\max\\limits_\\pi  E\\left[ {\\left. R \\right|\\pi } \\right] \\to {\\text{original problem} }<br>\\\\  {\\Downarrow} \\\\<br>{\\text{policy parameterization by } } \\pi_{\\theta}: \\Theta  \\to \\Pi \\\\ {\\Downarrow} $$</p>\n<p>$$\\pi^* = \\arg \\max\\limits_{\\theta}  E\\left[ {\\left. R \\right|\\theta } \\right] \\to {\\text{policy search problem} }<br>$$</p>\n<p>policy search 방법은 크게 두 가지로 분류할 수 있습니다. 첫 번째는 gradient-based optimization으로 policy gradient method가 이에 속합니다. 두 번째는 gradient-free optimization으로 진화(evolutionary) 연산을 이용하는 것이 이에 속합니다.<br><br></p>\n<h2 id=\"1-3-How-to-Obtain-the-Expected-Return\"><a href=\"#1-3-How-to-Obtain-the-Expected-Return\" class=\"headerlink\" title=\"1.3 How to Obtain the Expected Return\"></a>1.3 How to Obtain the Expected Return</h2><p>expected return을 최대화하는 방향으로 policy를 update한다고 하였습니다. 그렇다면 expected return은 어떻게 구할 수 있을까요? 여기에도 크게 두 가지 방법이 있습니다. 첫 번째는 deterministic approximation으로 Markov decision process의 dynamics를 모델링한 후 수식적으로 구하는 것입니다. 두 번째 방법은 monte carlo estimation으로 dynamics에 대한 모델을 하지 않고 많은 sample들을 얻은 후 empirical하게 expected return을 계산하는 방법입니다. 두 번째 방법이 좀 더 현실적이지만 gradient를 구하는 것은 더 어렵습니다. 결국 gradient를 esimate하는방법을 고안해야 합니다. 가장 유명한 gradient estimation 방법이 1992년 R. J. Williams에 의해서 제안된 REINFORCE 기법입니다. REINFORCE는 Monte Carlo estimate 또는 likelihood-ratio estimate라고 부르는 방법을 이용해서 gradient와 expectation의 위치를 서로 바꿔줍니다.</p>\n<h3 id=\"Monte-Carlo-Gradient-Estimation\"><a href=\"#Monte-Carlo-Gradient-Estimation\" class=\"headerlink\" title=\"Monte Carlo Gradient Estimation\"></a>Monte Carlo Gradient Estimation</h3><p>다음과 같은 parameter $\\theta$를 가지는 random variable $X$가 있습니다: $X:\\Omega\\mapsto\\mathcal{X}$. 그리고 이 $x$에 대한 함수 $f$가 있습니다: $f:\\mathcal{X}\\mapsto\\mathbb{R}$. expected return처럼 $E[f(x)]$를 최대화하고자 합니다. 이를 위해서는 $\\nabla_{\\theta}E[f(x)]$를 구해야 합니다. 이 때 다음과 같은 log derivate trick을 이용하여 다음과 같이 수식을 변형시킬 수 있습니다.</p>\n<p>$$<br>\\begin{align}<br>\\nabla_{\\theta}E_{p(x;\\theta)}\\left[ {f(x)} \\right]<br>&amp;= \\nabla_\\theta\\int{f\\left( x \\right)p\\left( {x;\\theta} \\right) dx}<br>\\\\<br>&amp;= \\int { {\\nabla_\\theta }p\\left( {x;\\theta } \\right)f\\left( x \\right)dx}<br>\\\\<br>&amp;= \\int {\\frac{ {p\\left( {x;\\theta } \\right)} }{ {p\\left( {x;\\theta } \\right)} }{\\nabla_\\theta }p\\left( {x;\\theta } \\right)f\\left( x \\right)dx}<br>\\\\<br>&amp;= \\int {p\\left( {x;\\theta } \\right){\\nabla_\\theta }\\log p\\left( {x;\\theta } \\right)f\\left( x \\right)dx}<br>\\\\<br>&amp;= E_{p(x;\\theta)}[f(x)\\nabla_\\theta \\log p({x;\\theta})]<br>\\end{align}<br>$$</p>\n<p>이제 expectation은 많은 샘플들을 모아서 평균을 취하는 Monte Carlo 기법으로 근사화할 수 있습니다.</p>\n<p>$${\\nabla_{\\theta} }{E_{p\\left( {x;\\theta } \\right)} }\\left[ {f\\left( x \\right)} \\right] = \\frac{1}{N}\\sum_{n=1}^N f\\left(X_n\\right)\\nabla_\\theta\\log p\\left(X_n;\\theta\\right)$$</p>\n<p>이 방법을 사용하기 위해서 필요한 조건은 $\\log p\\left(X_n;\\theta\\right)$가 미분가능해야 한다는 것 뿐입니다. 그러나 이 방법은 얻은 샘플들에 의존하기 때문에 경우에 따라서 큰 variance를 가질 수 있습니다.</p>\n<p><br><br></p>\n<hr>\n<h1 id=\"2-Policy-Gradient-Methods\"><a href=\"#2-Policy-Gradient-Methods\" class=\"headerlink\" title=\"2. Policy Gradient Methods\"></a>2. Policy Gradient Methods</h1><p>이제 본격적으로 policy gradient 기법에 대해서 알아보겠습니다.</p>\n<h2 id=\"2-1-Policy-Gradent-Approach\"><a href=\"#2-1-Policy-Gradent-Approach\" class=\"headerlink\" title=\"2.1 Policy Gradent Approach\"></a>2.1 Policy Gradent Approach</h2><p>policy gradient는 stochastic policy를 자체적인 파리미터를 가진 function approximator를 이용해서 근사화시킵니다. 몇 가지 notation들을 정의하겠습니다.</p>\n<ul>\n<li>$\\theta$: policy parameter들의 vector  </li>\n<li>$\\rho$: 해당 policy들의 성능을 나타내는 척도 (예. average reward per step)  </li>\n<li>$\\alpha$: positive-definite한 step size </li>\n</ul>\n<p>policy parameter는 gradient에 비례하여 업데이트됩니다.</p>\n<p>$$\\Delta\\theta \\approx \\alpha\\frac{\\partial\\rho}{\\partial\\theta}$$</p>\n<p>local optimal policy로 수렴합니다. 논문의 가장 중요한 contribution은 특정 조건을 만족하는 function approximator를 이용하여 경험(experience, sample)을 축적하고 이것들을 이용하여 위의 gradient를 unbiased estimate할 수 있다는 것을 증명한 것입니다.<br><br></p>\n<h2 id=\"2-2-System-Model\"><a href=\"#2-2-System-Model\" class=\"headerlink\" title=\"2.2 System Model\"></a>2.2 System Model</h2><p>시스템모델은 다음과 같습니다.</p>\n<ul>\n<li>Markov decision process (MDP) at each time $t\\in{0,1,2,…}$</li>\n<li>$S_t\\in\\mathcal{S}$: state</li>\n<li>$A_t\\in\\mathcal{A}$: action</li>\n<li>$r_t\\in E$: reward</li>\n<li>$\\theta\\in\\mathbb{R}^l$: vector of policy parameters, for $l\\ll\\left|\\mathcal{S}\\right|$</li>\n<li>$\\mathcal{P}<em>{s s’}^a = \\Pr[S</em>{t+1}=s’|S_t=s,A_t=a]$: state transition probabilities</li>\n<li>$\\mathcal{R}<em>{s s’}^a = E[R</em>{t+1}|S_t=s,A_t=a]$: expected reward</li>\n<li>$\\pi(s,a,\\theta)=\\Pr[A_t=a|S_t=s,\\theta]$: policy, shortened as $\\pi(s,a)$ or $\\pi(a|s)$</li>\n</ul>\n<p>다음으로는 reward를 표현하는 두 가지 방법에 대해서 알아보겠습니다.<br><br></p>\n<h2 id=\"2-3-Average-Reward-Formulation\"><a href=\"#2-3-Average-Reward-Formulation\" class=\"headerlink\" title=\"2.3 Average Reward Formulation\"></a>2.3 Average Reward Formulation</h2><p>Average rewward formulation은 시간의 흐름에 따른 reward를 표현한다기보다는 모든 시간의 reward를 평균을 내서 표현하는 방법입니다.</p>\n<ul>\n<li><p>Long‐term expected reward per step<br>$$\\rho(\\pi)=\\lim_{n\\to\\infty}\\frac{1}{n}E[r_1+r_2+\\cdots+r_n|\\pi] = \\sum_s d^\\pi(s)\\sum_a\\pi(s,a)\\mathcal{R}_s^a$$</p>\n<ul>\n<li>$d^\\pi(s): \\lim_{t\\to\\infty}\\Pr[S_t=s|s_0,\\pi]$: stationary distribution of states under $\\pi$</li>\n<li>$d^\\pi(s)$가 존재한다고 가정합니다. 그리고 모든 policy에 대해서 $d^\\pi(s)$는 $s_0$와 independent합니다.</li>\n<li>위의 두 표현식은 왜 같을까요? Time average와 Ensemble average가 같다는 뜻으로 ergodic한 system에서 성립합니다.</li>\n</ul>\n</li>\n<li><p>Value of a state-action pair given a policy<br>$$Q^{\\pi(s,a)} = \\sum_{t=1}^\\infty E[r_t - \\rho(\\pi)|S_0=s, A_0=a, \\pi]$$</p>\n</li>\n<li>State-valu function<br>$$V^\\pi(s) = \\sum_{a} \\pi(s,a)Q^\\pi(s,a)$$<br><br></li>\n</ul>\n<h2 id=\"2-4-Start-State-Formulation\"><a href=\"#2-4-Start-State-Formulation\" class=\"headerlink\" title=\"2.4 Start-State Formulation\"></a>2.4 Start-State Formulation</h2><p>start-state formulation은 시간의 흐름에 따라 감소하는 reward들을 표현합니다.</p>\n<ul>\n<li><p>Long‐term expected reward per step with a designated start state $S_0$<br>$$\\rho(\\pi) = E\\left[\\left.\\sum_{t=1}^\\infty \\gamma^{t-1}r_t\\right|S_0,\\pi\\right]$$</p>\n<ul>\n<li>$\\gamma\\in\\left[0,1\\right]$: discount rate</li>\n</ul>\n</li>\n<li><p>Value of a state‐action pair given a policy<br>$$Q^\\pi(s,a) = E\\left[\\left.\\sum_{k=1}^\\infty \\gamma^{k-1} r_{t+k}\\right|S_t=s, A_t=a, \\pi\\right]$$</p>\n</li>\n<li><p>Alternative form of $Q^\\pi(s,a)$<br>$$<br>Q^\\pi(s,a) =R_s^a + \\gamma\\sum_{s’}P_{s s’}^a V^{\\pi}(s’)<br>$$<br><br></p>\n</li>\n</ul>\n<h2 id=\"2-5-Policy-Gradient-Theorem\"><a href=\"#2-5-Policy-Gradient-Theorem\" class=\"headerlink\" title=\"2.5 Policy Gradient Theorem\"></a>2.5 Policy Gradient Theorem</h2><p>논문의 중요한 결과인 Theorem 1은 다음과 같습니다.</p>\n<p><em>For any MDP, in either the average‐reward or start‐state formulations,</em><br>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)$$</p>\n<p>놀라운 부분은 expected return의 gradient를 취할 때 $\\frac{\\partial d^\\pi(s,a)}{\\partial\\theta}$를 구하지 않아도 된다는 점입니다. 즉, policy의 변화가 state distribution에 영향을 주지 않는다는 것입니다. 이것은 다시 말하면 아래 수식과 같이 우리는 $\\frac{\\partial\\rho}{\\partial\\theta}$에 대한 unbiased estimator를 구할 수 있다는 뜻입니다.</p>\n<p>$$E_s\\left[\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)\\right] = \\frac{\\partial\\rho}{\\partial\\theta}$$</p>\n<p>이로인해서 우리는 gradient를 sampling을 통해서 추정할 수 있습니다. 그렇지만 이것은 sample이 아주 많을 때만 성립합니다!</p>\n<p>또 한 가지 문제는 우리는 $Q^\\pi(s,a)$의 정확한 값을 알 수 없다는 것 입니다. 이것을 estimate하기 위해서 현재의 return값 $R_t$를 이용할 수 있습니다.<br><br></p>\n<h2 id=\"2-6-Proof-of-Policy-Gradient-Theorem\"><a href=\"#2-6-Proof-of-Policy-Gradient-Theorem\" class=\"headerlink\" title=\"2.6 Proof of Policy Gradient Theorem\"></a>2.6 Proof of Policy Gradient Theorem</h2><p>증명을 살펴보겠습니다.</p>\n<ul>\n<li>average-reward formulation을 이용할 때의 증명은 아래와 같습니다.</li>\n</ul>\n<p>$$<br>\\begin{align}<br>\\frac{\\partial V^\\pi(s)}{\\partial\\theta} &amp;\\overset{\\underset{\\mathrm{def} }{} }{=}\\frac{\\partial}{\\partial\\theta}\\sum_a \\pi(s,a)Q^\\pi(s,a)<br>\\\\<br>&amp;=\\sum_a \\frac{\\partial}{\\partial\\theta}\\left[\\pi(s,a)Q^\\pi(s,a)\\right]\\\\<br>&amp;=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\frac{\\partial Q^\\pi(s,a)}{\\partial\\theta}\\right]\\\\<br>&amp;=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\frac{\\partial}{\\partial\\theta}\\left[R_s^a -\\rho(\\pi) + \\sum_{s’}P_{s s’}^a V^{\\pi}(s’)\\right]\\right]\\\\<br>&amp;=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\left[-\\frac{\\partial\\rho}{\\partial\\theta} + \\sum_{s’}\\mathcal{P}_{s s’}^a \\frac{\\partial V^{\\pi}(s’)}{\\partial\\theta}\\right]\\right]\\\\<br>\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\sum_{s’}\\mathcal{P}_{s s’}^a \\frac{\\partial V^{\\pi}(s’)}{\\partial\\theta}\\right] - \\frac{\\partial V^\\pi(s)}{\\partial\\theta}<br>\\end{align}<br>$$</p>\n<p>양변에 stationary distribution에 대한 평균을 취합니다.<br>$$<br>\\begin{align}<br>\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+ \\sum_s d^\\pi(s)\\sum_a\\pi(s,a)\\sum_{s’}\\mathcal{P}_{s s’}^a \\frac{\\partial V^{\\pi}(s’)}{\\partial\\theta}<br>\\\\<br>&amp;- \\sum_s d^\\pi(s)\\frac{\\partial V^\\pi(s)}{\\partial\\theta}<br>\\end{align}<br>$$</p>\n<p>모든 state $s$에서 모든 state $s’$으로 이동하는 것은 state $s’$에 존재할 확률이므로 다음과 같이 표현할 수 있습니다.</p>\n<p>$$<br>\\begin{align}<br>\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+ \\sum_s d^\\pi(s’) \\frac{\\partial V^{\\pi}(s’)}{\\partial\\theta} - \\sum_s d^\\pi(s)\\frac{\\partial V^\\pi(s)}{\\partial\\theta}<br>\\end{align}<br>$$</p>\n<p>뒤의 두 항은 같은 식이므로 이것은 다시 다음과 같이 표현됩니다.<br>$$<br>\\begin{align}<br>\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)<br>\\end{align}<br>$$</p>\n<p>$\\frac{\\partial\\rho}{\\partial\\theta}$는 $s$에 대한 함수가 아니므로 $\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}=\\frac{\\partial\\rho}{\\partial\\theta}$입니다.<br>이를 이용해서 위의 식을 다시 표현하면 아래와 같고 이를 통해 증명이 완성됩니다.</p>\n<p>$$<br>\\begin{align}<br>\\therefore\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)<br>\\end{align}<br>$$</p>\n<p>다음으로 start-state formulation에 대한 증명입니다.</p>\n<ul>\n<li>start-state formulation은 이 논문에서도 나오고 서튼책에서도 나오는데, 기존의 논문과 다르게 좀 더 진행되는 부분이 있습니다. 그래서 논문에 있는 증명이 아닌 서튼책에 있는 증명은 아래와 같습니다. (서튼책 <a href=\"https://drive.google.com/file/d/1xeUDVGWGUUv1-ccUMAZHJLej2C7aAFWY/view\" target=\"_blank\" rel=\"noopener\">Link</a>)</li>\n</ul>\n<p>$$\\nabla{v_\\pi}(s)=\\nabla\\big[\\sum_a\\pi(a|s)q_\\pi(s,a)\\big]$$ </p>\n<p>다음으로 $(f\\cdot g)’=f’\\cdot g+f\\cdot g’$ (product rule of calculus)에 의해 다음과 같이 나타낼 수 있습니다.<br>$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\nabla q_\\pi(s,a)\\big]$$  </p>\n<p>이어서 $p(s’,r|s,a)\\cong Pr[S_t=s’, R_t=r|S_{t-1}=s,A_{t-1}=a ]$ (서튼책 3.2 공식) 에 의해 다음과 같이 나타낼 수 있습니다.</p>\n<p>$$=\\sum_a\\big[\\nabla\\pi(a|s)q_{\\pi(s,a)}+\\pi(a|s)\\nabla\\big[\\sum_{s’, r}p(s’,r|s,a)(r+v_{\\pi}(s’))\\big]\\big]$$  </p>\n<p>계속해서 $p(s’|s,a)\\cong Pr[S_t=s’, R_t=r|S_{t-1}=s,A_{t-1}=a ]=\\sum_{r\\in R}p(s’,r|s,a)$ (서튼책 3.4 공식) 에 의해 다음과 같이 나타낼 수 있습니다.<br>$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\sum_{s’}p(s’|s,a)\\nabla v_\\pi(s’) \\big]$$ </p>\n<p>여기서 $\\nabla v_\\pi(s’)$ 부분을 unrolling을 하면 다음과 같이 나타낼 수 있습니다.<br>$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\sum_{s’}p(s’|s,a)\\sum_{a’}\\big[\\nabla\\pi(a’|s’)q_\\pi(s’,a’)\\\\+\\pi(a’|s’)\\sum_{s’’}p(s’’|s’,a’)\\nabla v_\\pi(s’’))\\big] \\big]$$ </p>\n<p>unrolling을 반복하다보면 다음과 같이 나타낼 수 있습니다.<br>$$=\\sum_{x\\in S}\\sum_{k=0}^{\\infty}Pr(s\\rightarrow x,k,\\pi)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$</p>\n<p>여기에서 $Pr(s\\rightarrow x,k,\\pi)$ 는 policy $\\pi$ 에 대해 state s에서 state x까지 k step만큼 움직일 때의 변환 확률입니다. 따라서 위의 수식을 아래처럼 다시 나타낼 수 있습니다.<br>$$ \\begin{align} \\nabla J(\\theta)= &amp;\\nabla v_\\pi (s_0) \\ =&amp;\\sum_s(\\sum_{k=0}^{\\infty}Pr(s_0\\rightarrow s,k,\\pi))\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a) \\ =&amp;\\sum_s \\eta(s) \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a) \\end{align}$$</p>\n<ul>\n<li>서튼책에서 $\\eta(s)$는 다음과 같이 표현할 수 있습니다.<ul>\n<li>$\\eta(s)$ = 평균적으로 하나의 episode마다 state에서 머무른 time step의 수 (page 199)</li>\n</ul>\n</li>\n<li>논문에서는 $\\eta(s)$를 다음과 같이 표현합니다.<ul>\n<li>$d^\\pi(s)\\\\=\\sum_{t=0}^{\\infty}\\gamma^t Pr(s_t=s|s_0,\\pi)$ ($\\gamma=1$ is allowed only in episodic tasks) = discounted weighting of states</li>\n</ul>\n</li>\n</ul>\n<p>여기서부터가 논문에는 없는 내용입니다. 위의 최종 수식을 다시 한 번 적으면 아래와 같습니다.<br>$$\\sum_s \\eta(s) \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$</p>\n<p>위의 수식을 아래의 수식처럼 바꿀 수 있습니다.<br>$$\\sum_{s’} \\eta(s’) \\cdot\\sum_s \\frac{\\eta(s)}{\\sum_{s’}\\eta(s’)} \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$</p>\n<p>그러면 이것을 새로운 기호로 다시 나타낼 수 있습니다.<br>$$\\sum_{s’} \\eta(s’) \\cdot\\sum_s \\mu(s)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$ </p>\n<ul>\n<li>여기서 $\\mu(s)$ 는 state distribution이라고 하며, weight를 state distribution으로 바꿔주는 과정이라고 생각할 수 있습니다. 쉽게 말해 어떠한 state에 agent가 머무르는 확률입니다.</li>\n</ul>\n<p>위의 수식은 아래의 수식과 비례합니다. 따라서 최종 형태는 다음과 같습니다.<br>$$\\propto \\sum_s \\mu(s)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$<br><br><br></p>\n<hr>\n<h1 id=\"3-Policy-Gradient-with-Approximation\"><a href=\"#3-Policy-Gradient-with-Approximation\" class=\"headerlink\" title=\"3. Policy Gradient with Approximation\"></a>3. Policy Gradient with Approximation</h1><p>이번 section은 앞서 다뤘던 Theorem 1 중에 $Q^\\pi$에 대해서 중점적으로 다룹니다. 어떠한 $Q^\\pi$가 학습된 function approximator로 근사된다고 합시다. </p>\n<p>$f_w$ ($f_w$는 $S × A \\rightarrow \\Re$)를 parameter $w$를 가지는 어떠한 $Q^\\pi$ 에 대한 근사치입니다. 학습된 function approximator에 대해서 생각하기 때문에, 학습된 $f_w$는 error를 최소화하는 방향으로 parameter $w$를 업데이트된 것을 의미합니다. 그러면 하나의 rule에 의해서 $\\pi$를 따르고 $w$를 업데이트하는 $f_w$에 대해서 다음과 같이 생각할 수 있습니다.<br>$$\\Delta w_t \\propto \\frac{\\partial}{\\partial w}\\big[ {\\hat Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]^2 \\propto \\big[{\\hat Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s_t,a_t)}{\\partial w}$$<br>(여기서 ${\\hat Q}^\\pi(s_t,a_t)$ 는 $Q^\\pi(s_t,a_t)$의 unbiased estimator $R_t$입니다.)</p>\n<p>그러면 위와 같은 수식이 local optimum에 수렴을 했을 때, 다음과 같이 나타낼 수 있습니다.<br>$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$</p>\n<ul>\n<li>위의 수식에 대한 추가 설명<ul>\n<li>local optimum으로 수렴을 하기 때문에 당연히 위의 수식은 0이 됩니다.</li>\n<li>또한 stochastic policy이기 때문에 local optimum으로 수렴하려면 모든 state, action에 대해 expectation을 해야합니다. 따라서 $\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)$이 붙게 됩니다.<br><br></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"3-1-Theorem-2-Policy-Gradient-with-Function-Approximation\"><a href=\"#3-1-Theorem-2-Policy-Gradient-with-Function-Approximation\" class=\"headerlink\" title=\"3.1 Theorem 2: Policy Gradient with Function Approximation\"></a>3.1 Theorem 2: Policy Gradient with Function Approximation</h2><p>만약 $f_w$가 아래의 등식을 만족한다고 합시다.<br>$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$</p>\n<p>또한 $f_w$가 policy parameterization과 양립할 수 있다고 합시다. policy parameterization에 대한 여러가지 의미가 있는데 그 중에서 이 논문에서는 다음을 의미합니다.<br>$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}$$</p>\n<ul>\n<li>위의 수식에 대한 추가 설명<ul>\n<li>이 수식은 Monte-Calro Estimation을 이용하기 위한 조건입니다.</li>\n<li>Compatibility Condition이라고 부릅니다.</li>\n<li>우변(= $\\nabla\\log\\pi(s,a)$)으로 유도함으로써 아래의 수식과 같이 sampling을 이용한 추정이 가능해집니다.</li>\n</ul>\n</li>\n</ul>\n<p>따라서 위의 두 수식을 이용하여 아래와 같이 나타낼 수 있다.<br>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}f_w(s,a)$$<br><br></p>\n<h2 id=\"3-2-Proof-of-Theorem-2\"><a href=\"#3-2-Proof-of-Theorem-2\" class=\"headerlink\" title=\"3.2 Proof of Theorem 2\"></a>3.2 Proof of Theorem 2</h2><p>$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$</p>\n<p>$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}$$</p>\n<p>위의 두 수식을 합치면 다음과 같이 나타낼 수 있습니다.<br>$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}=0$$</p>\n<p>분자와 분모에 있는 $\\pi(s,a)$를 지우면 아래와 같습니다.<br>$$\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]=0$$</p>\n<p>이어서 위의 수식이 0이기 때문에 policy gradient theorem(앞서 다뤘던 Theorem 1)에서 위의 수식을 뺄 수 있습니다.<br>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)-\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ Q^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]$$</p>\n<p>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ Q^\\pi(s_t,a_t)-Q^\\pi(s_t,a_t)+f_w(s_t,a_t) \\big]$$</p>\n<p>$$\\therefore\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}f_w(s_t,a_t)$$<br><br><br></p>\n<hr>\n<h1 id=\"4-Application-to-Deriving-Algorithms-and-Advantages\"><a href=\"#4-Application-to-Deriving-Algorithms-and-Advantages\" class=\"headerlink\" title=\"4. Application to Deriving Algorithms and Advantages\"></a>4. Application to Deriving Algorithms and Advantages</h1><h2 id=\"4-1-Application-to-Deriving-Algorithms\"><a href=\"#4-1-Application-to-Deriving-Algorithms\" class=\"headerlink\" title=\"4.1 Application to Deriving Algorithms\"></a>4.1 Application to Deriving Algorithms</h2><p>feature의 linear combination에서 Gibbs distribution (softmax)를 하나의 policy로 생각해볼 수 있습니다.<br>$$\\pi(s,a)=\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_b e^{\\theta^{T}\\phi_{sb} }}$$</p>\n<ul>\n<li>($\\phi_{sa}$: state-action pair s,a을 나타내는 l-dimensional feature vector)</li>\n</ul>\n<p>compatible condition을 추가하면 다음과 같습니다. (4.2 Proof of application of compatible condition 참고)<br>$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\frac{1}{\\pi(s,a)}=\\phi_{sa}-\\sum_b \\pi(s,b)\\phi_{sb}$$ </p>\n<p>이어서 $f_w$을 적분한 natural parameterization은 다음과 같습니다.<br>$$f_w(s,a)=w^T\\big[ \\phi_{sa}-\\sum_b \\pi(s,b)\\phi_{sb}\\big]$$</p>\n<ul>\n<li>위의 수식에 대한 추가 설명<ul>\n<li>$f_w$는 policy로서 같은 feature들에 대해 linear합니다.</li>\n<li>$f_w$는 각각의 state에 대해서 평균이 0입니다. ($\\sum_a\\pi(s,a)f_w(s,a)=0$)</li>\n<li>advantage function $A^\\pi(s,a)=Q^\\pi(s,a)-V^\\pi(s)$의 하나의 근사치로서 $f_w$를 생각하는 것 좋습니다.<br><br></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"4-2-Proof-of-application-of-compatible-condition\"><a href=\"#4-2-Proof-of-application-of-compatible-condition\" class=\"headerlink\" title=\"4.2 Proof of application of compatible condition\"></a>4.2 Proof of application of compatible condition</h2><p>증명을 하기 전에 다음을 가정합니다.</p>\n<ol>\n<li>$(\\frac{f(x)}{g(x)})’=f’(x)\\cdot \\frac{1}{g(x)}-f(x)\\cdot \\frac{g’(x)}{g(x)^{2} }=\\frac{f’(x)\\cdot g(x)-f(x)\\cdot g’(x)}{g(x)^{2} }$</li>\n<li>$f(\\theta)=e^{\\theta^{T}\\phi_{sa} }$, $g(\\theta)=\\sum_be^{\\theta^{T}\\phi_{sb} }$</li>\n</ol>\n<p>$$<br>\\begin{align}<br>\\frac{\\partial f_w(s,a)}{\\partial w}&amp;=\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\frac{1}{\\pi(s,a)}<br>\\\\<br>&amp;=\\big(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }}\\big)’\\frac{1}{\\pi(s,a)}<br>\\\\<br>&amp;=\\big[ \\phi_{sa}(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }})-(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }})\\frac{\\sum_b\\phi_{sb}e^{ {\\theta^T}\\phi_{sb} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }}\\big]\\frac{1}{\\pi(s,a)}\\\\<br>&amp;=\\big[ \\phi_{sa}\\pi(s,a)-\\pi(s,a)\\sum_b\\phi_{sb}\\pi(s,b)\\big]\\frac{1}{\\pi(s,a)}<br>\\\\<br>&amp;=\\pi(s,a)\\big[ \\phi_{sa}-\\sum_b\\pi(s,b)\\phi_{sb}\\big]\\frac{1}{\\pi(s,a)}<br>\\\\<br>&amp;\\therefore\\phi_{sa}-\\sum_b\\pi(s,b)\\phi_{sb}<br>\\end{align}<br>$$</p>\n<p><br></p>\n<h2 id=\"4-3-Application-to-Advantages\"><a href=\"#4-3-Application-to-Advantages\" class=\"headerlink\" title=\"4.3 Application to Advantages\"></a>4.3 Application to Advantages</h2><p>Policy Gradient with Function Approximation Theorem (Theorem 2)는 advantage function으로 확장될 수 있습니다.<br>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_sd^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ f_w(s,a)+v(s) \\big]$$</p>\n<ul>\n<li>위의 수식에 대한 추가설명<ul>\n<li>$v$ ($v$는 $S\\rightarrow\\Re$) 는 arbitrary function입니다.</li>\n<li>이 수식은 $\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}=0$이기 때문에 가능해집니다.<ul>\n<li>즉, 이 수식은 $\\pi(s,a)$의 gradient에만 dependent하기 때문에 advantage 역할을 하는 함수들을 넣어도 아무런 상관이 없습니다.</li>\n</ul>\n</li>\n<li>$v$의 선택은 Theorem들에 영향을 미치지 못하지만, 실질적으로 gradient estimator의 variance에 영향을 미칩니다.</li>\n<li>이러한 문제는 전체적으로 이전의 연구에 reinforcement baseline의 사용에 있어서 유사합니다.</li>\n<li>(comment) 위의 수식에서 $f_w(s,a)+v(s)$와 Application to Deriving Algorithms의 $f_w(s,a)$와는 다른 것입니다. Application to Deriving Algorithms의 $f_w(s,a)$은 softmax에 의해 스스로 advantage function의 역할을 할 수 있습니다. 하지만 보통의 경우에는 그러지 못할 수도 있기 때문에 위의 수식처럼 $f_w(s,a)+v(s)$을 추가하여 zero mean만들어서 variance를 줄일 수 있습니다.<br><br><br></li>\n</ul>\n</li>\n</ul>\n<hr>\n<h1 id=\"5-Convergence-of-Policy-Iteration-with-Function-Approximation\"><a href=\"#5-Convergence-of-Policy-Iteration-with-Function-Approximation\" class=\"headerlink\" title=\"5. Convergence of Policy Iteration with Function Approximation\"></a>5. Convergence of Policy Iteration with Function Approximation</h1><h2 id=\"5-1-Theorem-3-Convergence-of-Policy-Iteration-with-Function-Approximation\"><a href=\"#5-1-Theorem-3-Convergence-of-Policy-Iteration-with-Function-Approximation\" class=\"headerlink\" title=\"5.1 Theorem 3: Convergence of Policy Iteration with Function Approximation\"></a>5.1 Theorem 3: Convergence of Policy Iteration with Function Approximation</h2><p>policy iteration with function approximation의 형태는 locally optimal policy에 수렴합니다. $\\pi$와 $f_w$를 </p>\n<p>compatibility condition을 만족하는 policy와 value function에 대해 $\\max_{\\theta, s, a, i, j}\\big| \\frac{\\partial^2\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}\\big|&lt;B&lt;\\infty$ 을<br>만족하는 어떠한 미분가능한 function approximator라고 합시다.</p>\n<p>(comment) $\\pi(s,a)$라는 function은 이계미분값이 존재하고, 임의의 상수인 (Bound) B에 유계하기 때문에 function의 그래프는 smooth하다고 볼 수 있습니다. (아래의 그림 중 빨간색 그래프 참고)</p>\n<p><img src=\"https://www.dropbox.com/s/xx02ejfg5ao19ps/Screen%20Shot%202018-07-10%20at%203.41.06%20PM.png?dl=1\"></p>\n<p>${\\alpha_k}$는 $\\lim_{k\\rightarrow\\infty}\\alpha_k=0$ 이며 $\\sum_k\\alpha_k=\\infty$를 만족하는 step-size sequence라고 합시다.</p>\n<p>그 때, bounded reward를 가진 MDP에 대해</p>\n<ul>\n<li><ol>\n<li>어떠한 $\\theta_0, \\pi_k=\\pi(\\cdot,\\cdot,\\theta_k)$ </li>\n</ol>\n</li>\n<li><ol start=\"2\">\n<li>$\\sum_sd^{\\pi_{k} }(s)\\sum_a\\pi_k(s,a)\\big[ Q^{\\pi_{k} }(s,a)-f_w(s,a)\\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$로 인하여 $w_k=w$, $\\theta_{k+1}=\\theta_k+\\alpha_k\\sum_sd^{\\pi_{k} }(s)\\sum_a\\frac{\\partial\\pi_k(s,a)}{\\partial\\theta}f_{w_{k} }(s,a)$</li>\n</ol>\n</li>\n</ul>\n<p>으로 정의된 sequence $\\rho(\\pi_k)$는 $\\lim_{k\\rightarrow\\infty}\\frac{\\partial\\rho(\\pi_k)}{\\partial\\theta}=0$이기 때문에 수렴합니다.</p>\n<ul>\n<li>sequence $\\rho(\\pi_k)_{k=0}^\\infty$에 대한 추가 설명<ul>\n<li>$\\theta_{k+1}=\\theta_k+\\alpha_k\\sum_sd^{\\pi_{k} }(s)\\sum_a\\frac{\\partial\\pi_k(s,a)}{\\partial\\theta}f_{w_{k} }(s,a)$ 에 따라 $\\theta$가 1, 2, …, $\\infty$로 갈텐데, 거기에 따른 objective function or performance의 sequence입니다.</li>\n<li>(comment) 굳이 sequence라는 표현이 없어도 될 것 같다. 어짜피 k가 $\\infty$로 가면 $\\rho(\\pi_k)$가 수렴한다는 의미이기 때문에 불필요해보입니다.</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"5-2-Proof-of-Theorem-3\"><a href=\"#5-2-Proof-of-Theorem-3\" class=\"headerlink\" title=\"5.2 Proof of Theorem 3\"></a>5.2 Proof of Theorem 3</h2><ul>\n<li>Theorem 2는 $\\theta_k$ update가 gradient의 error를 최소화한다는 것을 증명했습니다.</li>\n<li>$\\frac{\\partial^{2}\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}$와 MDP의 reward에서의 bound는, $\\frac{\\partial^{2}\\rho}{\\partial\\theta_i\\partial\\theta_j}$ 또한 bound된다는 것을 증명합니다.</li>\n<li>step-size 필요조건 때문에 이러한 bound된 것들은 Proposition 3.5 from page 94 of Bertsekas and Tsitsiklis(1996)에 적용하기 위해 필요한 조건입니다.</li>\n<li>Proposition 3.5 from page 94 of Bertsekas and Tsitsiklis(1996)은 local optimum으로 수렴한다는 것을 증명했습니다.<br><br><br></li>\n</ul>\n<hr>\n<h1 id=\"6-Summary\"><a href=\"#6-Summary\" class=\"headerlink\" title=\"6. Summary\"></a>6. Summary</h1><p>논문에서 설명한 policy gradient 기법을 요약하면 다음과 같습니다.</p>\n<ul>\n<li>Original maximization problem: $\\max\\rho_{\\theta}=E\\left[R\\right]$</li>\n<li>gradient ascent method를 이용한 parameter update: $\\theta_{t+1} = \\theta_{t} + \\alpha\\left.\\frac{\\partial E[R]}{\\partial\\theta}\\right|_{\\theta_t}$<ul>\n<li>그러나 우리는 gradient를 모르고, 추정하기도 어렵습니다. 왜냐하면 expectation이 안에 있기 때문입니다.</li>\n<li>그렇다면 이것을 추정 가능한 형태로 바꿔야합니다. 다시 말해 expectation이 밖에 있는 형태로 바꾸는 것입니다.</li>\n<li>Expectation이 밖에 있으면 왜 추정이 유리할까요? 바로 Sample mean을 취하면 되기 때문입니다.</li>\n</ul>\n</li>\n<li>Approximate gradient<ul>\n<li>방법1: 논문의 Theorem 1을 이용하면 gradient와 expectation의 위치를 바꿀 수 있습니다.<br>$$\\frac{\\partial E[R]}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)$$</li>\n<li>방법2: Log derivative trick을 이용하면 gradient와 expectation의 위치를 바꿀 수 있습니다. (Theorem 2)<br>$$\\theta_{t+1} = \\theta_{t} + \\alpha E[R\\frac{\\partial}{\\partial\\theta}\\log p_{\\theta}|\\theta_t]$$</li>\n<li>그리고 특정 trajectory를 따라가면서 return값을 구하고 이것을 여러 번 수행하여 sample mean을 취하면 gradient를 추정하는 것이 가능합니다.</li>\n<li>이러한 기법을 제시한 기존의 연구가 REINFORCE입니다.</li>\n<li>여러 Trajectory를 이용하므로 variance가 높을 수 밖에 없습니다.</li>\n<li>Advantage를 활용하거나 하는 방식으로 이후 여러 연구가 진행되었습니다.<br><br></li>\n</ul>\n</li>\n</ul>\n"},{"title":"Deterministic Policy Gradient Algorithms","date":"2018-06-16T08:21:48.000Z","author":"이웅원","subtitle":"피지여행 2번째 논문","_content":"\nAuthors: David Silver<sup>1)</sup>, Guy Lever<sup>2)</sup>, Nicloas Heess<sup>1)</sup>, Thomas Degris<sup>1)</sup>, Daan Wierstra<sup>1)</sup>, Martin Riedmiller<sup>1)</sup>\nAffiliation: 1) Google Deepmind, 2) University College London (UCL)\nProceeding: International Conference on Machine Learning (ICML) 2014\n\n* [Deterministic Policy Gradient Algorithms](#deterministic-policy-gradient-algorithms)\n\t* [Summary](#summary)\n\t* [Background](#background)\n\t\t* [Performance objective function](#performance-objective-function)\n\t\t* [SPG Theorem](#spg-theorem)\n\t\t* [Stochastic Actor-Critic Algorithms](#stochastic-actor-critic-algorithms)\n\t\t* [Off-policy Actor-Critic](#off-policy-actor-critic)\n\t* [Gradient of Deterministic Policies](#gradient-of-deterministic-policies)\n\t\t* [Regulariy Conditions](#regulariy-conditions)\n\t\t* [Deterministic Policy Gradient Theorem](#deterministic-policy-gradient-theorem)\n\t\t* [DPG 형태에 대한 informal intuition](#dpg-형태에-대한-informal-intuition)\n\t\t* [DPG 는 SPG 의 limiting case 임](#dpg-는-spg-의-limiting-case-임)\n\t* [Deterministic Actor-Critic Algorithms](#deterministic-actor-critic-algorithms)\n\t* [Experiments](#experiments)\n\t\t* [Continuous Bandit](#continuous-bandit)\n\t\t* [Continuous Reinforcement Learning](#continuous-reinforcement-learning)\n\t\t* [Octopus Arm](#octopus-arm)\n\n<!-- /code_chunk_output -->\n\n<br>\n\n---\n## 1. Summary\n- Deterministic Policy Gradient (DPG) Theorem 제안함 [[Theorem 1](#deterministic-policy-gradient-theorem)]\n    1) DPG는 존재하며,\n    2) DPG는 Expected gradient of the action-value function의 형태를 띈다.\n- Policy variance가 0에 수렴할 경우, DPG는 Stochastic Policy Gradient(SPG)와 동일해짐을 보임 [[Theorem 2](#dpg-는-spg-의-limiting-case-임)]\n    - Theorem 2 로 인해 기존 Policy Gradient (PG) 와 관련된 기법들을 DPG 에 적용할 수 있게 됨\n        - 예. Sutton PG, natural gradients, actor-critic, episodic/batch methods\n- 적절한 exploration 을 위해 model-free, off-policy Actor-Critic algorithm 을 제안함\n    - Action-value function approximator 사용으로 인해 policy gradient가 bias되는 것을 방지하기 위해 compatibility conditions을 제공 [[Theorem 3](##deterministic-actor-critic-algorithms)]\n- DPG 는 SPG 보다 성능이 좋음\n    - 특히 high dimensional action spaces 을 가지는 tasks에서의 성능 향상이 큼\n        - SPG의 policy gradient는 state와 action spaces 모두에 대해서, DPG의 policy gradient는  state spaces에 대해서만 평균을 취함\n        - 결과적으로, action spaces의 dimension이 커질수록 data efficiency가 높은 DPG의 학습이 더 잘 이뤄지게 됨\n        - 무한정 학습을 시키게 되면, SPG도 최적으로 수렴할 것으로 예상되기에 위 성능 비교는 일정 iteration 내로 한정함\n    - 기존 기법들에 비해 computation 양이 많지 않음\n        - Computation 은 action dimensionality 와 policy parameters 수에 비례함\n<br>\n\n---\n## 2. Background\n### 2.1 Performance objective function\n\n$$\n\\begin{align}\nJ(\\pi_{\\theta}) &= \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\pi_{\\theta}(s,a)r(s,a)dads = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[r(s,a)]\n\\end{align}\n$$\n\n### 2.2 SPG Theorem\n- State distribution $ \\rho^{\\pi}(s) $ 은 policy parameters에 영향을 받지만, policy gradient 를 계산할 때는 state distribution 의 gradient 를 고려할 필요가 없다.\n- $$\\begin{eqnarray}\\nabla_{\\theta}J(\\pi_{\\theta}) &=& \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\\\ &=& E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]\n\\end{eqnarray}$$\n\n### 2.3 Stochastic Actor-Critic Algorithms\n- Actor 와 Critic 이 번갈아가면서 동작하며 stochastic policy 를 최적화하는 기법\n- Actor: $ Q^{\\pi}(s,a) $ 를 근사한 $ Q^w(s,a) $ 를 이용해 stochastic policy gradient 를 ascent 하는 방향으로 policy parameter $ \\theta $ 를 업데이트함으로써 stochastic policy 를 발전시킴\n    - $ \\nabla_{\\theta}J(\\pi_{\\theta}) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}log\\pi_{\\theta}(a|s)Q^{w}(s,a)] $\n- Critic: SARSA 나 Q-learning 같은 Temporal-difference (TD) learning 을 이용해 action-value function의 parameter, $ w $ 를 업데이트함으로써 $ Q^w(s,a) $ 가 $ Q^{\\pi}(s,a) $ 과 유사해지도록 함\n- 실제 값인 $ Q^{\\pi}(s,a) $ 대신 이를 근사한 $ Q^w(s,a) $ 를 사용하게 되면, 일반적으로 bias 가 발생하게 된다. 하지만, compatible condition 에 부합하는 $ Q^w(s,a) $ 를 사용하게 되면, bias 가 발생하지 않는다.\n\n### 2.4 Off-policy Actor-Critic\n- Distinct behavior policy $ \\beta(a|s) ( \\neq \\pi_{\\theta}(a|s) ) $ 로부터 샘플링된 trajectories 를 이용한 Actor-Critic\n- Performance objective function\n    - $\\begin{eqnarray}\n        J_{\\beta}(\\pi_{\\theta}) \n        &=& \\int_{S}\\rho^{\\beta}(s)V^{\\pi}(s)ds \\nonumber \\\\\\\\\n        &=& \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads\n        \\end{eqnarray} $\n- off-policy policy gradient\n    - $ \\begin{eqnarray}\n        \\nabla_{\\theta}J_{\\beta}(\\pi_{\\theta}) &\\approx& \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\end{eqnarray} $\n      $=E_{s \\sim \\rho^{\\beta}, a \\sim \\beta}[\\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)}\\nabla_{\\theta}log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]$\n    - off-policy policy gradient 식에서의 물결 표시는 [Degris, 2012b] 논문에 근거함\n        - [Degris, 2012b] \"Linear off-policy actor-critic,\" ICML 2012\n        - Exact off-policy policy gradient 와 이를 approximate 한 policy gradient 는 아래와 같음. (빨간색 상자에 있는 항목을 삭제함으로써 근사함)\n            - <img src=\"https://www.dropbox.com/s/xzpv3okc139c1fs/Screenshot%202018-06-16%2017.48.51.png?dl=1\" width=500px>\n        - [Degris, 2012b] Theorem 1 에 의해 policy parameter 가 approximated policy gradient ( $\\nabla_{u}𝑄^{\\pi,\\gamma}(𝑠,𝑎)$ term 제거)에 따라 업데이트되어도 policy 는 improve 가 됨이 보장되기에 exact off-policy policy gradient 대신 approximated off-policy policy gradient 를 사용해도 괜찮음.\n            - <img src=\"https://www.dropbox.com/s/mk13931r4scjngo/Screenshot%202018-06-16%2017.49.24.png?dl=1\" width=500px>\n    - off-policy policy gradient 식에서 $ \\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)} $ 는 importance sampling ratio 임\n        - off-policy actor-critic 에서는 $ \\beta $ 에 의해 샘플링된 trajectory 를 이용해서 stochastic policy $ \\pi $ 를 예측하는 것이기 때문에 imnportance sampling 이 필요함\n<br>\n\n---\n## 3. Gradient of Deterministic Policies\n### 3.1 Regulariy Conditions\n- 어떠한 이론이 성립하기 위한 전제 조건\n- Regularity conditions A.1\n    - $ p(s'|s,a), \\nabla_{a}p(s'|s,a), \\mu_{\\theta}(s), \\nabla_{\\theta}\\mu_{\\theta}(s), r(s,a), \\nabla_{a}r(s,a), p_{1}(s) $ are continuous in all parameters and variables $ s, a, s' $ and $ x $.\n- regularity conditions A.2\n    - There exists a $ b $ and $ L $ such that $ \\sup_{s}p_{1}(s) < b $, $ \\sup_{a,s,s'}p(s'|s,a) < b $, $ \\sup_{a,s}r(s,a) < b $, $ \\sup_{a,s,s'}\\|\\nabla_{a}p(s'|s,a)\\| < L $, and $ \\sup_{a,s}\\|\\nabla_{a}r(s,a)\\| < L $.\n\n### 3.2 Deterministic Policy Gradient Theorem\n- Deterministic policy\n    - $ \\mu_{\\theta} : S \\to A $ with parameter vector $ \\theta \\in \\rm I \\! R^n $\n- Probability distribution\n    - $ p(s \\to s', t, \\mu) $\n- Discounted state distribution\n    - $ \\rho^{\\mu}(s) $\n- Performance objective\n\n$$\nJ(\\mu_{\\theta}) = E[r^{\\gamma}_{1} | \\mu] \n$$\n\n$$\n= \\int_{S}\\rho^{\\mu}(s)r(s,\\mu_{\\theta}(s))ds \n= E_{s \\sim \\rho^{\\mu}}[r(s,\\mu_{\\theta}(s))]\n$$\n\n- DPG Theorem\n    - MDP 가 A.1 만족한다면, 아래 식(9)이 성립함\n    $\\nabla_{\\theta}J(\\mu_{\\theta}) = \\int_{S}\\rho^{\\mu}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}ds \\nonumber$\n    $= E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]   \\nonumber $ (9)\n    \n\t- DPG는 State space 에 대해서만 평균을 취하면 되기에, State와 Action space 모두에 대해 평균을 취해야 하는 [SPG](#spg-theorem)에 비해 data efficiency가 좋다. 즉, 더 적은 양의 데이터로도 학습이 잘 이뤄지게 된다.\n\n    \n### 3.3 DPG 형태에 대한 informal intuition\n- Generalized policy iteration\n    - 정책 평가와 정책 발전을 한 번 씩 번갈아 가면서 실행하는 정책 iteration\n        - 위와 같이 해도 정책 평가에서 예측한 가치함수가 최적 가치함수에 수렴함\n- 정책 평가\n    - action-value function $ Q^{\\pi}(s,a) $ or $ Q^{\\mu}(s,a) $ 을 estimate 하는 것\n- 정책 발전\n    - 위 estimated action-value function 에 따라 정책을 update 하는 것\n    - 주로 action-value function 에 대한 greedy maximisation 사용함\n        - $ \\mu^{k+1}(s) = \\arg\\max\\limits_{a}Q^{\\mu^{k}}(s,a) $\n        - greedy 정책 발전은 매 단계마다 global maximization을 해야하는데, 이는 continuous action spaces 에서 계산량이 급격히 늘어나게 됨.\n    - 그렇기에 policy gradient 방법이 나옴\n        - policy 를 $ \\theta $ 에 대해서 parameterize 함\n        - 매 단계마다 global maximisation 수행하는 대신, 방문하는 state $ s $ 마다 policy parameter 를 action-value function $ Q $ 의 $ \\theta $ 에 대한 gradient $ \\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s)) $ 방향으로 proportional 하게 update 함\n        - 하지만 각 state 는 다른 방향을 제시할 수 있기에, state distribution $ \\rho^{\\mu}(s) $ 에 대한 기대값을 취해 policy parameter 를 update 할 수도 있음\n            - $ \\theta^{k+1} = \\theta^{k} + \\alpha \\rm I\\!E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s))] $\n        - 이는 chain-rule 에 따라 아래와 같이 분리될 수 있음\n            - $ \\theta^{k+1} = \\theta^{k} + \\alpha \\rm I\\!E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu^{k}}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $ (7)\n            - chain rule: $ \\frac{\\partial Q}{\\partial \\theta} = \\frac{\\partial a}{\\partial \\theta} \\frac{\\partial Q}{\\partial a} $\n        - 하지만 state distribution $ \\rho^{\\mu} $ 은 정책에 dependent 함\n            - 정책이 바꾸게 되면, 바뀐 정책에 따라 방문하게 되는 state 가 변하게 되기 때문에 state distribution이 변하게 됨\n        - 그렇기에 정책 update 시 state distribution에 대한 gradient 를 고려하지 않는데 정책 발전이 이뤄진다는 것은 직관적으로 와닿지 않을 수 있음\n        - deterministic policy gradient theorem 은 state distribution에 대한 gradient 계산없이 위 식(7) 대로만 update 해도 performance objective 의 gradient 를 정확하게 따름을 의미한다.\n\n\n### 3.4 DPG 는 SPG 의 limiting case 임\n- stochastic policy parameterization\n    - $ \\pi_{\\mu_{\\theta},\\sigma} $ by a deterministic policy $ \\mu_{\\theta} : S \\to A $ and a variance parameter $ \\sigma $\n    - $ \\sigma = 0 $ 이면, $ \\pi_{\\mu_{\\theta},\\sigma} \\equiv \\mu_{\\theta} $\n- Theorem 2. Policy 의 variance 가 0 에 수렴하면, 즉, $ \\sigma \\to 0 $, stochastic policy gradient 와 deterministic policy gradient 는 동일해짐\n    - 조건 : stochastic policy $ \\pi_{\\mu_{\\theta},\\sigma} = \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $\n        - $ \\sigma $ 는 variance\n        - $ \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $ 는 conditions B.1 만족\n        - MDP 는 conditions A.1 및 A.2 만족\n    - 결과 :\n        - $ \\lim\\limits_{\\sigma\\downarrow0}\\nabla_{\\theta}J(\\pi_{\\mu_{\\theta},\\sigma}) = \\nabla_{\\theta}J(\\mu_{\\theta})  $\n            - 좌변은 standard stochastic gradient 이며, 우변은 deterministic gradient.\n    - 의미 :\n        - deterministic policy gradient 는 stochastic policy gradient 의 특수 case 임\n        - 기존 유명한 policy gradients 기법들에 deterministic policy gradients 를 적용할 수 있음\n            - 기존 기법들 예: compatible function approximation (Sutton, 1999), natural gradients (Kakade, 2001), actor-critic (Bhatnagar, 2007) or episodic/batch methods (Peters, 2005)\n<br>\n \n---\n## 4. Deterministic Actor-Critic Algorithms\n1. 살사 critic 을 이용한 on-policy actor-critic\n    - 단점\n        - deterministic policy 에 의해 행동하면 exploration 이 잘 되지 않기에, sub-optimal 에 빠지기 쉬움\n    - 목적\n        - 교훈/정보제공\n        - 환경에서 충분한 noise 를 제공하여 exploration을 시킬 수 있다면, deterministic policy 를 사용한다고 하여도 좋은 학습 결과를 얻을 수도 있음$\n            - 예. 바람이 agent의 행동에 영향(noise)을 줌\n    - Remind: 살사(SARSA) update rule\n        - $ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma Q(s_{t+1},a_{t+1}) - Q(s_{t},a_{t})) $\n    - Algorithm\n        - Critic 은 MSE 를 $ \\bf minimize $ 하는 방향, 즉, action-value function 을 stochastic gradient $ \\bf descent $ 방법으로 update 함\n            - $ MSE = [Q^{\\mu}(s,a) - Q^{w}(s,a)]^2 $\n                - critic 은 실제 $ Q^{\\mu}(s,a) $ 대신 미분 가능한 $ Q^{w}(s,a) $ 로 대체하여 action-value function 을 estimate 하며, 이 둘 간 Mean Square Error 를 최소화하는 것이 목표\n            - $ \\nabla_{w}MSE \\approx -2 * [r + \\gamma Q^{w}(s',a') - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $\n                - $ \\nabla_{w}MSE = -2 * [Q^{\\mu}(s,a) - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $\n                - $ Q^{\\mu}(s,a) $ 를 $ r + \\gamma Q^{w}(s',a') $ 로 대체\n                    - $ Q^{\\mu}(s,a) = r + \\gamma Q^{\\mu}(s',a') $\n            - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $\n                - $w_{t+1} = w_{t} - \\alpha_{w}\\nabla_{w}MSE  \\nonumber$\n                $ \\approx w_{t} - \\alpha_{w} * (-2 * [r + \\gamma Q^{w}(s',a') - Q^{w}(s,a)] \\nabla_{w}Q^{w}(s,a)$\n                - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},a_{t+1}) - Q^{w}(s_{t},a_{t}) $\n        - Actor 는 식(9)에 따라 보상이 $ \\bf maximize $ 되는 방향, 즉, deterministic policy 를 stochastic gradient $ \\bf ascent $ 방법으로 update함\n            - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $\n2. Q-learning 을 이용한 off-policy actor-critic\n    - stochastic behavior policy $ \\beta(a|s) $ 에 의해 생성된 trajectories 로부터 deterministic target policy $ \\mu_{\\theta}(s) $ 를 학습하는 off-policy actor-critic\n    - performance objective\n        - $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)V^{\\mu}(s)ds \\nonumber \\\\\\\\$\n          $= \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds \\nonumber \\\\\\\\$\n          $= E_{s \\sim \\rho^{\\beta}}[Q^{\\mu}(s,\\mu_{\\theta}(s))]$\n    - off-policy deterministic policy gradient\n        - $ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $\n            - 논문에는 아래와 같이 나와있는데, 물결 표시 부분은 오류로 판단됨.\n            - $ \\begin{eqnarray}\n                \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) &\\approx& \\int_{S}\\rho^{\\beta}(s)\\nabla_{\\theta}\\mu_{\\theta}(a|s)Q^{\\mu}(s,a)ds \\nonumber \\\\\n                &=& E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]\n                \\end{eqnarray} $\n            - 근거: Action이 deterministic 하기에 stochastic 경우와는 다르게 performance objective 에서 action 에 대해 평균을 구할 필요가 없음. 그렇기에, 곱의 미분이 있을 필요가 없고, [Degris, 2012b] 에서처럼 곱의 미분을 통해 생기는 action-value function 에 대한 gradient term 를 생략할 필요가 사라짐. [참고](#off-policy-actor-critic)\n    - Remind: 큐러닝(Q-learning) update rule\n        - $ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma \\max\\limits_{a}Q(s_{t+1},a) - Q(s_{t},a_{t})) $\n    - algorithm: OPDAC (Off-Policy Deterministic Actor-Critic)\n        - 살사를 이용한 on-policy deterministic actor-critic 과 아래 부분을 제외하고는 같음\n            - target policy 는 $ \\beta(a|s) $ 에 의해 생성된 trajectories 를 통해 학습함\n            - 업데이트 목표 부분에 실제 행동 값 $ a_{t+1} $ 이 아니라 정책으로부터 나온 행동 값 $ \\mu_{\\theta}(s_{t+1}) $ 사용함\n                - $ \\mu_{\\theta}(s_{t+1}) $ 는 가장 높은 Q 값을 가지는 행동. 즉, Q-learning.\n        - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $\n        - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $\n        - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $\n    - Stochastic off-policy actor-critic 은 대개 actor 와 critic 모두 importance sampling을 필요로 하지만, deterministic policy gradient 에선 importance sampling이 필요없음\n        - Actor 는 deterministic 이기에 sampling 자체가 필요없음\n            - Stochastic policy 인 경우, Actor 에서 importance sampling이 필요한 이유는 상태 $ s $ 에서의 가치 함수 값 $ V^{\\pi}(s) $ 을 estimate 하기 위해 $ \\pi $ 가 아니라 $ \\beta $ 에 따라 sampling을 한 후, 평균을 내기 때문임.\n            - 하지만 Deterministic policy 인 경우, 상태 $ s $ 에서의 가치 함수 값 $ V^{\\pi}(s) = Q^{\\pi}(s,\\mu_{\\theta}) $ 즉, action 이 상태 s 에 대해 deterministic 이기에 sampling 을 통해 estimate 할 필요가 없고, 따라서 importance sampling도 필요없어짐.\n            - stochastic vs. deterministic performance objective\n                - stochastic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads $\n                - deterministic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds $\n        - Critic 이 사용하는 Q-learning 은 importance sampling이 필요없는 off policy 알고리즘임.\n            - Q-learning 도 업데이트 목표를 특정 분포에서 샘플링을 통해 estimate 하는 것이 아니라 Q 함수를 최대화하는 action을 선택하는 것이기에 위 actor 에서의 deterministic 경우와 비슷하게 볼 수 있음\n3. compatible function approximation 및 gradient temporal-difference learning 을 이용한 actor-critic\n    - 위 살사/Q-learning 기반 on/off-policy 는 아래와 같은 문제가 존재\n        - function approximator 에 의한 bias\n            - 일반적으로 $ Q^{\\mu}(s,a) $ 를 $ Q^{w}(s,a) $ 로 대체하여 deterministic policy gradient 를 구하면, 그 gradient 는 ascent 하는 방향이 아닐 수도 있음\n        - off-policy learning 에 의한 instabilities\n    - 그래서 stochastic 처럼 $ \\nabla_{a}Q^{\\mu}(s,a) $ 를 $ \\nabla_{a}Q^{w}(s,a) $ 로 대체해도 deterministic policy gradient 에 영향을 미치지 않을 compatible function approximator $ Q^{w}(s,a) $ 를 찾아야 함\n    - Theorem 3. 아래 두 조건을 만족하면, $ Q^{w}(s,a) $ 는 deterministic policy $ \\mu_{\\theta}(s) $ 와 compatible 함. 즉, $ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $\n        - $ \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $\n        - $ w $ 는 $ MSE(\\theta, w) = E[\\epsilon(s;\\theta,w)^{\\top}\\epsilon(s;\\theta,w)] $ 를 최소화함\n            - $ \\epsilon(s;\\theta,w) = \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} - \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}  $\n    - Theorem 3 은 on-policy 뿐만 아니라 off-policy 에도 적용 가능함\n    - $ Q^{w}(s,a) = (a-\\mu_{\\theta}(s))^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top} w + V^{v}(s) $\n        - 어떠한 deterministic policy 에 대해서도 위 형태와 같은 compatible function approximator 가 존재함.\n        - 앞의 term 은 advantage 를, 뒤의 term 은 value 로 볼 수 있음\n    - $ Q^{w}(s,a) = \\phi(s,a)^{\\top} w + v^{\\top}\\phi(s) $\n        - 정의 : $ \\phi(s,a) \\overset{\\underset{\\mathrm{def}}{}}{=} \\nabla_{\\theta}\\mu_{\\theta}(s)(a-\\mu_{\\theta}(s)) $\n        - 일례 : $ V^{v}(s) = v^{\\top}\\phi(s) $\n        - Theorem 3 만족 여부\n            - 첫 번째 조건 만족.\n            - 두 번째 조건은 대강 만족.\n                - $ \\nabla_{a}Q^{\\mu}(s,a) $ 에 대한 unbiased sample 을 획득하기는 매우 어렵기에, 일반적인 정책 평가 방법들로 $ w $ 학습.\n                - 이 정책 평가 방법들을 이용하면 $ Q^{w}(s,a) \\approx Q^{\\mu}(s,a) $ 인 reasonable solution을 찾을 수 있기에 대강 $ \\nabla_{a}Q^{w}(s,a) \\approx \\nabla_{a}Q^{\\mu}(s,a) $ 이 될 것.\n        - action-value function 에 대한 linear function approximator 는 큰 값을 가지는 actions 에 대해선 diverge할 수 있어 global 하게 action-values 예측하기에는 좋지 않지만, local critic 에 사용할 때는 매우 유용하다.\n            - 즉, 절대값이 아니라 작은 변화량을 다루는 gradient method 경우엔 $ A^{w}(s,\\mu_{\\theta}(s)+\\delta) = \\delta^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $ 로, diverge 하지 않고, 값을 얻을 수 있음\n    - COPDAC-Q algorithm (Compatible Off-Policy Deterministic Actor-Critic Q-learning critic)\n        - Critic: 실제 action-value function 에 대한 linear function approximator 인 $ Q^{w}(s,a) = \\phi(s,a)^{\\top}w $ 를 estimate\n            - $ \\phi(s,a) = a^{\\top}\\nabla_{\\theta}\\mu_{\\theta} $\n            - Behavior policy $ \\beta(a|s) $ 로부터 얻은 samples를 이용하여 Q-learning 이나 gradient Q-learning 과 같은 off-policy algorithm 으로 학습 가능함\n        - Actor: estimated action-value function 에 대한 gradient 를 $ \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $ 로 치환 후, 정책을 업데이트함\n        - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $\n        - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) $\n        - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $\n    - off-policy Q-learning은 linear function approximation을 이용하면 diverge 할 수도 있음\n        - $ \\mu_{\\theta}(s_{t+1}) $ 이 diverge 할 수도 있기 때문으로 판단됨.\n        - 그렇기에 simple Q-learning 대신 다른 기법이 필요함.\n    - 그렇기에 critic 에 gradient Q-learning 사용한 COPDAC-GQ (Gradient Q-learning critic) algorithm 제안\n        - gradient temporal-difference learning 에 기반한 기법들은 true gradient descent algorithm 이며, converge가 보장됨. (Sutton, 2009)\n            - 기본 아이디어는 stochastic gradient descent 로 Mean-squared projected Bellman error (MSPBE) 를 최소화하는 것\n            - critic 이 actor 보다 빠른 time-scale 로 update 되도록 step size 들을 잘 조절하면, critic 은 MSPBE 를 최소화하는 parameters 로 converge 하게 됨\n            - critic 에 gradient temporal-difference learning 의 일종인 gradient Q-learning 사용한 논문 (Maei, 2010)\n    - COPDAC-GQ algorithm\n        - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $\n        - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $\n        - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) - \\alpha_{w}\\gamma\\phi(s_{t+1}, \\mu_{\\theta}(s_{t+1}))(\\phi(s_{t},a_{t})^{\\top} u_{t}) $\n        - $ v_{t+1} = v_{t} + \\alpha_{v}\\delta_{t}\\phi(s_{t}) - \\alpha_{v}\\gamma\\phi(s_{t+1})(\\phi(s_{t},a_{t})^{\\top} u_{t}) $\n        - $ u_{t+1} = u_{t} + \\alpha_{u}(\\delta_{t}-\\phi(s_{t}, a_{t})^{\\top} u_{t})\\phi(s_{t}, a_{t}) $\n    - stochastic actor-critic 과 같이 매 time-step 마다 update 시 필요한 계산의 복잡도는 $ O(mn) $\n        - m 은 action dimensions, n 은 number of policy parameters\n    - Natural policy gradient 를 이용해 deterministic policies 를 찾을 수 있음\n        - $ M(\\theta)^{-1}\\nabla_{\\theta}J(\\mu_{\\theta}) $ 는 any metric $ M(\\theta) $ 에 대한 our performance objective (식(14)) 의 steepest ascent direction 임 (Toussaint, 2012)\n        - Natural gradient 는 Fisher information metric $ M_{\\pi}(\\theta) $ 에 대한 steepest ascent direction 임\n            -  $ M_{\\pi}(\\theta) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)^{\\top}] $\n            - Fisher information metric 은 policy reparameterization 에 대해 불변임 (Bagnell, 2003)\n        - deterministic policies 에 대해 metric 으로 $ M_{\\mu}(\\theta) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}] $ 을 사용.\n        \t- 이는 variance 가 0 인 policy 에 대한 Fisher information metric 으로 볼 수 있음\n        \t- $ \\frac{\\nabla_{\\theta}\\pi_{\\theta}(a\\vert s)}{\\pi_{\\theta}(a\\vert s)}$ 에서 policy variance 가 0 이면, 특정 s 에 대한 $ \\pi_{\\theta}(a|s)$ 만 1 이 되고, 나머지는 0 임\n        - deterministic policy gradient theorem 과 compatible function approximation 을 결합하면 $ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] $ 이 됨\n            - $ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $\n            - $ \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)} \\approx \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $\n        - 그렇기에 steepest ascent direction 은 $ M_{\\mu}(\\theta)^{-1}\\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = w $ 이 됨\n            - $ E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}]^{-1}E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] = w $\n        - 이 알고리즘은 COPDAC-Q 혹은 COPDAC-GQ 에서 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $ 식을 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta}w_{t} $ 로 바꿔주기만 하면 됨\n\n## Experiments\n### Continuous Bandit\n- Stochastic Actor-Critic (SAC)과 COPDAC 간 성능 비교 수행\n    - Action dimension이 커질수록 성능 차이가 심함\n    - 빠르게 수렴하는 것을 통해 DPG의 data efficiency가 SPG에 비해 좋다는 것을 확인할 수 있지만, 반면, time-step이 증가할수록 SAC와 COPDAC 간 성능 차이가 줄어드는 것을 통해 성능 차이가 심하다는 것은 일정 time step 내에서만 해당하는 것이라고 유추해볼 수 있음\n    - <img src=\"https://www.dropbox.com/s/hrkyq0s2f24z66r/Screenshot%202018-06-16%2017.47.38.png?dl=1\">\n\n### Continuous Reinforcement Learning\n- COPDAC-Q가 SAC와 off-policy stochastic actor-critic(OffPAC-TD) 간 성능 비교 수행\n    - COPDAC-Q의 성능이 약간 더 좋음\n    - COPDAC-Q의 학습이 더 빨리 이뤄짐\n    - <img src=\"https://www.dropbox.com/s/qdca4augapmzsxi/Screenshot%202018-06-16%2017.47.07.png?dl=1\">\n### Octopus Arm\n- 목표: 6 segments octopus arm (20 action dimensions & 50 state dimensions)을 control하여 target을 맞추는 것\n    - COPDAC-Q 사용 시, action space dimension이 큰 octopus arm을 잘 control하여 target을 맞춤\n    - <img src=\"https://www.dropbox.com/s/xrxb0a52wntekld/Screenshot%202018-06-16%2017.46.28.png?dl=1\" width=600px>\n    - 기존 기법들은 action spaces 혹은 action 과 state spaces 둘 다 작은 경우들에 대해서만 실험했다고 하며, 비교하고 있지 않음.\n        - 기존 기법들이 6 segments octopus arm에서 동작을 잘 안 했을 것 같긴한데, 그래도 실험해서 결과를 보여주지...왜 안 했을까?\n    - 8 segment arm 동영상이 저자 홈페이지에 있다고 하는데, 안 보임.\n- [참고] Octopus Arm 이란?\n    - [OctopusArm Youtube Link](https://www.youtube.com/watch?v=AxeeHif0euY)\n    - <img src=\"https://www.dropbox.com/s/950ycj06sudakjx/Screenshot%202018-06-16%2017.45.52.png?dl=1\">\n","source":"_posts/dpg.md","raw":"---\ntitle: Deterministic Policy Gradient Algorithms\ndate: 2018-06-16 17:21:48\ntags: [\"프로젝트\", \"피지여행\"]\ncategories: 프로젝트\nauthor: 이웅원\nsubtitle: 피지여행 2번째 논문\n---\n\nAuthors: David Silver<sup>1)</sup>, Guy Lever<sup>2)</sup>, Nicloas Heess<sup>1)</sup>, Thomas Degris<sup>1)</sup>, Daan Wierstra<sup>1)</sup>, Martin Riedmiller<sup>1)</sup>\nAffiliation: 1) Google Deepmind, 2) University College London (UCL)\nProceeding: International Conference on Machine Learning (ICML) 2014\n\n* [Deterministic Policy Gradient Algorithms](#deterministic-policy-gradient-algorithms)\n\t* [Summary](#summary)\n\t* [Background](#background)\n\t\t* [Performance objective function](#performance-objective-function)\n\t\t* [SPG Theorem](#spg-theorem)\n\t\t* [Stochastic Actor-Critic Algorithms](#stochastic-actor-critic-algorithms)\n\t\t* [Off-policy Actor-Critic](#off-policy-actor-critic)\n\t* [Gradient of Deterministic Policies](#gradient-of-deterministic-policies)\n\t\t* [Regulariy Conditions](#regulariy-conditions)\n\t\t* [Deterministic Policy Gradient Theorem](#deterministic-policy-gradient-theorem)\n\t\t* [DPG 형태에 대한 informal intuition](#dpg-형태에-대한-informal-intuition)\n\t\t* [DPG 는 SPG 의 limiting case 임](#dpg-는-spg-의-limiting-case-임)\n\t* [Deterministic Actor-Critic Algorithms](#deterministic-actor-critic-algorithms)\n\t* [Experiments](#experiments)\n\t\t* [Continuous Bandit](#continuous-bandit)\n\t\t* [Continuous Reinforcement Learning](#continuous-reinforcement-learning)\n\t\t* [Octopus Arm](#octopus-arm)\n\n<!-- /code_chunk_output -->\n\n<br>\n\n---\n## 1. Summary\n- Deterministic Policy Gradient (DPG) Theorem 제안함 [[Theorem 1](#deterministic-policy-gradient-theorem)]\n    1) DPG는 존재하며,\n    2) DPG는 Expected gradient of the action-value function의 형태를 띈다.\n- Policy variance가 0에 수렴할 경우, DPG는 Stochastic Policy Gradient(SPG)와 동일해짐을 보임 [[Theorem 2](#dpg-는-spg-의-limiting-case-임)]\n    - Theorem 2 로 인해 기존 Policy Gradient (PG) 와 관련된 기법들을 DPG 에 적용할 수 있게 됨\n        - 예. Sutton PG, natural gradients, actor-critic, episodic/batch methods\n- 적절한 exploration 을 위해 model-free, off-policy Actor-Critic algorithm 을 제안함\n    - Action-value function approximator 사용으로 인해 policy gradient가 bias되는 것을 방지하기 위해 compatibility conditions을 제공 [[Theorem 3](##deterministic-actor-critic-algorithms)]\n- DPG 는 SPG 보다 성능이 좋음\n    - 특히 high dimensional action spaces 을 가지는 tasks에서의 성능 향상이 큼\n        - SPG의 policy gradient는 state와 action spaces 모두에 대해서, DPG의 policy gradient는  state spaces에 대해서만 평균을 취함\n        - 결과적으로, action spaces의 dimension이 커질수록 data efficiency가 높은 DPG의 학습이 더 잘 이뤄지게 됨\n        - 무한정 학습을 시키게 되면, SPG도 최적으로 수렴할 것으로 예상되기에 위 성능 비교는 일정 iteration 내로 한정함\n    - 기존 기법들에 비해 computation 양이 많지 않음\n        - Computation 은 action dimensionality 와 policy parameters 수에 비례함\n<br>\n\n---\n## 2. Background\n### 2.1 Performance objective function\n\n$$\n\\begin{align}\nJ(\\pi_{\\theta}) &= \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\pi_{\\theta}(s,a)r(s,a)dads = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[r(s,a)]\n\\end{align}\n$$\n\n### 2.2 SPG Theorem\n- State distribution $ \\rho^{\\pi}(s) $ 은 policy parameters에 영향을 받지만, policy gradient 를 계산할 때는 state distribution 의 gradient 를 고려할 필요가 없다.\n- $$\\begin{eqnarray}\\nabla_{\\theta}J(\\pi_{\\theta}) &=& \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\\\ &=& E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]\n\\end{eqnarray}$$\n\n### 2.3 Stochastic Actor-Critic Algorithms\n- Actor 와 Critic 이 번갈아가면서 동작하며 stochastic policy 를 최적화하는 기법\n- Actor: $ Q^{\\pi}(s,a) $ 를 근사한 $ Q^w(s,a) $ 를 이용해 stochastic policy gradient 를 ascent 하는 방향으로 policy parameter $ \\theta $ 를 업데이트함으로써 stochastic policy 를 발전시킴\n    - $ \\nabla_{\\theta}J(\\pi_{\\theta}) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}log\\pi_{\\theta}(a|s)Q^{w}(s,a)] $\n- Critic: SARSA 나 Q-learning 같은 Temporal-difference (TD) learning 을 이용해 action-value function의 parameter, $ w $ 를 업데이트함으로써 $ Q^w(s,a) $ 가 $ Q^{\\pi}(s,a) $ 과 유사해지도록 함\n- 실제 값인 $ Q^{\\pi}(s,a) $ 대신 이를 근사한 $ Q^w(s,a) $ 를 사용하게 되면, 일반적으로 bias 가 발생하게 된다. 하지만, compatible condition 에 부합하는 $ Q^w(s,a) $ 를 사용하게 되면, bias 가 발생하지 않는다.\n\n### 2.4 Off-policy Actor-Critic\n- Distinct behavior policy $ \\beta(a|s) ( \\neq \\pi_{\\theta}(a|s) ) $ 로부터 샘플링된 trajectories 를 이용한 Actor-Critic\n- Performance objective function\n    - $\\begin{eqnarray}\n        J_{\\beta}(\\pi_{\\theta}) \n        &=& \\int_{S}\\rho^{\\beta}(s)V^{\\pi}(s)ds \\nonumber \\\\\\\\\n        &=& \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads\n        \\end{eqnarray} $\n- off-policy policy gradient\n    - $ \\begin{eqnarray}\n        \\nabla_{\\theta}J_{\\beta}(\\pi_{\\theta}) &\\approx& \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\end{eqnarray} $\n      $=E_{s \\sim \\rho^{\\beta}, a \\sim \\beta}[\\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)}\\nabla_{\\theta}log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]$\n    - off-policy policy gradient 식에서의 물결 표시는 [Degris, 2012b] 논문에 근거함\n        - [Degris, 2012b] \"Linear off-policy actor-critic,\" ICML 2012\n        - Exact off-policy policy gradient 와 이를 approximate 한 policy gradient 는 아래와 같음. (빨간색 상자에 있는 항목을 삭제함으로써 근사함)\n            - <img src=\"https://www.dropbox.com/s/xzpv3okc139c1fs/Screenshot%202018-06-16%2017.48.51.png?dl=1\" width=500px>\n        - [Degris, 2012b] Theorem 1 에 의해 policy parameter 가 approximated policy gradient ( $\\nabla_{u}𝑄^{\\pi,\\gamma}(𝑠,𝑎)$ term 제거)에 따라 업데이트되어도 policy 는 improve 가 됨이 보장되기에 exact off-policy policy gradient 대신 approximated off-policy policy gradient 를 사용해도 괜찮음.\n            - <img src=\"https://www.dropbox.com/s/mk13931r4scjngo/Screenshot%202018-06-16%2017.49.24.png?dl=1\" width=500px>\n    - off-policy policy gradient 식에서 $ \\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)} $ 는 importance sampling ratio 임\n        - off-policy actor-critic 에서는 $ \\beta $ 에 의해 샘플링된 trajectory 를 이용해서 stochastic policy $ \\pi $ 를 예측하는 것이기 때문에 imnportance sampling 이 필요함\n<br>\n\n---\n## 3. Gradient of Deterministic Policies\n### 3.1 Regulariy Conditions\n- 어떠한 이론이 성립하기 위한 전제 조건\n- Regularity conditions A.1\n    - $ p(s'|s,a), \\nabla_{a}p(s'|s,a), \\mu_{\\theta}(s), \\nabla_{\\theta}\\mu_{\\theta}(s), r(s,a), \\nabla_{a}r(s,a), p_{1}(s) $ are continuous in all parameters and variables $ s, a, s' $ and $ x $.\n- regularity conditions A.2\n    - There exists a $ b $ and $ L $ such that $ \\sup_{s}p_{1}(s) < b $, $ \\sup_{a,s,s'}p(s'|s,a) < b $, $ \\sup_{a,s}r(s,a) < b $, $ \\sup_{a,s,s'}\\|\\nabla_{a}p(s'|s,a)\\| < L $, and $ \\sup_{a,s}\\|\\nabla_{a}r(s,a)\\| < L $.\n\n### 3.2 Deterministic Policy Gradient Theorem\n- Deterministic policy\n    - $ \\mu_{\\theta} : S \\to A $ with parameter vector $ \\theta \\in \\rm I \\! R^n $\n- Probability distribution\n    - $ p(s \\to s', t, \\mu) $\n- Discounted state distribution\n    - $ \\rho^{\\mu}(s) $\n- Performance objective\n\n$$\nJ(\\mu_{\\theta}) = E[r^{\\gamma}_{1} | \\mu] \n$$\n\n$$\n= \\int_{S}\\rho^{\\mu}(s)r(s,\\mu_{\\theta}(s))ds \n= E_{s \\sim \\rho^{\\mu}}[r(s,\\mu_{\\theta}(s))]\n$$\n\n- DPG Theorem\n    - MDP 가 A.1 만족한다면, 아래 식(9)이 성립함\n    $\\nabla_{\\theta}J(\\mu_{\\theta}) = \\int_{S}\\rho^{\\mu}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}ds \\nonumber$\n    $= E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]   \\nonumber $ (9)\n    \n\t- DPG는 State space 에 대해서만 평균을 취하면 되기에, State와 Action space 모두에 대해 평균을 취해야 하는 [SPG](#spg-theorem)에 비해 data efficiency가 좋다. 즉, 더 적은 양의 데이터로도 학습이 잘 이뤄지게 된다.\n\n    \n### 3.3 DPG 형태에 대한 informal intuition\n- Generalized policy iteration\n    - 정책 평가와 정책 발전을 한 번 씩 번갈아 가면서 실행하는 정책 iteration\n        - 위와 같이 해도 정책 평가에서 예측한 가치함수가 최적 가치함수에 수렴함\n- 정책 평가\n    - action-value function $ Q^{\\pi}(s,a) $ or $ Q^{\\mu}(s,a) $ 을 estimate 하는 것\n- 정책 발전\n    - 위 estimated action-value function 에 따라 정책을 update 하는 것\n    - 주로 action-value function 에 대한 greedy maximisation 사용함\n        - $ \\mu^{k+1}(s) = \\arg\\max\\limits_{a}Q^{\\mu^{k}}(s,a) $\n        - greedy 정책 발전은 매 단계마다 global maximization을 해야하는데, 이는 continuous action spaces 에서 계산량이 급격히 늘어나게 됨.\n    - 그렇기에 policy gradient 방법이 나옴\n        - policy 를 $ \\theta $ 에 대해서 parameterize 함\n        - 매 단계마다 global maximisation 수행하는 대신, 방문하는 state $ s $ 마다 policy parameter 를 action-value function $ Q $ 의 $ \\theta $ 에 대한 gradient $ \\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s)) $ 방향으로 proportional 하게 update 함\n        - 하지만 각 state 는 다른 방향을 제시할 수 있기에, state distribution $ \\rho^{\\mu}(s) $ 에 대한 기대값을 취해 policy parameter 를 update 할 수도 있음\n            - $ \\theta^{k+1} = \\theta^{k} + \\alpha \\rm I\\!E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s))] $\n        - 이는 chain-rule 에 따라 아래와 같이 분리될 수 있음\n            - $ \\theta^{k+1} = \\theta^{k} + \\alpha \\rm I\\!E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu^{k}}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $ (7)\n            - chain rule: $ \\frac{\\partial Q}{\\partial \\theta} = \\frac{\\partial a}{\\partial \\theta} \\frac{\\partial Q}{\\partial a} $\n        - 하지만 state distribution $ \\rho^{\\mu} $ 은 정책에 dependent 함\n            - 정책이 바꾸게 되면, 바뀐 정책에 따라 방문하게 되는 state 가 변하게 되기 때문에 state distribution이 변하게 됨\n        - 그렇기에 정책 update 시 state distribution에 대한 gradient 를 고려하지 않는데 정책 발전이 이뤄진다는 것은 직관적으로 와닿지 않을 수 있음\n        - deterministic policy gradient theorem 은 state distribution에 대한 gradient 계산없이 위 식(7) 대로만 update 해도 performance objective 의 gradient 를 정확하게 따름을 의미한다.\n\n\n### 3.4 DPG 는 SPG 의 limiting case 임\n- stochastic policy parameterization\n    - $ \\pi_{\\mu_{\\theta},\\sigma} $ by a deterministic policy $ \\mu_{\\theta} : S \\to A $ and a variance parameter $ \\sigma $\n    - $ \\sigma = 0 $ 이면, $ \\pi_{\\mu_{\\theta},\\sigma} \\equiv \\mu_{\\theta} $\n- Theorem 2. Policy 의 variance 가 0 에 수렴하면, 즉, $ \\sigma \\to 0 $, stochastic policy gradient 와 deterministic policy gradient 는 동일해짐\n    - 조건 : stochastic policy $ \\pi_{\\mu_{\\theta},\\sigma} = \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $\n        - $ \\sigma $ 는 variance\n        - $ \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $ 는 conditions B.1 만족\n        - MDP 는 conditions A.1 및 A.2 만족\n    - 결과 :\n        - $ \\lim\\limits_{\\sigma\\downarrow0}\\nabla_{\\theta}J(\\pi_{\\mu_{\\theta},\\sigma}) = \\nabla_{\\theta}J(\\mu_{\\theta})  $\n            - 좌변은 standard stochastic gradient 이며, 우변은 deterministic gradient.\n    - 의미 :\n        - deterministic policy gradient 는 stochastic policy gradient 의 특수 case 임\n        - 기존 유명한 policy gradients 기법들에 deterministic policy gradients 를 적용할 수 있음\n            - 기존 기법들 예: compatible function approximation (Sutton, 1999), natural gradients (Kakade, 2001), actor-critic (Bhatnagar, 2007) or episodic/batch methods (Peters, 2005)\n<br>\n \n---\n## 4. Deterministic Actor-Critic Algorithms\n1. 살사 critic 을 이용한 on-policy actor-critic\n    - 단점\n        - deterministic policy 에 의해 행동하면 exploration 이 잘 되지 않기에, sub-optimal 에 빠지기 쉬움\n    - 목적\n        - 교훈/정보제공\n        - 환경에서 충분한 noise 를 제공하여 exploration을 시킬 수 있다면, deterministic policy 를 사용한다고 하여도 좋은 학습 결과를 얻을 수도 있음$\n            - 예. 바람이 agent의 행동에 영향(noise)을 줌\n    - Remind: 살사(SARSA) update rule\n        - $ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma Q(s_{t+1},a_{t+1}) - Q(s_{t},a_{t})) $\n    - Algorithm\n        - Critic 은 MSE 를 $ \\bf minimize $ 하는 방향, 즉, action-value function 을 stochastic gradient $ \\bf descent $ 방법으로 update 함\n            - $ MSE = [Q^{\\mu}(s,a) - Q^{w}(s,a)]^2 $\n                - critic 은 실제 $ Q^{\\mu}(s,a) $ 대신 미분 가능한 $ Q^{w}(s,a) $ 로 대체하여 action-value function 을 estimate 하며, 이 둘 간 Mean Square Error 를 최소화하는 것이 목표\n            - $ \\nabla_{w}MSE \\approx -2 * [r + \\gamma Q^{w}(s',a') - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $\n                - $ \\nabla_{w}MSE = -2 * [Q^{\\mu}(s,a) - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $\n                - $ Q^{\\mu}(s,a) $ 를 $ r + \\gamma Q^{w}(s',a') $ 로 대체\n                    - $ Q^{\\mu}(s,a) = r + \\gamma Q^{\\mu}(s',a') $\n            - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $\n                - $w_{t+1} = w_{t} - \\alpha_{w}\\nabla_{w}MSE  \\nonumber$\n                $ \\approx w_{t} - \\alpha_{w} * (-2 * [r + \\gamma Q^{w}(s',a') - Q^{w}(s,a)] \\nabla_{w}Q^{w}(s,a)$\n                - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},a_{t+1}) - Q^{w}(s_{t},a_{t}) $\n        - Actor 는 식(9)에 따라 보상이 $ \\bf maximize $ 되는 방향, 즉, deterministic policy 를 stochastic gradient $ \\bf ascent $ 방법으로 update함\n            - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $\n2. Q-learning 을 이용한 off-policy actor-critic\n    - stochastic behavior policy $ \\beta(a|s) $ 에 의해 생성된 trajectories 로부터 deterministic target policy $ \\mu_{\\theta}(s) $ 를 학습하는 off-policy actor-critic\n    - performance objective\n        - $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)V^{\\mu}(s)ds \\nonumber \\\\\\\\$\n          $= \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds \\nonumber \\\\\\\\$\n          $= E_{s \\sim \\rho^{\\beta}}[Q^{\\mu}(s,\\mu_{\\theta}(s))]$\n    - off-policy deterministic policy gradient\n        - $ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $\n            - 논문에는 아래와 같이 나와있는데, 물결 표시 부분은 오류로 판단됨.\n            - $ \\begin{eqnarray}\n                \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) &\\approx& \\int_{S}\\rho^{\\beta}(s)\\nabla_{\\theta}\\mu_{\\theta}(a|s)Q^{\\mu}(s,a)ds \\nonumber \\\\\n                &=& E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]\n                \\end{eqnarray} $\n            - 근거: Action이 deterministic 하기에 stochastic 경우와는 다르게 performance objective 에서 action 에 대해 평균을 구할 필요가 없음. 그렇기에, 곱의 미분이 있을 필요가 없고, [Degris, 2012b] 에서처럼 곱의 미분을 통해 생기는 action-value function 에 대한 gradient term 를 생략할 필요가 사라짐. [참고](#off-policy-actor-critic)\n    - Remind: 큐러닝(Q-learning) update rule\n        - $ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma \\max\\limits_{a}Q(s_{t+1},a) - Q(s_{t},a_{t})) $\n    - algorithm: OPDAC (Off-Policy Deterministic Actor-Critic)\n        - 살사를 이용한 on-policy deterministic actor-critic 과 아래 부분을 제외하고는 같음\n            - target policy 는 $ \\beta(a|s) $ 에 의해 생성된 trajectories 를 통해 학습함\n            - 업데이트 목표 부분에 실제 행동 값 $ a_{t+1} $ 이 아니라 정책으로부터 나온 행동 값 $ \\mu_{\\theta}(s_{t+1}) $ 사용함\n                - $ \\mu_{\\theta}(s_{t+1}) $ 는 가장 높은 Q 값을 가지는 행동. 즉, Q-learning.\n        - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $\n        - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $\n        - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $\n    - Stochastic off-policy actor-critic 은 대개 actor 와 critic 모두 importance sampling을 필요로 하지만, deterministic policy gradient 에선 importance sampling이 필요없음\n        - Actor 는 deterministic 이기에 sampling 자체가 필요없음\n            - Stochastic policy 인 경우, Actor 에서 importance sampling이 필요한 이유는 상태 $ s $ 에서의 가치 함수 값 $ V^{\\pi}(s) $ 을 estimate 하기 위해 $ \\pi $ 가 아니라 $ \\beta $ 에 따라 sampling을 한 후, 평균을 내기 때문임.\n            - 하지만 Deterministic policy 인 경우, 상태 $ s $ 에서의 가치 함수 값 $ V^{\\pi}(s) = Q^{\\pi}(s,\\mu_{\\theta}) $ 즉, action 이 상태 s 에 대해 deterministic 이기에 sampling 을 통해 estimate 할 필요가 없고, 따라서 importance sampling도 필요없어짐.\n            - stochastic vs. deterministic performance objective\n                - stochastic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads $\n                - deterministic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds $\n        - Critic 이 사용하는 Q-learning 은 importance sampling이 필요없는 off policy 알고리즘임.\n            - Q-learning 도 업데이트 목표를 특정 분포에서 샘플링을 통해 estimate 하는 것이 아니라 Q 함수를 최대화하는 action을 선택하는 것이기에 위 actor 에서의 deterministic 경우와 비슷하게 볼 수 있음\n3. compatible function approximation 및 gradient temporal-difference learning 을 이용한 actor-critic\n    - 위 살사/Q-learning 기반 on/off-policy 는 아래와 같은 문제가 존재\n        - function approximator 에 의한 bias\n            - 일반적으로 $ Q^{\\mu}(s,a) $ 를 $ Q^{w}(s,a) $ 로 대체하여 deterministic policy gradient 를 구하면, 그 gradient 는 ascent 하는 방향이 아닐 수도 있음\n        - off-policy learning 에 의한 instabilities\n    - 그래서 stochastic 처럼 $ \\nabla_{a}Q^{\\mu}(s,a) $ 를 $ \\nabla_{a}Q^{w}(s,a) $ 로 대체해도 deterministic policy gradient 에 영향을 미치지 않을 compatible function approximator $ Q^{w}(s,a) $ 를 찾아야 함\n    - Theorem 3. 아래 두 조건을 만족하면, $ Q^{w}(s,a) $ 는 deterministic policy $ \\mu_{\\theta}(s) $ 와 compatible 함. 즉, $ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $\n        - $ \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $\n        - $ w $ 는 $ MSE(\\theta, w) = E[\\epsilon(s;\\theta,w)^{\\top}\\epsilon(s;\\theta,w)] $ 를 최소화함\n            - $ \\epsilon(s;\\theta,w) = \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} - \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}  $\n    - Theorem 3 은 on-policy 뿐만 아니라 off-policy 에도 적용 가능함\n    - $ Q^{w}(s,a) = (a-\\mu_{\\theta}(s))^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top} w + V^{v}(s) $\n        - 어떠한 deterministic policy 에 대해서도 위 형태와 같은 compatible function approximator 가 존재함.\n        - 앞의 term 은 advantage 를, 뒤의 term 은 value 로 볼 수 있음\n    - $ Q^{w}(s,a) = \\phi(s,a)^{\\top} w + v^{\\top}\\phi(s) $\n        - 정의 : $ \\phi(s,a) \\overset{\\underset{\\mathrm{def}}{}}{=} \\nabla_{\\theta}\\mu_{\\theta}(s)(a-\\mu_{\\theta}(s)) $\n        - 일례 : $ V^{v}(s) = v^{\\top}\\phi(s) $\n        - Theorem 3 만족 여부\n            - 첫 번째 조건 만족.\n            - 두 번째 조건은 대강 만족.\n                - $ \\nabla_{a}Q^{\\mu}(s,a) $ 에 대한 unbiased sample 을 획득하기는 매우 어렵기에, 일반적인 정책 평가 방법들로 $ w $ 학습.\n                - 이 정책 평가 방법들을 이용하면 $ Q^{w}(s,a) \\approx Q^{\\mu}(s,a) $ 인 reasonable solution을 찾을 수 있기에 대강 $ \\nabla_{a}Q^{w}(s,a) \\approx \\nabla_{a}Q^{\\mu}(s,a) $ 이 될 것.\n        - action-value function 에 대한 linear function approximator 는 큰 값을 가지는 actions 에 대해선 diverge할 수 있어 global 하게 action-values 예측하기에는 좋지 않지만, local critic 에 사용할 때는 매우 유용하다.\n            - 즉, 절대값이 아니라 작은 변화량을 다루는 gradient method 경우엔 $ A^{w}(s,\\mu_{\\theta}(s)+\\delta) = \\delta^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $ 로, diverge 하지 않고, 값을 얻을 수 있음\n    - COPDAC-Q algorithm (Compatible Off-Policy Deterministic Actor-Critic Q-learning critic)\n        - Critic: 실제 action-value function 에 대한 linear function approximator 인 $ Q^{w}(s,a) = \\phi(s,a)^{\\top}w $ 를 estimate\n            - $ \\phi(s,a) = a^{\\top}\\nabla_{\\theta}\\mu_{\\theta} $\n            - Behavior policy $ \\beta(a|s) $ 로부터 얻은 samples를 이용하여 Q-learning 이나 gradient Q-learning 과 같은 off-policy algorithm 으로 학습 가능함\n        - Actor: estimated action-value function 에 대한 gradient 를 $ \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $ 로 치환 후, 정책을 업데이트함\n        - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $\n        - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) $\n        - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $\n    - off-policy Q-learning은 linear function approximation을 이용하면 diverge 할 수도 있음\n        - $ \\mu_{\\theta}(s_{t+1}) $ 이 diverge 할 수도 있기 때문으로 판단됨.\n        - 그렇기에 simple Q-learning 대신 다른 기법이 필요함.\n    - 그렇기에 critic 에 gradient Q-learning 사용한 COPDAC-GQ (Gradient Q-learning critic) algorithm 제안\n        - gradient temporal-difference learning 에 기반한 기법들은 true gradient descent algorithm 이며, converge가 보장됨. (Sutton, 2009)\n            - 기본 아이디어는 stochastic gradient descent 로 Mean-squared projected Bellman error (MSPBE) 를 최소화하는 것\n            - critic 이 actor 보다 빠른 time-scale 로 update 되도록 step size 들을 잘 조절하면, critic 은 MSPBE 를 최소화하는 parameters 로 converge 하게 됨\n            - critic 에 gradient temporal-difference learning 의 일종인 gradient Q-learning 사용한 논문 (Maei, 2010)\n    - COPDAC-GQ algorithm\n        - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $\n        - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $\n        - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) - \\alpha_{w}\\gamma\\phi(s_{t+1}, \\mu_{\\theta}(s_{t+1}))(\\phi(s_{t},a_{t})^{\\top} u_{t}) $\n        - $ v_{t+1} = v_{t} + \\alpha_{v}\\delta_{t}\\phi(s_{t}) - \\alpha_{v}\\gamma\\phi(s_{t+1})(\\phi(s_{t},a_{t})^{\\top} u_{t}) $\n        - $ u_{t+1} = u_{t} + \\alpha_{u}(\\delta_{t}-\\phi(s_{t}, a_{t})^{\\top} u_{t})\\phi(s_{t}, a_{t}) $\n    - stochastic actor-critic 과 같이 매 time-step 마다 update 시 필요한 계산의 복잡도는 $ O(mn) $\n        - m 은 action dimensions, n 은 number of policy parameters\n    - Natural policy gradient 를 이용해 deterministic policies 를 찾을 수 있음\n        - $ M(\\theta)^{-1}\\nabla_{\\theta}J(\\mu_{\\theta}) $ 는 any metric $ M(\\theta) $ 에 대한 our performance objective (식(14)) 의 steepest ascent direction 임 (Toussaint, 2012)\n        - Natural gradient 는 Fisher information metric $ M_{\\pi}(\\theta) $ 에 대한 steepest ascent direction 임\n            -  $ M_{\\pi}(\\theta) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)^{\\top}] $\n            - Fisher information metric 은 policy reparameterization 에 대해 불변임 (Bagnell, 2003)\n        - deterministic policies 에 대해 metric 으로 $ M_{\\mu}(\\theta) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}] $ 을 사용.\n        \t- 이는 variance 가 0 인 policy 에 대한 Fisher information metric 으로 볼 수 있음\n        \t- $ \\frac{\\nabla_{\\theta}\\pi_{\\theta}(a\\vert s)}{\\pi_{\\theta}(a\\vert s)}$ 에서 policy variance 가 0 이면, 특정 s 에 대한 $ \\pi_{\\theta}(a|s)$ 만 1 이 되고, 나머지는 0 임\n        - deterministic policy gradient theorem 과 compatible function approximation 을 결합하면 $ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] $ 이 됨\n            - $ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $\n            - $ \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)} \\approx \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $\n        - 그렇기에 steepest ascent direction 은 $ M_{\\mu}(\\theta)^{-1}\\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = w $ 이 됨\n            - $ E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}]^{-1}E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] = w $\n        - 이 알고리즘은 COPDAC-Q 혹은 COPDAC-GQ 에서 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $ 식을 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta}w_{t} $ 로 바꿔주기만 하면 됨\n\n## Experiments\n### Continuous Bandit\n- Stochastic Actor-Critic (SAC)과 COPDAC 간 성능 비교 수행\n    - Action dimension이 커질수록 성능 차이가 심함\n    - 빠르게 수렴하는 것을 통해 DPG의 data efficiency가 SPG에 비해 좋다는 것을 확인할 수 있지만, 반면, time-step이 증가할수록 SAC와 COPDAC 간 성능 차이가 줄어드는 것을 통해 성능 차이가 심하다는 것은 일정 time step 내에서만 해당하는 것이라고 유추해볼 수 있음\n    - <img src=\"https://www.dropbox.com/s/hrkyq0s2f24z66r/Screenshot%202018-06-16%2017.47.38.png?dl=1\">\n\n### Continuous Reinforcement Learning\n- COPDAC-Q가 SAC와 off-policy stochastic actor-critic(OffPAC-TD) 간 성능 비교 수행\n    - COPDAC-Q의 성능이 약간 더 좋음\n    - COPDAC-Q의 학습이 더 빨리 이뤄짐\n    - <img src=\"https://www.dropbox.com/s/qdca4augapmzsxi/Screenshot%202018-06-16%2017.47.07.png?dl=1\">\n### Octopus Arm\n- 목표: 6 segments octopus arm (20 action dimensions & 50 state dimensions)을 control하여 target을 맞추는 것\n    - COPDAC-Q 사용 시, action space dimension이 큰 octopus arm을 잘 control하여 target을 맞춤\n    - <img src=\"https://www.dropbox.com/s/xrxb0a52wntekld/Screenshot%202018-06-16%2017.46.28.png?dl=1\" width=600px>\n    - 기존 기법들은 action spaces 혹은 action 과 state spaces 둘 다 작은 경우들에 대해서만 실험했다고 하며, 비교하고 있지 않음.\n        - 기존 기법들이 6 segments octopus arm에서 동작을 잘 안 했을 것 같긴한데, 그래도 실험해서 결과를 보여주지...왜 안 했을까?\n    - 8 segment arm 동영상이 저자 홈페이지에 있다고 하는데, 안 보임.\n- [참고] Octopus Arm 이란?\n    - [OctopusArm Youtube Link](https://www.youtube.com/watch?v=AxeeHif0euY)\n    - <img src=\"https://www.dropbox.com/s/950ycj06sudakjx/Screenshot%202018-06-16%2017.45.52.png?dl=1\">\n","slug":"dpg","published":1,"updated":"2018-06-16T14:09:54.593Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjiinnj6q0005j28a4kpaxi6x","content":"<p>Authors: David Silver<sup>1)</sup>, Guy Lever<sup>2)</sup>, Nicloas Heess<sup>1)</sup>, Thomas Degris<sup>1)</sup>, Daan Wierstra<sup>1)</sup>, Martin Riedmiller<sup>1)</sup><br>Affiliation: 1) Google Deepmind, 2) University College London (UCL)<br>Proceeding: International Conference on Machine Learning (ICML) 2014</p>\n<ul>\n<li><a href=\"#deterministic-policy-gradient-algorithms\">Deterministic Policy Gradient Algorithms</a><ul>\n<li><a href=\"#summary\">Summary</a></li>\n<li><a href=\"#background\">Background</a><ul>\n<li><a href=\"#performance-objective-function\">Performance objective function</a></li>\n<li><a href=\"#spg-theorem\">SPG Theorem</a></li>\n<li><a href=\"#stochastic-actor-critic-algorithms\">Stochastic Actor-Critic Algorithms</a></li>\n<li><a href=\"#off-policy-actor-critic\">Off-policy Actor-Critic</a></li>\n</ul>\n</li>\n<li><a href=\"#gradient-of-deterministic-policies\">Gradient of Deterministic Policies</a><ul>\n<li><a href=\"#regulariy-conditions\">Regulariy Conditions</a></li>\n<li><a href=\"#deterministic-policy-gradient-theorem\">Deterministic Policy Gradient Theorem</a></li>\n<li><a href=\"#dpg-형태에-대한-informal-intuition\">DPG 형태에 대한 informal intuition</a></li>\n<li><a href=\"#dpg-는-spg-의-limiting-case-임\">DPG 는 SPG 의 limiting case 임</a></li>\n</ul>\n</li>\n<li><a href=\"#deterministic-actor-critic-algorithms\">Deterministic Actor-Critic Algorithms</a></li>\n<li><a href=\"#experiments\">Experiments</a><ul>\n<li><a href=\"#continuous-bandit\">Continuous Bandit</a></li>\n<li><a href=\"#continuous-reinforcement-learning\">Continuous Reinforcement Learning</a></li>\n<li><a href=\"#octopus-arm\">Octopus Arm</a></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<!-- /code_chunk_output -->\n<p><br></p>\n<hr>\n<h2 id=\"1-Summary\"><a href=\"#1-Summary\" class=\"headerlink\" title=\"1. Summary\"></a>1. Summary</h2><ul>\n<li>Deterministic Policy Gradient (DPG) Theorem 제안함 [<a href=\"#deterministic-policy-gradient-theorem\">Theorem 1</a>]<br>  1) DPG는 존재하며,<br>  2) DPG는 Expected gradient of the action-value function의 형태를 띈다.</li>\n<li>Policy variance가 0에 수렴할 경우, DPG는 Stochastic Policy Gradient(SPG)와 동일해짐을 보임 [<a href=\"#dpg-는-spg-의-limiting-case-임\">Theorem 2</a>]<ul>\n<li>Theorem 2 로 인해 기존 Policy Gradient (PG) 와 관련된 기법들을 DPG 에 적용할 수 있게 됨<ul>\n<li>예. Sutton PG, natural gradients, actor-critic, episodic/batch methods</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>적절한 exploration 을 위해 model-free, off-policy Actor-Critic algorithm 을 제안함<ul>\n<li>Action-value function approximator 사용으로 인해 policy gradient가 bias되는 것을 방지하기 위해 compatibility conditions을 제공 [<a href=\"##deterministic-actor-critic-algorithms\">Theorem 3</a>]</li>\n</ul>\n</li>\n<li>DPG 는 SPG 보다 성능이 좋음<ul>\n<li>특히 high dimensional action spaces 을 가지는 tasks에서의 성능 향상이 큼<ul>\n<li>SPG의 policy gradient는 state와 action spaces 모두에 대해서, DPG의 policy gradient는  state spaces에 대해서만 평균을 취함</li>\n<li>결과적으로, action spaces의 dimension이 커질수록 data efficiency가 높은 DPG의 학습이 더 잘 이뤄지게 됨</li>\n<li>무한정 학습을 시키게 되면, SPG도 최적으로 수렴할 것으로 예상되기에 위 성능 비교는 일정 iteration 내로 한정함</li>\n</ul>\n</li>\n<li>기존 기법들에 비해 computation 양이 많지 않음<ul>\n<li>Computation 은 action dimensionality 와 policy parameters 수에 비례함<br><br></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2 id=\"2-Background\"><a href=\"#2-Background\" class=\"headerlink\" title=\"2. Background\"></a>2. Background</h2><h3 id=\"2-1-Performance-objective-function\"><a href=\"#2-1-Performance-objective-function\" class=\"headerlink\" title=\"2.1 Performance objective function\"></a>2.1 Performance objective function</h3><p>$$<br>\\begin{align}<br>J(\\pi_{\\theta}) &amp;= \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\pi_{\\theta}(s,a)r(s,a)dads = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[r(s,a)]<br>\\end{align}<br>$$</p>\n<h3 id=\"2-2-SPG-Theorem\"><a href=\"#2-2-SPG-Theorem\" class=\"headerlink\" title=\"2.2 SPG Theorem\"></a>2.2 SPG Theorem</h3><ul>\n<li>State distribution $ \\rho^{\\pi}(s) $ 은 policy parameters에 영향을 받지만, policy gradient 를 계산할 때는 state distribution 의 gradient 를 고려할 필요가 없다.</li>\n<li>$$\\begin{eqnarray}\\nabla_{\\theta}J(\\pi_{\\theta}) &amp;=&amp; \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\ &amp;=&amp; E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]<br>\\end{eqnarray}$$</li>\n</ul>\n<h3 id=\"2-3-Stochastic-Actor-Critic-Algorithms\"><a href=\"#2-3-Stochastic-Actor-Critic-Algorithms\" class=\"headerlink\" title=\"2.3 Stochastic Actor-Critic Algorithms\"></a>2.3 Stochastic Actor-Critic Algorithms</h3><ul>\n<li>Actor 와 Critic 이 번갈아가면서 동작하며 stochastic policy 를 최적화하는 기법</li>\n<li>Actor: $ Q^{\\pi}(s,a) $ 를 근사한 $ Q^w(s,a) $ 를 이용해 stochastic policy gradient 를 ascent 하는 방향으로 policy parameter $ \\theta $ 를 업데이트함으로써 stochastic policy 를 발전시킴<ul>\n<li>$ \\nabla_{\\theta}J(\\pi_{\\theta}) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}log\\pi_{\\theta}(a|s)Q^{w}(s,a)] $</li>\n</ul>\n</li>\n<li>Critic: SARSA 나 Q-learning 같은 Temporal-difference (TD) learning 을 이용해 action-value function의 parameter, $ w $ 를 업데이트함으로써 $ Q^w(s,a) $ 가 $ Q^{\\pi}(s,a) $ 과 유사해지도록 함</li>\n<li>실제 값인 $ Q^{\\pi}(s,a) $ 대신 이를 근사한 $ Q^w(s,a) $ 를 사용하게 되면, 일반적으로 bias 가 발생하게 된다. 하지만, compatible condition 에 부합하는 $ Q^w(s,a) $ 를 사용하게 되면, bias 가 발생하지 않는다.</li>\n</ul>\n<h3 id=\"2-4-Off-policy-Actor-Critic\"><a href=\"#2-4-Off-policy-Actor-Critic\" class=\"headerlink\" title=\"2.4 Off-policy Actor-Critic\"></a>2.4 Off-policy Actor-Critic</h3><ul>\n<li>Distinct behavior policy $ \\beta(a|s) ( \\neq \\pi_{\\theta}(a|s) ) $ 로부터 샘플링된 trajectories 를 이용한 Actor-Critic</li>\n<li>Performance objective function<ul>\n<li>$\\begin{eqnarray}<br>  J_{\\beta}(\\pi_{\\theta})<br>  &amp;=&amp; \\int_{S}\\rho^{\\beta}(s)V^{\\pi}(s)ds \\nonumber \\\\<br>  &amp;=&amp; \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads<br>  \\end{eqnarray} $</li>\n</ul>\n</li>\n<li>off-policy policy gradient<ul>\n<li>$ \\begin{eqnarray}<br>  \\nabla_{\\theta}J_{\\beta}(\\pi_{\\theta}) &amp;\\approx&amp; \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\end{eqnarray} $<br>$=E_{s \\sim \\rho^{\\beta}, a \\sim \\beta}[\\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)}\\nabla_{\\theta}log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]$</li>\n<li>off-policy policy gradient 식에서의 물결 표시는 [Degris, 2012b] 논문에 근거함<ul>\n<li>[Degris, 2012b] “Linear off-policy actor-critic,” ICML 2012</li>\n<li>Exact off-policy policy gradient 와 이를 approximate 한 policy gradient 는 아래와 같음. (빨간색 상자에 있는 항목을 삭제함으로써 근사함)<ul>\n<li><img src=\"https://www.dropbox.com/s/xzpv3okc139c1fs/Screenshot%202018-06-16%2017.48.51.png?dl=1\" width=\"500px\"></li>\n</ul>\n</li>\n<li>[Degris, 2012b] Theorem 1 에 의해 policy parameter 가 approximated policy gradient ( $\\nabla_{u}𝑄^{\\pi,\\gamma}(𝑠,𝑎)$ term 제거)에 따라 업데이트되어도 policy 는 improve 가 됨이 보장되기에 exact off-policy policy gradient 대신 approximated off-policy policy gradient 를 사용해도 괜찮음.<ul>\n<li><img src=\"https://www.dropbox.com/s/mk13931r4scjngo/Screenshot%202018-06-16%2017.49.24.png?dl=1\" width=\"500px\"></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>off-policy policy gradient 식에서 $ \\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)} $ 는 importance sampling ratio 임<ul>\n<li>off-policy actor-critic 에서는 $ \\beta $ 에 의해 샘플링된 trajectory 를 이용해서 stochastic policy $ \\pi $ 를 예측하는 것이기 때문에 imnportance sampling 이 필요함<br><br></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2 id=\"3-Gradient-of-Deterministic-Policies\"><a href=\"#3-Gradient-of-Deterministic-Policies\" class=\"headerlink\" title=\"3. Gradient of Deterministic Policies\"></a>3. Gradient of Deterministic Policies</h2><h3 id=\"3-1-Regulariy-Conditions\"><a href=\"#3-1-Regulariy-Conditions\" class=\"headerlink\" title=\"3.1 Regulariy Conditions\"></a>3.1 Regulariy Conditions</h3><ul>\n<li>어떠한 이론이 성립하기 위한 전제 조건</li>\n<li>Regularity conditions A.1<ul>\n<li>$ p(s’|s,a), \\nabla_{a}p(s’|s,a), \\mu_{\\theta}(s), \\nabla_{\\theta}\\mu_{\\theta}(s), r(s,a), \\nabla_{a}r(s,a), p_{1}(s) $ are continuous in all parameters and variables $ s, a, s’ $ and $ x $.</li>\n</ul>\n</li>\n<li>regularity conditions A.2<ul>\n<li>There exists a $ b $ and $ L $ such that $ \\sup_{s}p_{1}(s) &lt; b $, $ \\sup_{a,s,s’}p(s’|s,a) &lt; b $, $ \\sup_{a,s}r(s,a) &lt; b $, $ \\sup_{a,s,s’}|\\nabla_{a}p(s’|s,a)| &lt; L $, and $ \\sup_{a,s}|\\nabla_{a}r(s,a)| &lt; L $.</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"3-2-Deterministic-Policy-Gradient-Theorem\"><a href=\"#3-2-Deterministic-Policy-Gradient-Theorem\" class=\"headerlink\" title=\"3.2 Deterministic Policy Gradient Theorem\"></a>3.2 Deterministic Policy Gradient Theorem</h3><ul>\n<li>Deterministic policy<ul>\n<li>$ \\mu_{\\theta} : S \\to A $ with parameter vector $ \\theta \\in \\rm I ! R^n $</li>\n</ul>\n</li>\n<li>Probability distribution<ul>\n<li>$ p(s \\to s’, t, \\mu) $</li>\n</ul>\n</li>\n<li>Discounted state distribution<ul>\n<li>$ \\rho^{\\mu}(s) $</li>\n</ul>\n</li>\n<li>Performance objective</li>\n</ul>\n<p>$$<br>J(\\mu_{\\theta}) = E[r^{\\gamma}_{1} | \\mu]<br>$$</p>\n<p>$$<br>= \\int_{S}\\rho^{\\mu}(s)r(s,\\mu_{\\theta}(s))ds<br>= E_{s \\sim \\rho^{\\mu}}[r(s,\\mu_{\\theta}(s))]<br>$$</p>\n<ul>\n<li><p>DPG Theorem</p>\n<ul>\n<li><p>MDP 가 A.1 만족한다면, 아래 식(9)이 성립함<br>$\\nabla_{\\theta}J(\\mu_{\\theta}) = \\int_{S}\\rho^{\\mu}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}ds \\nonumber$<br>$= E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]   \\nonumber $ (9)</p>\n</li>\n<li><p>DPG는 State space 에 대해서만 평균을 취하면 되기에, State와 Action space 모두에 대해 평균을 취해야 하는 <a href=\"#spg-theorem\">SPG</a>에 비해 data efficiency가 좋다. 즉, 더 적은 양의 데이터로도 학습이 잘 이뤄지게 된다.</p>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"3-3-DPG-형태에-대한-informal-intuition\"><a href=\"#3-3-DPG-형태에-대한-informal-intuition\" class=\"headerlink\" title=\"3.3 DPG 형태에 대한 informal intuition\"></a>3.3 DPG 형태에 대한 informal intuition</h3><ul>\n<li>Generalized policy iteration<ul>\n<li>정책 평가와 정책 발전을 한 번 씩 번갈아 가면서 실행하는 정책 iteration<ul>\n<li>위와 같이 해도 정책 평가에서 예측한 가치함수가 최적 가치함수에 수렴함</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>정책 평가<ul>\n<li>action-value function $ Q^{\\pi}(s,a) $ or $ Q^{\\mu}(s,a) $ 을 estimate 하는 것</li>\n</ul>\n</li>\n<li>정책 발전<ul>\n<li>위 estimated action-value function 에 따라 정책을 update 하는 것</li>\n<li>주로 action-value function 에 대한 greedy maximisation 사용함<ul>\n<li>$ \\mu^{k+1}(s) = \\arg\\max\\limits_{a}Q^{\\mu^{k}}(s,a) $</li>\n<li>greedy 정책 발전은 매 단계마다 global maximization을 해야하는데, 이는 continuous action spaces 에서 계산량이 급격히 늘어나게 됨.</li>\n</ul>\n</li>\n<li>그렇기에 policy gradient 방법이 나옴<ul>\n<li>policy 를 $ \\theta $ 에 대해서 parameterize 함</li>\n<li>매 단계마다 global maximisation 수행하는 대신, 방문하는 state $ s $ 마다 policy parameter 를 action-value function $ Q $ 의 $ \\theta $ 에 대한 gradient $ \\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s)) $ 방향으로 proportional 하게 update 함</li>\n<li>하지만 각 state 는 다른 방향을 제시할 수 있기에, state distribution $ \\rho^{\\mu}(s) $ 에 대한 기대값을 취해 policy parameter 를 update 할 수도 있음<ul>\n<li>$ \\theta^{k+1} = \\theta^{k} + \\alpha \\rm I!E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s))] $</li>\n</ul>\n</li>\n<li>이는 chain-rule 에 따라 아래와 같이 분리될 수 있음<ul>\n<li>$ \\theta^{k+1} = \\theta^{k} + \\alpha \\rm I!E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu^{k}}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $ (7)</li>\n<li>chain rule: $ \\frac{\\partial Q}{\\partial \\theta} = \\frac{\\partial a}{\\partial \\theta} \\frac{\\partial Q}{\\partial a} $</li>\n</ul>\n</li>\n<li>하지만 state distribution $ \\rho^{\\mu} $ 은 정책에 dependent 함<ul>\n<li>정책이 바꾸게 되면, 바뀐 정책에 따라 방문하게 되는 state 가 변하게 되기 때문에 state distribution이 변하게 됨</li>\n</ul>\n</li>\n<li>그렇기에 정책 update 시 state distribution에 대한 gradient 를 고려하지 않는데 정책 발전이 이뤄진다는 것은 직관적으로 와닿지 않을 수 있음</li>\n<li>deterministic policy gradient theorem 은 state distribution에 대한 gradient 계산없이 위 식(7) 대로만 update 해도 performance objective 의 gradient 를 정확하게 따름을 의미한다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"3-4-DPG-는-SPG-의-limiting-case-임\"><a href=\"#3-4-DPG-는-SPG-의-limiting-case-임\" class=\"headerlink\" title=\"3.4 DPG 는 SPG 의 limiting case 임\"></a>3.4 DPG 는 SPG 의 limiting case 임</h3><ul>\n<li>stochastic policy parameterization<ul>\n<li>$ \\pi_{\\mu_{\\theta},\\sigma} $ by a deterministic policy $ \\mu_{\\theta} : S \\to A $ and a variance parameter $ \\sigma $</li>\n<li>$ \\sigma = 0 $ 이면, $ \\pi_{\\mu_{\\theta},\\sigma} \\equiv \\mu_{\\theta} $</li>\n</ul>\n</li>\n<li>Theorem 2. Policy 의 variance 가 0 에 수렴하면, 즉, $ \\sigma \\to 0 $, stochastic policy gradient 와 deterministic policy gradient 는 동일해짐<ul>\n<li>조건 : stochastic policy $ \\pi_{\\mu_{\\theta},\\sigma} = \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $<ul>\n<li>$ \\sigma $ 는 variance</li>\n<li>$ \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $ 는 conditions B.1 만족</li>\n<li>MDP 는 conditions A.1 및 A.2 만족</li>\n</ul>\n</li>\n<li>결과 :<ul>\n<li>$ \\lim\\limits_{\\sigma\\downarrow0}\\nabla_{\\theta}J(\\pi_{\\mu_{\\theta},\\sigma}) = \\nabla_{\\theta}J(\\mu_{\\theta})  $<ul>\n<li>좌변은 standard stochastic gradient 이며, 우변은 deterministic gradient.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>의미 :<ul>\n<li>deterministic policy gradient 는 stochastic policy gradient 의 특수 case 임</li>\n<li>기존 유명한 policy gradients 기법들에 deterministic policy gradients 를 적용할 수 있음<ul>\n<li>기존 기법들 예: compatible function approximation (Sutton, 1999), natural gradients (Kakade, 2001), actor-critic (Bhatnagar, 2007) or episodic/batch methods (Peters, 2005)<br><br></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2 id=\"4-Deterministic-Actor-Critic-Algorithms\"><a href=\"#4-Deterministic-Actor-Critic-Algorithms\" class=\"headerlink\" title=\"4. Deterministic Actor-Critic Algorithms\"></a>4. Deterministic Actor-Critic Algorithms</h2><ol>\n<li>살사 critic 을 이용한 on-policy actor-critic<ul>\n<li>단점<ul>\n<li>deterministic policy 에 의해 행동하면 exploration 이 잘 되지 않기에, sub-optimal 에 빠지기 쉬움</li>\n</ul>\n</li>\n<li>목적<ul>\n<li>교훈/정보제공</li>\n<li>환경에서 충분한 noise 를 제공하여 exploration을 시킬 수 있다면, deterministic policy 를 사용한다고 하여도 좋은 학습 결과를 얻을 수도 있음$<ul>\n<li>예. 바람이 agent의 행동에 영향(noise)을 줌</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Remind: 살사(SARSA) update rule<ul>\n<li>$ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma Q(s_{t+1},a_{t+1}) - Q(s_{t},a_{t})) $</li>\n</ul>\n</li>\n<li>Algorithm<ul>\n<li>Critic 은 MSE 를 $ \\bf minimize $ 하는 방향, 즉, action-value function 을 stochastic gradient $ \\bf descent $ 방법으로 update 함<ul>\n<li>$ MSE = [Q^{\\mu}(s,a) - Q^{w}(s,a)]^2 $<ul>\n<li>critic 은 실제 $ Q^{\\mu}(s,a) $ 대신 미분 가능한 $ Q^{w}(s,a) $ 로 대체하여 action-value function 을 estimate 하며, 이 둘 간 Mean Square Error 를 최소화하는 것이 목표</li>\n</ul>\n</li>\n<li>$ \\nabla_{w}MSE \\approx -2 * [r + \\gamma Q^{w}(s’,a’) - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $<ul>\n<li>$ \\nabla_{w}MSE = -2 * [Q^{\\mu}(s,a) - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $</li>\n<li>$ Q^{\\mu}(s,a) $ 를 $ r + \\gamma Q^{w}(s’,a’) $ 로 대체<ul>\n<li>$ Q^{\\mu}(s,a) = r + \\gamma Q^{\\mu}(s’,a’) $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $<ul>\n<li>$w_{t+1} = w_{t} - \\alpha_{w}\\nabla_{w}MSE  \\nonumber$<br>$ \\approx w_{t} - \\alpha_{w} <em> (-2 </em> [r + \\gamma Q^{w}(s’,a’) - Q^{w}(s,a)] \\nabla_{w}Q^{w}(s,a)$</li>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},a_{t+1}) - Q^{w}(s_{t},a_{t}) $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Actor 는 식(9)에 따라 보상이 $ \\bf maximize $ 되는 방향, 즉, deterministic policy 를 stochastic gradient $ \\bf ascent $ 방법으로 update함<ul>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Q-learning 을 이용한 off-policy actor-critic<ul>\n<li>stochastic behavior policy $ \\beta(a|s) $ 에 의해 생성된 trajectories 로부터 deterministic target policy $ \\mu_{\\theta}(s) $ 를 학습하는 off-policy actor-critic</li>\n<li>performance objective<ul>\n<li>$ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)V^{\\mu}(s)ds \\nonumber \\\\$<br>$= \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds \\nonumber \\\\$<br>$= E_{s \\sim \\rho^{\\beta}}[Q^{\\mu}(s,\\mu_{\\theta}(s))]$</li>\n</ul>\n</li>\n<li>off-policy deterministic policy gradient<ul>\n<li>$ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $<ul>\n<li>논문에는 아래와 같이 나와있는데, 물결 표시 부분은 오류로 판단됨.</li>\n<li>$ \\begin{eqnarray}<br>  \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) &amp;\\approx&amp; \\int_{S}\\rho^{\\beta}(s)\\nabla_{\\theta}\\mu_{\\theta}(a|s)Q^{\\mu}(s,a)ds \\nonumber \\<br>  &amp;=&amp; E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]<br>  \\end{eqnarray} $</li>\n<li>근거: Action이 deterministic 하기에 stochastic 경우와는 다르게 performance objective 에서 action 에 대해 평균을 구할 필요가 없음. 그렇기에, 곱의 미분이 있을 필요가 없고, [Degris, 2012b] 에서처럼 곱의 미분을 통해 생기는 action-value function 에 대한 gradient term 를 생략할 필요가 사라짐. <a href=\"#off-policy-actor-critic\">참고</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Remind: 큐러닝(Q-learning) update rule<ul>\n<li>$ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma \\max\\limits_{a}Q(s_{t+1},a) - Q(s_{t},a_{t})) $</li>\n</ul>\n</li>\n<li>algorithm: OPDAC (Off-Policy Deterministic Actor-Critic)<ul>\n<li>살사를 이용한 on-policy deterministic actor-critic 과 아래 부분을 제외하고는 같음<ul>\n<li>target policy 는 $ \\beta(a|s) $ 에 의해 생성된 trajectories 를 통해 학습함</li>\n<li>업데이트 목표 부분에 실제 행동 값 $ a_{t+1} $ 이 아니라 정책으로부터 나온 행동 값 $ \\mu_{\\theta}(s_{t+1}) $ 사용함<ul>\n<li>$ \\mu_{\\theta}(s_{t+1}) $ 는 가장 높은 Q 값을 가지는 행동. 즉, Q-learning.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $</li>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $</li>\n</ul>\n</li>\n<li>Stochastic off-policy actor-critic 은 대개 actor 와 critic 모두 importance sampling을 필요로 하지만, deterministic policy gradient 에선 importance sampling이 필요없음<ul>\n<li>Actor 는 deterministic 이기에 sampling 자체가 필요없음<ul>\n<li>Stochastic policy 인 경우, Actor 에서 importance sampling이 필요한 이유는 상태 $ s $ 에서의 가치 함수 값 $ V^{\\pi}(s) $ 을 estimate 하기 위해 $ \\pi $ 가 아니라 $ \\beta $ 에 따라 sampling을 한 후, 평균을 내기 때문임.</li>\n<li>하지만 Deterministic policy 인 경우, 상태 $ s $ 에서의 가치 함수 값 $ V^{\\pi}(s) = Q^{\\pi}(s,\\mu_{\\theta}) $ 즉, action 이 상태 s 에 대해 deterministic 이기에 sampling 을 통해 estimate 할 필요가 없고, 따라서 importance sampling도 필요없어짐.</li>\n<li>stochastic vs. deterministic performance objective<ul>\n<li>stochastic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads $</li>\n<li>deterministic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Critic 이 사용하는 Q-learning 은 importance sampling이 필요없는 off policy 알고리즘임.<ul>\n<li>Q-learning 도 업데이트 목표를 특정 분포에서 샘플링을 통해 estimate 하는 것이 아니라 Q 함수를 최대화하는 action을 선택하는 것이기에 위 actor 에서의 deterministic 경우와 비슷하게 볼 수 있음</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>compatible function approximation 및 gradient temporal-difference learning 을 이용한 actor-critic<ul>\n<li>위 살사/Q-learning 기반 on/off-policy 는 아래와 같은 문제가 존재<ul>\n<li>function approximator 에 의한 bias<ul>\n<li>일반적으로 $ Q^{\\mu}(s,a) $ 를 $ Q^{w}(s,a) $ 로 대체하여 deterministic policy gradient 를 구하면, 그 gradient 는 ascent 하는 방향이 아닐 수도 있음</li>\n</ul>\n</li>\n<li>off-policy learning 에 의한 instabilities</li>\n</ul>\n</li>\n<li>그래서 stochastic 처럼 $ \\nabla_{a}Q^{\\mu}(s,a) $ 를 $ \\nabla_{a}Q^{w}(s,a) $ 로 대체해도 deterministic policy gradient 에 영향을 미치지 않을 compatible function approximator $ Q^{w}(s,a) $ 를 찾아야 함</li>\n<li>Theorem 3. 아래 두 조건을 만족하면, $ Q^{w}(s,a) $ 는 deterministic policy $ \\mu_{\\theta}(s) $ 와 compatible 함. 즉, $ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $<ul>\n<li>$ \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $</li>\n<li>$ w $ 는 $ MSE(\\theta, w) = E[\\epsilon(s;\\theta,w)^{\\top}\\epsilon(s;\\theta,w)] $ 를 최소화함<ul>\n<li>$ \\epsilon(s;\\theta,w) = \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} - \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}  $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Theorem 3 은 on-policy 뿐만 아니라 off-policy 에도 적용 가능함</li>\n<li>$ Q^{w}(s,a) = (a-\\mu_{\\theta}(s))^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top} w + V^{v}(s) $<ul>\n<li>어떠한 deterministic policy 에 대해서도 위 형태와 같은 compatible function approximator 가 존재함.</li>\n<li>앞의 term 은 advantage 를, 뒤의 term 은 value 로 볼 수 있음</li>\n</ul>\n</li>\n<li>$ Q^{w}(s,a) = \\phi(s,a)^{\\top} w + v^{\\top}\\phi(s) $<ul>\n<li>정의 : $ \\phi(s,a) \\overset{\\underset{\\mathrm{def}}{}}{=} \\nabla_{\\theta}\\mu_{\\theta}(s)(a-\\mu_{\\theta}(s)) $</li>\n<li>일례 : $ V^{v}(s) = v^{\\top}\\phi(s) $</li>\n<li>Theorem 3 만족 여부<ul>\n<li>첫 번째 조건 만족.</li>\n<li>두 번째 조건은 대강 만족.<ul>\n<li>$ \\nabla_{a}Q^{\\mu}(s,a) $ 에 대한 unbiased sample 을 획득하기는 매우 어렵기에, 일반적인 정책 평가 방법들로 $ w $ 학습.</li>\n<li>이 정책 평가 방법들을 이용하면 $ Q^{w}(s,a) \\approx Q^{\\mu}(s,a) $ 인 reasonable solution을 찾을 수 있기에 대강 $ \\nabla_{a}Q^{w}(s,a) \\approx \\nabla_{a}Q^{\\mu}(s,a) $ 이 될 것.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>action-value function 에 대한 linear function approximator 는 큰 값을 가지는 actions 에 대해선 diverge할 수 있어 global 하게 action-values 예측하기에는 좋지 않지만, local critic 에 사용할 때는 매우 유용하다.<ul>\n<li>즉, 절대값이 아니라 작은 변화량을 다루는 gradient method 경우엔 $ A^{w}(s,\\mu_{\\theta}(s)+\\delta) = \\delta^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $ 로, diverge 하지 않고, 값을 얻을 수 있음</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>COPDAC-Q algorithm (Compatible Off-Policy Deterministic Actor-Critic Q-learning critic)<ul>\n<li>Critic: 실제 action-value function 에 대한 linear function approximator 인 $ Q^{w}(s,a) = \\phi(s,a)^{\\top}w $ 를 estimate<ul>\n<li>$ \\phi(s,a) = a^{\\top}\\nabla_{\\theta}\\mu_{\\theta} $</li>\n<li>Behavior policy $ \\beta(a|s) $ 로부터 얻은 samples를 이용하여 Q-learning 이나 gradient Q-learning 과 같은 off-policy algorithm 으로 학습 가능함</li>\n</ul>\n</li>\n<li>Actor: estimated action-value function 에 대한 gradient 를 $ \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $ 로 치환 후, 정책을 업데이트함</li>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) $</li>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $</li>\n</ul>\n</li>\n<li>off-policy Q-learning은 linear function approximation을 이용하면 diverge 할 수도 있음<ul>\n<li>$ \\mu_{\\theta}(s_{t+1}) $ 이 diverge 할 수도 있기 때문으로 판단됨.</li>\n<li>그렇기에 simple Q-learning 대신 다른 기법이 필요함.</li>\n</ul>\n</li>\n<li>그렇기에 critic 에 gradient Q-learning 사용한 COPDAC-GQ (Gradient Q-learning critic) algorithm 제안<ul>\n<li>gradient temporal-difference learning 에 기반한 기법들은 true gradient descent algorithm 이며, converge가 보장됨. (Sutton, 2009)<ul>\n<li>기본 아이디어는 stochastic gradient descent 로 Mean-squared projected Bellman error (MSPBE) 를 최소화하는 것</li>\n<li>critic 이 actor 보다 빠른 time-scale 로 update 되도록 step size 들을 잘 조절하면, critic 은 MSPBE 를 최소화하는 parameters 로 converge 하게 됨</li>\n<li>critic 에 gradient temporal-difference learning 의 일종인 gradient Q-learning 사용한 논문 (Maei, 2010)</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>COPDAC-GQ algorithm<ul>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $</li>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) - \\alpha_{w}\\gamma\\phi(s_{t+1}, \\mu_{\\theta}(s_{t+1}))(\\phi(s_{t},a_{t})^{\\top} u_{t}) $</li>\n<li>$ v_{t+1} = v_{t} + \\alpha_{v}\\delta_{t}\\phi(s_{t}) - \\alpha_{v}\\gamma\\phi(s_{t+1})(\\phi(s_{t},a_{t})^{\\top} u_{t}) $</li>\n<li>$ u_{t+1} = u_{t} + \\alpha_{u}(\\delta_{t}-\\phi(s_{t}, a_{t})^{\\top} u_{t})\\phi(s_{t}, a_{t}) $</li>\n</ul>\n</li>\n<li>stochastic actor-critic 과 같이 매 time-step 마다 update 시 필요한 계산의 복잡도는 $ O(mn) $<ul>\n<li>m 은 action dimensions, n 은 number of policy parameters</li>\n</ul>\n</li>\n<li>Natural policy gradient 를 이용해 deterministic policies 를 찾을 수 있음<ul>\n<li>$ M(\\theta)^{-1}\\nabla_{\\theta}J(\\mu_{\\theta}) $ 는 any metric $ M(\\theta) $ 에 대한 our performance objective (식(14)) 의 steepest ascent direction 임 (Toussaint, 2012)</li>\n<li>Natural gradient 는 Fisher information metric $ M_{\\pi}(\\theta) $ 에 대한 steepest ascent direction 임<ul>\n<li>$ M_{\\pi}(\\theta) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)^{\\top}] $</li>\n<li>Fisher information metric 은 policy reparameterization 에 대해 불변임 (Bagnell, 2003)</li>\n</ul>\n</li>\n<li>deterministic policies 에 대해 metric 으로 $ M_{\\mu}(\\theta) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}] $ 을 사용.<ul>\n<li>이는 variance 가 0 인 policy 에 대한 Fisher information metric 으로 볼 수 있음</li>\n<li>$ \\frac{\\nabla_{\\theta}\\pi_{\\theta}(a\\vert s)}{\\pi_{\\theta}(a\\vert s)}$ 에서 policy variance 가 0 이면, 특정 s 에 대한 $ \\pi_{\\theta}(a|s)$ 만 1 이 되고, 나머지는 0 임</li>\n</ul>\n</li>\n<li>deterministic policy gradient theorem 과 compatible function approximation 을 결합하면 $ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] $ 이 됨<ul>\n<li>$ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $</li>\n<li>$ \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)} \\approx \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $</li>\n</ul>\n</li>\n<li>그렇기에 steepest ascent direction 은 $ M_{\\mu}(\\theta)^{-1}\\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = w $ 이 됨<ul>\n<li>$ E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}]^{-1}E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] = w $</li>\n</ul>\n</li>\n<li>이 알고리즘은 COPDAC-Q 혹은 COPDAC-GQ 에서 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $ 식을 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta}w_{t} $ 로 바꿔주기만 하면 됨</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"Experiments\"><a href=\"#Experiments\" class=\"headerlink\" title=\"Experiments\"></a>Experiments</h2><h3 id=\"Continuous-Bandit\"><a href=\"#Continuous-Bandit\" class=\"headerlink\" title=\"Continuous Bandit\"></a>Continuous Bandit</h3><ul>\n<li>Stochastic Actor-Critic (SAC)과 COPDAC 간 성능 비교 수행<ul>\n<li>Action dimension이 커질수록 성능 차이가 심함</li>\n<li>빠르게 수렴하는 것을 통해 DPG의 data efficiency가 SPG에 비해 좋다는 것을 확인할 수 있지만, 반면, time-step이 증가할수록 SAC와 COPDAC 간 성능 차이가 줄어드는 것을 통해 성능 차이가 심하다는 것은 일정 time step 내에서만 해당하는 것이라고 유추해볼 수 있음</li>\n<li><img src=\"https://www.dropbox.com/s/hrkyq0s2f24z66r/Screenshot%202018-06-16%2017.47.38.png?dl=1\"></li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Continuous-Reinforcement-Learning\"><a href=\"#Continuous-Reinforcement-Learning\" class=\"headerlink\" title=\"Continuous Reinforcement Learning\"></a>Continuous Reinforcement Learning</h3><ul>\n<li>COPDAC-Q가 SAC와 off-policy stochastic actor-critic(OffPAC-TD) 간 성능 비교 수행<ul>\n<li>COPDAC-Q의 성능이 약간 더 좋음</li>\n<li>COPDAC-Q의 학습이 더 빨리 이뤄짐</li>\n<li><img src=\"https://www.dropbox.com/s/qdca4augapmzsxi/Screenshot%202018-06-16%2017.47.07.png?dl=1\"><h3 id=\"Octopus-Arm\"><a href=\"#Octopus-Arm\" class=\"headerlink\" title=\"Octopus Arm\"></a>Octopus Arm</h3></li>\n</ul>\n</li>\n<li>목표: 6 segments octopus arm (20 action dimensions &amp; 50 state dimensions)을 control하여 target을 맞추는 것<ul>\n<li>COPDAC-Q 사용 시, action space dimension이 큰 octopus arm을 잘 control하여 target을 맞춤</li>\n<li><img src=\"https://www.dropbox.com/s/xrxb0a52wntekld/Screenshot%202018-06-16%2017.46.28.png?dl=1\" width=\"600px\"></li>\n<li>기존 기법들은 action spaces 혹은 action 과 state spaces 둘 다 작은 경우들에 대해서만 실험했다고 하며, 비교하고 있지 않음.<ul>\n<li>기존 기법들이 6 segments octopus arm에서 동작을 잘 안 했을 것 같긴한데, 그래도 실험해서 결과를 보여주지…왜 안 했을까?</li>\n</ul>\n</li>\n<li>8 segment arm 동영상이 저자 홈페이지에 있다고 하는데, 안 보임.</li>\n</ul>\n</li>\n<li>[참고] Octopus Arm 이란?<ul>\n<li><a href=\"https://www.youtube.com/watch?v=AxeeHif0euY\" target=\"_blank\" rel=\"noopener\">OctopusArm Youtube Link</a></li>\n<li><img src=\"https://www.dropbox.com/s/950ycj06sudakjx/Screenshot%202018-06-16%2017.45.52.png?dl=1\"></li>\n</ul>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>Authors: David Silver<sup>1)</sup>, Guy Lever<sup>2)</sup>, Nicloas Heess<sup>1)</sup>, Thomas Degris<sup>1)</sup>, Daan Wierstra<sup>1)</sup>, Martin Riedmiller<sup>1)</sup><br>Affiliation: 1) Google Deepmind, 2) University College London (UCL)<br>Proceeding: International Conference on Machine Learning (ICML) 2014</p>\n<ul>\n<li><a href=\"#deterministic-policy-gradient-algorithms\">Deterministic Policy Gradient Algorithms</a><ul>\n<li><a href=\"#summary\">Summary</a></li>\n<li><a href=\"#background\">Background</a><ul>\n<li><a href=\"#performance-objective-function\">Performance objective function</a></li>\n<li><a href=\"#spg-theorem\">SPG Theorem</a></li>\n<li><a href=\"#stochastic-actor-critic-algorithms\">Stochastic Actor-Critic Algorithms</a></li>\n<li><a href=\"#off-policy-actor-critic\">Off-policy Actor-Critic</a></li>\n</ul>\n</li>\n<li><a href=\"#gradient-of-deterministic-policies\">Gradient of Deterministic Policies</a><ul>\n<li><a href=\"#regulariy-conditions\">Regulariy Conditions</a></li>\n<li><a href=\"#deterministic-policy-gradient-theorem\">Deterministic Policy Gradient Theorem</a></li>\n<li><a href=\"#dpg-형태에-대한-informal-intuition\">DPG 형태에 대한 informal intuition</a></li>\n<li><a href=\"#dpg-는-spg-의-limiting-case-임\">DPG 는 SPG 의 limiting case 임</a></li>\n</ul>\n</li>\n<li><a href=\"#deterministic-actor-critic-algorithms\">Deterministic Actor-Critic Algorithms</a></li>\n<li><a href=\"#experiments\">Experiments</a><ul>\n<li><a href=\"#continuous-bandit\">Continuous Bandit</a></li>\n<li><a href=\"#continuous-reinforcement-learning\">Continuous Reinforcement Learning</a></li>\n<li><a href=\"#octopus-arm\">Octopus Arm</a></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<!-- /code_chunk_output -->\n<p><br></p>\n<hr>\n<h2 id=\"1-Summary\"><a href=\"#1-Summary\" class=\"headerlink\" title=\"1. Summary\"></a>1. Summary</h2><ul>\n<li>Deterministic Policy Gradient (DPG) Theorem 제안함 [<a href=\"#deterministic-policy-gradient-theorem\">Theorem 1</a>]<br>  1) DPG는 존재하며,<br>  2) DPG는 Expected gradient of the action-value function의 형태를 띈다.</li>\n<li>Policy variance가 0에 수렴할 경우, DPG는 Stochastic Policy Gradient(SPG)와 동일해짐을 보임 [<a href=\"#dpg-는-spg-의-limiting-case-임\">Theorem 2</a>]<ul>\n<li>Theorem 2 로 인해 기존 Policy Gradient (PG) 와 관련된 기법들을 DPG 에 적용할 수 있게 됨<ul>\n<li>예. Sutton PG, natural gradients, actor-critic, episodic/batch methods</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>적절한 exploration 을 위해 model-free, off-policy Actor-Critic algorithm 을 제안함<ul>\n<li>Action-value function approximator 사용으로 인해 policy gradient가 bias되는 것을 방지하기 위해 compatibility conditions을 제공 [<a href=\"##deterministic-actor-critic-algorithms\">Theorem 3</a>]</li>\n</ul>\n</li>\n<li>DPG 는 SPG 보다 성능이 좋음<ul>\n<li>특히 high dimensional action spaces 을 가지는 tasks에서의 성능 향상이 큼<ul>\n<li>SPG의 policy gradient는 state와 action spaces 모두에 대해서, DPG의 policy gradient는  state spaces에 대해서만 평균을 취함</li>\n<li>결과적으로, action spaces의 dimension이 커질수록 data efficiency가 높은 DPG의 학습이 더 잘 이뤄지게 됨</li>\n<li>무한정 학습을 시키게 되면, SPG도 최적으로 수렴할 것으로 예상되기에 위 성능 비교는 일정 iteration 내로 한정함</li>\n</ul>\n</li>\n<li>기존 기법들에 비해 computation 양이 많지 않음<ul>\n<li>Computation 은 action dimensionality 와 policy parameters 수에 비례함<br><br></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2 id=\"2-Background\"><a href=\"#2-Background\" class=\"headerlink\" title=\"2. Background\"></a>2. Background</h2><h3 id=\"2-1-Performance-objective-function\"><a href=\"#2-1-Performance-objective-function\" class=\"headerlink\" title=\"2.1 Performance objective function\"></a>2.1 Performance objective function</h3><p>$$<br>\\begin{align}<br>J(\\pi_{\\theta}) &amp;= \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\pi_{\\theta}(s,a)r(s,a)dads = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[r(s,a)]<br>\\end{align}<br>$$</p>\n<h3 id=\"2-2-SPG-Theorem\"><a href=\"#2-2-SPG-Theorem\" class=\"headerlink\" title=\"2.2 SPG Theorem\"></a>2.2 SPG Theorem</h3><ul>\n<li>State distribution $ \\rho^{\\pi}(s) $ 은 policy parameters에 영향을 받지만, policy gradient 를 계산할 때는 state distribution 의 gradient 를 고려할 필요가 없다.</li>\n<li>$$\\begin{eqnarray}\\nabla_{\\theta}J(\\pi_{\\theta}) &amp;=&amp; \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\ &amp;=&amp; E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]<br>\\end{eqnarray}$$</li>\n</ul>\n<h3 id=\"2-3-Stochastic-Actor-Critic-Algorithms\"><a href=\"#2-3-Stochastic-Actor-Critic-Algorithms\" class=\"headerlink\" title=\"2.3 Stochastic Actor-Critic Algorithms\"></a>2.3 Stochastic Actor-Critic Algorithms</h3><ul>\n<li>Actor 와 Critic 이 번갈아가면서 동작하며 stochastic policy 를 최적화하는 기법</li>\n<li>Actor: $ Q^{\\pi}(s,a) $ 를 근사한 $ Q^w(s,a) $ 를 이용해 stochastic policy gradient 를 ascent 하는 방향으로 policy parameter $ \\theta $ 를 업데이트함으로써 stochastic policy 를 발전시킴<ul>\n<li>$ \\nabla_{\\theta}J(\\pi_{\\theta}) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}log\\pi_{\\theta}(a|s)Q^{w}(s,a)] $</li>\n</ul>\n</li>\n<li>Critic: SARSA 나 Q-learning 같은 Temporal-difference (TD) learning 을 이용해 action-value function의 parameter, $ w $ 를 업데이트함으로써 $ Q^w(s,a) $ 가 $ Q^{\\pi}(s,a) $ 과 유사해지도록 함</li>\n<li>실제 값인 $ Q^{\\pi}(s,a) $ 대신 이를 근사한 $ Q^w(s,a) $ 를 사용하게 되면, 일반적으로 bias 가 발생하게 된다. 하지만, compatible condition 에 부합하는 $ Q^w(s,a) $ 를 사용하게 되면, bias 가 발생하지 않는다.</li>\n</ul>\n<h3 id=\"2-4-Off-policy-Actor-Critic\"><a href=\"#2-4-Off-policy-Actor-Critic\" class=\"headerlink\" title=\"2.4 Off-policy Actor-Critic\"></a>2.4 Off-policy Actor-Critic</h3><ul>\n<li>Distinct behavior policy $ \\beta(a|s) ( \\neq \\pi_{\\theta}(a|s) ) $ 로부터 샘플링된 trajectories 를 이용한 Actor-Critic</li>\n<li>Performance objective function<ul>\n<li>$\\begin{eqnarray}<br>  J_{\\beta}(\\pi_{\\theta})<br>  &amp;=&amp; \\int_{S}\\rho^{\\beta}(s)V^{\\pi}(s)ds \\nonumber \\\\<br>  &amp;=&amp; \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads<br>  \\end{eqnarray} $</li>\n</ul>\n</li>\n<li>off-policy policy gradient<ul>\n<li>$ \\begin{eqnarray}<br>  \\nabla_{\\theta}J_{\\beta}(\\pi_{\\theta}) &amp;\\approx&amp; \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\end{eqnarray} $<br>$=E_{s \\sim \\rho^{\\beta}, a \\sim \\beta}[\\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)}\\nabla_{\\theta}log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]$</li>\n<li>off-policy policy gradient 식에서의 물결 표시는 [Degris, 2012b] 논문에 근거함<ul>\n<li>[Degris, 2012b] “Linear off-policy actor-critic,” ICML 2012</li>\n<li>Exact off-policy policy gradient 와 이를 approximate 한 policy gradient 는 아래와 같음. (빨간색 상자에 있는 항목을 삭제함으로써 근사함)<ul>\n<li><img src=\"https://www.dropbox.com/s/xzpv3okc139c1fs/Screenshot%202018-06-16%2017.48.51.png?dl=1\" width=\"500px\"></li>\n</ul>\n</li>\n<li>[Degris, 2012b] Theorem 1 에 의해 policy parameter 가 approximated policy gradient ( $\\nabla_{u}𝑄^{\\pi,\\gamma}(𝑠,𝑎)$ term 제거)에 따라 업데이트되어도 policy 는 improve 가 됨이 보장되기에 exact off-policy policy gradient 대신 approximated off-policy policy gradient 를 사용해도 괜찮음.<ul>\n<li><img src=\"https://www.dropbox.com/s/mk13931r4scjngo/Screenshot%202018-06-16%2017.49.24.png?dl=1\" width=\"500px\"></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>off-policy policy gradient 식에서 $ \\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)} $ 는 importance sampling ratio 임<ul>\n<li>off-policy actor-critic 에서는 $ \\beta $ 에 의해 샘플링된 trajectory 를 이용해서 stochastic policy $ \\pi $ 를 예측하는 것이기 때문에 imnportance sampling 이 필요함<br><br></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2 id=\"3-Gradient-of-Deterministic-Policies\"><a href=\"#3-Gradient-of-Deterministic-Policies\" class=\"headerlink\" title=\"3. Gradient of Deterministic Policies\"></a>3. Gradient of Deterministic Policies</h2><h3 id=\"3-1-Regulariy-Conditions\"><a href=\"#3-1-Regulariy-Conditions\" class=\"headerlink\" title=\"3.1 Regulariy Conditions\"></a>3.1 Regulariy Conditions</h3><ul>\n<li>어떠한 이론이 성립하기 위한 전제 조건</li>\n<li>Regularity conditions A.1<ul>\n<li>$ p(s’|s,a), \\nabla_{a}p(s’|s,a), \\mu_{\\theta}(s), \\nabla_{\\theta}\\mu_{\\theta}(s), r(s,a), \\nabla_{a}r(s,a), p_{1}(s) $ are continuous in all parameters and variables $ s, a, s’ $ and $ x $.</li>\n</ul>\n</li>\n<li>regularity conditions A.2<ul>\n<li>There exists a $ b $ and $ L $ such that $ \\sup_{s}p_{1}(s) &lt; b $, $ \\sup_{a,s,s’}p(s’|s,a) &lt; b $, $ \\sup_{a,s}r(s,a) &lt; b $, $ \\sup_{a,s,s’}|\\nabla_{a}p(s’|s,a)| &lt; L $, and $ \\sup_{a,s}|\\nabla_{a}r(s,a)| &lt; L $.</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"3-2-Deterministic-Policy-Gradient-Theorem\"><a href=\"#3-2-Deterministic-Policy-Gradient-Theorem\" class=\"headerlink\" title=\"3.2 Deterministic Policy Gradient Theorem\"></a>3.2 Deterministic Policy Gradient Theorem</h3><ul>\n<li>Deterministic policy<ul>\n<li>$ \\mu_{\\theta} : S \\to A $ with parameter vector $ \\theta \\in \\rm I ! R^n $</li>\n</ul>\n</li>\n<li>Probability distribution<ul>\n<li>$ p(s \\to s’, t, \\mu) $</li>\n</ul>\n</li>\n<li>Discounted state distribution<ul>\n<li>$ \\rho^{\\mu}(s) $</li>\n</ul>\n</li>\n<li>Performance objective</li>\n</ul>\n<p>$$<br>J(\\mu_{\\theta}) = E[r^{\\gamma}_{1} | \\mu]<br>$$</p>\n<p>$$<br>= \\int_{S}\\rho^{\\mu}(s)r(s,\\mu_{\\theta}(s))ds<br>= E_{s \\sim \\rho^{\\mu}}[r(s,\\mu_{\\theta}(s))]<br>$$</p>\n<ul>\n<li><p>DPG Theorem</p>\n<ul>\n<li><p>MDP 가 A.1 만족한다면, 아래 식(9)이 성립함<br>$\\nabla_{\\theta}J(\\mu_{\\theta}) = \\int_{S}\\rho^{\\mu}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}ds \\nonumber$<br>$= E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]   \\nonumber $ (9)</p>\n</li>\n<li><p>DPG는 State space 에 대해서만 평균을 취하면 되기에, State와 Action space 모두에 대해 평균을 취해야 하는 <a href=\"#spg-theorem\">SPG</a>에 비해 data efficiency가 좋다. 즉, 더 적은 양의 데이터로도 학습이 잘 이뤄지게 된다.</p>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"3-3-DPG-형태에-대한-informal-intuition\"><a href=\"#3-3-DPG-형태에-대한-informal-intuition\" class=\"headerlink\" title=\"3.3 DPG 형태에 대한 informal intuition\"></a>3.3 DPG 형태에 대한 informal intuition</h3><ul>\n<li>Generalized policy iteration<ul>\n<li>정책 평가와 정책 발전을 한 번 씩 번갈아 가면서 실행하는 정책 iteration<ul>\n<li>위와 같이 해도 정책 평가에서 예측한 가치함수가 최적 가치함수에 수렴함</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>정책 평가<ul>\n<li>action-value function $ Q^{\\pi}(s,a) $ or $ Q^{\\mu}(s,a) $ 을 estimate 하는 것</li>\n</ul>\n</li>\n<li>정책 발전<ul>\n<li>위 estimated action-value function 에 따라 정책을 update 하는 것</li>\n<li>주로 action-value function 에 대한 greedy maximisation 사용함<ul>\n<li>$ \\mu^{k+1}(s) = \\arg\\max\\limits_{a}Q^{\\mu^{k}}(s,a) $</li>\n<li>greedy 정책 발전은 매 단계마다 global maximization을 해야하는데, 이는 continuous action spaces 에서 계산량이 급격히 늘어나게 됨.</li>\n</ul>\n</li>\n<li>그렇기에 policy gradient 방법이 나옴<ul>\n<li>policy 를 $ \\theta $ 에 대해서 parameterize 함</li>\n<li>매 단계마다 global maximisation 수행하는 대신, 방문하는 state $ s $ 마다 policy parameter 를 action-value function $ Q $ 의 $ \\theta $ 에 대한 gradient $ \\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s)) $ 방향으로 proportional 하게 update 함</li>\n<li>하지만 각 state 는 다른 방향을 제시할 수 있기에, state distribution $ \\rho^{\\mu}(s) $ 에 대한 기대값을 취해 policy parameter 를 update 할 수도 있음<ul>\n<li>$ \\theta^{k+1} = \\theta^{k} + \\alpha \\rm I!E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s))] $</li>\n</ul>\n</li>\n<li>이는 chain-rule 에 따라 아래와 같이 분리될 수 있음<ul>\n<li>$ \\theta^{k+1} = \\theta^{k} + \\alpha \\rm I!E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu^{k}}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $ (7)</li>\n<li>chain rule: $ \\frac{\\partial Q}{\\partial \\theta} = \\frac{\\partial a}{\\partial \\theta} \\frac{\\partial Q}{\\partial a} $</li>\n</ul>\n</li>\n<li>하지만 state distribution $ \\rho^{\\mu} $ 은 정책에 dependent 함<ul>\n<li>정책이 바꾸게 되면, 바뀐 정책에 따라 방문하게 되는 state 가 변하게 되기 때문에 state distribution이 변하게 됨</li>\n</ul>\n</li>\n<li>그렇기에 정책 update 시 state distribution에 대한 gradient 를 고려하지 않는데 정책 발전이 이뤄진다는 것은 직관적으로 와닿지 않을 수 있음</li>\n<li>deterministic policy gradient theorem 은 state distribution에 대한 gradient 계산없이 위 식(7) 대로만 update 해도 performance objective 의 gradient 를 정확하게 따름을 의미한다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"3-4-DPG-는-SPG-의-limiting-case-임\"><a href=\"#3-4-DPG-는-SPG-의-limiting-case-임\" class=\"headerlink\" title=\"3.4 DPG 는 SPG 의 limiting case 임\"></a>3.4 DPG 는 SPG 의 limiting case 임</h3><ul>\n<li>stochastic policy parameterization<ul>\n<li>$ \\pi_{\\mu_{\\theta},\\sigma} $ by a deterministic policy $ \\mu_{\\theta} : S \\to A $ and a variance parameter $ \\sigma $</li>\n<li>$ \\sigma = 0 $ 이면, $ \\pi_{\\mu_{\\theta},\\sigma} \\equiv \\mu_{\\theta} $</li>\n</ul>\n</li>\n<li>Theorem 2. Policy 의 variance 가 0 에 수렴하면, 즉, $ \\sigma \\to 0 $, stochastic policy gradient 와 deterministic policy gradient 는 동일해짐<ul>\n<li>조건 : stochastic policy $ \\pi_{\\mu_{\\theta},\\sigma} = \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $<ul>\n<li>$ \\sigma $ 는 variance</li>\n<li>$ \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $ 는 conditions B.1 만족</li>\n<li>MDP 는 conditions A.1 및 A.2 만족</li>\n</ul>\n</li>\n<li>결과 :<ul>\n<li>$ \\lim\\limits_{\\sigma\\downarrow0}\\nabla_{\\theta}J(\\pi_{\\mu_{\\theta},\\sigma}) = \\nabla_{\\theta}J(\\mu_{\\theta})  $<ul>\n<li>좌변은 standard stochastic gradient 이며, 우변은 deterministic gradient.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>의미 :<ul>\n<li>deterministic policy gradient 는 stochastic policy gradient 의 특수 case 임</li>\n<li>기존 유명한 policy gradients 기법들에 deterministic policy gradients 를 적용할 수 있음<ul>\n<li>기존 기법들 예: compatible function approximation (Sutton, 1999), natural gradients (Kakade, 2001), actor-critic (Bhatnagar, 2007) or episodic/batch methods (Peters, 2005)<br><br></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2 id=\"4-Deterministic-Actor-Critic-Algorithms\"><a href=\"#4-Deterministic-Actor-Critic-Algorithms\" class=\"headerlink\" title=\"4. Deterministic Actor-Critic Algorithms\"></a>4. Deterministic Actor-Critic Algorithms</h2><ol>\n<li>살사 critic 을 이용한 on-policy actor-critic<ul>\n<li>단점<ul>\n<li>deterministic policy 에 의해 행동하면 exploration 이 잘 되지 않기에, sub-optimal 에 빠지기 쉬움</li>\n</ul>\n</li>\n<li>목적<ul>\n<li>교훈/정보제공</li>\n<li>환경에서 충분한 noise 를 제공하여 exploration을 시킬 수 있다면, deterministic policy 를 사용한다고 하여도 좋은 학습 결과를 얻을 수도 있음$<ul>\n<li>예. 바람이 agent의 행동에 영향(noise)을 줌</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Remind: 살사(SARSA) update rule<ul>\n<li>$ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma Q(s_{t+1},a_{t+1}) - Q(s_{t},a_{t})) $</li>\n</ul>\n</li>\n<li>Algorithm<ul>\n<li>Critic 은 MSE 를 $ \\bf minimize $ 하는 방향, 즉, action-value function 을 stochastic gradient $ \\bf descent $ 방법으로 update 함<ul>\n<li>$ MSE = [Q^{\\mu}(s,a) - Q^{w}(s,a)]^2 $<ul>\n<li>critic 은 실제 $ Q^{\\mu}(s,a) $ 대신 미분 가능한 $ Q^{w}(s,a) $ 로 대체하여 action-value function 을 estimate 하며, 이 둘 간 Mean Square Error 를 최소화하는 것이 목표</li>\n</ul>\n</li>\n<li>$ \\nabla_{w}MSE \\approx -2 * [r + \\gamma Q^{w}(s’,a’) - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $<ul>\n<li>$ \\nabla_{w}MSE = -2 * [Q^{\\mu}(s,a) - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $</li>\n<li>$ Q^{\\mu}(s,a) $ 를 $ r + \\gamma Q^{w}(s’,a’) $ 로 대체<ul>\n<li>$ Q^{\\mu}(s,a) = r + \\gamma Q^{\\mu}(s’,a’) $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $<ul>\n<li>$w_{t+1} = w_{t} - \\alpha_{w}\\nabla_{w}MSE  \\nonumber$<br>$ \\approx w_{t} - \\alpha_{w} <em> (-2 </em> [r + \\gamma Q^{w}(s’,a’) - Q^{w}(s,a)] \\nabla_{w}Q^{w}(s,a)$</li>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},a_{t+1}) - Q^{w}(s_{t},a_{t}) $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Actor 는 식(9)에 따라 보상이 $ \\bf maximize $ 되는 방향, 즉, deterministic policy 를 stochastic gradient $ \\bf ascent $ 방법으로 update함<ul>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Q-learning 을 이용한 off-policy actor-critic<ul>\n<li>stochastic behavior policy $ \\beta(a|s) $ 에 의해 생성된 trajectories 로부터 deterministic target policy $ \\mu_{\\theta}(s) $ 를 학습하는 off-policy actor-critic</li>\n<li>performance objective<ul>\n<li>$ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)V^{\\mu}(s)ds \\nonumber \\\\$<br>$= \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds \\nonumber \\\\$<br>$= E_{s \\sim \\rho^{\\beta}}[Q^{\\mu}(s,\\mu_{\\theta}(s))]$</li>\n</ul>\n</li>\n<li>off-policy deterministic policy gradient<ul>\n<li>$ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $<ul>\n<li>논문에는 아래와 같이 나와있는데, 물결 표시 부분은 오류로 판단됨.</li>\n<li>$ \\begin{eqnarray}<br>  \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) &amp;\\approx&amp; \\int_{S}\\rho^{\\beta}(s)\\nabla_{\\theta}\\mu_{\\theta}(a|s)Q^{\\mu}(s,a)ds \\nonumber \\<br>  &amp;=&amp; E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]<br>  \\end{eqnarray} $</li>\n<li>근거: Action이 deterministic 하기에 stochastic 경우와는 다르게 performance objective 에서 action 에 대해 평균을 구할 필요가 없음. 그렇기에, 곱의 미분이 있을 필요가 없고, [Degris, 2012b] 에서처럼 곱의 미분을 통해 생기는 action-value function 에 대한 gradient term 를 생략할 필요가 사라짐. <a href=\"#off-policy-actor-critic\">참고</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Remind: 큐러닝(Q-learning) update rule<ul>\n<li>$ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma \\max\\limits_{a}Q(s_{t+1},a) - Q(s_{t},a_{t})) $</li>\n</ul>\n</li>\n<li>algorithm: OPDAC (Off-Policy Deterministic Actor-Critic)<ul>\n<li>살사를 이용한 on-policy deterministic actor-critic 과 아래 부분을 제외하고는 같음<ul>\n<li>target policy 는 $ \\beta(a|s) $ 에 의해 생성된 trajectories 를 통해 학습함</li>\n<li>업데이트 목표 부분에 실제 행동 값 $ a_{t+1} $ 이 아니라 정책으로부터 나온 행동 값 $ \\mu_{\\theta}(s_{t+1}) $ 사용함<ul>\n<li>$ \\mu_{\\theta}(s_{t+1}) $ 는 가장 높은 Q 값을 가지는 행동. 즉, Q-learning.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $</li>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $</li>\n</ul>\n</li>\n<li>Stochastic off-policy actor-critic 은 대개 actor 와 critic 모두 importance sampling을 필요로 하지만, deterministic policy gradient 에선 importance sampling이 필요없음<ul>\n<li>Actor 는 deterministic 이기에 sampling 자체가 필요없음<ul>\n<li>Stochastic policy 인 경우, Actor 에서 importance sampling이 필요한 이유는 상태 $ s $ 에서의 가치 함수 값 $ V^{\\pi}(s) $ 을 estimate 하기 위해 $ \\pi $ 가 아니라 $ \\beta $ 에 따라 sampling을 한 후, 평균을 내기 때문임.</li>\n<li>하지만 Deterministic policy 인 경우, 상태 $ s $ 에서의 가치 함수 값 $ V^{\\pi}(s) = Q^{\\pi}(s,\\mu_{\\theta}) $ 즉, action 이 상태 s 에 대해 deterministic 이기에 sampling 을 통해 estimate 할 필요가 없고, 따라서 importance sampling도 필요없어짐.</li>\n<li>stochastic vs. deterministic performance objective<ul>\n<li>stochastic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads $</li>\n<li>deterministic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Critic 이 사용하는 Q-learning 은 importance sampling이 필요없는 off policy 알고리즘임.<ul>\n<li>Q-learning 도 업데이트 목표를 특정 분포에서 샘플링을 통해 estimate 하는 것이 아니라 Q 함수를 최대화하는 action을 선택하는 것이기에 위 actor 에서의 deterministic 경우와 비슷하게 볼 수 있음</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>compatible function approximation 및 gradient temporal-difference learning 을 이용한 actor-critic<ul>\n<li>위 살사/Q-learning 기반 on/off-policy 는 아래와 같은 문제가 존재<ul>\n<li>function approximator 에 의한 bias<ul>\n<li>일반적으로 $ Q^{\\mu}(s,a) $ 를 $ Q^{w}(s,a) $ 로 대체하여 deterministic policy gradient 를 구하면, 그 gradient 는 ascent 하는 방향이 아닐 수도 있음</li>\n</ul>\n</li>\n<li>off-policy learning 에 의한 instabilities</li>\n</ul>\n</li>\n<li>그래서 stochastic 처럼 $ \\nabla_{a}Q^{\\mu}(s,a) $ 를 $ \\nabla_{a}Q^{w}(s,a) $ 로 대체해도 deterministic policy gradient 에 영향을 미치지 않을 compatible function approximator $ Q^{w}(s,a) $ 를 찾아야 함</li>\n<li>Theorem 3. 아래 두 조건을 만족하면, $ Q^{w}(s,a) $ 는 deterministic policy $ \\mu_{\\theta}(s) $ 와 compatible 함. 즉, $ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $<ul>\n<li>$ \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $</li>\n<li>$ w $ 는 $ MSE(\\theta, w) = E[\\epsilon(s;\\theta,w)^{\\top}\\epsilon(s;\\theta,w)] $ 를 최소화함<ul>\n<li>$ \\epsilon(s;\\theta,w) = \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} - \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}  $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Theorem 3 은 on-policy 뿐만 아니라 off-policy 에도 적용 가능함</li>\n<li>$ Q^{w}(s,a) = (a-\\mu_{\\theta}(s))^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top} w + V^{v}(s) $<ul>\n<li>어떠한 deterministic policy 에 대해서도 위 형태와 같은 compatible function approximator 가 존재함.</li>\n<li>앞의 term 은 advantage 를, 뒤의 term 은 value 로 볼 수 있음</li>\n</ul>\n</li>\n<li>$ Q^{w}(s,a) = \\phi(s,a)^{\\top} w + v^{\\top}\\phi(s) $<ul>\n<li>정의 : $ \\phi(s,a) \\overset{\\underset{\\mathrm{def}}{}}{=} \\nabla_{\\theta}\\mu_{\\theta}(s)(a-\\mu_{\\theta}(s)) $</li>\n<li>일례 : $ V^{v}(s) = v^{\\top}\\phi(s) $</li>\n<li>Theorem 3 만족 여부<ul>\n<li>첫 번째 조건 만족.</li>\n<li>두 번째 조건은 대강 만족.<ul>\n<li>$ \\nabla_{a}Q^{\\mu}(s,a) $ 에 대한 unbiased sample 을 획득하기는 매우 어렵기에, 일반적인 정책 평가 방법들로 $ w $ 학습.</li>\n<li>이 정책 평가 방법들을 이용하면 $ Q^{w}(s,a) \\approx Q^{\\mu}(s,a) $ 인 reasonable solution을 찾을 수 있기에 대강 $ \\nabla_{a}Q^{w}(s,a) \\approx \\nabla_{a}Q^{\\mu}(s,a) $ 이 될 것.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>action-value function 에 대한 linear function approximator 는 큰 값을 가지는 actions 에 대해선 diverge할 수 있어 global 하게 action-values 예측하기에는 좋지 않지만, local critic 에 사용할 때는 매우 유용하다.<ul>\n<li>즉, 절대값이 아니라 작은 변화량을 다루는 gradient method 경우엔 $ A^{w}(s,\\mu_{\\theta}(s)+\\delta) = \\delta^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $ 로, diverge 하지 않고, 값을 얻을 수 있음</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>COPDAC-Q algorithm (Compatible Off-Policy Deterministic Actor-Critic Q-learning critic)<ul>\n<li>Critic: 실제 action-value function 에 대한 linear function approximator 인 $ Q^{w}(s,a) = \\phi(s,a)^{\\top}w $ 를 estimate<ul>\n<li>$ \\phi(s,a) = a^{\\top}\\nabla_{\\theta}\\mu_{\\theta} $</li>\n<li>Behavior policy $ \\beta(a|s) $ 로부터 얻은 samples를 이용하여 Q-learning 이나 gradient Q-learning 과 같은 off-policy algorithm 으로 학습 가능함</li>\n</ul>\n</li>\n<li>Actor: estimated action-value function 에 대한 gradient 를 $ \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $ 로 치환 후, 정책을 업데이트함</li>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) $</li>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $</li>\n</ul>\n</li>\n<li>off-policy Q-learning은 linear function approximation을 이용하면 diverge 할 수도 있음<ul>\n<li>$ \\mu_{\\theta}(s_{t+1}) $ 이 diverge 할 수도 있기 때문으로 판단됨.</li>\n<li>그렇기에 simple Q-learning 대신 다른 기법이 필요함.</li>\n</ul>\n</li>\n<li>그렇기에 critic 에 gradient Q-learning 사용한 COPDAC-GQ (Gradient Q-learning critic) algorithm 제안<ul>\n<li>gradient temporal-difference learning 에 기반한 기법들은 true gradient descent algorithm 이며, converge가 보장됨. (Sutton, 2009)<ul>\n<li>기본 아이디어는 stochastic gradient descent 로 Mean-squared projected Bellman error (MSPBE) 를 최소화하는 것</li>\n<li>critic 이 actor 보다 빠른 time-scale 로 update 되도록 step size 들을 잘 조절하면, critic 은 MSPBE 를 최소화하는 parameters 로 converge 하게 됨</li>\n<li>critic 에 gradient temporal-difference learning 의 일종인 gradient Q-learning 사용한 논문 (Maei, 2010)</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>COPDAC-GQ algorithm<ul>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $</li>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) - \\alpha_{w}\\gamma\\phi(s_{t+1}, \\mu_{\\theta}(s_{t+1}))(\\phi(s_{t},a_{t})^{\\top} u_{t}) $</li>\n<li>$ v_{t+1} = v_{t} + \\alpha_{v}\\delta_{t}\\phi(s_{t}) - \\alpha_{v}\\gamma\\phi(s_{t+1})(\\phi(s_{t},a_{t})^{\\top} u_{t}) $</li>\n<li>$ u_{t+1} = u_{t} + \\alpha_{u}(\\delta_{t}-\\phi(s_{t}, a_{t})^{\\top} u_{t})\\phi(s_{t}, a_{t}) $</li>\n</ul>\n</li>\n<li>stochastic actor-critic 과 같이 매 time-step 마다 update 시 필요한 계산의 복잡도는 $ O(mn) $<ul>\n<li>m 은 action dimensions, n 은 number of policy parameters</li>\n</ul>\n</li>\n<li>Natural policy gradient 를 이용해 deterministic policies 를 찾을 수 있음<ul>\n<li>$ M(\\theta)^{-1}\\nabla_{\\theta}J(\\mu_{\\theta}) $ 는 any metric $ M(\\theta) $ 에 대한 our performance objective (식(14)) 의 steepest ascent direction 임 (Toussaint, 2012)</li>\n<li>Natural gradient 는 Fisher information metric $ M_{\\pi}(\\theta) $ 에 대한 steepest ascent direction 임<ul>\n<li>$ M_{\\pi}(\\theta) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)^{\\top}] $</li>\n<li>Fisher information metric 은 policy reparameterization 에 대해 불변임 (Bagnell, 2003)</li>\n</ul>\n</li>\n<li>deterministic policies 에 대해 metric 으로 $ M_{\\mu}(\\theta) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}] $ 을 사용.<ul>\n<li>이는 variance 가 0 인 policy 에 대한 Fisher information metric 으로 볼 수 있음</li>\n<li>$ \\frac{\\nabla_{\\theta}\\pi_{\\theta}(a\\vert s)}{\\pi_{\\theta}(a\\vert s)}$ 에서 policy variance 가 0 이면, 특정 s 에 대한 $ \\pi_{\\theta}(a|s)$ 만 1 이 되고, 나머지는 0 임</li>\n</ul>\n</li>\n<li>deterministic policy gradient theorem 과 compatible function approximation 을 결합하면 $ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] $ 이 됨<ul>\n<li>$ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $</li>\n<li>$ \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)} \\approx \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $</li>\n</ul>\n</li>\n<li>그렇기에 steepest ascent direction 은 $ M_{\\mu}(\\theta)^{-1}\\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = w $ 이 됨<ul>\n<li>$ E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}]^{-1}E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] = w $</li>\n</ul>\n</li>\n<li>이 알고리즘은 COPDAC-Q 혹은 COPDAC-GQ 에서 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $ 식을 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta}w_{t} $ 로 바꿔주기만 하면 됨</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"Experiments\"><a href=\"#Experiments\" class=\"headerlink\" title=\"Experiments\"></a>Experiments</h2><h3 id=\"Continuous-Bandit\"><a href=\"#Continuous-Bandit\" class=\"headerlink\" title=\"Continuous Bandit\"></a>Continuous Bandit</h3><ul>\n<li>Stochastic Actor-Critic (SAC)과 COPDAC 간 성능 비교 수행<ul>\n<li>Action dimension이 커질수록 성능 차이가 심함</li>\n<li>빠르게 수렴하는 것을 통해 DPG의 data efficiency가 SPG에 비해 좋다는 것을 확인할 수 있지만, 반면, time-step이 증가할수록 SAC와 COPDAC 간 성능 차이가 줄어드는 것을 통해 성능 차이가 심하다는 것은 일정 time step 내에서만 해당하는 것이라고 유추해볼 수 있음</li>\n<li><img src=\"https://www.dropbox.com/s/hrkyq0s2f24z66r/Screenshot%202018-06-16%2017.47.38.png?dl=1\"></li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Continuous-Reinforcement-Learning\"><a href=\"#Continuous-Reinforcement-Learning\" class=\"headerlink\" title=\"Continuous Reinforcement Learning\"></a>Continuous Reinforcement Learning</h3><ul>\n<li>COPDAC-Q가 SAC와 off-policy stochastic actor-critic(OffPAC-TD) 간 성능 비교 수행<ul>\n<li>COPDAC-Q의 성능이 약간 더 좋음</li>\n<li>COPDAC-Q의 학습이 더 빨리 이뤄짐</li>\n<li><img src=\"https://www.dropbox.com/s/qdca4augapmzsxi/Screenshot%202018-06-16%2017.47.07.png?dl=1\"><h3 id=\"Octopus-Arm\"><a href=\"#Octopus-Arm\" class=\"headerlink\" title=\"Octopus Arm\"></a>Octopus Arm</h3></li>\n</ul>\n</li>\n<li>목표: 6 segments octopus arm (20 action dimensions &amp; 50 state dimensions)을 control하여 target을 맞추는 것<ul>\n<li>COPDAC-Q 사용 시, action space dimension이 큰 octopus arm을 잘 control하여 target을 맞춤</li>\n<li><img src=\"https://www.dropbox.com/s/xrxb0a52wntekld/Screenshot%202018-06-16%2017.46.28.png?dl=1\" width=\"600px\"></li>\n<li>기존 기법들은 action spaces 혹은 action 과 state spaces 둘 다 작은 경우들에 대해서만 실험했다고 하며, 비교하고 있지 않음.<ul>\n<li>기존 기법들이 6 segments octopus arm에서 동작을 잘 안 했을 것 같긴한데, 그래도 실험해서 결과를 보여주지…왜 안 했을까?</li>\n</ul>\n</li>\n<li>8 segment arm 동영상이 저자 홈페이지에 있다고 하는데, 안 보임.</li>\n</ul>\n</li>\n<li>[참고] Octopus Arm 이란?<ul>\n<li><a href=\"https://www.youtube.com/watch?v=AxeeHif0euY\" target=\"_blank\" rel=\"noopener\">OctopusArm Youtube Link</a></li>\n<li><img src=\"https://www.dropbox.com/s/950ycj06sudakjx/Screenshot%202018-06-16%2017.45.52.png?dl=1\"></li>\n</ul>\n</li>\n</ul>\n"},{"title":"High-Dimensional Continuous Control using Generalized Advantage Estimation","date":"2018-07-11T10:18:45.000Z","author":"이동민, 양혁렬","subtitle":"피지여행 7번째 논문","_content":"\n# High-Dimensional Continuous Control using Generalized Advantage Estimation\n\n## 1. Abstract\n현존하는 Policy Gradient Method들의 목적은 누적되는 reward들을 optimization하는 것입니다. 하지만 학습할 때에 많은 양의 sample이 필요로 하고, 들어오는 data가 nonstationarity임에도 불구하고 stable and steady improvement가 어렵습니다.\n\n그래서 이 논문에서는 다음과 같은 방법을 제시합니다.\n- TD($\\lambda$)와 유사한 advantage function의 exponentially-weighted estimator를 사용하여 policy gradient estimate의 variance를 줄이는 것 \n- policy와 value function에 대한 Trust Region Optimization 사용하는 것\n\n3D locomotion tasks에 대한 empirical results는 다음과 같습니다.\n- bipedal and quadrupedal simulated robots의 달리는 자세를 학습\n- bipedal 사람이 땅에 누워있다가 일어서는 것을 학습\n\n## 2. Introduction\n기본적으로 \"parameterized stochastic policy\"를 가정합니다. 이 때 expected total returns의 gradient에 대한 unbiased estimate를 얻을 수 있는데 이것을 REINFORCE라고 부릅니다. 하지만 하나의 action의 결과가 과거와 미래의 action의 결과로 혼동되기 때문에 gradient estimator의 high variance는 시간에 따라 scaling됩니다.\n\n또 다른 방법은 Actor-Critic이 있습니다. 이 방법은 empirical returns보다 하나의 value function을 사용합니다. 또한 bias하고 lower variance를 가진 estimator입니다. 구체적으로 말하자면, high variance하다면 더 sampling을 하면 되는 반면에 bias는 매우 치명적입니다. 다시 말해 bias는 algorithm이 converge하는 데에 실패하거나 또는 local optimum이 아닌 poor solution에 converge하도록 만듭니다.\n\n따라서 이 논문에서는 $\\gamma\\in [0,1]$ and $\\lambda\\in [0,1]$에 의해 parameterized estimation scheme인 Generalized Advantage Estimator(GAE)를 다룹니다.\n\n이 논문을 대략적으로 요약하자면 다음과 같습니다.\n- 효과적인 variance reduction scheme인 GAE를 다룹니다. 또한 실험할 때 batch trust region algorithm에 더하여 다양한 algorithm들에 적용됩니다.\n- value function에 대해 trust region optimization method를 사용합니다. 이렇게 함으로서 더 robust하고 efficient한 방법이 됩니다.\n- A와 B를 합쳐서, 실험적으로 control task에 neural network policies를 learning하는 데에 있어서 효과적인 algorithm을 얻습니다. 이러한 결과는 high-demensional continuous control에 RL을 사용함으로서 state of the art로 확장되었습니다.\n\n## 3. Preliminaries\n먼저 policy optimization의 \"undiscounted formulation\"을 가정합니다. (undiscounted formulation에 주목합시다.)\n\n- initial state $s_0$는 distribution $\\rho_0$으로부터 sampling된 것입니다.\n- 하나의 trajectory ($s_0, a_0, s_1, a_1, ...$)는 terminal state에 도달될 때 까지 policy $a_t$ ~ $\\pi(a_t | s_t)$에 따라서 action을 sampling하고, dynamics $s_{t+1}$ ~ $P(s_{t+1} | s_t, a_t)$에 따라서 state를 sampling함으로써 생성됩니다.\n- reward $r_t = r(s_t, a_t, s_{t+1})$은 매 time step마다 받아집니다.\n- 목표는 모든 policies에 대해 finite하다고 가정됨으로서 expected total reward $\\sum_{t=0}^{\\infty} r_t$를 maximize하는 것입니다.\n\n여기서 중요한 점은 $\\gamma$를 discount의 parameter로 사용하는 것이 아니라 \"bias-variance tradeoff를 조절하는 parameter로 사용한다\" 는 것입니다.\n\npolicy gradient method는 gradient $g := \\nabla_\\theta \\mathbb{E} [\\sum_{t=0}^\\infty r_t]$를 반복적으로 estimate함으로써 expected total reward를 maximize하는 것인데, policy gradient에는 여러 다른 표현들이 있습니다.\n$$g = \\mathbb{E} [\\sum_{t=0}^\\infty \\Phi_t \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$\n여기서 $\\Phi_t$는 아래 중의 하나일 수 있습니다.\n1. $\\sum_{t=0}^\\infty r_t$: trajectory의 total reward\n2. $\\sum_{t'=t}^\\infty r_t'$: action $a_t$ 후의 reward\n3. $\\sum_{t'=t}^\\infty r_t' - b(s_t)$: 2의 baselined version\n4. $Q^\\pi (s_t, a_t)$: state-action value function\n5. $A^\\pi (s_t, a_t)$: advantage function\n6. $r_t + V^\\pi (s_{t+1}) - V^\\pi (s_t)$: TD residual\n\n위의 번호 중 4, 5, 6의 수식들은 다음의 정의를 사용합니다.\n- $V^\\pi (s_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_t:\\infty} [\\sum_{l=0}^\\infty r_{t+1}]$\n- $Q^\\pi (s_t, a_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty} [\\sum_{l=0}^\\infty r_{t+l}]$\n- $A^\\pi (s_t, a_t) := Q^\\pi (s_t, a_t) - V^\\pi (s_t)$, (Advantage function)\n\n추가적으로 colon notation $a : b$는 포괄적인 범위 $(a, a+1, ... , b)$입니다. (잘 기억해둡시다. 뒤에 계속해서 colon notation이 나옵니다.)\n\n여기서부터 parameter $\\gamma$에 대해 좀 더 자세히 알아봅시다. parameter $\\gamma$는 bias하면서 동시에 reward를 downweighting함으로써 variance를 줄입니다. 다시 말해 MDP의 discounted formulation에서 사용된 discounted factor와 일치하지만, 이 논문에서는 \"$\\gamma$를 undiscounted MDP에서 variance reduction parameter\"로 다룹니다. -> 결과는 같지만, 의미와 의도가 다릅니다.\n\ndiscounted value function들은 다음과 같습니다.\n- $V^{\\pi, \\gamma} (s_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_t:\\infty} [\\sum_{l=0}^\\infty \\gamma^l r_{t+l}]$ \n- $Q^\\pi (s_t, a_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty} [\\sum_{l=0}^\\infty \\gamma^l r_{t+l}]$\n- $A^{\\pi, \\gamma} (s_t, a_t) := Q^{\\pi, \\gamma} (s_t, a_t) - V^{\\pi, \\gamma} (s_t)$\n\n따라서 policy gradient에서의 discounted approximation은 다음과 같이 나타낼 수 있습니다.\n$$g^\\gamma := \\mathbb{E}_{s_{0:\\infty} a_{0:\\infty}} [\\sum_{t=0}^\\infty A^{\\pi, \\gamma} (s_t, a_t) \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$\n뒤이어 나오는 section에서는 위의 수식에서 $A^{\\pi, \\gamma}$에 대해 biased (but not too biased) estimator를 얻는 방법에 대해서 나옵니다.\n\n다음으로 advantage function에서 새롭게 접하는 \"$\\gamma$-just estimator\"의 notation에 대해 알아봅시다.\n- 먼저 $\\gamma$-just estimator는 $g^\\gamma$ ${}^1$를 estimate하기 위해 위의 수식에서 $A^{\\pi, \\gamma}$를 $\\gamma$-just estimator로 사용할 때, bias하지 않은 estimator라고 합시다. 그리고 이 $\\gamma$-just advantage estimator를 $\\hat{A}_t (s_{0:\\infty} , a_{0:\\infty})$라고 하고, 전체의 trajectory에 대한 하나의 function이라고 합시다.\n- ${}^1$에서 이 논문의 저자가 하고 싶은 말이 있는 것 같습니다. 개인적으로 $\\gamma$-just estimator를 이해하는 데에 있어서 중요한 주석이라 정확히 해석하고자 합니다.\n    - $A^\\pi$를 $A^{\\pi, \\gamma}$로 사용함으로써 이미 bias하다라고 말했지만, \"이 논문에서 하고자 하는 것은 $g^\\gamma$에 대해 unbiased estimate를 얻고 싶은 것입니다.\" 하지만 undiscounted MDP의 policy gradient에 대해서는 당연히 $\\gamma$를 사용하기 때문에 biased estimate를 얻습니다. 개인적으로 이것은 일단 무시하고 $\\gamma$를 사용할 때 어떻게 unbiased estimate를 얻을 지에 대해 좀 더 포커스를 맞추고 있는 것 같습니다.\n    - 그러니까 저 $A^{\\pi, \\gamma}$를 $\\gamma$-just estimator로 바꿔줌으로써 unbiased estimate를 하고 싶다는 것이 뒤이어 나오는 정의와 명제의 핵심이라고 할 수 있습니다.\n\nDefinition 1.\n먼저 가정을 합니다. (가정을 바탕으로 이루어지는 정의라는 것을 주목합시다.) 만약\n$$\\mathbb{E}_{s_{0:\\infty} a_{0:\\infty}} [\\hat{A}_t (s_{0:\\infty}, a_{0:\\infty}) \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)] = \\mathbb{E}_{s_{0:\\infty} a_{0:\\infty}} [A^{\\pi, \\gamma} (s_t, a_t) \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$\n두 수식이 같다면, estimator $\\hat{A}_t$는 $\\gamma$-just입니다.\n\n그리고 만약 모든 t에 대해서 $\\hat{A}_t$이 $\\gamma$-just이라면, 다음과 같이 표현할 수 있습니다.\n$$\\mathbb{E}_{s_{0:\\infty} a_{0:\\infty}} [\\hat{A}_t (s_{0:\\infty}, a_{0:\\infty}) \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)] = g^\\gamma$$\n위의 수식이 바로 unbiased estimate입니다.\n\n$\\gamma$-just인 $\\hat{A}_t$에 대한 한가지 조건은 $\\hat{A}_t$이 두 가지 function $Q_t$ and $b_t$로 나뉠 수 있다는 것입니다.\n- $Q_t$는 $\\gamma$-discounted Q-function의 unbiased estimator입니다.\n- $b_t$는 action $a_t$전에 sampling된 states and actions의 arbitrary function이다. \n\nProposition 1.\n모든 $(s_t, a_t)$에 대해,\n$$\\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty | s_t, a_t} [Q_t (s_{t:\\infty}, a_{t:\\infty})] = Q^{\\pi, \\gamma} (s_t, a_t)$$\n로 인하여 $\\hat{A}_t$이 \n$$\\hat{A}_{s_{0:\\infty}, a_{0:\\infty}} = Q_t (s_{0:\\infty}, a_{0:\\infty}) - b_t (s_{0:t}, a_{0:t-1})$$\n형태라고 가정합시다. (가정을 바탕으로 이루어지는 명제라는 점을 주목합시다.)\n\n그 때, $\\hat{A}_t$은 $\\gamma$-just입니다.\n\n이 명제에 대한 증명은 Appendix B에 있습니. 그리고 $\\hat{A}_t$에 대한 $\\gamma$-just advantage estimator들은 다음과 같습니다.\n- $\\sum_{l=0}^\\infty \\gamma^l r_{t+1}$\n- $A^{\\pi, \\gamma} (s_t, a_t)$\n- $Q^{\\pi, \\gamma} (s_t, a_t)$\n- $r_t + \\gamma V^{\\pi, \\gamma} (s_{t+1}) - V^{\\pi, \\gamma} (s_t)$\n\n증명을 보기 전에 먼저 Advantage function에 대해서 먼저 살펴봅시다. 아래의 내용은 Sutton이 쓴 논문인 Policy Gradient Methods for Reinforcement Learning with Function Approximation(2000)에서 나온 내용을 리뷰한 것입니다. \n\n<center> <img src=\"https://www.dropbox.com/s/x737yq97ut6gp1a/Screen%20Shot%202018-07-15%20at%201.16.09%20PM.png?dl=1\" width=\"300\"> </center>\n\n여기서는 \"이 수식은 $\\sum_a \\frac{\\partial \\pi (s,a)}{\\partial \\theta} = 0$이기 때문에 가능해진다.\", \"즉, 이 수식은 $\\pi(s,a)$의 gradient에만 dependent하기 때문에 advantage 역할을 하는 함수들을 넣어도 아무런 상관이 없다\"라는 두 문장을 기억해둡시다.\n\n이제 증명을 보면 됩니다.\n\n<center> <img src=\"https://www.dropbox.com/s/e7sj2fwcm1f7hof/figure2.jpg?dl=1\" width=\"300\"> </center>\n<center> <img src=\"https://www.dropbox.com/s/kyk4jes1202az4m/figure3.jpg?dl=1\" width=\"300\"> </center>\n\n위의 증명 수식에서 Q와 b의 각각 세번째 수식과 마지막 수식을 자세히 봅시다.\n\n1. $Q$에 대한 증명\n    - 빨간색 부분\n        - $\\mathbb{E}_{s_{t+1:\\infty}, a_{t+1:\\infty}} [Q_t (s_{0:\\infty}, a_{0:\\infty})]$를 쉽게 말하자면 \"모든(과거, 현재, 미래) state와 action에 대해서 expectation을 $t+1$부터 $\\infty$까지 하겠다.\"라는 말입니다. 이렇게 함으로써 원래는 $Q^\\pi (s_t, a_t)$가 나와야 맞는 건데, 앞에서 살펴봤다시피 4번째 수식 밑줄 친 자리에는 advantage 역할을 하는 함수들을 넣어도 결과값에 아무런 영향을 끼치지 않기 때문에 의도적으로 $A^\\pi (s_t, a_t)$로 바꾼 것입니다. 또한 이 증명을 바탕으로 $\\hat{A}$이 $\\gamma$-just라는 것을 표현하고 싶기 때문이라고 봐도 됩니다.\n\n    - 파란색 부분\n        - 또한 $a_{0:t}$에서 $a_{0:t-1}$로 변한 것은 $A^\\pi (s_t, a_t)$에 baseline이 들어가기 때문에 바꿔준 것입니다.\n\n2. $b$에 대한 증명\n    - 빨간색 부분\n        - $\\mathbb{E}_{s_{t+1:\\infty}, a_{t+1:\\infty}} [\\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$가 0으로 바뀌는 것은 위에서 설명했듯이 $\\nabla_\\theta$자체가 $\\log \\pi_\\theta$만 gradient하기 때문에 이것을 expectation을 취하면 0이 됩니다.\n\n## 4. Advantage Function Estimation\n이번 section에는 discounted advantage function $A^{\\pi, \\gamma} (s_t, a_t)$의 accurate estimate $\\hat{A}_t$에 대해서 살펴봅시다. 이에 따른 수식은 다음과 같습니다.\n\n$$\\hat{g} = \\frac{1}{N} \\sum_{n=1}^N \\sum_{t=0}^\\infty \\hat{A}_t^n \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t^n | s_t^n)$$\n\n여기서 n은 a batch of episodes에 대하여 index한 것입니다.\n\n$V$를 approximate value function이라고 합시다. 그리고 $\\delta_t^V = r_t + \\gamma V(s_{t+1}) - V(s_t)$이라고 합시다.\n\n만약 (이전과 마찬가지로 가정부터 합니다.) correct value function $V = V^{\\pi, \\gamma}$가 있다고 한다면, 이것은 $\\gamma$-just advantage estimator입니다. 실제로, $A^{\\pi, \\gamma}$의 unbiased estimator는 다음과 같습니다.\n$$\\mathbb{E}_{s_{t+1}} [\\delta_t^{V^{\\pi, \\gamma}}] = \\mathbb{E}_{s_{t+1}} [r_t + \\gamma V^{\\pi, \\gamma} (s_{t+1}) - V^{\\pi, \\gamma} (s_t)]$$\n$$= \\mathbb{E}_{s_{t+1}} [Q^{\\pi, \\gamma} (s_t, a_t) - V^{\\pi, \\gamma} (s_t)] = A^{\\pi, \\gamma} (s_t, a_t)$$\n그러나, \"이 estimator는 유일하게 $V = V^{\\pi, \\gamma}$에 대한 $\\gamma$-just입니다.\" 다른 경우라면 이것은 biased policy gradient estimate일 것입니다. (우리가 하고 싶은 것은 $V$에 대해서만 unbiased estimator가 아니라 advantage function에 대해서 일반화된 unbiased estimator를 얻고 싶은 것입니다. 그래서 아래에서도 나오겠지만, $\\gamma$와 함께 $\\lambda$를 이용한 estimator가 나옵니다.ㅏ)\n\n그래서 $\\delta$에 대해 $k$의 sum으로 생각해봅시다. 이것을 $\\hat{A}_t^{(k)}$라고 하자. 그러면 아래와 같이 표현할 수 있습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/ra7hxksveg2hz45/figure4.jpg?dl=1\" width=\"300\"> </center>\n\n$\\hat{A}_t^{(k)}$은 returns의 $k$-step estimate와 연관지을 수 있고, $\\delta_t^V = \\hat{A}_t^{(1)}$의 case와 유사하게도 $\\hat{A}_t^{(k)}$를 advantage function의 estimator로 생각할 수 있습니다.\n\n여기서 $k \\rightarrow \\infty$로 생각해보면 bias가 일반적으로 점점 더 작아집니다. 왜냐하면 $\\gamma^k V(s_{t+k})$가 점점 많이 discounted되서 $-V(s_t)$가 bias에 영향을 미치지 못하기 때문입니다. $k \\rightarrow \\infty$를 취하면 다음과 같은 수식이 나옵니다.\n\n$$\\hat{A}_t^{(\\infty)} = \\sum_{l=0}^\\infty \\gamma^l \\delta_{t+l}^V = -V(s_t) + \\sum_{l=0}^\\infty \\gamma^l r_{t+l}$$\n우변의 수식과 같이 empirical returns에서 value function baseline을 뺀 것으로 나타낼 수 있습니다.\n\nGeneralized Advantage Estimator GAE($\\gamma, \\lambda$)는 $k$-step estimators의 exponentially-weighted average로 나타낼 수 있습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/yg1ybmfkep3towu/figure5.jpg?dl=1\" width=\"300\"> </center>\n\n(TD($\\lambda$)를 떠올려주세요..!)\n\n위의 마지막 수식에서도 알 수 있듯이, advantage estimator는 Bellman residual terms의 discounted sum과 관련있는 간단한 수식입니다. 다음 section에서 modified reward function을 가진 MDP에 returns로서의 관점에서 위의 마지막 수식을 더 자세히 살펴봅시다. 위의 수식은 TD($\\lambda$)와 많이 유사합니다. 그러나 TD($\\lambda$)는 value function를 estimator하고, 여기서는 advantage function을 estimator합니다.\n\n위의 수식에서 $\\lambda = 0$ and $\\lambda = 1$에 대해서는 특별한 case가 존재한다. 수식으로 표현하면 다음과 같습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/ufglxzanhcnbi36/figure6.jpg?dl=1\" width=\"300\"> </center>\n\n- $GAE(\\gamma, 1)$은 $V$의 정확도와 관계없이 $\\gamma$-just입니다. 그러나 returns의 sum때문에 high variance합니다.\n- $GAE(\\gamma, 0)$은 $V = V^{\\pi, \\gamma}$에 대해 $\\gamma$-just입니다. 그리고 bias하지만 일반적으로 훨씬 lower variance를 가집니다. \n- $0 < \\lambda < 1$에 대해 GAE는 parameter $\\lambda$를 control하여 bias와 variance사이에 compromise를 만듭니다.\n\n두 가지 별개의 parameter $\\gamma$ and $\\lambda$를 가진 advantage estimator는 bias-variance tradeoff에 도움을 줍니다. 그러나 이 두 가지 parameter는 각각 다른 목적을 가지고 작동합니다.\n- $\\gamma$는 가장 중요하게 value function $V^{\\pi, \\gamma}$ 의 scale을 결정합니다. 또한 $\\gamma$는 $\\lambda$에 의존하지 않습니다. $\\gamma < 1$로 정하는 것은 policy gradient estimate에서 bias합니다.\n- 반면에, $\\lambda < 1$는 유일하게 value function이 부정확할 때 bias합니다. 그리고 경험상, $\\lambda$의 best value는 $\\gamma$의 best value보다 훨씬 더 낮습니다. 왜냐하면 $\\lambda$가 정확한 value function에 대해 $\\gamma$보다 훨씬 덜 bias하기 때문입니다.\n\nGAE를 사용할 때, $g^\\gamma$의 biased estimator를 구성할 수 있습니다. 수식은 다음과 같습니다.\n$$g^\\gamma \\approx \\mathbb{E} [\\sum_{t=0}^\\infty] \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) \\hat{A}_t^{GAE(\\gamma, \\lambda)}] = \\mathbb{E} [\\sum_{t=0}^{\\infty} \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) \\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+1}^V]$$\n여기서 $\\lambda = 1$일 때 동일해집니다.\n\n## 5. Interpretation as Reward Shaping\n이번 section에서는 앞서 다뤘던 수식 $\\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^V$를 modified reward function의 MDP의 관점으로 생각해봅시다. 조금 더 구체적으로 말하자면, MDP에서 reward shaping transformation을 실행한 후에 적용된 extra discounted factor로서 $\\lambda$를 어떻게 볼 것인지에 대해서 다룹니다. \n\n(개인적인 comment) 한 가지 먼저 언급하자면, 본래의 목적은 reward shaping이 아니라 variance reduction입니다. 이번 section은 그저 이전에 이러한 개념이 있었고, gae를 다른 관점에서 생각해보자라는 뜻에서 나온 section인 것 같습니다. '아~ 이러한 개념이 있구나~!' 정도로만 알면 될 것 같습니다. 'gae가 reward shaping의 효과까지 있어서 이러한 section을 넣은 것일까?' 라는 생각도 해봤지만 아직 잘 모르겠습니다. 실험부분에도 딱히 하지 않은 걸로 봐서는.. 아닌 것 같기도 하고.. 아직까지는 그저 'reward shaping의 관점에서 봤을 때에도 gae로 만들어줄 수 있다.' 정도만 생각하려고 합니다.\n\nReward Shaping이란 개념 자체는 일반적인 알고리즘 분야(최단경로 문제 등)에도 있습니다. 하지만 이 개념이 머신러닝에 적용된 건 Andrew Y. Ng이 쓴 Policy Invariance under Reward Transformations Theory and Application to Reward Shaping(1999) 논문입니다. 논문에서 Reward Shaping을 설명하는 부분은 간략하게 다음과 같습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/0w9cuxjkhz2mbcy/figure7.jpg?dl=1\" width=\"300\"> </center>\n<center> <img src=\"https://www.dropbox.com/s/xcp9eqvcbv3268g/figure8.jpg?dl=1\" width=\"300\"> </center>\n\n위의 글만 보고 이해가 잘 안됩니다. 그래서 다른 자료들을 찾던 중에 다음의 그림을 찾게 되었습니다. (아래의 그림은 Youtube에 있는 [Udacity 영상](https://www.youtube.com/watch?v=xManAGjbx2k&t=95s)에서 있는 그림입니다.)\n\n<center> <img src=\"https://www.dropbox.com/s/87kacngez9cl2e6/figure9.jpg?dl=1\" width=\"300\"> </center>\n\n이 그림을 통해서 Reward Shaping을 이해해봅시다.\n\nReward Shaping(보상 형성)을 통해서 뭘 하고 싶은지 목적부터 봅시다. reward space가 sparse 한 경우에 reward가 너무 드문드문 나옵니다. 따라서 이것을 꾸준히 reward를 받을 수 있도록 바꿉니다. potential-based shaping function인 $\\Phi$를 만들어서 더하고 빼줍니다. 저 $\\Phi$ 자리에는 대표적으로 state value function이 많이 들어간다고 합니다. (아직 목적이 이해가 안될 수 있습니다. Reward Shaping에 대해서 다 보고 나서 다시 한 번 읽어봅시다.)\n\n위의 그림은 돌고래가 점프하여 불구멍을 통과해야하는 환경입니다. 하지만 저 불구멍을 통과하기 위해서는 (1) 점프도 해야하고, (2) 불도 피해야하고, (3) 알맞게 착지까지 완료해야합니다. 이렇게 해야 reward를 +1 얻습니다. \n\n생각해봅시다. 어느 세월에 점프도 해야하고.. 불도 피해야하고.. 알맞게 착지까지해서 reward를 +1 얻겠습니까? 따라서 그 전에도 잘하고 있는 지, 못하고 있는 지를 판단하기 위해, 다시 말해 reward를 꾸준히 받도록 다음과 같이 transformed reward function $\\tilde{r}$을 정의합니다.\n\n$$\\tilde{r} (s, a, s') = r(s, a, s') + \\gamma \\Phi (s') - \\Phi (s)$$\n\n- 여기서 $\\Phi : S \\rightarrow \\mathbb{R}$를 state space에서의 arbitrary scalar-valued function을 나타냅니다. 그리고 $\\Phi$자리에는 대표적으로 state value function이 들어간다고 생각합시다.\n- 형태가 TD residual term의 결과와 비슷하지만 의미와 의도가 다릅니다. reward shaping은 sparse reward 때문이고, 이전 section에서 봤던 gae는 variance reduction때문에 나온 것입니다.\n\n<center> <img src=\"https://www.dropbox.com/s/tkfpqniaazwfbld/figure10.jpg?dl=1\" width=\"300\"> </center>\n\n이 transformation은 discounted advantage function $A^{\\pi, \\gamma}$으로도 둘 수 있습니다. state $s_t$를 시작하여 하나의 trajectory의 rewards의 discounted sum을 표현하면 다음과 같습니다.\n$$\\sum_{l=0}^\\infty \\gamma^l \\tilde{r} (s_{t+l}, a_t, s_{t+l+1}) = \\sum_{l=0}^\\infty \\gamma^l r(s_{t+l}, a_{t+l}, s_{t+l+1}) - \\Phi(s_t)$$\n\n$\\tilde{Q}^{\\pi, \\gamma}, \\tilde{V}^{\\pi, \\gamma}, \\tilde{A}^{\\pi, \\gamma}$를  transformed MDP의 value function과 advantage function이라고 하면, 다음과 같은 수식이 나옵니다.\n$$\\tilde{Q}^{\\pi, \\gamma} (s, a) = Q^{\\pi, \\gamma} (s, a) - \\Phi (s)$$\n$$\\tilde{V}^{\\pi, \\gamma} (s, a) = V^{\\pi, \\gamma} (s, a) - \\Phi (s)$$\n$$\\tilde{A}^{\\pi, \\gamma} (s, a) = (Q^{\\pi, \\gamma} (s, a) - \\Phi (s)) - (V^\\pi (s) - \\Phi (s)) = A^{\\pi, \\gamma} (s, a)$$\n\n이제 reward shaping의 idea를 가지고 어떻게 policy gradient estimate를 얻을 수 있는 지에 대해서 알아봅시다. \n- $0 \\le \\lambda \\le 1$의 범위에 있는 steeper discount $\\gamma \\lambda$를 사용합니다.\n- shaped reward $\\tilde{r}$는 Bellman residual term $\\delta^V$와 동일합니다.\n- 그리고 $\\Phi = V$와 같다고 보면 다음과 같은 수식이 나옵니다.\n$$\\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\tilde{r} (s_{t+l}, a_t, s_{t+l+1}) = \\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^V = \\hat{A}_t^{GAE(\\gamma, \\lambda)}$$\n이렇게 shaped rewards의 $\\gamma \\lambda$-discounted sum을 고려함으로써 GAE를 얻을 수 있습니다. 더 정확하게 $\\lambda = 1$은 $g^\\gamma$의 unbiased estimate이고, 반면에 $\\lambda < 1$은 biased estimate입니다.\n\n좀 더 나아가서, shaping transformation과 parameters $\\gamma$ and $\\lambda$의 결과를 보기 위해, response function $\\chi$를 이용하면 다음과 같은 수식이 나옵니다.\n$$\\chi (l; s_t, a_t) = \\mathbb{E} [r_{t+l} | s_t, a_t] - \\mathbb{E} [r_{t+l} | s_t]$$\n추가적으로 $A^{\\pi, \\gamma} (s, a) = \\sum_{l=0}^\\infty \\gamma^l \\chi (l; s, a)$입니다.\n그래서 discounted policy gradient estimator는 다음과 같이 쓸 수 있다.\n$$\\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) A^{\\pi, \\gamma} (s_t, a_t) = \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) \\sum_{l=0}^\\infty \\gamma^l \\chi (l; s, a)$$\n\n## 6. Value Fuction Estimation\n이번 section에서는 Trust Region Optimization Scheme에 따라 Value function을 Estimation합니다.\n- \n### 6.1 Simplest approach\n$ minimize_{\\phi} \\sum_{n=1}^{N} \\vert\\vert V_{\\phi}(s_n) - \\hat{V_n} \\vert\\vert^{2} $$\n\n- 위는 가장 간단하게 non-linear approximation 으로 푸는 방법입니다. \n- $\\hat{V_t} = \\sum_{l=0}^{\\infty}\\gamma^l r_{t+l}$ 은 reward 에 대한 discounted sum 을 의미합니다. \n\n### 6.2 Trust region method to optimize the value function\n\n- Value function 을 최적화 하기 위해 trust region method 를 사용합니다. \n- Trust region 은 최근 데이터에 대해 overfitting 되는 것을 막아줍니다. \n\nTrust region 문제를 풀기 위해서는 다음 스텝을 따릅니다.\n\n- $\\sigma^2 = \\frac{1}{N} \\sum_{n=1}^{N} \\vert\\vert V_{\\phi old}(s_n) - \\hat{V_n} \\vert\\vert^{2}$ 을 계산합니다. \n- 그 후에 다음과 같은 constrained opimization 문제를 풉니다. \n\n$$minimize_{\\phi} \\, \\sum_{n=1}^N \\parallel V_\\phi (s_n) - \\hat{V}_n \\parallel^2$$\n$$subject \\, \\, to \\, \\frac{1}{N} \\sum_{n=1}^N \\frac{\\parallel V_\\phi (s_n) - V_{\\phi old} (s_n) \\parallel^2}{2 \\sigma^2} \\le \\epsilon$$\n\n- 위의 수식은 사실 old Value function과 새로운 Value function KL distance 가 $\\epsilon$ 보다 작아야한다는 수식과 같습니다. Value function이 평균은 $V_{\\phi}(s)$ 이고 분산이 $\\sigma^2$인 conditional Gaussian distribution 으로 parameterize되었을 뿐입니다. \n<img width =\"400px\" src=\"https://www.dropbox.com/s/uw0v05feu8chqmc/Screenshot%202018-07-08%2010.04.38.png?dl=1\"> \n\n이 trust region 문제의 답을 conjudate gradient algorithm 을 이용하여 근사값을 구할 수 있습니다. 특히, 다음과 같은  quadratic program 을 풀게됩니다. \n\n$$minimize_{\\phi} \\, g^T (\\phi - \\phi_{old})$$\n$$subject \\, \\, to \\, \\frac{1}{N} \\sum_{n=1}^N (\\phi - \\phi_{old})^T H(\\phi - \\phi_{old}) \\le \\epsilon$$\n- 여기서 $g$는 objective 의 gradient입니다. \n- $j_n = \\nabla_{\\phi} V_{\\phi}(s_n)$ 일때, $H = \\frac{1}{N} \\sum_{n} j_n j^T_n$ 이며, $H$는 objective의 hessian에 대해서 gaussian newton method 로 근사한 값입니다. 따라서, value function 을 conditional probability로 해석한다면 Fisher information matrix가 됩니다. \n- 구현할때의 방법은 TRPO 에서 사용한 방법과 모두 같습니다. \n\n## 7.Experiments\n\n실험은 다음 두 가지 물음에 대해서 디자인 되었습니다.\n\n- GAE 에 따라서 episodic total reward 를 최적화 할때, $\\lambda$와 $\\gamma$ 가 변함에 따라서 어떤 경험적인 효과를 볼 수 있는지 ?\n- GAE 와 trust region alogorithm 을 policy 와 value function 모두에 함께 사용했을 때 어려운 문제에 적용되는 큰 뉴럴넷을 최적화 할 수 있을까 ?\n\n### 6.1 Policy Optimization Algorithm\n\nPolicy update는 TRPO 를 사용합니다. TRPO 에 대한 설명은 여기서는 생략하겠습니다. TRPO 포스트를 보고 돌아와주세요 !\n\n- 이전 TRPO에서 이미 TRPO 와 다른 많은 알고리즘들을 비교하였기 때문에, 여기서는 똑같은 짓을 반복하지 않고 $\\lambda$, $\\gamma$ 가 변함에 따라 어떤 영향이 있는 지에 대한 실험에 집중하겠다고 합니다. (귀찮았던거죠)\n\n- TRPO 를 적용한 GAE의 최종 알고리즘은 다음과 같습니다.\n<center> <img width = \"600px\" src=\"https://www.dropbox.com/s/b1klz11f2frrvg4/Screenshot%202018-07-08%2010.33.54.png?dl=1\"> </center>\n<center> <img width = \"300px\" src=\"https://www.dropbox.com/s/35fxte05pqtiel7/Screenshot%202018-07-08%2010.35.22.png?dl=1\"> </center>\n<center> <img width = \"300px\" src=\"https://www.dropbox.com/s/u35pjow3w50bvz1/Screenshot%202018-07-08%2010.36.03.png?dl=1\"> </center>\n\n\n- 여기서 주의할 점은 Policy update ($\\theta_i \\rightarrow \\theta_{i+1}$)에서 $V_{\\phi_i}$ 를 사용했다는 점입니다. \n- 만약 Value function을 먼저 update하게 된다면 추가적인 bias가 발생합니다.\n- 극단적으로 생각해보아서, 우리가 Value function을 완벽하게 overfit 해낸다면 Bellman residual ($r_t + \\gamma V(s_{t+1}) - V(S_t)$)은 0이 됩니다. 그럼 Policy gradient의 estimation도 거의 0이 될 것입니다. \n\n    \n### 7.2 Expermint details\n\n#### 7.2.1 Environment \n실험에서 사용된 환경은 다음 네 가지 입니다. \n\n1. classic cart-pole (x 3D)\n2. bipedal locomotion \n3. quadrupedal locomotion \n4. dynamically standing up for the biped\n\n#### 7.2.2 Architecture\n\n- 3D robot task에 대해서는 같은 모델을 사용하였습니다. \n    - layers  = [100, 50, 25] 각각 tanh 사용.(Policy와 Value 네트워크 모두)\n    - Final output layer은 linear\n- Cartpole 에 대해서는 1개의 layer 안에 20개의 hidden unit 만 있는 linear policy를 사용했다고 합니다. \n\n#### 7.2.3 Task\n\n- Cartpole\n    - 한 배치당 20 개의 trajectory 를 모았고, maximum length 는 1000 입니다. \n- 3D biped locomotion\n    - 33 dim state , 10 dim action\n    - 50000 time step per batch\n- 3D quadruped locomotion\n    - 29 dim state, 8 dim action\n    - 200000 time step per batch \n- 3D biped locomotion Standing\n    - 33 dim state , 10 dim action\n    - 200000 time step per batch \n\n\n#### 7.2.3 results\ncost 의 관점에서 결과를 나타내었다고 합니다. Cost 는 negative reward와 이것이 최소화 되었는가로 정의되었다고 하는데, 정확히는 안나와있습니다. \n\n##### 7.2.3.1 Cartpole\n\n<center> <img width = \"500px\" src=\"https://www.dropbox.com/s/x9pbms1wvg38lda/Screenshot%202018-07-08%2011.08.22.png?dl=1\"> </center>\n\n- 왼쪽 그림은 $\\gamma$ 를 0.99로 고정시켜놓은 상태에서 $\\lambda$ 를 변화시킴에 따라서 cost 를 측정한 것입니다. \n- 오른쪽은 $\\gamma$ 와 $\\lambda$ 를 둘 다 변화 시키면서 performance 를 그림으로 나타낸 표입니다. 흰색에 가까울 수록 좋은 퍼포먼스입니다. \n\n##### 7.2.3.2 3D BIPEDAL LOCOMOTINO\n<center> <img width = \"500px\" src=\"https://www.dropbox.com/s/i9wj4p6ijojsy82/Screenshot%202018-07-08%2011.25.08.png?dl=1\"> </center>\n\n\n- 다른 random seed 로 부터 9번 씩 시도한 결과를 mean 을 취해서 사용합니다.\n- Best performance 는   $\\gamma \\in [0.99, 0.995]$ 그리고  $\\lambda \\in [0.96, 0.99]$ 일때. \n- 1000 iteration 후에 빠르고 부드럽고 안정적인 걸음거이가 나옵니다.\n- 실제로 걸린 시간은 0.01(타입스텝당 시간) * 50000(배치당 타임스텝) * 1000(배치) * 3600(초->시간) * 24 = 5.8일 정도가 걸렸습니다.\n\n##### 7.2.3.3 다른 ROBOT TASKS\n<center> <img width = \"500px\" src=\"https://www.dropbox.com/s/fuuat65we52quht/Screenshot%202018-07-08%2011.33.11.png?dl=1\"> </center>\n\n- 다른 로봇 TASK에 대해서는 아주 제한적인 실험만 진행합니다.(시간이 부족했던듯)\n- Quadruped 에 대해서는 $\\gamma$ =0.995  로 fix, $\\lambda \\in ${0, 0.96}\n- Standingup 에 대해서는 $\\gamma$ =0.99  로 fix, $\\lambda \\in ${0, 0.96}\n\n\n## 8. Discussion\n\n### 8.1 Main discussion\n지금까지 복잡하고 어려운 control problem에서 Reinforcement Learning(RL)은 high sample complexity 때문에 제한이 되어왔습니다. 따라서 이 논문에서 그 제한을 풀고자 advantage function의 good estimate를 얻는 \"variance reduction\"에 대해 연구하였습니다.\n\n\"Generalized Advantage Estimator(GAE)\"라는 것을 제안했고, 이것은 bias-variance tradeoff를 조절하는 두 개의 parameter $\\gamma,\\lambda$를 가집니다.\n또한 어떻게 Trust Region Policy Optimization과 value function을 optimize하는 Trust Region Algorithm의 idea를 합치는 지를 보였습니다.\n\n이렇게 함으로써 보다 더 복잡하고 어려운 control task들을 해결할 수 있었습니다.\n\nGAE의 실험적인 입증으로는 robotic locomotion을 simulation하는 domain입니다. 실험에서도 보여준 것처럼 [0.9, 0.99]의 범위에서 $\\lambda$의 적절한 중간의 값을 통해 best performance를 얻습니다. 좀 더 나아가 연구되어야할 점은 adaptive or automatic하도록 estimator parameter $\\gamma,\\lambda$를 조절하는 방법입니다.\n\n### 8.2 Future work\n\nValue function estimation error 와 Policy gradient estimation error 사이의 관계를 알아낸다면, 우리는 Value function fitting 에 더 잘 맞는 error metric 을 사용할 수 있습니다. (policy gradient estimation 의 정확성과 더 잘 맞는 value function)\n\nPolicy와 Value function의 파라미터를 공유하는 모델을 만드는 것은 아주 흥미롭고 이점이 많습니다. 하지만 수렴을 보장하도록 적절한 numerical optimization을 제시하여야 합니다.\n\nDDPG는 별로입니다. TD(0) 는 bias 가 너무 크고, poor performance 로 이끕니다. 특히나 이 paper 에서는 low-dimention 의 쉬운 문제들만 해결했습니다.\n\n<center> <img width = \"500px\" src=\"https://www.dropbox.com/s/nhc7t9psul5lr3x/Screenshot%202018-07-08%2011.45.15.png?dl=1\"> </center>\n\n### 8.3 FAQ \n\n- Compatible features 와는 무슨 관계 ?\n     - Compatible features 는 value function을 이용하는 policy gradient 알고리즘들과 함께 자주 언급됩니다.\n     - Actor Critic 의 저자는 policy의 제한된 representation power 때문에, policy gradient 는 단지 advantage function space 의 subspace에만 의존하게 됩니다.\n     - 이 subspace는 compatible features 에 의해 span 됩니다.\n     - 이 이론은 현재 문제 구조를 어떻게 이용해야 advantage function에 대해 더 나은 estimation 을 할 수 있는 지에 대한 지침을 주지 않습니다. GAE paper 의 idea 와 orthogonal 합니다. \n- 왜 Q function을 사용하지 않는가 ?\n     - 먼저 state-value function이 더 낮은 차원의 input 을 가진다. 그래서 Q function보다 더 배우기 습니다.\n     - 두 번째로 이 페이퍼에서 제안하는 방법으로는 high bias estimator 에서 low bias estimator 로 $\\lambda$를 통해서 부드럽게 interpolate 할 수 있습니다. \n     - 반면에 Q 를 사용하면 단지 high-bias estimator 밖에 사용할 수 없습니다. \n     - 특히나 return에 대한 one-step estimation 은 엄두를 못낼 정도로 bias 가 큽니다.\n","source":"_posts/gae.md","raw":"---\ntitle: High-Dimensional Continuous Control using Generalized Advantage Estimation\ndate: 2018-07-11 19:18:45\ntags: [\"프로젝트\", \"피지여행\"]\ncategories: 프로젝트\nauthor: 이동민, 양혁렬\nsubtitle: 피지여행 7번째 논문\n---\n\n# High-Dimensional Continuous Control using Generalized Advantage Estimation\n\n## 1. Abstract\n현존하는 Policy Gradient Method들의 목적은 누적되는 reward들을 optimization하는 것입니다. 하지만 학습할 때에 많은 양의 sample이 필요로 하고, 들어오는 data가 nonstationarity임에도 불구하고 stable and steady improvement가 어렵습니다.\n\n그래서 이 논문에서는 다음과 같은 방법을 제시합니다.\n- TD($\\lambda$)와 유사한 advantage function의 exponentially-weighted estimator를 사용하여 policy gradient estimate의 variance를 줄이는 것 \n- policy와 value function에 대한 Trust Region Optimization 사용하는 것\n\n3D locomotion tasks에 대한 empirical results는 다음과 같습니다.\n- bipedal and quadrupedal simulated robots의 달리는 자세를 학습\n- bipedal 사람이 땅에 누워있다가 일어서는 것을 학습\n\n## 2. Introduction\n기본적으로 \"parameterized stochastic policy\"를 가정합니다. 이 때 expected total returns의 gradient에 대한 unbiased estimate를 얻을 수 있는데 이것을 REINFORCE라고 부릅니다. 하지만 하나의 action의 결과가 과거와 미래의 action의 결과로 혼동되기 때문에 gradient estimator의 high variance는 시간에 따라 scaling됩니다.\n\n또 다른 방법은 Actor-Critic이 있습니다. 이 방법은 empirical returns보다 하나의 value function을 사용합니다. 또한 bias하고 lower variance를 가진 estimator입니다. 구체적으로 말하자면, high variance하다면 더 sampling을 하면 되는 반면에 bias는 매우 치명적입니다. 다시 말해 bias는 algorithm이 converge하는 데에 실패하거나 또는 local optimum이 아닌 poor solution에 converge하도록 만듭니다.\n\n따라서 이 논문에서는 $\\gamma\\in [0,1]$ and $\\lambda\\in [0,1]$에 의해 parameterized estimation scheme인 Generalized Advantage Estimator(GAE)를 다룹니다.\n\n이 논문을 대략적으로 요약하자면 다음과 같습니다.\n- 효과적인 variance reduction scheme인 GAE를 다룹니다. 또한 실험할 때 batch trust region algorithm에 더하여 다양한 algorithm들에 적용됩니다.\n- value function에 대해 trust region optimization method를 사용합니다. 이렇게 함으로서 더 robust하고 efficient한 방법이 됩니다.\n- A와 B를 합쳐서, 실험적으로 control task에 neural network policies를 learning하는 데에 있어서 효과적인 algorithm을 얻습니다. 이러한 결과는 high-demensional continuous control에 RL을 사용함으로서 state of the art로 확장되었습니다.\n\n## 3. Preliminaries\n먼저 policy optimization의 \"undiscounted formulation\"을 가정합니다. (undiscounted formulation에 주목합시다.)\n\n- initial state $s_0$는 distribution $\\rho_0$으로부터 sampling된 것입니다.\n- 하나의 trajectory ($s_0, a_0, s_1, a_1, ...$)는 terminal state에 도달될 때 까지 policy $a_t$ ~ $\\pi(a_t | s_t)$에 따라서 action을 sampling하고, dynamics $s_{t+1}$ ~ $P(s_{t+1} | s_t, a_t)$에 따라서 state를 sampling함으로써 생성됩니다.\n- reward $r_t = r(s_t, a_t, s_{t+1})$은 매 time step마다 받아집니다.\n- 목표는 모든 policies에 대해 finite하다고 가정됨으로서 expected total reward $\\sum_{t=0}^{\\infty} r_t$를 maximize하는 것입니다.\n\n여기서 중요한 점은 $\\gamma$를 discount의 parameter로 사용하는 것이 아니라 \"bias-variance tradeoff를 조절하는 parameter로 사용한다\" 는 것입니다.\n\npolicy gradient method는 gradient $g := \\nabla_\\theta \\mathbb{E} [\\sum_{t=0}^\\infty r_t]$를 반복적으로 estimate함으로써 expected total reward를 maximize하는 것인데, policy gradient에는 여러 다른 표현들이 있습니다.\n$$g = \\mathbb{E} [\\sum_{t=0}^\\infty \\Phi_t \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$\n여기서 $\\Phi_t$는 아래 중의 하나일 수 있습니다.\n1. $\\sum_{t=0}^\\infty r_t$: trajectory의 total reward\n2. $\\sum_{t'=t}^\\infty r_t'$: action $a_t$ 후의 reward\n3. $\\sum_{t'=t}^\\infty r_t' - b(s_t)$: 2의 baselined version\n4. $Q^\\pi (s_t, a_t)$: state-action value function\n5. $A^\\pi (s_t, a_t)$: advantage function\n6. $r_t + V^\\pi (s_{t+1}) - V^\\pi (s_t)$: TD residual\n\n위의 번호 중 4, 5, 6의 수식들은 다음의 정의를 사용합니다.\n- $V^\\pi (s_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_t:\\infty} [\\sum_{l=0}^\\infty r_{t+1}]$\n- $Q^\\pi (s_t, a_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty} [\\sum_{l=0}^\\infty r_{t+l}]$\n- $A^\\pi (s_t, a_t) := Q^\\pi (s_t, a_t) - V^\\pi (s_t)$, (Advantage function)\n\n추가적으로 colon notation $a : b$는 포괄적인 범위 $(a, a+1, ... , b)$입니다. (잘 기억해둡시다. 뒤에 계속해서 colon notation이 나옵니다.)\n\n여기서부터 parameter $\\gamma$에 대해 좀 더 자세히 알아봅시다. parameter $\\gamma$는 bias하면서 동시에 reward를 downweighting함으로써 variance를 줄입니다. 다시 말해 MDP의 discounted formulation에서 사용된 discounted factor와 일치하지만, 이 논문에서는 \"$\\gamma$를 undiscounted MDP에서 variance reduction parameter\"로 다룹니다. -> 결과는 같지만, 의미와 의도가 다릅니다.\n\ndiscounted value function들은 다음과 같습니다.\n- $V^{\\pi, \\gamma} (s_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_t:\\infty} [\\sum_{l=0}^\\infty \\gamma^l r_{t+l}]$ \n- $Q^\\pi (s_t, a_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty} [\\sum_{l=0}^\\infty \\gamma^l r_{t+l}]$\n- $A^{\\pi, \\gamma} (s_t, a_t) := Q^{\\pi, \\gamma} (s_t, a_t) - V^{\\pi, \\gamma} (s_t)$\n\n따라서 policy gradient에서의 discounted approximation은 다음과 같이 나타낼 수 있습니다.\n$$g^\\gamma := \\mathbb{E}_{s_{0:\\infty} a_{0:\\infty}} [\\sum_{t=0}^\\infty A^{\\pi, \\gamma} (s_t, a_t) \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$\n뒤이어 나오는 section에서는 위의 수식에서 $A^{\\pi, \\gamma}$에 대해 biased (but not too biased) estimator를 얻는 방법에 대해서 나옵니다.\n\n다음으로 advantage function에서 새롭게 접하는 \"$\\gamma$-just estimator\"의 notation에 대해 알아봅시다.\n- 먼저 $\\gamma$-just estimator는 $g^\\gamma$ ${}^1$를 estimate하기 위해 위의 수식에서 $A^{\\pi, \\gamma}$를 $\\gamma$-just estimator로 사용할 때, bias하지 않은 estimator라고 합시다. 그리고 이 $\\gamma$-just advantage estimator를 $\\hat{A}_t (s_{0:\\infty} , a_{0:\\infty})$라고 하고, 전체의 trajectory에 대한 하나의 function이라고 합시다.\n- ${}^1$에서 이 논문의 저자가 하고 싶은 말이 있는 것 같습니다. 개인적으로 $\\gamma$-just estimator를 이해하는 데에 있어서 중요한 주석이라 정확히 해석하고자 합니다.\n    - $A^\\pi$를 $A^{\\pi, \\gamma}$로 사용함으로써 이미 bias하다라고 말했지만, \"이 논문에서 하고자 하는 것은 $g^\\gamma$에 대해 unbiased estimate를 얻고 싶은 것입니다.\" 하지만 undiscounted MDP의 policy gradient에 대해서는 당연히 $\\gamma$를 사용하기 때문에 biased estimate를 얻습니다. 개인적으로 이것은 일단 무시하고 $\\gamma$를 사용할 때 어떻게 unbiased estimate를 얻을 지에 대해 좀 더 포커스를 맞추고 있는 것 같습니다.\n    - 그러니까 저 $A^{\\pi, \\gamma}$를 $\\gamma$-just estimator로 바꿔줌으로써 unbiased estimate를 하고 싶다는 것이 뒤이어 나오는 정의와 명제의 핵심이라고 할 수 있습니다.\n\nDefinition 1.\n먼저 가정을 합니다. (가정을 바탕으로 이루어지는 정의라는 것을 주목합시다.) 만약\n$$\\mathbb{E}_{s_{0:\\infty} a_{0:\\infty}} [\\hat{A}_t (s_{0:\\infty}, a_{0:\\infty}) \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)] = \\mathbb{E}_{s_{0:\\infty} a_{0:\\infty}} [A^{\\pi, \\gamma} (s_t, a_t) \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$\n두 수식이 같다면, estimator $\\hat{A}_t$는 $\\gamma$-just입니다.\n\n그리고 만약 모든 t에 대해서 $\\hat{A}_t$이 $\\gamma$-just이라면, 다음과 같이 표현할 수 있습니다.\n$$\\mathbb{E}_{s_{0:\\infty} a_{0:\\infty}} [\\hat{A}_t (s_{0:\\infty}, a_{0:\\infty}) \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)] = g^\\gamma$$\n위의 수식이 바로 unbiased estimate입니다.\n\n$\\gamma$-just인 $\\hat{A}_t$에 대한 한가지 조건은 $\\hat{A}_t$이 두 가지 function $Q_t$ and $b_t$로 나뉠 수 있다는 것입니다.\n- $Q_t$는 $\\gamma$-discounted Q-function의 unbiased estimator입니다.\n- $b_t$는 action $a_t$전에 sampling된 states and actions의 arbitrary function이다. \n\nProposition 1.\n모든 $(s_t, a_t)$에 대해,\n$$\\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty | s_t, a_t} [Q_t (s_{t:\\infty}, a_{t:\\infty})] = Q^{\\pi, \\gamma} (s_t, a_t)$$\n로 인하여 $\\hat{A}_t$이 \n$$\\hat{A}_{s_{0:\\infty}, a_{0:\\infty}} = Q_t (s_{0:\\infty}, a_{0:\\infty}) - b_t (s_{0:t}, a_{0:t-1})$$\n형태라고 가정합시다. (가정을 바탕으로 이루어지는 명제라는 점을 주목합시다.)\n\n그 때, $\\hat{A}_t$은 $\\gamma$-just입니다.\n\n이 명제에 대한 증명은 Appendix B에 있습니. 그리고 $\\hat{A}_t$에 대한 $\\gamma$-just advantage estimator들은 다음과 같습니다.\n- $\\sum_{l=0}^\\infty \\gamma^l r_{t+1}$\n- $A^{\\pi, \\gamma} (s_t, a_t)$\n- $Q^{\\pi, \\gamma} (s_t, a_t)$\n- $r_t + \\gamma V^{\\pi, \\gamma} (s_{t+1}) - V^{\\pi, \\gamma} (s_t)$\n\n증명을 보기 전에 먼저 Advantage function에 대해서 먼저 살펴봅시다. 아래의 내용은 Sutton이 쓴 논문인 Policy Gradient Methods for Reinforcement Learning with Function Approximation(2000)에서 나온 내용을 리뷰한 것입니다. \n\n<center> <img src=\"https://www.dropbox.com/s/x737yq97ut6gp1a/Screen%20Shot%202018-07-15%20at%201.16.09%20PM.png?dl=1\" width=\"300\"> </center>\n\n여기서는 \"이 수식은 $\\sum_a \\frac{\\partial \\pi (s,a)}{\\partial \\theta} = 0$이기 때문에 가능해진다.\", \"즉, 이 수식은 $\\pi(s,a)$의 gradient에만 dependent하기 때문에 advantage 역할을 하는 함수들을 넣어도 아무런 상관이 없다\"라는 두 문장을 기억해둡시다.\n\n이제 증명을 보면 됩니다.\n\n<center> <img src=\"https://www.dropbox.com/s/e7sj2fwcm1f7hof/figure2.jpg?dl=1\" width=\"300\"> </center>\n<center> <img src=\"https://www.dropbox.com/s/kyk4jes1202az4m/figure3.jpg?dl=1\" width=\"300\"> </center>\n\n위의 증명 수식에서 Q와 b의 각각 세번째 수식과 마지막 수식을 자세히 봅시다.\n\n1. $Q$에 대한 증명\n    - 빨간색 부분\n        - $\\mathbb{E}_{s_{t+1:\\infty}, a_{t+1:\\infty}} [Q_t (s_{0:\\infty}, a_{0:\\infty})]$를 쉽게 말하자면 \"모든(과거, 현재, 미래) state와 action에 대해서 expectation을 $t+1$부터 $\\infty$까지 하겠다.\"라는 말입니다. 이렇게 함으로써 원래는 $Q^\\pi (s_t, a_t)$가 나와야 맞는 건데, 앞에서 살펴봤다시피 4번째 수식 밑줄 친 자리에는 advantage 역할을 하는 함수들을 넣어도 결과값에 아무런 영향을 끼치지 않기 때문에 의도적으로 $A^\\pi (s_t, a_t)$로 바꾼 것입니다. 또한 이 증명을 바탕으로 $\\hat{A}$이 $\\gamma$-just라는 것을 표현하고 싶기 때문이라고 봐도 됩니다.\n\n    - 파란색 부분\n        - 또한 $a_{0:t}$에서 $a_{0:t-1}$로 변한 것은 $A^\\pi (s_t, a_t)$에 baseline이 들어가기 때문에 바꿔준 것입니다.\n\n2. $b$에 대한 증명\n    - 빨간색 부분\n        - $\\mathbb{E}_{s_{t+1:\\infty}, a_{t+1:\\infty}} [\\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$가 0으로 바뀌는 것은 위에서 설명했듯이 $\\nabla_\\theta$자체가 $\\log \\pi_\\theta$만 gradient하기 때문에 이것을 expectation을 취하면 0이 됩니다.\n\n## 4. Advantage Function Estimation\n이번 section에는 discounted advantage function $A^{\\pi, \\gamma} (s_t, a_t)$의 accurate estimate $\\hat{A}_t$에 대해서 살펴봅시다. 이에 따른 수식은 다음과 같습니다.\n\n$$\\hat{g} = \\frac{1}{N} \\sum_{n=1}^N \\sum_{t=0}^\\infty \\hat{A}_t^n \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t^n | s_t^n)$$\n\n여기서 n은 a batch of episodes에 대하여 index한 것입니다.\n\n$V$를 approximate value function이라고 합시다. 그리고 $\\delta_t^V = r_t + \\gamma V(s_{t+1}) - V(s_t)$이라고 합시다.\n\n만약 (이전과 마찬가지로 가정부터 합니다.) correct value function $V = V^{\\pi, \\gamma}$가 있다고 한다면, 이것은 $\\gamma$-just advantage estimator입니다. 실제로, $A^{\\pi, \\gamma}$의 unbiased estimator는 다음과 같습니다.\n$$\\mathbb{E}_{s_{t+1}} [\\delta_t^{V^{\\pi, \\gamma}}] = \\mathbb{E}_{s_{t+1}} [r_t + \\gamma V^{\\pi, \\gamma} (s_{t+1}) - V^{\\pi, \\gamma} (s_t)]$$\n$$= \\mathbb{E}_{s_{t+1}} [Q^{\\pi, \\gamma} (s_t, a_t) - V^{\\pi, \\gamma} (s_t)] = A^{\\pi, \\gamma} (s_t, a_t)$$\n그러나, \"이 estimator는 유일하게 $V = V^{\\pi, \\gamma}$에 대한 $\\gamma$-just입니다.\" 다른 경우라면 이것은 biased policy gradient estimate일 것입니다. (우리가 하고 싶은 것은 $V$에 대해서만 unbiased estimator가 아니라 advantage function에 대해서 일반화된 unbiased estimator를 얻고 싶은 것입니다. 그래서 아래에서도 나오겠지만, $\\gamma$와 함께 $\\lambda$를 이용한 estimator가 나옵니다.ㅏ)\n\n그래서 $\\delta$에 대해 $k$의 sum으로 생각해봅시다. 이것을 $\\hat{A}_t^{(k)}$라고 하자. 그러면 아래와 같이 표현할 수 있습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/ra7hxksveg2hz45/figure4.jpg?dl=1\" width=\"300\"> </center>\n\n$\\hat{A}_t^{(k)}$은 returns의 $k$-step estimate와 연관지을 수 있고, $\\delta_t^V = \\hat{A}_t^{(1)}$의 case와 유사하게도 $\\hat{A}_t^{(k)}$를 advantage function의 estimator로 생각할 수 있습니다.\n\n여기서 $k \\rightarrow \\infty$로 생각해보면 bias가 일반적으로 점점 더 작아집니다. 왜냐하면 $\\gamma^k V(s_{t+k})$가 점점 많이 discounted되서 $-V(s_t)$가 bias에 영향을 미치지 못하기 때문입니다. $k \\rightarrow \\infty$를 취하면 다음과 같은 수식이 나옵니다.\n\n$$\\hat{A}_t^{(\\infty)} = \\sum_{l=0}^\\infty \\gamma^l \\delta_{t+l}^V = -V(s_t) + \\sum_{l=0}^\\infty \\gamma^l r_{t+l}$$\n우변의 수식과 같이 empirical returns에서 value function baseline을 뺀 것으로 나타낼 수 있습니다.\n\nGeneralized Advantage Estimator GAE($\\gamma, \\lambda$)는 $k$-step estimators의 exponentially-weighted average로 나타낼 수 있습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/yg1ybmfkep3towu/figure5.jpg?dl=1\" width=\"300\"> </center>\n\n(TD($\\lambda$)를 떠올려주세요..!)\n\n위의 마지막 수식에서도 알 수 있듯이, advantage estimator는 Bellman residual terms의 discounted sum과 관련있는 간단한 수식입니다. 다음 section에서 modified reward function을 가진 MDP에 returns로서의 관점에서 위의 마지막 수식을 더 자세히 살펴봅시다. 위의 수식은 TD($\\lambda$)와 많이 유사합니다. 그러나 TD($\\lambda$)는 value function를 estimator하고, 여기서는 advantage function을 estimator합니다.\n\n위의 수식에서 $\\lambda = 0$ and $\\lambda = 1$에 대해서는 특별한 case가 존재한다. 수식으로 표현하면 다음과 같습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/ufglxzanhcnbi36/figure6.jpg?dl=1\" width=\"300\"> </center>\n\n- $GAE(\\gamma, 1)$은 $V$의 정확도와 관계없이 $\\gamma$-just입니다. 그러나 returns의 sum때문에 high variance합니다.\n- $GAE(\\gamma, 0)$은 $V = V^{\\pi, \\gamma}$에 대해 $\\gamma$-just입니다. 그리고 bias하지만 일반적으로 훨씬 lower variance를 가집니다. \n- $0 < \\lambda < 1$에 대해 GAE는 parameter $\\lambda$를 control하여 bias와 variance사이에 compromise를 만듭니다.\n\n두 가지 별개의 parameter $\\gamma$ and $\\lambda$를 가진 advantage estimator는 bias-variance tradeoff에 도움을 줍니다. 그러나 이 두 가지 parameter는 각각 다른 목적을 가지고 작동합니다.\n- $\\gamma$는 가장 중요하게 value function $V^{\\pi, \\gamma}$ 의 scale을 결정합니다. 또한 $\\gamma$는 $\\lambda$에 의존하지 않습니다. $\\gamma < 1$로 정하는 것은 policy gradient estimate에서 bias합니다.\n- 반면에, $\\lambda < 1$는 유일하게 value function이 부정확할 때 bias합니다. 그리고 경험상, $\\lambda$의 best value는 $\\gamma$의 best value보다 훨씬 더 낮습니다. 왜냐하면 $\\lambda$가 정확한 value function에 대해 $\\gamma$보다 훨씬 덜 bias하기 때문입니다.\n\nGAE를 사용할 때, $g^\\gamma$의 biased estimator를 구성할 수 있습니다. 수식은 다음과 같습니다.\n$$g^\\gamma \\approx \\mathbb{E} [\\sum_{t=0}^\\infty] \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) \\hat{A}_t^{GAE(\\gamma, \\lambda)}] = \\mathbb{E} [\\sum_{t=0}^{\\infty} \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) \\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+1}^V]$$\n여기서 $\\lambda = 1$일 때 동일해집니다.\n\n## 5. Interpretation as Reward Shaping\n이번 section에서는 앞서 다뤘던 수식 $\\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^V$를 modified reward function의 MDP의 관점으로 생각해봅시다. 조금 더 구체적으로 말하자면, MDP에서 reward shaping transformation을 실행한 후에 적용된 extra discounted factor로서 $\\lambda$를 어떻게 볼 것인지에 대해서 다룹니다. \n\n(개인적인 comment) 한 가지 먼저 언급하자면, 본래의 목적은 reward shaping이 아니라 variance reduction입니다. 이번 section은 그저 이전에 이러한 개념이 있었고, gae를 다른 관점에서 생각해보자라는 뜻에서 나온 section인 것 같습니다. '아~ 이러한 개념이 있구나~!' 정도로만 알면 될 것 같습니다. 'gae가 reward shaping의 효과까지 있어서 이러한 section을 넣은 것일까?' 라는 생각도 해봤지만 아직 잘 모르겠습니다. 실험부분에도 딱히 하지 않은 걸로 봐서는.. 아닌 것 같기도 하고.. 아직까지는 그저 'reward shaping의 관점에서 봤을 때에도 gae로 만들어줄 수 있다.' 정도만 생각하려고 합니다.\n\nReward Shaping이란 개념 자체는 일반적인 알고리즘 분야(최단경로 문제 등)에도 있습니다. 하지만 이 개념이 머신러닝에 적용된 건 Andrew Y. Ng이 쓴 Policy Invariance under Reward Transformations Theory and Application to Reward Shaping(1999) 논문입니다. 논문에서 Reward Shaping을 설명하는 부분은 간략하게 다음과 같습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/0w9cuxjkhz2mbcy/figure7.jpg?dl=1\" width=\"300\"> </center>\n<center> <img src=\"https://www.dropbox.com/s/xcp9eqvcbv3268g/figure8.jpg?dl=1\" width=\"300\"> </center>\n\n위의 글만 보고 이해가 잘 안됩니다. 그래서 다른 자료들을 찾던 중에 다음의 그림을 찾게 되었습니다. (아래의 그림은 Youtube에 있는 [Udacity 영상](https://www.youtube.com/watch?v=xManAGjbx2k&t=95s)에서 있는 그림입니다.)\n\n<center> <img src=\"https://www.dropbox.com/s/87kacngez9cl2e6/figure9.jpg?dl=1\" width=\"300\"> </center>\n\n이 그림을 통해서 Reward Shaping을 이해해봅시다.\n\nReward Shaping(보상 형성)을 통해서 뭘 하고 싶은지 목적부터 봅시다. reward space가 sparse 한 경우에 reward가 너무 드문드문 나옵니다. 따라서 이것을 꾸준히 reward를 받을 수 있도록 바꿉니다. potential-based shaping function인 $\\Phi$를 만들어서 더하고 빼줍니다. 저 $\\Phi$ 자리에는 대표적으로 state value function이 많이 들어간다고 합니다. (아직 목적이 이해가 안될 수 있습니다. Reward Shaping에 대해서 다 보고 나서 다시 한 번 읽어봅시다.)\n\n위의 그림은 돌고래가 점프하여 불구멍을 통과해야하는 환경입니다. 하지만 저 불구멍을 통과하기 위해서는 (1) 점프도 해야하고, (2) 불도 피해야하고, (3) 알맞게 착지까지 완료해야합니다. 이렇게 해야 reward를 +1 얻습니다. \n\n생각해봅시다. 어느 세월에 점프도 해야하고.. 불도 피해야하고.. 알맞게 착지까지해서 reward를 +1 얻겠습니까? 따라서 그 전에도 잘하고 있는 지, 못하고 있는 지를 판단하기 위해, 다시 말해 reward를 꾸준히 받도록 다음과 같이 transformed reward function $\\tilde{r}$을 정의합니다.\n\n$$\\tilde{r} (s, a, s') = r(s, a, s') + \\gamma \\Phi (s') - \\Phi (s)$$\n\n- 여기서 $\\Phi : S \\rightarrow \\mathbb{R}$를 state space에서의 arbitrary scalar-valued function을 나타냅니다. 그리고 $\\Phi$자리에는 대표적으로 state value function이 들어간다고 생각합시다.\n- 형태가 TD residual term의 결과와 비슷하지만 의미와 의도가 다릅니다. reward shaping은 sparse reward 때문이고, 이전 section에서 봤던 gae는 variance reduction때문에 나온 것입니다.\n\n<center> <img src=\"https://www.dropbox.com/s/tkfpqniaazwfbld/figure10.jpg?dl=1\" width=\"300\"> </center>\n\n이 transformation은 discounted advantage function $A^{\\pi, \\gamma}$으로도 둘 수 있습니다. state $s_t$를 시작하여 하나의 trajectory의 rewards의 discounted sum을 표현하면 다음과 같습니다.\n$$\\sum_{l=0}^\\infty \\gamma^l \\tilde{r} (s_{t+l}, a_t, s_{t+l+1}) = \\sum_{l=0}^\\infty \\gamma^l r(s_{t+l}, a_{t+l}, s_{t+l+1}) - \\Phi(s_t)$$\n\n$\\tilde{Q}^{\\pi, \\gamma}, \\tilde{V}^{\\pi, \\gamma}, \\tilde{A}^{\\pi, \\gamma}$를  transformed MDP의 value function과 advantage function이라고 하면, 다음과 같은 수식이 나옵니다.\n$$\\tilde{Q}^{\\pi, \\gamma} (s, a) = Q^{\\pi, \\gamma} (s, a) - \\Phi (s)$$\n$$\\tilde{V}^{\\pi, \\gamma} (s, a) = V^{\\pi, \\gamma} (s, a) - \\Phi (s)$$\n$$\\tilde{A}^{\\pi, \\gamma} (s, a) = (Q^{\\pi, \\gamma} (s, a) - \\Phi (s)) - (V^\\pi (s) - \\Phi (s)) = A^{\\pi, \\gamma} (s, a)$$\n\n이제 reward shaping의 idea를 가지고 어떻게 policy gradient estimate를 얻을 수 있는 지에 대해서 알아봅시다. \n- $0 \\le \\lambda \\le 1$의 범위에 있는 steeper discount $\\gamma \\lambda$를 사용합니다.\n- shaped reward $\\tilde{r}$는 Bellman residual term $\\delta^V$와 동일합니다.\n- 그리고 $\\Phi = V$와 같다고 보면 다음과 같은 수식이 나옵니다.\n$$\\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\tilde{r} (s_{t+l}, a_t, s_{t+l+1}) = \\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^V = \\hat{A}_t^{GAE(\\gamma, \\lambda)}$$\n이렇게 shaped rewards의 $\\gamma \\lambda$-discounted sum을 고려함으로써 GAE를 얻을 수 있습니다. 더 정확하게 $\\lambda = 1$은 $g^\\gamma$의 unbiased estimate이고, 반면에 $\\lambda < 1$은 biased estimate입니다.\n\n좀 더 나아가서, shaping transformation과 parameters $\\gamma$ and $\\lambda$의 결과를 보기 위해, response function $\\chi$를 이용하면 다음과 같은 수식이 나옵니다.\n$$\\chi (l; s_t, a_t) = \\mathbb{E} [r_{t+l} | s_t, a_t] - \\mathbb{E} [r_{t+l} | s_t]$$\n추가적으로 $A^{\\pi, \\gamma} (s, a) = \\sum_{l=0}^\\infty \\gamma^l \\chi (l; s, a)$입니다.\n그래서 discounted policy gradient estimator는 다음과 같이 쓸 수 있다.\n$$\\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) A^{\\pi, \\gamma} (s_t, a_t) = \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) \\sum_{l=0}^\\infty \\gamma^l \\chi (l; s, a)$$\n\n## 6. Value Fuction Estimation\n이번 section에서는 Trust Region Optimization Scheme에 따라 Value function을 Estimation합니다.\n- \n### 6.1 Simplest approach\n$ minimize_{\\phi} \\sum_{n=1}^{N} \\vert\\vert V_{\\phi}(s_n) - \\hat{V_n} \\vert\\vert^{2} $$\n\n- 위는 가장 간단하게 non-linear approximation 으로 푸는 방법입니다. \n- $\\hat{V_t} = \\sum_{l=0}^{\\infty}\\gamma^l r_{t+l}$ 은 reward 에 대한 discounted sum 을 의미합니다. \n\n### 6.2 Trust region method to optimize the value function\n\n- Value function 을 최적화 하기 위해 trust region method 를 사용합니다. \n- Trust region 은 최근 데이터에 대해 overfitting 되는 것을 막아줍니다. \n\nTrust region 문제를 풀기 위해서는 다음 스텝을 따릅니다.\n\n- $\\sigma^2 = \\frac{1}{N} \\sum_{n=1}^{N} \\vert\\vert V_{\\phi old}(s_n) - \\hat{V_n} \\vert\\vert^{2}$ 을 계산합니다. \n- 그 후에 다음과 같은 constrained opimization 문제를 풉니다. \n\n$$minimize_{\\phi} \\, \\sum_{n=1}^N \\parallel V_\\phi (s_n) - \\hat{V}_n \\parallel^2$$\n$$subject \\, \\, to \\, \\frac{1}{N} \\sum_{n=1}^N \\frac{\\parallel V_\\phi (s_n) - V_{\\phi old} (s_n) \\parallel^2}{2 \\sigma^2} \\le \\epsilon$$\n\n- 위의 수식은 사실 old Value function과 새로운 Value function KL distance 가 $\\epsilon$ 보다 작아야한다는 수식과 같습니다. Value function이 평균은 $V_{\\phi}(s)$ 이고 분산이 $\\sigma^2$인 conditional Gaussian distribution 으로 parameterize되었을 뿐입니다. \n<img width =\"400px\" src=\"https://www.dropbox.com/s/uw0v05feu8chqmc/Screenshot%202018-07-08%2010.04.38.png?dl=1\"> \n\n이 trust region 문제의 답을 conjudate gradient algorithm 을 이용하여 근사값을 구할 수 있습니다. 특히, 다음과 같은  quadratic program 을 풀게됩니다. \n\n$$minimize_{\\phi} \\, g^T (\\phi - \\phi_{old})$$\n$$subject \\, \\, to \\, \\frac{1}{N} \\sum_{n=1}^N (\\phi - \\phi_{old})^T H(\\phi - \\phi_{old}) \\le \\epsilon$$\n- 여기서 $g$는 objective 의 gradient입니다. \n- $j_n = \\nabla_{\\phi} V_{\\phi}(s_n)$ 일때, $H = \\frac{1}{N} \\sum_{n} j_n j^T_n$ 이며, $H$는 objective의 hessian에 대해서 gaussian newton method 로 근사한 값입니다. 따라서, value function 을 conditional probability로 해석한다면 Fisher information matrix가 됩니다. \n- 구현할때의 방법은 TRPO 에서 사용한 방법과 모두 같습니다. \n\n## 7.Experiments\n\n실험은 다음 두 가지 물음에 대해서 디자인 되었습니다.\n\n- GAE 에 따라서 episodic total reward 를 최적화 할때, $\\lambda$와 $\\gamma$ 가 변함에 따라서 어떤 경험적인 효과를 볼 수 있는지 ?\n- GAE 와 trust region alogorithm 을 policy 와 value function 모두에 함께 사용했을 때 어려운 문제에 적용되는 큰 뉴럴넷을 최적화 할 수 있을까 ?\n\n### 6.1 Policy Optimization Algorithm\n\nPolicy update는 TRPO 를 사용합니다. TRPO 에 대한 설명은 여기서는 생략하겠습니다. TRPO 포스트를 보고 돌아와주세요 !\n\n- 이전 TRPO에서 이미 TRPO 와 다른 많은 알고리즘들을 비교하였기 때문에, 여기서는 똑같은 짓을 반복하지 않고 $\\lambda$, $\\gamma$ 가 변함에 따라 어떤 영향이 있는 지에 대한 실험에 집중하겠다고 합니다. (귀찮았던거죠)\n\n- TRPO 를 적용한 GAE의 최종 알고리즘은 다음과 같습니다.\n<center> <img width = \"600px\" src=\"https://www.dropbox.com/s/b1klz11f2frrvg4/Screenshot%202018-07-08%2010.33.54.png?dl=1\"> </center>\n<center> <img width = \"300px\" src=\"https://www.dropbox.com/s/35fxte05pqtiel7/Screenshot%202018-07-08%2010.35.22.png?dl=1\"> </center>\n<center> <img width = \"300px\" src=\"https://www.dropbox.com/s/u35pjow3w50bvz1/Screenshot%202018-07-08%2010.36.03.png?dl=1\"> </center>\n\n\n- 여기서 주의할 점은 Policy update ($\\theta_i \\rightarrow \\theta_{i+1}$)에서 $V_{\\phi_i}$ 를 사용했다는 점입니다. \n- 만약 Value function을 먼저 update하게 된다면 추가적인 bias가 발생합니다.\n- 극단적으로 생각해보아서, 우리가 Value function을 완벽하게 overfit 해낸다면 Bellman residual ($r_t + \\gamma V(s_{t+1}) - V(S_t)$)은 0이 됩니다. 그럼 Policy gradient의 estimation도 거의 0이 될 것입니다. \n\n    \n### 7.2 Expermint details\n\n#### 7.2.1 Environment \n실험에서 사용된 환경은 다음 네 가지 입니다. \n\n1. classic cart-pole (x 3D)\n2. bipedal locomotion \n3. quadrupedal locomotion \n4. dynamically standing up for the biped\n\n#### 7.2.2 Architecture\n\n- 3D robot task에 대해서는 같은 모델을 사용하였습니다. \n    - layers  = [100, 50, 25] 각각 tanh 사용.(Policy와 Value 네트워크 모두)\n    - Final output layer은 linear\n- Cartpole 에 대해서는 1개의 layer 안에 20개의 hidden unit 만 있는 linear policy를 사용했다고 합니다. \n\n#### 7.2.3 Task\n\n- Cartpole\n    - 한 배치당 20 개의 trajectory 를 모았고, maximum length 는 1000 입니다. \n- 3D biped locomotion\n    - 33 dim state , 10 dim action\n    - 50000 time step per batch\n- 3D quadruped locomotion\n    - 29 dim state, 8 dim action\n    - 200000 time step per batch \n- 3D biped locomotion Standing\n    - 33 dim state , 10 dim action\n    - 200000 time step per batch \n\n\n#### 7.2.3 results\ncost 의 관점에서 결과를 나타내었다고 합니다. Cost 는 negative reward와 이것이 최소화 되었는가로 정의되었다고 하는데, 정확히는 안나와있습니다. \n\n##### 7.2.3.1 Cartpole\n\n<center> <img width = \"500px\" src=\"https://www.dropbox.com/s/x9pbms1wvg38lda/Screenshot%202018-07-08%2011.08.22.png?dl=1\"> </center>\n\n- 왼쪽 그림은 $\\gamma$ 를 0.99로 고정시켜놓은 상태에서 $\\lambda$ 를 변화시킴에 따라서 cost 를 측정한 것입니다. \n- 오른쪽은 $\\gamma$ 와 $\\lambda$ 를 둘 다 변화 시키면서 performance 를 그림으로 나타낸 표입니다. 흰색에 가까울 수록 좋은 퍼포먼스입니다. \n\n##### 7.2.3.2 3D BIPEDAL LOCOMOTINO\n<center> <img width = \"500px\" src=\"https://www.dropbox.com/s/i9wj4p6ijojsy82/Screenshot%202018-07-08%2011.25.08.png?dl=1\"> </center>\n\n\n- 다른 random seed 로 부터 9번 씩 시도한 결과를 mean 을 취해서 사용합니다.\n- Best performance 는   $\\gamma \\in [0.99, 0.995]$ 그리고  $\\lambda \\in [0.96, 0.99]$ 일때. \n- 1000 iteration 후에 빠르고 부드럽고 안정적인 걸음거이가 나옵니다.\n- 실제로 걸린 시간은 0.01(타입스텝당 시간) * 50000(배치당 타임스텝) * 1000(배치) * 3600(초->시간) * 24 = 5.8일 정도가 걸렸습니다.\n\n##### 7.2.3.3 다른 ROBOT TASKS\n<center> <img width = \"500px\" src=\"https://www.dropbox.com/s/fuuat65we52quht/Screenshot%202018-07-08%2011.33.11.png?dl=1\"> </center>\n\n- 다른 로봇 TASK에 대해서는 아주 제한적인 실험만 진행합니다.(시간이 부족했던듯)\n- Quadruped 에 대해서는 $\\gamma$ =0.995  로 fix, $\\lambda \\in ${0, 0.96}\n- Standingup 에 대해서는 $\\gamma$ =0.99  로 fix, $\\lambda \\in ${0, 0.96}\n\n\n## 8. Discussion\n\n### 8.1 Main discussion\n지금까지 복잡하고 어려운 control problem에서 Reinforcement Learning(RL)은 high sample complexity 때문에 제한이 되어왔습니다. 따라서 이 논문에서 그 제한을 풀고자 advantage function의 good estimate를 얻는 \"variance reduction\"에 대해 연구하였습니다.\n\n\"Generalized Advantage Estimator(GAE)\"라는 것을 제안했고, 이것은 bias-variance tradeoff를 조절하는 두 개의 parameter $\\gamma,\\lambda$를 가집니다.\n또한 어떻게 Trust Region Policy Optimization과 value function을 optimize하는 Trust Region Algorithm의 idea를 합치는 지를 보였습니다.\n\n이렇게 함으로써 보다 더 복잡하고 어려운 control task들을 해결할 수 있었습니다.\n\nGAE의 실험적인 입증으로는 robotic locomotion을 simulation하는 domain입니다. 실험에서도 보여준 것처럼 [0.9, 0.99]의 범위에서 $\\lambda$의 적절한 중간의 값을 통해 best performance를 얻습니다. 좀 더 나아가 연구되어야할 점은 adaptive or automatic하도록 estimator parameter $\\gamma,\\lambda$를 조절하는 방법입니다.\n\n### 8.2 Future work\n\nValue function estimation error 와 Policy gradient estimation error 사이의 관계를 알아낸다면, 우리는 Value function fitting 에 더 잘 맞는 error metric 을 사용할 수 있습니다. (policy gradient estimation 의 정확성과 더 잘 맞는 value function)\n\nPolicy와 Value function의 파라미터를 공유하는 모델을 만드는 것은 아주 흥미롭고 이점이 많습니다. 하지만 수렴을 보장하도록 적절한 numerical optimization을 제시하여야 합니다.\n\nDDPG는 별로입니다. TD(0) 는 bias 가 너무 크고, poor performance 로 이끕니다. 특히나 이 paper 에서는 low-dimention 의 쉬운 문제들만 해결했습니다.\n\n<center> <img width = \"500px\" src=\"https://www.dropbox.com/s/nhc7t9psul5lr3x/Screenshot%202018-07-08%2011.45.15.png?dl=1\"> </center>\n\n### 8.3 FAQ \n\n- Compatible features 와는 무슨 관계 ?\n     - Compatible features 는 value function을 이용하는 policy gradient 알고리즘들과 함께 자주 언급됩니다.\n     - Actor Critic 의 저자는 policy의 제한된 representation power 때문에, policy gradient 는 단지 advantage function space 의 subspace에만 의존하게 됩니다.\n     - 이 subspace는 compatible features 에 의해 span 됩니다.\n     - 이 이론은 현재 문제 구조를 어떻게 이용해야 advantage function에 대해 더 나은 estimation 을 할 수 있는 지에 대한 지침을 주지 않습니다. GAE paper 의 idea 와 orthogonal 합니다. \n- 왜 Q function을 사용하지 않는가 ?\n     - 먼저 state-value function이 더 낮은 차원의 input 을 가진다. 그래서 Q function보다 더 배우기 습니다.\n     - 두 번째로 이 페이퍼에서 제안하는 방법으로는 high bias estimator 에서 low bias estimator 로 $\\lambda$를 통해서 부드럽게 interpolate 할 수 있습니다. \n     - 반면에 Q 를 사용하면 단지 high-bias estimator 밖에 사용할 수 없습니다. \n     - 특히나 return에 대한 one-step estimation 은 엄두를 못낼 정도로 bias 가 큽니다.\n","slug":"gae","published":1,"updated":"2018-07-15T04:52:44.205Z","_id":"cjjgzyg440000xo155e3g8dvy","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"High-Dimensional-Continuous-Control-using-Generalized-Advantage-Estimation\"><a href=\"#High-Dimensional-Continuous-Control-using-Generalized-Advantage-Estimation\" class=\"headerlink\" title=\"High-Dimensional Continuous Control using Generalized Advantage Estimation\"></a>High-Dimensional Continuous Control using Generalized Advantage Estimation</h1><h2 id=\"1-Abstract\"><a href=\"#1-Abstract\" class=\"headerlink\" title=\"1. Abstract\"></a>1. Abstract</h2><p>현존하는 Policy Gradient Method들의 목적은 누적되는 reward들을 optimization하는 것입니다. 하지만 학습할 때에 많은 양의 sample이 필요로 하고, 들어오는 data가 nonstationarity임에도 불구하고 stable and steady improvement가 어렵습니다.</p>\n<p>그래서 이 논문에서는 다음과 같은 방법을 제시합니다.</p>\n<ul>\n<li>TD($\\lambda$)와 유사한 advantage function의 exponentially-weighted estimator를 사용하여 policy gradient estimate의 variance를 줄이는 것 </li>\n<li>policy와 value function에 대한 Trust Region Optimization 사용하는 것</li>\n</ul>\n<p>3D locomotion tasks에 대한 empirical results는 다음과 같습니다.</p>\n<ul>\n<li>bipedal and quadrupedal simulated robots의 달리는 자세를 학습</li>\n<li>bipedal 사람이 땅에 누워있다가 일어서는 것을 학습</li>\n</ul>\n<h2 id=\"2-Introduction\"><a href=\"#2-Introduction\" class=\"headerlink\" title=\"2. Introduction\"></a>2. Introduction</h2><p>기본적으로 “parameterized stochastic policy”를 가정합니다. 이 때 expected total returns의 gradient에 대한 unbiased estimate를 얻을 수 있는데 이것을 REINFORCE라고 부릅니다. 하지만 하나의 action의 결과가 과거와 미래의 action의 결과로 혼동되기 때문에 gradient estimator의 high variance는 시간에 따라 scaling됩니다.</p>\n<p>또 다른 방법은 Actor-Critic이 있습니다. 이 방법은 empirical returns보다 하나의 value function을 사용합니다. 또한 bias하고 lower variance를 가진 estimator입니다. 구체적으로 말하자면, high variance하다면 더 sampling을 하면 되는 반면에 bias는 매우 치명적입니다. 다시 말해 bias는 algorithm이 converge하는 데에 실패하거나 또는 local optimum이 아닌 poor solution에 converge하도록 만듭니다.</p>\n<p>따라서 이 논문에서는 $\\gamma\\in [0,1]$ and $\\lambda\\in [0,1]$에 의해 parameterized estimation scheme인 Generalized Advantage Estimator(GAE)를 다룹니다.</p>\n<p>이 논문을 대략적으로 요약하자면 다음과 같습니다.</p>\n<ul>\n<li>효과적인 variance reduction scheme인 GAE를 다룹니다. 또한 실험할 때 batch trust region algorithm에 더하여 다양한 algorithm들에 적용됩니다.</li>\n<li>value function에 대해 trust region optimization method를 사용합니다. 이렇게 함으로서 더 robust하고 efficient한 방법이 됩니다.</li>\n<li>A와 B를 합쳐서, 실험적으로 control task에 neural network policies를 learning하는 데에 있어서 효과적인 algorithm을 얻습니다. 이러한 결과는 high-demensional continuous control에 RL을 사용함으로서 state of the art로 확장되었습니다.</li>\n</ul>\n<h2 id=\"3-Preliminaries\"><a href=\"#3-Preliminaries\" class=\"headerlink\" title=\"3. Preliminaries\"></a>3. Preliminaries</h2><p>먼저 policy optimization의 “undiscounted formulation”을 가정합니다. (undiscounted formulation에 주목합시다.)</p>\n<ul>\n<li>initial state $s_0$는 distribution $\\rho_0$으로부터 sampling된 것입니다.</li>\n<li>하나의 trajectory ($s_0, a_0, s_1, a_1, …$)는 terminal state에 도달될 때 까지 policy $a_t$ ~ $\\pi(a_t | s_t)$에 따라서 action을 sampling하고, dynamics $s_{t+1}$ ~ $P(s_{t+1} | s_t, a_t)$에 따라서 state를 sampling함으로써 생성됩니다.</li>\n<li>reward $r_t = r(s_t, a_t, s_{t+1})$은 매 time step마다 받아집니다.</li>\n<li>목표는 모든 policies에 대해 finite하다고 가정됨으로서 expected total reward $\\sum_{t=0}^{\\infty} r_t$를 maximize하는 것입니다.</li>\n</ul>\n<p>여기서 중요한 점은 $\\gamma$를 discount의 parameter로 사용하는 것이 아니라 “bias-variance tradeoff를 조절하는 parameter로 사용한다” 는 것입니다.</p>\n<p>policy gradient method는 gradient $g := \\nabla_\\theta \\mathbb{E} [\\sum_{t=0}^\\infty r_t]$를 반복적으로 estimate함으로써 expected total reward를 maximize하는 것인데, policy gradient에는 여러 다른 표현들이 있습니다.<br>$$g = \\mathbb{E} [\\sum_{t=0}^\\infty \\Phi_t \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$<br>여기서 $\\Phi_t$는 아래 중의 하나일 수 있습니다.</p>\n<ol>\n<li>$\\sum_{t=0}^\\infty r_t$: trajectory의 total reward</li>\n<li>$\\sum_{t’=t}^\\infty r_t’$: action $a_t$ 후의 reward</li>\n<li>$\\sum_{t’=t}^\\infty r_t’ - b(s_t)$: 2의 baselined version</li>\n<li>$Q^\\pi (s_t, a_t)$: state-action value function</li>\n<li>$A^\\pi (s_t, a_t)$: advantage function</li>\n<li>$r_t + V^\\pi (s_{t+1}) - V^\\pi (s_t)$: TD residual</li>\n</ol>\n<p>위의 번호 중 4, 5, 6의 수식들은 다음의 정의를 사용합니다.</p>\n<ul>\n<li>$V^\\pi (s_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_t:\\infty} [\\sum_{l=0}^\\infty r_{t+1}]$</li>\n<li>$Q^\\pi (s_t, a_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty} [\\sum_{l=0}^\\infty r_{t+l}]$</li>\n<li>$A^\\pi (s_t, a_t) := Q^\\pi (s_t, a_t) - V^\\pi (s_t)$, (Advantage function)</li>\n</ul>\n<p>추가적으로 colon notation $a : b$는 포괄적인 범위 $(a, a+1, … , b)$입니다. (잘 기억해둡시다. 뒤에 계속해서 colon notation이 나옵니다.)</p>\n<p>여기서부터 parameter $\\gamma$에 대해 좀 더 자세히 알아봅시다. parameter $\\gamma$는 bias하면서 동시에 reward를 downweighting함으로써 variance를 줄입니다. 다시 말해 MDP의 discounted formulation에서 사용된 discounted factor와 일치하지만, 이 논문에서는 “$\\gamma$를 undiscounted MDP에서 variance reduction parameter”로 다룹니다. -&gt; 결과는 같지만, 의미와 의도가 다릅니다.</p>\n<p>discounted value function들은 다음과 같습니다.</p>\n<ul>\n<li>$V^{\\pi, \\gamma} (s_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_t:\\infty} [\\sum_{l=0}^\\infty \\gamma^l r_{t+l}]$ </li>\n<li>$Q^\\pi (s_t, a_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty} [\\sum_{l=0}^\\infty \\gamma^l r_{t+l}]$</li>\n<li>$A^{\\pi, \\gamma} (s_t, a_t) := Q^{\\pi, \\gamma} (s_t, a_t) - V^{\\pi, \\gamma} (s_t)$</li>\n</ul>\n<p>따라서 policy gradient에서의 discounted approximation은 다음과 같이 나타낼 수 있습니다.<br>$$g^\\gamma := \\mathbb{E}_{s_{0:\\infty} a_{0:\\infty}} [\\sum_{t=0}^\\infty A^{\\pi, \\gamma} (s_t, a_t) \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$<br>뒤이어 나오는 section에서는 위의 수식에서 $A^{\\pi, \\gamma}$에 대해 biased (but not too biased) estimator를 얻는 방법에 대해서 나옵니다.</p>\n<p>다음으로 advantage function에서 새롭게 접하는 “$\\gamma$-just estimator”의 notation에 대해 알아봅시다.</p>\n<ul>\n<li>먼저 $\\gamma$-just estimator는 $g^\\gamma$ ${}^1$를 estimate하기 위해 위의 수식에서 $A^{\\pi, \\gamma}$를 $\\gamma$-just estimator로 사용할 때, bias하지 않은 estimator라고 합시다. 그리고 이 $\\gamma$-just advantage estimator를 $\\hat{A}<em>t (s</em>{0:\\infty} , a_{0:\\infty})$라고 하고, 전체의 trajectory에 대한 하나의 function이라고 합시다.</li>\n<li>${}^1$에서 이 논문의 저자가 하고 싶은 말이 있는 것 같습니다. 개인적으로 $\\gamma$-just estimator를 이해하는 데에 있어서 중요한 주석이라 정확히 해석하고자 합니다.<ul>\n<li>$A^\\pi$를 $A^{\\pi, \\gamma}$로 사용함으로써 이미 bias하다라고 말했지만, “이 논문에서 하고자 하는 것은 $g^\\gamma$에 대해 unbiased estimate를 얻고 싶은 것입니다.” 하지만 undiscounted MDP의 policy gradient에 대해서는 당연히 $\\gamma$를 사용하기 때문에 biased estimate를 얻습니다. 개인적으로 이것은 일단 무시하고 $\\gamma$를 사용할 때 어떻게 unbiased estimate를 얻을 지에 대해 좀 더 포커스를 맞추고 있는 것 같습니다.</li>\n<li>그러니까 저 $A^{\\pi, \\gamma}$를 $\\gamma$-just estimator로 바꿔줌으로써 unbiased estimate를 하고 싶다는 것이 뒤이어 나오는 정의와 명제의 핵심이라고 할 수 있습니다.</li>\n</ul>\n</li>\n</ul>\n<p>Definition 1.<br>먼저 가정을 합니다. (가정을 바탕으로 이루어지는 정의라는 것을 주목합시다.) 만약<br>$$\\mathbb{E}_{s_{0:\\infty} a_{0:\\infty}} [\\hat{A}<em>t (s</em>{0:\\infty}, a_{0:\\infty}) \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)] = \\mathbb{E}_{s_{0:\\infty} a_{0:\\infty}} [A^{\\pi, \\gamma} (s_t, a_t) \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$<br>두 수식이 같다면, estimator $\\hat{A}_t$는 $\\gamma$-just입니다.</p>\n<p>그리고 만약 모든 t에 대해서 $\\hat{A}<em>t$이 $\\gamma$-just이라면, 다음과 같이 표현할 수 있습니다.<br>$$\\mathbb{E}</em>{s_{0:\\infty} a_{0:\\infty}} [\\hat{A}<em>t (s</em>{0:\\infty}, a_{0:\\infty}) \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)] = g^\\gamma$$<br>위의 수식이 바로 unbiased estimate입니다.</p>\n<p>$\\gamma$-just인 $\\hat{A}_t$에 대한 한가지 조건은 $\\hat{A}_t$이 두 가지 function $Q_t$ and $b_t$로 나뉠 수 있다는 것입니다.</p>\n<ul>\n<li>$Q_t$는 $\\gamma$-discounted Q-function의 unbiased estimator입니다.</li>\n<li>$b_t$는 action $a_t$전에 sampling된 states and actions의 arbitrary function이다. </li>\n</ul>\n<p>Proposition 1.<br>모든 $(s_t, a_t)$에 대해,<br>$$\\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty | s_t, a_t} [Q_t (s_{t:\\infty}, a_{t:\\infty})] = Q^{\\pi, \\gamma} (s_t, a_t)$$<br>로 인하여 $\\hat{A}<em>t$이<br>$$\\hat{A}</em>{s_{0:\\infty}, a_{0:\\infty}} = Q_t (s_{0:\\infty}, a_{0:\\infty}) - b_t (s_{0:t}, a_{0:t-1})$$<br>형태라고 가정합시다. (가정을 바탕으로 이루어지는 명제라는 점을 주목합시다.)</p>\n<p>그 때, $\\hat{A}_t$은 $\\gamma$-just입니다.</p>\n<p>이 명제에 대한 증명은 Appendix B에 있습니. 그리고 $\\hat{A}_t$에 대한 $\\gamma$-just advantage estimator들은 다음과 같습니다.</p>\n<ul>\n<li>$\\sum_{l=0}^\\infty \\gamma^l r_{t+1}$</li>\n<li>$A^{\\pi, \\gamma} (s_t, a_t)$</li>\n<li>$Q^{\\pi, \\gamma} (s_t, a_t)$</li>\n<li>$r_t + \\gamma V^{\\pi, \\gamma} (s_{t+1}) - V^{\\pi, \\gamma} (s_t)$</li>\n</ul>\n<p>증명을 보기 전에 먼저 Advantage function에 대해서 먼저 살펴봅시다. 아래의 내용은 Sutton이 쓴 논문인 Policy Gradient Methods for Reinforcement Learning with Function Approximation(2000)에서 나온 내용을 리뷰한 것입니다. </p>\n<center> <img src=\"https://www.dropbox.com/s/x737yq97ut6gp1a/Screen%20Shot%202018-07-15%20at%201.16.09%20PM.png?dl=1\" width=\"300\"> </center>\n\n<p>여기서는 “이 수식은 $\\sum_a \\frac{\\partial \\pi (s,a)}{\\partial \\theta} = 0$이기 때문에 가능해진다.”, “즉, 이 수식은 $\\pi(s,a)$의 gradient에만 dependent하기 때문에 advantage 역할을 하는 함수들을 넣어도 아무런 상관이 없다”라는 두 문장을 기억해둡시다.</p>\n<p>이제 증명을 보면 됩니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/e7sj2fwcm1f7hof/figure2.jpg?dl=1\" width=\"300\"> </center><br><center> <img src=\"https://www.dropbox.com/s/kyk4jes1202az4m/figure3.jpg?dl=1\" width=\"300\"> </center>\n\n<p>위의 증명 수식에서 Q와 b의 각각 세번째 수식과 마지막 수식을 자세히 봅시다.</p>\n<ol>\n<li><p>$Q$에 대한 증명</p>\n<ul>\n<li><p>빨간색 부분</p>\n<ul>\n<li>$\\mathbb{E}_{s_{t+1:\\infty}, a_{t+1:\\infty}} [Q_t (s_{0:\\infty}, a_{0:\\infty})]$를 쉽게 말하자면 “모든(과거, 현재, 미래) state와 action에 대해서 expectation을 $t+1$부터 $\\infty$까지 하겠다.”라는 말입니다. 이렇게 함으로써 원래는 $Q^\\pi (s_t, a_t)$가 나와야 맞는 건데, 앞에서 살펴봤다시피 4번째 수식 밑줄 친 자리에는 advantage 역할을 하는 함수들을 넣어도 결과값에 아무런 영향을 끼치지 않기 때문에 의도적으로 $A^\\pi (s_t, a_t)$로 바꾼 것입니다. 또한 이 증명을 바탕으로 $\\hat{A}$이 $\\gamma$-just라는 것을 표현하고 싶기 때문이라고 봐도 됩니다.</li>\n</ul>\n</li>\n<li><p>파란색 부분</p>\n<ul>\n<li>또한 $a_{0:t}$에서 $a_{0:t-1}$로 변한 것은 $A^\\pi (s_t, a_t)$에 baseline이 들어가기 때문에 바꿔준 것입니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>$b$에 대한 증명</p>\n<ul>\n<li>빨간색 부분<ul>\n<li>$\\mathbb{E}_{s_{t+1:\\infty}, a_{t+1:\\infty}} [\\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$가 0으로 바뀌는 것은 위에서 설명했듯이 $\\nabla_\\theta$자체가 $\\log \\pi_\\theta$만 gradient하기 때문에 이것을 expectation을 취하면 0이 됩니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"4-Advantage-Function-Estimation\"><a href=\"#4-Advantage-Function-Estimation\" class=\"headerlink\" title=\"4. Advantage Function Estimation\"></a>4. Advantage Function Estimation</h2><p>이번 section에는 discounted advantage function $A^{\\pi, \\gamma} (s_t, a_t)$의 accurate estimate $\\hat{A}_t$에 대해서 살펴봅시다. 이에 따른 수식은 다음과 같습니다.</p>\n<p>$$\\hat{g} = \\frac{1}{N} \\sum_{n=1}^N \\sum_{t=0}^\\infty \\hat{A}<em>t^n \\nabla</em>{\\theta} \\log \\pi_{\\theta}(a_t^n | s_t^n)$$</p>\n<p>여기서 n은 a batch of episodes에 대하여 index한 것입니다.</p>\n<p>$V$를 approximate value function이라고 합시다. 그리고 $\\delta_t^V = r_t + \\gamma V(s_{t+1}) - V(s_t)$이라고 합시다.</p>\n<p>만약 (이전과 마찬가지로 가정부터 합니다.) correct value function $V = V^{\\pi, \\gamma}$가 있다고 한다면, 이것은 $\\gamma$-just advantage estimator입니다. 실제로, $A^{\\pi, \\gamma}$의 unbiased estimator는 다음과 같습니다.<br>$$\\mathbb{E}_{s_{t+1}} [\\delta_t^{V^{\\pi, \\gamma}}] = \\mathbb{E}_{s_{t+1}} [r_t + \\gamma V^{\\pi, \\gamma} (s_{t+1}) - V^{\\pi, \\gamma} (s_t)]$$<br>$$= \\mathbb{E}_{s_{t+1}} [Q^{\\pi, \\gamma} (s_t, a_t) - V^{\\pi, \\gamma} (s_t)] = A^{\\pi, \\gamma} (s_t, a_t)$$<br>그러나, “이 estimator는 유일하게 $V = V^{\\pi, \\gamma}$에 대한 $\\gamma$-just입니다.” 다른 경우라면 이것은 biased policy gradient estimate일 것입니다. (우리가 하고 싶은 것은 $V$에 대해서만 unbiased estimator가 아니라 advantage function에 대해서 일반화된 unbiased estimator를 얻고 싶은 것입니다. 그래서 아래에서도 나오겠지만, $\\gamma$와 함께 $\\lambda$를 이용한 estimator가 나옵니다.ㅏ)</p>\n<p>그래서 $\\delta$에 대해 $k$의 sum으로 생각해봅시다. 이것을 $\\hat{A}_t^{(k)}$라고 하자. 그러면 아래와 같이 표현할 수 있습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/ra7hxksveg2hz45/figure4.jpg?dl=1\" width=\"300\"> </center>\n\n<p>$\\hat{A}_t^{(k)}$은 returns의 $k$-step estimate와 연관지을 수 있고, $\\delta_t^V = \\hat{A}_t^{(1)}$의 case와 유사하게도 $\\hat{A}_t^{(k)}$를 advantage function의 estimator로 생각할 수 있습니다.</p>\n<p>여기서 $k \\rightarrow \\infty$로 생각해보면 bias가 일반적으로 점점 더 작아집니다. 왜냐하면 $\\gamma^k V(s_{t+k})$가 점점 많이 discounted되서 $-V(s_t)$가 bias에 영향을 미치지 못하기 때문입니다. $k \\rightarrow \\infty$를 취하면 다음과 같은 수식이 나옵니다.</p>\n<p>$$\\hat{A}<em>t^{(\\infty)} = \\sum</em>{l=0}^\\infty \\gamma^l \\delta_{t+l}^V = -V(s_t) + \\sum_{l=0}^\\infty \\gamma^l r_{t+l}$$<br>우변의 수식과 같이 empirical returns에서 value function baseline을 뺀 것으로 나타낼 수 있습니다.</p>\n<p>Generalized Advantage Estimator GAE($\\gamma, \\lambda$)는 $k$-step estimators의 exponentially-weighted average로 나타낼 수 있습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/yg1ybmfkep3towu/figure5.jpg?dl=1\" width=\"300\"> </center>\n\n<p>(TD($\\lambda$)를 떠올려주세요..!)</p>\n<p>위의 마지막 수식에서도 알 수 있듯이, advantage estimator는 Bellman residual terms의 discounted sum과 관련있는 간단한 수식입니다. 다음 section에서 modified reward function을 가진 MDP에 returns로서의 관점에서 위의 마지막 수식을 더 자세히 살펴봅시다. 위의 수식은 TD($\\lambda$)와 많이 유사합니다. 그러나 TD($\\lambda$)는 value function를 estimator하고, 여기서는 advantage function을 estimator합니다.</p>\n<p>위의 수식에서 $\\lambda = 0$ and $\\lambda = 1$에 대해서는 특별한 case가 존재한다. 수식으로 표현하면 다음과 같습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/ufglxzanhcnbi36/figure6.jpg?dl=1\" width=\"300\"> </center>\n\n<ul>\n<li>$GAE(\\gamma, 1)$은 $V$의 정확도와 관계없이 $\\gamma$-just입니다. 그러나 returns의 sum때문에 high variance합니다.</li>\n<li>$GAE(\\gamma, 0)$은 $V = V^{\\pi, \\gamma}$에 대해 $\\gamma$-just입니다. 그리고 bias하지만 일반적으로 훨씬 lower variance를 가집니다. </li>\n<li>$0 &lt; \\lambda &lt; 1$에 대해 GAE는 parameter $\\lambda$를 control하여 bias와 variance사이에 compromise를 만듭니다.</li>\n</ul>\n<p>두 가지 별개의 parameter $\\gamma$ and $\\lambda$를 가진 advantage estimator는 bias-variance tradeoff에 도움을 줍니다. 그러나 이 두 가지 parameter는 각각 다른 목적을 가지고 작동합니다.</p>\n<ul>\n<li>$\\gamma$는 가장 중요하게 value function $V^{\\pi, \\gamma}$ 의 scale을 결정합니다. 또한 $\\gamma$는 $\\lambda$에 의존하지 않습니다. $\\gamma &lt; 1$로 정하는 것은 policy gradient estimate에서 bias합니다.</li>\n<li>반면에, $\\lambda &lt; 1$는 유일하게 value function이 부정확할 때 bias합니다. 그리고 경험상, $\\lambda$의 best value는 $\\gamma$의 best value보다 훨씬 더 낮습니다. 왜냐하면 $\\lambda$가 정확한 value function에 대해 $\\gamma$보다 훨씬 덜 bias하기 때문입니다.</li>\n</ul>\n<p>GAE를 사용할 때, $g^\\gamma$의 biased estimator를 구성할 수 있습니다. 수식은 다음과 같습니다.<br>$$g^\\gamma \\approx \\mathbb{E} [\\sum_{t=0}^\\infty] \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) \\hat{A}<em>t^{GAE(\\gamma, \\lambda)}] = \\mathbb{E} [\\sum</em>{t=0}^{\\infty} \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) \\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+1}^V]$$<br>여기서 $\\lambda = 1$일 때 동일해집니다.</p>\n<h2 id=\"5-Interpretation-as-Reward-Shaping\"><a href=\"#5-Interpretation-as-Reward-Shaping\" class=\"headerlink\" title=\"5. Interpretation as Reward Shaping\"></a>5. Interpretation as Reward Shaping</h2><p>이번 section에서는 앞서 다뤘던 수식 $\\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^V$를 modified reward function의 MDP의 관점으로 생각해봅시다. 조금 더 구체적으로 말하자면, MDP에서 reward shaping transformation을 실행한 후에 적용된 extra discounted factor로서 $\\lambda$를 어떻게 볼 것인지에 대해서 다룹니다. </p>\n<p>(개인적인 comment) 한 가지 먼저 언급하자면, 본래의 목적은 reward shaping이 아니라 variance reduction입니다. 이번 section은 그저 이전에 이러한 개념이 있었고, gae를 다른 관점에서 생각해보자라는 뜻에서 나온 section인 것 같습니다. ‘아~ 이러한 개념이 있구나~!’ 정도로만 알면 될 것 같습니다. ‘gae가 reward shaping의 효과까지 있어서 이러한 section을 넣은 것일까?’ 라는 생각도 해봤지만 아직 잘 모르겠습니다. 실험부분에도 딱히 하지 않은 걸로 봐서는.. 아닌 것 같기도 하고.. 아직까지는 그저 ‘reward shaping의 관점에서 봤을 때에도 gae로 만들어줄 수 있다.’ 정도만 생각하려고 합니다.</p>\n<p>Reward Shaping이란 개념 자체는 일반적인 알고리즘 분야(최단경로 문제 등)에도 있습니다. 하지만 이 개념이 머신러닝에 적용된 건 Andrew Y. Ng이 쓴 Policy Invariance under Reward Transformations Theory and Application to Reward Shaping(1999) 논문입니다. 논문에서 Reward Shaping을 설명하는 부분은 간략하게 다음과 같습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/0w9cuxjkhz2mbcy/figure7.jpg?dl=1\" width=\"300\"> </center><br><center> <img src=\"https://www.dropbox.com/s/xcp9eqvcbv3268g/figure8.jpg?dl=1\" width=\"300\"> </center>\n\n<p>위의 글만 보고 이해가 잘 안됩니다. 그래서 다른 자료들을 찾던 중에 다음의 그림을 찾게 되었습니다. (아래의 그림은 Youtube에 있는 <a href=\"https://www.youtube.com/watch?v=xManAGjbx2k&amp;t=95s\" target=\"_blank\" rel=\"noopener\">Udacity 영상</a>에서 있는 그림입니다.)</p>\n<center> <img src=\"https://www.dropbox.com/s/87kacngez9cl2e6/figure9.jpg?dl=1\" width=\"300\"> </center>\n\n<p>이 그림을 통해서 Reward Shaping을 이해해봅시다.</p>\n<p>Reward Shaping(보상 형성)을 통해서 뭘 하고 싶은지 목적부터 봅시다. reward space가 sparse 한 경우에 reward가 너무 드문드문 나옵니다. 따라서 이것을 꾸준히 reward를 받을 수 있도록 바꿉니다. potential-based shaping function인 $\\Phi$를 만들어서 더하고 빼줍니다. 저 $\\Phi$ 자리에는 대표적으로 state value function이 많이 들어간다고 합니다. (아직 목적이 이해가 안될 수 있습니다. Reward Shaping에 대해서 다 보고 나서 다시 한 번 읽어봅시다.)</p>\n<p>위의 그림은 돌고래가 점프하여 불구멍을 통과해야하는 환경입니다. 하지만 저 불구멍을 통과하기 위해서는 (1) 점프도 해야하고, (2) 불도 피해야하고, (3) 알맞게 착지까지 완료해야합니다. 이렇게 해야 reward를 +1 얻습니다. </p>\n<p>생각해봅시다. 어느 세월에 점프도 해야하고.. 불도 피해야하고.. 알맞게 착지까지해서 reward를 +1 얻겠습니까? 따라서 그 전에도 잘하고 있는 지, 못하고 있는 지를 판단하기 위해, 다시 말해 reward를 꾸준히 받도록 다음과 같이 transformed reward function $\\tilde{r}$을 정의합니다.</p>\n<p>$$\\tilde{r} (s, a, s’) = r(s, a, s’) + \\gamma \\Phi (s’) - \\Phi (s)$$</p>\n<ul>\n<li>여기서 $\\Phi : S \\rightarrow \\mathbb{R}$를 state space에서의 arbitrary scalar-valued function을 나타냅니다. 그리고 $\\Phi$자리에는 대표적으로 state value function이 들어간다고 생각합시다.</li>\n<li>형태가 TD residual term의 결과와 비슷하지만 의미와 의도가 다릅니다. reward shaping은 sparse reward 때문이고, 이전 section에서 봤던 gae는 variance reduction때문에 나온 것입니다.</li>\n</ul>\n<center> <img src=\"https://www.dropbox.com/s/tkfpqniaazwfbld/figure10.jpg?dl=1\" width=\"300\"> </center>\n\n<p>이 transformation은 discounted advantage function $A^{\\pi, \\gamma}$으로도 둘 수 있습니다. state $s_t$를 시작하여 하나의 trajectory의 rewards의 discounted sum을 표현하면 다음과 같습니다.<br>$$\\sum_{l=0}^\\infty \\gamma^l \\tilde{r} (s_{t+l}, a_t, s_{t+l+1}) = \\sum_{l=0}^\\infty \\gamma^l r(s_{t+l}, a_{t+l}, s_{t+l+1}) - \\Phi(s_t)$$</p>\n<p>$\\tilde{Q}^{\\pi, \\gamma}, \\tilde{V}^{\\pi, \\gamma}, \\tilde{A}^{\\pi, \\gamma}$를  transformed MDP의 value function과 advantage function이라고 하면, 다음과 같은 수식이 나옵니다.<br>$$\\tilde{Q}^{\\pi, \\gamma} (s, a) = Q^{\\pi, \\gamma} (s, a) - \\Phi (s)$$<br>$$\\tilde{V}^{\\pi, \\gamma} (s, a) = V^{\\pi, \\gamma} (s, a) - \\Phi (s)$$<br>$$\\tilde{A}^{\\pi, \\gamma} (s, a) = (Q^{\\pi, \\gamma} (s, a) - \\Phi (s)) - (V^\\pi (s) - \\Phi (s)) = A^{\\pi, \\gamma} (s, a)$$</p>\n<p>이제 reward shaping의 idea를 가지고 어떻게 policy gradient estimate를 얻을 수 있는 지에 대해서 알아봅시다. </p>\n<ul>\n<li>$0 \\le \\lambda \\le 1$의 범위에 있는 steeper discount $\\gamma \\lambda$를 사용합니다.</li>\n<li>shaped reward $\\tilde{r}$는 Bellman residual term $\\delta^V$와 동일합니다.</li>\n<li>그리고 $\\Phi = V$와 같다고 보면 다음과 같은 수식이 나옵니다.<br>$$\\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\tilde{r} (s_{t+l}, a_t, s_{t+l+1}) = \\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^V = \\hat{A}_t^{GAE(\\gamma, \\lambda)}$$<br>이렇게 shaped rewards의 $\\gamma \\lambda$-discounted sum을 고려함으로써 GAE를 얻을 수 있습니다. 더 정확하게 $\\lambda = 1$은 $g^\\gamma$의 unbiased estimate이고, 반면에 $\\lambda &lt; 1$은 biased estimate입니다.</li>\n</ul>\n<p>좀 더 나아가서, shaping transformation과 parameters $\\gamma$ and $\\lambda$의 결과를 보기 위해, response function $\\chi$를 이용하면 다음과 같은 수식이 나옵니다.<br>$$\\chi (l; s_t, a_t) = \\mathbb{E} [r_{t+l} | s_t, a_t] - \\mathbb{E} [r_{t+l} | s_t]$$<br>추가적으로 $A^{\\pi, \\gamma} (s, a) = \\sum_{l=0}^\\infty \\gamma^l \\chi (l; s, a)$입니다.<br>그래서 discounted policy gradient estimator는 다음과 같이 쓸 수 있다.<br>$$\\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) A^{\\pi, \\gamma} (s_t, a_t) = \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) \\sum_{l=0}^\\infty \\gamma^l \\chi (l; s, a)$$</p>\n<h2 id=\"6-Value-Fuction-Estimation\"><a href=\"#6-Value-Fuction-Estimation\" class=\"headerlink\" title=\"6. Value Fuction Estimation\"></a>6. Value Fuction Estimation</h2><p>이번 section에서는 Trust Region Optimization Scheme에 따라 Value function을 Estimation합니다.</p>\n<ul>\n<li><h3 id=\"6-1-Simplest-approach\"><a href=\"#6-1-Simplest-approach\" class=\"headerlink\" title=\"6.1 Simplest approach\"></a>6.1 Simplest approach</h3><p>$ minimize_{\\phi} \\sum_{n=1}^{N} \\vert\\vert V_{\\phi}(s_n) - \\hat{V_n} \\vert\\vert^{2} $$</p>\n</li>\n<li><p>위는 가장 간단하게 non-linear approximation 으로 푸는 방법입니다. </p>\n</li>\n<li>$\\hat{V_t} = \\sum_{l=0}^{\\infty}\\gamma^l r_{t+l}$ 은 reward 에 대한 discounted sum 을 의미합니다. </li>\n</ul>\n<h3 id=\"6-2-Trust-region-method-to-optimize-the-value-function\"><a href=\"#6-2-Trust-region-method-to-optimize-the-value-function\" class=\"headerlink\" title=\"6.2 Trust region method to optimize the value function\"></a>6.2 Trust region method to optimize the value function</h3><ul>\n<li>Value function 을 최적화 하기 위해 trust region method 를 사용합니다. </li>\n<li>Trust region 은 최근 데이터에 대해 overfitting 되는 것을 막아줍니다. </li>\n</ul>\n<p>Trust region 문제를 풀기 위해서는 다음 스텝을 따릅니다.</p>\n<ul>\n<li>$\\sigma^2 = \\frac{1}{N} \\sum_{n=1}^{N} \\vert\\vert V_{\\phi old}(s_n) - \\hat{V_n} \\vert\\vert^{2}$ 을 계산합니다. </li>\n<li>그 후에 다음과 같은 constrained opimization 문제를 풉니다. </li>\n</ul>\n<p>$$minimize_{\\phi} \\, \\sum_{n=1}^N \\parallel V_\\phi (s_n) - \\hat{V}<em>n \\parallel^2$$<br>$$subject \\, \\, to \\, \\frac{1}{N} \\sum</em>{n=1}^N \\frac{\\parallel V_\\phi (s_n) - V_{\\phi old} (s_n) \\parallel^2}{2 \\sigma^2} \\le \\epsilon$$</p>\n<ul>\n<li>위의 수식은 사실 old Value function과 새로운 Value function KL distance 가 $\\epsilon$ 보다 작아야한다는 수식과 같습니다. Value function이 평균은 $V_{\\phi}(s)$ 이고 분산이 $\\sigma^2$인 conditional Gaussian distribution 으로 parameterize되었을 뿐입니다.<br><img width=\"400px\" src=\"https://www.dropbox.com/s/uw0v05feu8chqmc/Screenshot%202018-07-08%2010.04.38.png?dl=1\"> </li>\n</ul>\n<p>이 trust region 문제의 답을 conjudate gradient algorithm 을 이용하여 근사값을 구할 수 있습니다. 특히, 다음과 같은  quadratic program 을 풀게됩니다. </p>\n<p>$$minimize_{\\phi} \\, g^T (\\phi - \\phi_{old})$$<br>$$subject \\, \\, to \\, \\frac{1}{N} \\sum_{n=1}^N (\\phi - \\phi_{old})^T H(\\phi - \\phi_{old}) \\le \\epsilon$$</p>\n<ul>\n<li>여기서 $g$는 objective 의 gradient입니다. </li>\n<li>$j_n = \\nabla_{\\phi} V_{\\phi}(s_n)$ 일때, $H = \\frac{1}{N} \\sum_{n} j_n j^T_n$ 이며, $H$는 objective의 hessian에 대해서 gaussian newton method 로 근사한 값입니다. 따라서, value function 을 conditional probability로 해석한다면 Fisher information matrix가 됩니다. </li>\n<li>구현할때의 방법은 TRPO 에서 사용한 방법과 모두 같습니다. </li>\n</ul>\n<h2 id=\"7-Experiments\"><a href=\"#7-Experiments\" class=\"headerlink\" title=\"7.Experiments\"></a>7.Experiments</h2><p>실험은 다음 두 가지 물음에 대해서 디자인 되었습니다.</p>\n<ul>\n<li>GAE 에 따라서 episodic total reward 를 최적화 할때, $\\lambda$와 $\\gamma$ 가 변함에 따라서 어떤 경험적인 효과를 볼 수 있는지 ?</li>\n<li>GAE 와 trust region alogorithm 을 policy 와 value function 모두에 함께 사용했을 때 어려운 문제에 적용되는 큰 뉴럴넷을 최적화 할 수 있을까 ?</li>\n</ul>\n<h3 id=\"6-1-Policy-Optimization-Algorithm\"><a href=\"#6-1-Policy-Optimization-Algorithm\" class=\"headerlink\" title=\"6.1 Policy Optimization Algorithm\"></a>6.1 Policy Optimization Algorithm</h3><p>Policy update는 TRPO 를 사용합니다. TRPO 에 대한 설명은 여기서는 생략하겠습니다. TRPO 포스트를 보고 돌아와주세요 !</p>\n<ul>\n<li><p>이전 TRPO에서 이미 TRPO 와 다른 많은 알고리즘들을 비교하였기 때문에, 여기서는 똑같은 짓을 반복하지 않고 $\\lambda$, $\\gamma$ 가 변함에 따라 어떤 영향이 있는 지에 대한 실험에 집중하겠다고 합니다. (귀찮았던거죠)</p>\n</li>\n<li><p>TRPO 를 적용한 GAE의 최종 알고리즘은 다음과 같습니다.</p>\n<center> <img width=\"600px\" src=\"https://www.dropbox.com/s/b1klz11f2frrvg4/Screenshot%202018-07-08%2010.33.54.png?dl=1\"> </center><br><center> <img width=\"300px\" src=\"https://www.dropbox.com/s/35fxte05pqtiel7/Screenshot%202018-07-08%2010.35.22.png?dl=1\"> </center><br><center> <img width=\"300px\" src=\"https://www.dropbox.com/s/u35pjow3w50bvz1/Screenshot%202018-07-08%2010.36.03.png?dl=1\"> </center>\n\n\n</li>\n</ul>\n<ul>\n<li>여기서 주의할 점은 Policy update ($\\theta_i \\rightarrow \\theta_{i+1}$)에서 $V_{\\phi_i}$ 를 사용했다는 점입니다. </li>\n<li>만약 Value function을 먼저 update하게 된다면 추가적인 bias가 발생합니다.</li>\n<li>극단적으로 생각해보아서, 우리가 Value function을 완벽하게 overfit 해낸다면 Bellman residual ($r_t + \\gamma V(s_{t+1}) - V(S_t)$)은 0이 됩니다. 그럼 Policy gradient의 estimation도 거의 0이 될 것입니다. </li>\n</ul>\n<h3 id=\"7-2-Expermint-details\"><a href=\"#7-2-Expermint-details\" class=\"headerlink\" title=\"7.2 Expermint details\"></a>7.2 Expermint details</h3><h4 id=\"7-2-1-Environment\"><a href=\"#7-2-1-Environment\" class=\"headerlink\" title=\"7.2.1 Environment\"></a>7.2.1 Environment</h4><p>실험에서 사용된 환경은 다음 네 가지 입니다. </p>\n<ol>\n<li>classic cart-pole (x 3D)</li>\n<li>bipedal locomotion </li>\n<li>quadrupedal locomotion </li>\n<li>dynamically standing up for the biped</li>\n</ol>\n<h4 id=\"7-2-2-Architecture\"><a href=\"#7-2-2-Architecture\" class=\"headerlink\" title=\"7.2.2 Architecture\"></a>7.2.2 Architecture</h4><ul>\n<li>3D robot task에 대해서는 같은 모델을 사용하였습니다. <ul>\n<li>layers  = [100, 50, 25] 각각 tanh 사용.(Policy와 Value 네트워크 모두)</li>\n<li>Final output layer은 linear</li>\n</ul>\n</li>\n<li>Cartpole 에 대해서는 1개의 layer 안에 20개의 hidden unit 만 있는 linear policy를 사용했다고 합니다. </li>\n</ul>\n<h4 id=\"7-2-3-Task\"><a href=\"#7-2-3-Task\" class=\"headerlink\" title=\"7.2.3 Task\"></a>7.2.3 Task</h4><ul>\n<li>Cartpole<ul>\n<li>한 배치당 20 개의 trajectory 를 모았고, maximum length 는 1000 입니다. </li>\n</ul>\n</li>\n<li>3D biped locomotion<ul>\n<li>33 dim state , 10 dim action</li>\n<li>50000 time step per batch</li>\n</ul>\n</li>\n<li>3D quadruped locomotion<ul>\n<li>29 dim state, 8 dim action</li>\n<li>200000 time step per batch </li>\n</ul>\n</li>\n<li>3D biped locomotion Standing<ul>\n<li>33 dim state , 10 dim action</li>\n<li>200000 time step per batch </li>\n</ul>\n</li>\n</ul>\n<h4 id=\"7-2-3-results\"><a href=\"#7-2-3-results\" class=\"headerlink\" title=\"7.2.3 results\"></a>7.2.3 results</h4><p>cost 의 관점에서 결과를 나타내었다고 합니다. Cost 는 negative reward와 이것이 최소화 되었는가로 정의되었다고 하는데, 정확히는 안나와있습니다. </p>\n<h5 id=\"7-2-3-1-Cartpole\"><a href=\"#7-2-3-1-Cartpole\" class=\"headerlink\" title=\"7.2.3.1 Cartpole\"></a>7.2.3.1 Cartpole</h5><center> <img width=\"500px\" src=\"https://www.dropbox.com/s/x9pbms1wvg38lda/Screenshot%202018-07-08%2011.08.22.png?dl=1\"> </center>\n\n<ul>\n<li>왼쪽 그림은 $\\gamma$ 를 0.99로 고정시켜놓은 상태에서 $\\lambda$ 를 변화시킴에 따라서 cost 를 측정한 것입니다. </li>\n<li>오른쪽은 $\\gamma$ 와 $\\lambda$ 를 둘 다 변화 시키면서 performance 를 그림으로 나타낸 표입니다. 흰색에 가까울 수록 좋은 퍼포먼스입니다. </li>\n</ul>\n<h5 id=\"7-2-3-2-3D-BIPEDAL-LOCOMOTINO\"><a href=\"#7-2-3-2-3D-BIPEDAL-LOCOMOTINO\" class=\"headerlink\" title=\"7.2.3.2 3D BIPEDAL LOCOMOTINO\"></a>7.2.3.2 3D BIPEDAL LOCOMOTINO</h5><center> <img width=\"500px\" src=\"https://www.dropbox.com/s/i9wj4p6ijojsy82/Screenshot%202018-07-08%2011.25.08.png?dl=1\"> </center>\n\n\n<ul>\n<li>다른 random seed 로 부터 9번 씩 시도한 결과를 mean 을 취해서 사용합니다.</li>\n<li>Best performance 는   $\\gamma \\in [0.99, 0.995]$ 그리고  $\\lambda \\in [0.96, 0.99]$ 일때. </li>\n<li>1000 iteration 후에 빠르고 부드럽고 안정적인 걸음거이가 나옵니다.</li>\n<li>실제로 걸린 시간은 0.01(타입스텝당 시간) <em> 50000(배치당 타임스텝) </em> 1000(배치) <em> 3600(초-&gt;시간) </em> 24 = 5.8일 정도가 걸렸습니다.</li>\n</ul>\n<h5 id=\"7-2-3-3-다른-ROBOT-TASKS\"><a href=\"#7-2-3-3-다른-ROBOT-TASKS\" class=\"headerlink\" title=\"7.2.3.3 다른 ROBOT TASKS\"></a>7.2.3.3 다른 ROBOT TASKS</h5><center> <img width=\"500px\" src=\"https://www.dropbox.com/s/fuuat65we52quht/Screenshot%202018-07-08%2011.33.11.png?dl=1\"> </center>\n\n<ul>\n<li>다른 로봇 TASK에 대해서는 아주 제한적인 실험만 진행합니다.(시간이 부족했던듯)</li>\n<li>Quadruped 에 대해서는 $\\gamma$ =0.995  로 fix, $\\lambda \\in ${0, 0.96}</li>\n<li>Standingup 에 대해서는 $\\gamma$ =0.99  로 fix, $\\lambda \\in ${0, 0.96}</li>\n</ul>\n<h2 id=\"8-Discussion\"><a href=\"#8-Discussion\" class=\"headerlink\" title=\"8. Discussion\"></a>8. Discussion</h2><h3 id=\"8-1-Main-discussion\"><a href=\"#8-1-Main-discussion\" class=\"headerlink\" title=\"8.1 Main discussion\"></a>8.1 Main discussion</h3><p>지금까지 복잡하고 어려운 control problem에서 Reinforcement Learning(RL)은 high sample complexity 때문에 제한이 되어왔습니다. 따라서 이 논문에서 그 제한을 풀고자 advantage function의 good estimate를 얻는 “variance reduction”에 대해 연구하였습니다.</p>\n<p>“Generalized Advantage Estimator(GAE)”라는 것을 제안했고, 이것은 bias-variance tradeoff를 조절하는 두 개의 parameter $\\gamma,\\lambda$를 가집니다.<br>또한 어떻게 Trust Region Policy Optimization과 value function을 optimize하는 Trust Region Algorithm의 idea를 합치는 지를 보였습니다.</p>\n<p>이렇게 함으로써 보다 더 복잡하고 어려운 control task들을 해결할 수 있었습니다.</p>\n<p>GAE의 실험적인 입증으로는 robotic locomotion을 simulation하는 domain입니다. 실험에서도 보여준 것처럼 [0.9, 0.99]의 범위에서 $\\lambda$의 적절한 중간의 값을 통해 best performance를 얻습니다. 좀 더 나아가 연구되어야할 점은 adaptive or automatic하도록 estimator parameter $\\gamma,\\lambda$를 조절하는 방법입니다.</p>\n<h3 id=\"8-2-Future-work\"><a href=\"#8-2-Future-work\" class=\"headerlink\" title=\"8.2 Future work\"></a>8.2 Future work</h3><p>Value function estimation error 와 Policy gradient estimation error 사이의 관계를 알아낸다면, 우리는 Value function fitting 에 더 잘 맞는 error metric 을 사용할 수 있습니다. (policy gradient estimation 의 정확성과 더 잘 맞는 value function)</p>\n<p>Policy와 Value function의 파라미터를 공유하는 모델을 만드는 것은 아주 흥미롭고 이점이 많습니다. 하지만 수렴을 보장하도록 적절한 numerical optimization을 제시하여야 합니다.</p>\n<p>DDPG는 별로입니다. TD(0) 는 bias 가 너무 크고, poor performance 로 이끕니다. 특히나 이 paper 에서는 low-dimention 의 쉬운 문제들만 해결했습니다.</p>\n<center> <img width=\"500px\" src=\"https://www.dropbox.com/s/nhc7t9psul5lr3x/Screenshot%202018-07-08%2011.45.15.png?dl=1\"> </center>\n\n<h3 id=\"8-3-FAQ\"><a href=\"#8-3-FAQ\" class=\"headerlink\" title=\"8.3 FAQ\"></a>8.3 FAQ</h3><ul>\n<li>Compatible features 와는 무슨 관계 ?<ul>\n<li>Compatible features 는 value function을 이용하는 policy gradient 알고리즘들과 함께 자주 언급됩니다.</li>\n<li>Actor Critic 의 저자는 policy의 제한된 representation power 때문에, policy gradient 는 단지 advantage function space 의 subspace에만 의존하게 됩니다.</li>\n<li>이 subspace는 compatible features 에 의해 span 됩니다.</li>\n<li>이 이론은 현재 문제 구조를 어떻게 이용해야 advantage function에 대해 더 나은 estimation 을 할 수 있는 지에 대한 지침을 주지 않습니다. GAE paper 의 idea 와 orthogonal 합니다. </li>\n</ul>\n</li>\n<li>왜 Q function을 사용하지 않는가 ?<ul>\n<li>먼저 state-value function이 더 낮은 차원의 input 을 가진다. 그래서 Q function보다 더 배우기 습니다.</li>\n<li>두 번째로 이 페이퍼에서 제안하는 방법으로는 high bias estimator 에서 low bias estimator 로 $\\lambda$를 통해서 부드럽게 interpolate 할 수 있습니다. </li>\n<li>반면에 Q 를 사용하면 단지 high-bias estimator 밖에 사용할 수 없습니다. </li>\n<li>특히나 return에 대한 one-step estimation 은 엄두를 못낼 정도로 bias 가 큽니다.</li>\n</ul>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"High-Dimensional-Continuous-Control-using-Generalized-Advantage-Estimation\"><a href=\"#High-Dimensional-Continuous-Control-using-Generalized-Advantage-Estimation\" class=\"headerlink\" title=\"High-Dimensional Continuous Control using Generalized Advantage Estimation\"></a>High-Dimensional Continuous Control using Generalized Advantage Estimation</h1><h2 id=\"1-Abstract\"><a href=\"#1-Abstract\" class=\"headerlink\" title=\"1. Abstract\"></a>1. Abstract</h2><p>현존하는 Policy Gradient Method들의 목적은 누적되는 reward들을 optimization하는 것입니다. 하지만 학습할 때에 많은 양의 sample이 필요로 하고, 들어오는 data가 nonstationarity임에도 불구하고 stable and steady improvement가 어렵습니다.</p>\n<p>그래서 이 논문에서는 다음과 같은 방법을 제시합니다.</p>\n<ul>\n<li>TD($\\lambda$)와 유사한 advantage function의 exponentially-weighted estimator를 사용하여 policy gradient estimate의 variance를 줄이는 것 </li>\n<li>policy와 value function에 대한 Trust Region Optimization 사용하는 것</li>\n</ul>\n<p>3D locomotion tasks에 대한 empirical results는 다음과 같습니다.</p>\n<ul>\n<li>bipedal and quadrupedal simulated robots의 달리는 자세를 학습</li>\n<li>bipedal 사람이 땅에 누워있다가 일어서는 것을 학습</li>\n</ul>\n<h2 id=\"2-Introduction\"><a href=\"#2-Introduction\" class=\"headerlink\" title=\"2. Introduction\"></a>2. Introduction</h2><p>기본적으로 “parameterized stochastic policy”를 가정합니다. 이 때 expected total returns의 gradient에 대한 unbiased estimate를 얻을 수 있는데 이것을 REINFORCE라고 부릅니다. 하지만 하나의 action의 결과가 과거와 미래의 action의 결과로 혼동되기 때문에 gradient estimator의 high variance는 시간에 따라 scaling됩니다.</p>\n<p>또 다른 방법은 Actor-Critic이 있습니다. 이 방법은 empirical returns보다 하나의 value function을 사용합니다. 또한 bias하고 lower variance를 가진 estimator입니다. 구체적으로 말하자면, high variance하다면 더 sampling을 하면 되는 반면에 bias는 매우 치명적입니다. 다시 말해 bias는 algorithm이 converge하는 데에 실패하거나 또는 local optimum이 아닌 poor solution에 converge하도록 만듭니다.</p>\n<p>따라서 이 논문에서는 $\\gamma\\in [0,1]$ and $\\lambda\\in [0,1]$에 의해 parameterized estimation scheme인 Generalized Advantage Estimator(GAE)를 다룹니다.</p>\n<p>이 논문을 대략적으로 요약하자면 다음과 같습니다.</p>\n<ul>\n<li>효과적인 variance reduction scheme인 GAE를 다룹니다. 또한 실험할 때 batch trust region algorithm에 더하여 다양한 algorithm들에 적용됩니다.</li>\n<li>value function에 대해 trust region optimization method를 사용합니다. 이렇게 함으로서 더 robust하고 efficient한 방법이 됩니다.</li>\n<li>A와 B를 합쳐서, 실험적으로 control task에 neural network policies를 learning하는 데에 있어서 효과적인 algorithm을 얻습니다. 이러한 결과는 high-demensional continuous control에 RL을 사용함으로서 state of the art로 확장되었습니다.</li>\n</ul>\n<h2 id=\"3-Preliminaries\"><a href=\"#3-Preliminaries\" class=\"headerlink\" title=\"3. Preliminaries\"></a>3. Preliminaries</h2><p>먼저 policy optimization의 “undiscounted formulation”을 가정합니다. (undiscounted formulation에 주목합시다.)</p>\n<ul>\n<li>initial state $s_0$는 distribution $\\rho_0$으로부터 sampling된 것입니다.</li>\n<li>하나의 trajectory ($s_0, a_0, s_1, a_1, …$)는 terminal state에 도달될 때 까지 policy $a_t$ ~ $\\pi(a_t | s_t)$에 따라서 action을 sampling하고, dynamics $s_{t+1}$ ~ $P(s_{t+1} | s_t, a_t)$에 따라서 state를 sampling함으로써 생성됩니다.</li>\n<li>reward $r_t = r(s_t, a_t, s_{t+1})$은 매 time step마다 받아집니다.</li>\n<li>목표는 모든 policies에 대해 finite하다고 가정됨으로서 expected total reward $\\sum_{t=0}^{\\infty} r_t$를 maximize하는 것입니다.</li>\n</ul>\n<p>여기서 중요한 점은 $\\gamma$를 discount의 parameter로 사용하는 것이 아니라 “bias-variance tradeoff를 조절하는 parameter로 사용한다” 는 것입니다.</p>\n<p>policy gradient method는 gradient $g := \\nabla_\\theta \\mathbb{E} [\\sum_{t=0}^\\infty r_t]$를 반복적으로 estimate함으로써 expected total reward를 maximize하는 것인데, policy gradient에는 여러 다른 표현들이 있습니다.<br>$$g = \\mathbb{E} [\\sum_{t=0}^\\infty \\Phi_t \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$<br>여기서 $\\Phi_t$는 아래 중의 하나일 수 있습니다.</p>\n<ol>\n<li>$\\sum_{t=0}^\\infty r_t$: trajectory의 total reward</li>\n<li>$\\sum_{t’=t}^\\infty r_t’$: action $a_t$ 후의 reward</li>\n<li>$\\sum_{t’=t}^\\infty r_t’ - b(s_t)$: 2의 baselined version</li>\n<li>$Q^\\pi (s_t, a_t)$: state-action value function</li>\n<li>$A^\\pi (s_t, a_t)$: advantage function</li>\n<li>$r_t + V^\\pi (s_{t+1}) - V^\\pi (s_t)$: TD residual</li>\n</ol>\n<p>위의 번호 중 4, 5, 6의 수식들은 다음의 정의를 사용합니다.</p>\n<ul>\n<li>$V^\\pi (s_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_t:\\infty} [\\sum_{l=0}^\\infty r_{t+1}]$</li>\n<li>$Q^\\pi (s_t, a_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty} [\\sum_{l=0}^\\infty r_{t+l}]$</li>\n<li>$A^\\pi (s_t, a_t) := Q^\\pi (s_t, a_t) - V^\\pi (s_t)$, (Advantage function)</li>\n</ul>\n<p>추가적으로 colon notation $a : b$는 포괄적인 범위 $(a, a+1, … , b)$입니다. (잘 기억해둡시다. 뒤에 계속해서 colon notation이 나옵니다.)</p>\n<p>여기서부터 parameter $\\gamma$에 대해 좀 더 자세히 알아봅시다. parameter $\\gamma$는 bias하면서 동시에 reward를 downweighting함으로써 variance를 줄입니다. 다시 말해 MDP의 discounted formulation에서 사용된 discounted factor와 일치하지만, 이 논문에서는 “$\\gamma$를 undiscounted MDP에서 variance reduction parameter”로 다룹니다. -&gt; 결과는 같지만, 의미와 의도가 다릅니다.</p>\n<p>discounted value function들은 다음과 같습니다.</p>\n<ul>\n<li>$V^{\\pi, \\gamma} (s_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_t:\\infty} [\\sum_{l=0}^\\infty \\gamma^l r_{t+l}]$ </li>\n<li>$Q^\\pi (s_t, a_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty} [\\sum_{l=0}^\\infty \\gamma^l r_{t+l}]$</li>\n<li>$A^{\\pi, \\gamma} (s_t, a_t) := Q^{\\pi, \\gamma} (s_t, a_t) - V^{\\pi, \\gamma} (s_t)$</li>\n</ul>\n<p>따라서 policy gradient에서의 discounted approximation은 다음과 같이 나타낼 수 있습니다.<br>$$g^\\gamma := \\mathbb{E}_{s_{0:\\infty} a_{0:\\infty}} [\\sum_{t=0}^\\infty A^{\\pi, \\gamma} (s_t, a_t) \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$<br>뒤이어 나오는 section에서는 위의 수식에서 $A^{\\pi, \\gamma}$에 대해 biased (but not too biased) estimator를 얻는 방법에 대해서 나옵니다.</p>\n<p>다음으로 advantage function에서 새롭게 접하는 “$\\gamma$-just estimator”의 notation에 대해 알아봅시다.</p>\n<ul>\n<li>먼저 $\\gamma$-just estimator는 $g^\\gamma$ ${}^1$를 estimate하기 위해 위의 수식에서 $A^{\\pi, \\gamma}$를 $\\gamma$-just estimator로 사용할 때, bias하지 않은 estimator라고 합시다. 그리고 이 $\\gamma$-just advantage estimator를 $\\hat{A}<em>t (s</em>{0:\\infty} , a_{0:\\infty})$라고 하고, 전체의 trajectory에 대한 하나의 function이라고 합시다.</li>\n<li>${}^1$에서 이 논문의 저자가 하고 싶은 말이 있는 것 같습니다. 개인적으로 $\\gamma$-just estimator를 이해하는 데에 있어서 중요한 주석이라 정확히 해석하고자 합니다.<ul>\n<li>$A^\\pi$를 $A^{\\pi, \\gamma}$로 사용함으로써 이미 bias하다라고 말했지만, “이 논문에서 하고자 하는 것은 $g^\\gamma$에 대해 unbiased estimate를 얻고 싶은 것입니다.” 하지만 undiscounted MDP의 policy gradient에 대해서는 당연히 $\\gamma$를 사용하기 때문에 biased estimate를 얻습니다. 개인적으로 이것은 일단 무시하고 $\\gamma$를 사용할 때 어떻게 unbiased estimate를 얻을 지에 대해 좀 더 포커스를 맞추고 있는 것 같습니다.</li>\n<li>그러니까 저 $A^{\\pi, \\gamma}$를 $\\gamma$-just estimator로 바꿔줌으로써 unbiased estimate를 하고 싶다는 것이 뒤이어 나오는 정의와 명제의 핵심이라고 할 수 있습니다.</li>\n</ul>\n</li>\n</ul>\n<p>Definition 1.<br>먼저 가정을 합니다. (가정을 바탕으로 이루어지는 정의라는 것을 주목합시다.) 만약<br>$$\\mathbb{E}_{s_{0:\\infty} a_{0:\\infty}} [\\hat{A}<em>t (s</em>{0:\\infty}, a_{0:\\infty}) \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)] = \\mathbb{E}_{s_{0:\\infty} a_{0:\\infty}} [A^{\\pi, \\gamma} (s_t, a_t) \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$<br>두 수식이 같다면, estimator $\\hat{A}_t$는 $\\gamma$-just입니다.</p>\n<p>그리고 만약 모든 t에 대해서 $\\hat{A}<em>t$이 $\\gamma$-just이라면, 다음과 같이 표현할 수 있습니다.<br>$$\\mathbb{E}</em>{s_{0:\\infty} a_{0:\\infty}} [\\hat{A}<em>t (s</em>{0:\\infty}, a_{0:\\infty}) \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)] = g^\\gamma$$<br>위의 수식이 바로 unbiased estimate입니다.</p>\n<p>$\\gamma$-just인 $\\hat{A}_t$에 대한 한가지 조건은 $\\hat{A}_t$이 두 가지 function $Q_t$ and $b_t$로 나뉠 수 있다는 것입니다.</p>\n<ul>\n<li>$Q_t$는 $\\gamma$-discounted Q-function의 unbiased estimator입니다.</li>\n<li>$b_t$는 action $a_t$전에 sampling된 states and actions의 arbitrary function이다. </li>\n</ul>\n<p>Proposition 1.<br>모든 $(s_t, a_t)$에 대해,<br>$$\\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty | s_t, a_t} [Q_t (s_{t:\\infty}, a_{t:\\infty})] = Q^{\\pi, \\gamma} (s_t, a_t)$$<br>로 인하여 $\\hat{A}<em>t$이<br>$$\\hat{A}</em>{s_{0:\\infty}, a_{0:\\infty}} = Q_t (s_{0:\\infty}, a_{0:\\infty}) - b_t (s_{0:t}, a_{0:t-1})$$<br>형태라고 가정합시다. (가정을 바탕으로 이루어지는 명제라는 점을 주목합시다.)</p>\n<p>그 때, $\\hat{A}_t$은 $\\gamma$-just입니다.</p>\n<p>이 명제에 대한 증명은 Appendix B에 있습니. 그리고 $\\hat{A}_t$에 대한 $\\gamma$-just advantage estimator들은 다음과 같습니다.</p>\n<ul>\n<li>$\\sum_{l=0}^\\infty \\gamma^l r_{t+1}$</li>\n<li>$A^{\\pi, \\gamma} (s_t, a_t)$</li>\n<li>$Q^{\\pi, \\gamma} (s_t, a_t)$</li>\n<li>$r_t + \\gamma V^{\\pi, \\gamma} (s_{t+1}) - V^{\\pi, \\gamma} (s_t)$</li>\n</ul>\n<p>증명을 보기 전에 먼저 Advantage function에 대해서 먼저 살펴봅시다. 아래의 내용은 Sutton이 쓴 논문인 Policy Gradient Methods for Reinforcement Learning with Function Approximation(2000)에서 나온 내용을 리뷰한 것입니다. </p>\n<center> <img src=\"https://www.dropbox.com/s/x737yq97ut6gp1a/Screen%20Shot%202018-07-15%20at%201.16.09%20PM.png?dl=1\" width=\"300\"> </center>\n\n<p>여기서는 “이 수식은 $\\sum_a \\frac{\\partial \\pi (s,a)}{\\partial \\theta} = 0$이기 때문에 가능해진다.”, “즉, 이 수식은 $\\pi(s,a)$의 gradient에만 dependent하기 때문에 advantage 역할을 하는 함수들을 넣어도 아무런 상관이 없다”라는 두 문장을 기억해둡시다.</p>\n<p>이제 증명을 보면 됩니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/e7sj2fwcm1f7hof/figure2.jpg?dl=1\" width=\"300\"> </center><br><center> <img src=\"https://www.dropbox.com/s/kyk4jes1202az4m/figure3.jpg?dl=1\" width=\"300\"> </center>\n\n<p>위의 증명 수식에서 Q와 b의 각각 세번째 수식과 마지막 수식을 자세히 봅시다.</p>\n<ol>\n<li><p>$Q$에 대한 증명</p>\n<ul>\n<li><p>빨간색 부분</p>\n<ul>\n<li>$\\mathbb{E}_{s_{t+1:\\infty}, a_{t+1:\\infty}} [Q_t (s_{0:\\infty}, a_{0:\\infty})]$를 쉽게 말하자면 “모든(과거, 현재, 미래) state와 action에 대해서 expectation을 $t+1$부터 $\\infty$까지 하겠다.”라는 말입니다. 이렇게 함으로써 원래는 $Q^\\pi (s_t, a_t)$가 나와야 맞는 건데, 앞에서 살펴봤다시피 4번째 수식 밑줄 친 자리에는 advantage 역할을 하는 함수들을 넣어도 결과값에 아무런 영향을 끼치지 않기 때문에 의도적으로 $A^\\pi (s_t, a_t)$로 바꾼 것입니다. 또한 이 증명을 바탕으로 $\\hat{A}$이 $\\gamma$-just라는 것을 표현하고 싶기 때문이라고 봐도 됩니다.</li>\n</ul>\n</li>\n<li><p>파란색 부분</p>\n<ul>\n<li>또한 $a_{0:t}$에서 $a_{0:t-1}$로 변한 것은 $A^\\pi (s_t, a_t)$에 baseline이 들어가기 때문에 바꿔준 것입니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>$b$에 대한 증명</p>\n<ul>\n<li>빨간색 부분<ul>\n<li>$\\mathbb{E}_{s_{t+1:\\infty}, a_{t+1:\\infty}} [\\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$가 0으로 바뀌는 것은 위에서 설명했듯이 $\\nabla_\\theta$자체가 $\\log \\pi_\\theta$만 gradient하기 때문에 이것을 expectation을 취하면 0이 됩니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"4-Advantage-Function-Estimation\"><a href=\"#4-Advantage-Function-Estimation\" class=\"headerlink\" title=\"4. Advantage Function Estimation\"></a>4. Advantage Function Estimation</h2><p>이번 section에는 discounted advantage function $A^{\\pi, \\gamma} (s_t, a_t)$의 accurate estimate $\\hat{A}_t$에 대해서 살펴봅시다. 이에 따른 수식은 다음과 같습니다.</p>\n<p>$$\\hat{g} = \\frac{1}{N} \\sum_{n=1}^N \\sum_{t=0}^\\infty \\hat{A}<em>t^n \\nabla</em>{\\theta} \\log \\pi_{\\theta}(a_t^n | s_t^n)$$</p>\n<p>여기서 n은 a batch of episodes에 대하여 index한 것입니다.</p>\n<p>$V$를 approximate value function이라고 합시다. 그리고 $\\delta_t^V = r_t + \\gamma V(s_{t+1}) - V(s_t)$이라고 합시다.</p>\n<p>만약 (이전과 마찬가지로 가정부터 합니다.) correct value function $V = V^{\\pi, \\gamma}$가 있다고 한다면, 이것은 $\\gamma$-just advantage estimator입니다. 실제로, $A^{\\pi, \\gamma}$의 unbiased estimator는 다음과 같습니다.<br>$$\\mathbb{E}_{s_{t+1}} [\\delta_t^{V^{\\pi, \\gamma}}] = \\mathbb{E}_{s_{t+1}} [r_t + \\gamma V^{\\pi, \\gamma} (s_{t+1}) - V^{\\pi, \\gamma} (s_t)]$$<br>$$= \\mathbb{E}_{s_{t+1}} [Q^{\\pi, \\gamma} (s_t, a_t) - V^{\\pi, \\gamma} (s_t)] = A^{\\pi, \\gamma} (s_t, a_t)$$<br>그러나, “이 estimator는 유일하게 $V = V^{\\pi, \\gamma}$에 대한 $\\gamma$-just입니다.” 다른 경우라면 이것은 biased policy gradient estimate일 것입니다. (우리가 하고 싶은 것은 $V$에 대해서만 unbiased estimator가 아니라 advantage function에 대해서 일반화된 unbiased estimator를 얻고 싶은 것입니다. 그래서 아래에서도 나오겠지만, $\\gamma$와 함께 $\\lambda$를 이용한 estimator가 나옵니다.ㅏ)</p>\n<p>그래서 $\\delta$에 대해 $k$의 sum으로 생각해봅시다. 이것을 $\\hat{A}_t^{(k)}$라고 하자. 그러면 아래와 같이 표현할 수 있습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/ra7hxksveg2hz45/figure4.jpg?dl=1\" width=\"300\"> </center>\n\n<p>$\\hat{A}_t^{(k)}$은 returns의 $k$-step estimate와 연관지을 수 있고, $\\delta_t^V = \\hat{A}_t^{(1)}$의 case와 유사하게도 $\\hat{A}_t^{(k)}$를 advantage function의 estimator로 생각할 수 있습니다.</p>\n<p>여기서 $k \\rightarrow \\infty$로 생각해보면 bias가 일반적으로 점점 더 작아집니다. 왜냐하면 $\\gamma^k V(s_{t+k})$가 점점 많이 discounted되서 $-V(s_t)$가 bias에 영향을 미치지 못하기 때문입니다. $k \\rightarrow \\infty$를 취하면 다음과 같은 수식이 나옵니다.</p>\n<p>$$\\hat{A}<em>t^{(\\infty)} = \\sum</em>{l=0}^\\infty \\gamma^l \\delta_{t+l}^V = -V(s_t) + \\sum_{l=0}^\\infty \\gamma^l r_{t+l}$$<br>우변의 수식과 같이 empirical returns에서 value function baseline을 뺀 것으로 나타낼 수 있습니다.</p>\n<p>Generalized Advantage Estimator GAE($\\gamma, \\lambda$)는 $k$-step estimators의 exponentially-weighted average로 나타낼 수 있습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/yg1ybmfkep3towu/figure5.jpg?dl=1\" width=\"300\"> </center>\n\n<p>(TD($\\lambda$)를 떠올려주세요..!)</p>\n<p>위의 마지막 수식에서도 알 수 있듯이, advantage estimator는 Bellman residual terms의 discounted sum과 관련있는 간단한 수식입니다. 다음 section에서 modified reward function을 가진 MDP에 returns로서의 관점에서 위의 마지막 수식을 더 자세히 살펴봅시다. 위의 수식은 TD($\\lambda$)와 많이 유사합니다. 그러나 TD($\\lambda$)는 value function를 estimator하고, 여기서는 advantage function을 estimator합니다.</p>\n<p>위의 수식에서 $\\lambda = 0$ and $\\lambda = 1$에 대해서는 특별한 case가 존재한다. 수식으로 표현하면 다음과 같습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/ufglxzanhcnbi36/figure6.jpg?dl=1\" width=\"300\"> </center>\n\n<ul>\n<li>$GAE(\\gamma, 1)$은 $V$의 정확도와 관계없이 $\\gamma$-just입니다. 그러나 returns의 sum때문에 high variance합니다.</li>\n<li>$GAE(\\gamma, 0)$은 $V = V^{\\pi, \\gamma}$에 대해 $\\gamma$-just입니다. 그리고 bias하지만 일반적으로 훨씬 lower variance를 가집니다. </li>\n<li>$0 &lt; \\lambda &lt; 1$에 대해 GAE는 parameter $\\lambda$를 control하여 bias와 variance사이에 compromise를 만듭니다.</li>\n</ul>\n<p>두 가지 별개의 parameter $\\gamma$ and $\\lambda$를 가진 advantage estimator는 bias-variance tradeoff에 도움을 줍니다. 그러나 이 두 가지 parameter는 각각 다른 목적을 가지고 작동합니다.</p>\n<ul>\n<li>$\\gamma$는 가장 중요하게 value function $V^{\\pi, \\gamma}$ 의 scale을 결정합니다. 또한 $\\gamma$는 $\\lambda$에 의존하지 않습니다. $\\gamma &lt; 1$로 정하는 것은 policy gradient estimate에서 bias합니다.</li>\n<li>반면에, $\\lambda &lt; 1$는 유일하게 value function이 부정확할 때 bias합니다. 그리고 경험상, $\\lambda$의 best value는 $\\gamma$의 best value보다 훨씬 더 낮습니다. 왜냐하면 $\\lambda$가 정확한 value function에 대해 $\\gamma$보다 훨씬 덜 bias하기 때문입니다.</li>\n</ul>\n<p>GAE를 사용할 때, $g^\\gamma$의 biased estimator를 구성할 수 있습니다. 수식은 다음과 같습니다.<br>$$g^\\gamma \\approx \\mathbb{E} [\\sum_{t=0}^\\infty] \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) \\hat{A}<em>t^{GAE(\\gamma, \\lambda)}] = \\mathbb{E} [\\sum</em>{t=0}^{\\infty} \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) \\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+1}^V]$$<br>여기서 $\\lambda = 1$일 때 동일해집니다.</p>\n<h2 id=\"5-Interpretation-as-Reward-Shaping\"><a href=\"#5-Interpretation-as-Reward-Shaping\" class=\"headerlink\" title=\"5. Interpretation as Reward Shaping\"></a>5. Interpretation as Reward Shaping</h2><p>이번 section에서는 앞서 다뤘던 수식 $\\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^V$를 modified reward function의 MDP의 관점으로 생각해봅시다. 조금 더 구체적으로 말하자면, MDP에서 reward shaping transformation을 실행한 후에 적용된 extra discounted factor로서 $\\lambda$를 어떻게 볼 것인지에 대해서 다룹니다. </p>\n<p>(개인적인 comment) 한 가지 먼저 언급하자면, 본래의 목적은 reward shaping이 아니라 variance reduction입니다. 이번 section은 그저 이전에 이러한 개념이 있었고, gae를 다른 관점에서 생각해보자라는 뜻에서 나온 section인 것 같습니다. ‘아~ 이러한 개념이 있구나~!’ 정도로만 알면 될 것 같습니다. ‘gae가 reward shaping의 효과까지 있어서 이러한 section을 넣은 것일까?’ 라는 생각도 해봤지만 아직 잘 모르겠습니다. 실험부분에도 딱히 하지 않은 걸로 봐서는.. 아닌 것 같기도 하고.. 아직까지는 그저 ‘reward shaping의 관점에서 봤을 때에도 gae로 만들어줄 수 있다.’ 정도만 생각하려고 합니다.</p>\n<p>Reward Shaping이란 개념 자체는 일반적인 알고리즘 분야(최단경로 문제 등)에도 있습니다. 하지만 이 개념이 머신러닝에 적용된 건 Andrew Y. Ng이 쓴 Policy Invariance under Reward Transformations Theory and Application to Reward Shaping(1999) 논문입니다. 논문에서 Reward Shaping을 설명하는 부분은 간략하게 다음과 같습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/0w9cuxjkhz2mbcy/figure7.jpg?dl=1\" width=\"300\"> </center><br><center> <img src=\"https://www.dropbox.com/s/xcp9eqvcbv3268g/figure8.jpg?dl=1\" width=\"300\"> </center>\n\n<p>위의 글만 보고 이해가 잘 안됩니다. 그래서 다른 자료들을 찾던 중에 다음의 그림을 찾게 되었습니다. (아래의 그림은 Youtube에 있는 <a href=\"https://www.youtube.com/watch?v=xManAGjbx2k&amp;t=95s\" target=\"_blank\" rel=\"noopener\">Udacity 영상</a>에서 있는 그림입니다.)</p>\n<center> <img src=\"https://www.dropbox.com/s/87kacngez9cl2e6/figure9.jpg?dl=1\" width=\"300\"> </center>\n\n<p>이 그림을 통해서 Reward Shaping을 이해해봅시다.</p>\n<p>Reward Shaping(보상 형성)을 통해서 뭘 하고 싶은지 목적부터 봅시다. reward space가 sparse 한 경우에 reward가 너무 드문드문 나옵니다. 따라서 이것을 꾸준히 reward를 받을 수 있도록 바꿉니다. potential-based shaping function인 $\\Phi$를 만들어서 더하고 빼줍니다. 저 $\\Phi$ 자리에는 대표적으로 state value function이 많이 들어간다고 합니다. (아직 목적이 이해가 안될 수 있습니다. Reward Shaping에 대해서 다 보고 나서 다시 한 번 읽어봅시다.)</p>\n<p>위의 그림은 돌고래가 점프하여 불구멍을 통과해야하는 환경입니다. 하지만 저 불구멍을 통과하기 위해서는 (1) 점프도 해야하고, (2) 불도 피해야하고, (3) 알맞게 착지까지 완료해야합니다. 이렇게 해야 reward를 +1 얻습니다. </p>\n<p>생각해봅시다. 어느 세월에 점프도 해야하고.. 불도 피해야하고.. 알맞게 착지까지해서 reward를 +1 얻겠습니까? 따라서 그 전에도 잘하고 있는 지, 못하고 있는 지를 판단하기 위해, 다시 말해 reward를 꾸준히 받도록 다음과 같이 transformed reward function $\\tilde{r}$을 정의합니다.</p>\n<p>$$\\tilde{r} (s, a, s’) = r(s, a, s’) + \\gamma \\Phi (s’) - \\Phi (s)$$</p>\n<ul>\n<li>여기서 $\\Phi : S \\rightarrow \\mathbb{R}$를 state space에서의 arbitrary scalar-valued function을 나타냅니다. 그리고 $\\Phi$자리에는 대표적으로 state value function이 들어간다고 생각합시다.</li>\n<li>형태가 TD residual term의 결과와 비슷하지만 의미와 의도가 다릅니다. reward shaping은 sparse reward 때문이고, 이전 section에서 봤던 gae는 variance reduction때문에 나온 것입니다.</li>\n</ul>\n<center> <img src=\"https://www.dropbox.com/s/tkfpqniaazwfbld/figure10.jpg?dl=1\" width=\"300\"> </center>\n\n<p>이 transformation은 discounted advantage function $A^{\\pi, \\gamma}$으로도 둘 수 있습니다. state $s_t$를 시작하여 하나의 trajectory의 rewards의 discounted sum을 표현하면 다음과 같습니다.<br>$$\\sum_{l=0}^\\infty \\gamma^l \\tilde{r} (s_{t+l}, a_t, s_{t+l+1}) = \\sum_{l=0}^\\infty \\gamma^l r(s_{t+l}, a_{t+l}, s_{t+l+1}) - \\Phi(s_t)$$</p>\n<p>$\\tilde{Q}^{\\pi, \\gamma}, \\tilde{V}^{\\pi, \\gamma}, \\tilde{A}^{\\pi, \\gamma}$를  transformed MDP의 value function과 advantage function이라고 하면, 다음과 같은 수식이 나옵니다.<br>$$\\tilde{Q}^{\\pi, \\gamma} (s, a) = Q^{\\pi, \\gamma} (s, a) - \\Phi (s)$$<br>$$\\tilde{V}^{\\pi, \\gamma} (s, a) = V^{\\pi, \\gamma} (s, a) - \\Phi (s)$$<br>$$\\tilde{A}^{\\pi, \\gamma} (s, a) = (Q^{\\pi, \\gamma} (s, a) - \\Phi (s)) - (V^\\pi (s) - \\Phi (s)) = A^{\\pi, \\gamma} (s, a)$$</p>\n<p>이제 reward shaping의 idea를 가지고 어떻게 policy gradient estimate를 얻을 수 있는 지에 대해서 알아봅시다. </p>\n<ul>\n<li>$0 \\le \\lambda \\le 1$의 범위에 있는 steeper discount $\\gamma \\lambda$를 사용합니다.</li>\n<li>shaped reward $\\tilde{r}$는 Bellman residual term $\\delta^V$와 동일합니다.</li>\n<li>그리고 $\\Phi = V$와 같다고 보면 다음과 같은 수식이 나옵니다.<br>$$\\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\tilde{r} (s_{t+l}, a_t, s_{t+l+1}) = \\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^V = \\hat{A}_t^{GAE(\\gamma, \\lambda)}$$<br>이렇게 shaped rewards의 $\\gamma \\lambda$-discounted sum을 고려함으로써 GAE를 얻을 수 있습니다. 더 정확하게 $\\lambda = 1$은 $g^\\gamma$의 unbiased estimate이고, 반면에 $\\lambda &lt; 1$은 biased estimate입니다.</li>\n</ul>\n<p>좀 더 나아가서, shaping transformation과 parameters $\\gamma$ and $\\lambda$의 결과를 보기 위해, response function $\\chi$를 이용하면 다음과 같은 수식이 나옵니다.<br>$$\\chi (l; s_t, a_t) = \\mathbb{E} [r_{t+l} | s_t, a_t] - \\mathbb{E} [r_{t+l} | s_t]$$<br>추가적으로 $A^{\\pi, \\gamma} (s, a) = \\sum_{l=0}^\\infty \\gamma^l \\chi (l; s, a)$입니다.<br>그래서 discounted policy gradient estimator는 다음과 같이 쓸 수 있다.<br>$$\\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) A^{\\pi, \\gamma} (s_t, a_t) = \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) \\sum_{l=0}^\\infty \\gamma^l \\chi (l; s, a)$$</p>\n<h2 id=\"6-Value-Fuction-Estimation\"><a href=\"#6-Value-Fuction-Estimation\" class=\"headerlink\" title=\"6. Value Fuction Estimation\"></a>6. Value Fuction Estimation</h2><p>이번 section에서는 Trust Region Optimization Scheme에 따라 Value function을 Estimation합니다.</p>\n<ul>\n<li><h3 id=\"6-1-Simplest-approach\"><a href=\"#6-1-Simplest-approach\" class=\"headerlink\" title=\"6.1 Simplest approach\"></a>6.1 Simplest approach</h3><p>$ minimize_{\\phi} \\sum_{n=1}^{N} \\vert\\vert V_{\\phi}(s_n) - \\hat{V_n} \\vert\\vert^{2} $$</p>\n</li>\n<li><p>위는 가장 간단하게 non-linear approximation 으로 푸는 방법입니다. </p>\n</li>\n<li>$\\hat{V_t} = \\sum_{l=0}^{\\infty}\\gamma^l r_{t+l}$ 은 reward 에 대한 discounted sum 을 의미합니다. </li>\n</ul>\n<h3 id=\"6-2-Trust-region-method-to-optimize-the-value-function\"><a href=\"#6-2-Trust-region-method-to-optimize-the-value-function\" class=\"headerlink\" title=\"6.2 Trust region method to optimize the value function\"></a>6.2 Trust region method to optimize the value function</h3><ul>\n<li>Value function 을 최적화 하기 위해 trust region method 를 사용합니다. </li>\n<li>Trust region 은 최근 데이터에 대해 overfitting 되는 것을 막아줍니다. </li>\n</ul>\n<p>Trust region 문제를 풀기 위해서는 다음 스텝을 따릅니다.</p>\n<ul>\n<li>$\\sigma^2 = \\frac{1}{N} \\sum_{n=1}^{N} \\vert\\vert V_{\\phi old}(s_n) - \\hat{V_n} \\vert\\vert^{2}$ 을 계산합니다. </li>\n<li>그 후에 다음과 같은 constrained opimization 문제를 풉니다. </li>\n</ul>\n<p>$$minimize_{\\phi} \\, \\sum_{n=1}^N \\parallel V_\\phi (s_n) - \\hat{V}<em>n \\parallel^2$$<br>$$subject \\, \\, to \\, \\frac{1}{N} \\sum</em>{n=1}^N \\frac{\\parallel V_\\phi (s_n) - V_{\\phi old} (s_n) \\parallel^2}{2 \\sigma^2} \\le \\epsilon$$</p>\n<ul>\n<li>위의 수식은 사실 old Value function과 새로운 Value function KL distance 가 $\\epsilon$ 보다 작아야한다는 수식과 같습니다. Value function이 평균은 $V_{\\phi}(s)$ 이고 분산이 $\\sigma^2$인 conditional Gaussian distribution 으로 parameterize되었을 뿐입니다.<br><img width=\"400px\" src=\"https://www.dropbox.com/s/uw0v05feu8chqmc/Screenshot%202018-07-08%2010.04.38.png?dl=1\"> </li>\n</ul>\n<p>이 trust region 문제의 답을 conjudate gradient algorithm 을 이용하여 근사값을 구할 수 있습니다. 특히, 다음과 같은  quadratic program 을 풀게됩니다. </p>\n<p>$$minimize_{\\phi} \\, g^T (\\phi - \\phi_{old})$$<br>$$subject \\, \\, to \\, \\frac{1}{N} \\sum_{n=1}^N (\\phi - \\phi_{old})^T H(\\phi - \\phi_{old}) \\le \\epsilon$$</p>\n<ul>\n<li>여기서 $g$는 objective 의 gradient입니다. </li>\n<li>$j_n = \\nabla_{\\phi} V_{\\phi}(s_n)$ 일때, $H = \\frac{1}{N} \\sum_{n} j_n j^T_n$ 이며, $H$는 objective의 hessian에 대해서 gaussian newton method 로 근사한 값입니다. 따라서, value function 을 conditional probability로 해석한다면 Fisher information matrix가 됩니다. </li>\n<li>구현할때의 방법은 TRPO 에서 사용한 방법과 모두 같습니다. </li>\n</ul>\n<h2 id=\"7-Experiments\"><a href=\"#7-Experiments\" class=\"headerlink\" title=\"7.Experiments\"></a>7.Experiments</h2><p>실험은 다음 두 가지 물음에 대해서 디자인 되었습니다.</p>\n<ul>\n<li>GAE 에 따라서 episodic total reward 를 최적화 할때, $\\lambda$와 $\\gamma$ 가 변함에 따라서 어떤 경험적인 효과를 볼 수 있는지 ?</li>\n<li>GAE 와 trust region alogorithm 을 policy 와 value function 모두에 함께 사용했을 때 어려운 문제에 적용되는 큰 뉴럴넷을 최적화 할 수 있을까 ?</li>\n</ul>\n<h3 id=\"6-1-Policy-Optimization-Algorithm\"><a href=\"#6-1-Policy-Optimization-Algorithm\" class=\"headerlink\" title=\"6.1 Policy Optimization Algorithm\"></a>6.1 Policy Optimization Algorithm</h3><p>Policy update는 TRPO 를 사용합니다. TRPO 에 대한 설명은 여기서는 생략하겠습니다. TRPO 포스트를 보고 돌아와주세요 !</p>\n<ul>\n<li><p>이전 TRPO에서 이미 TRPO 와 다른 많은 알고리즘들을 비교하였기 때문에, 여기서는 똑같은 짓을 반복하지 않고 $\\lambda$, $\\gamma$ 가 변함에 따라 어떤 영향이 있는 지에 대한 실험에 집중하겠다고 합니다. (귀찮았던거죠)</p>\n</li>\n<li><p>TRPO 를 적용한 GAE의 최종 알고리즘은 다음과 같습니다.</p>\n<center> <img width=\"600px\" src=\"https://www.dropbox.com/s/b1klz11f2frrvg4/Screenshot%202018-07-08%2010.33.54.png?dl=1\"> </center><br><center> <img width=\"300px\" src=\"https://www.dropbox.com/s/35fxte05pqtiel7/Screenshot%202018-07-08%2010.35.22.png?dl=1\"> </center><br><center> <img width=\"300px\" src=\"https://www.dropbox.com/s/u35pjow3w50bvz1/Screenshot%202018-07-08%2010.36.03.png?dl=1\"> </center>\n\n\n</li>\n</ul>\n<ul>\n<li>여기서 주의할 점은 Policy update ($\\theta_i \\rightarrow \\theta_{i+1}$)에서 $V_{\\phi_i}$ 를 사용했다는 점입니다. </li>\n<li>만약 Value function을 먼저 update하게 된다면 추가적인 bias가 발생합니다.</li>\n<li>극단적으로 생각해보아서, 우리가 Value function을 완벽하게 overfit 해낸다면 Bellman residual ($r_t + \\gamma V(s_{t+1}) - V(S_t)$)은 0이 됩니다. 그럼 Policy gradient의 estimation도 거의 0이 될 것입니다. </li>\n</ul>\n<h3 id=\"7-2-Expermint-details\"><a href=\"#7-2-Expermint-details\" class=\"headerlink\" title=\"7.2 Expermint details\"></a>7.2 Expermint details</h3><h4 id=\"7-2-1-Environment\"><a href=\"#7-2-1-Environment\" class=\"headerlink\" title=\"7.2.1 Environment\"></a>7.2.1 Environment</h4><p>실험에서 사용된 환경은 다음 네 가지 입니다. </p>\n<ol>\n<li>classic cart-pole (x 3D)</li>\n<li>bipedal locomotion </li>\n<li>quadrupedal locomotion </li>\n<li>dynamically standing up for the biped</li>\n</ol>\n<h4 id=\"7-2-2-Architecture\"><a href=\"#7-2-2-Architecture\" class=\"headerlink\" title=\"7.2.2 Architecture\"></a>7.2.2 Architecture</h4><ul>\n<li>3D robot task에 대해서는 같은 모델을 사용하였습니다. <ul>\n<li>layers  = [100, 50, 25] 각각 tanh 사용.(Policy와 Value 네트워크 모두)</li>\n<li>Final output layer은 linear</li>\n</ul>\n</li>\n<li>Cartpole 에 대해서는 1개의 layer 안에 20개의 hidden unit 만 있는 linear policy를 사용했다고 합니다. </li>\n</ul>\n<h4 id=\"7-2-3-Task\"><a href=\"#7-2-3-Task\" class=\"headerlink\" title=\"7.2.3 Task\"></a>7.2.3 Task</h4><ul>\n<li>Cartpole<ul>\n<li>한 배치당 20 개의 trajectory 를 모았고, maximum length 는 1000 입니다. </li>\n</ul>\n</li>\n<li>3D biped locomotion<ul>\n<li>33 dim state , 10 dim action</li>\n<li>50000 time step per batch</li>\n</ul>\n</li>\n<li>3D quadruped locomotion<ul>\n<li>29 dim state, 8 dim action</li>\n<li>200000 time step per batch </li>\n</ul>\n</li>\n<li>3D biped locomotion Standing<ul>\n<li>33 dim state , 10 dim action</li>\n<li>200000 time step per batch </li>\n</ul>\n</li>\n</ul>\n<h4 id=\"7-2-3-results\"><a href=\"#7-2-3-results\" class=\"headerlink\" title=\"7.2.3 results\"></a>7.2.3 results</h4><p>cost 의 관점에서 결과를 나타내었다고 합니다. Cost 는 negative reward와 이것이 최소화 되었는가로 정의되었다고 하는데, 정확히는 안나와있습니다. </p>\n<h5 id=\"7-2-3-1-Cartpole\"><a href=\"#7-2-3-1-Cartpole\" class=\"headerlink\" title=\"7.2.3.1 Cartpole\"></a>7.2.3.1 Cartpole</h5><center> <img width=\"500px\" src=\"https://www.dropbox.com/s/x9pbms1wvg38lda/Screenshot%202018-07-08%2011.08.22.png?dl=1\"> </center>\n\n<ul>\n<li>왼쪽 그림은 $\\gamma$ 를 0.99로 고정시켜놓은 상태에서 $\\lambda$ 를 변화시킴에 따라서 cost 를 측정한 것입니다. </li>\n<li>오른쪽은 $\\gamma$ 와 $\\lambda$ 를 둘 다 변화 시키면서 performance 를 그림으로 나타낸 표입니다. 흰색에 가까울 수록 좋은 퍼포먼스입니다. </li>\n</ul>\n<h5 id=\"7-2-3-2-3D-BIPEDAL-LOCOMOTINO\"><a href=\"#7-2-3-2-3D-BIPEDAL-LOCOMOTINO\" class=\"headerlink\" title=\"7.2.3.2 3D BIPEDAL LOCOMOTINO\"></a>7.2.3.2 3D BIPEDAL LOCOMOTINO</h5><center> <img width=\"500px\" src=\"https://www.dropbox.com/s/i9wj4p6ijojsy82/Screenshot%202018-07-08%2011.25.08.png?dl=1\"> </center>\n\n\n<ul>\n<li>다른 random seed 로 부터 9번 씩 시도한 결과를 mean 을 취해서 사용합니다.</li>\n<li>Best performance 는   $\\gamma \\in [0.99, 0.995]$ 그리고  $\\lambda \\in [0.96, 0.99]$ 일때. </li>\n<li>1000 iteration 후에 빠르고 부드럽고 안정적인 걸음거이가 나옵니다.</li>\n<li>실제로 걸린 시간은 0.01(타입스텝당 시간) <em> 50000(배치당 타임스텝) </em> 1000(배치) <em> 3600(초-&gt;시간) </em> 24 = 5.8일 정도가 걸렸습니다.</li>\n</ul>\n<h5 id=\"7-2-3-3-다른-ROBOT-TASKS\"><a href=\"#7-2-3-3-다른-ROBOT-TASKS\" class=\"headerlink\" title=\"7.2.3.3 다른 ROBOT TASKS\"></a>7.2.3.3 다른 ROBOT TASKS</h5><center> <img width=\"500px\" src=\"https://www.dropbox.com/s/fuuat65we52quht/Screenshot%202018-07-08%2011.33.11.png?dl=1\"> </center>\n\n<ul>\n<li>다른 로봇 TASK에 대해서는 아주 제한적인 실험만 진행합니다.(시간이 부족했던듯)</li>\n<li>Quadruped 에 대해서는 $\\gamma$ =0.995  로 fix, $\\lambda \\in ${0, 0.96}</li>\n<li>Standingup 에 대해서는 $\\gamma$ =0.99  로 fix, $\\lambda \\in ${0, 0.96}</li>\n</ul>\n<h2 id=\"8-Discussion\"><a href=\"#8-Discussion\" class=\"headerlink\" title=\"8. Discussion\"></a>8. Discussion</h2><h3 id=\"8-1-Main-discussion\"><a href=\"#8-1-Main-discussion\" class=\"headerlink\" title=\"8.1 Main discussion\"></a>8.1 Main discussion</h3><p>지금까지 복잡하고 어려운 control problem에서 Reinforcement Learning(RL)은 high sample complexity 때문에 제한이 되어왔습니다. 따라서 이 논문에서 그 제한을 풀고자 advantage function의 good estimate를 얻는 “variance reduction”에 대해 연구하였습니다.</p>\n<p>“Generalized Advantage Estimator(GAE)”라는 것을 제안했고, 이것은 bias-variance tradeoff를 조절하는 두 개의 parameter $\\gamma,\\lambda$를 가집니다.<br>또한 어떻게 Trust Region Policy Optimization과 value function을 optimize하는 Trust Region Algorithm의 idea를 합치는 지를 보였습니다.</p>\n<p>이렇게 함으로써 보다 더 복잡하고 어려운 control task들을 해결할 수 있었습니다.</p>\n<p>GAE의 실험적인 입증으로는 robotic locomotion을 simulation하는 domain입니다. 실험에서도 보여준 것처럼 [0.9, 0.99]의 범위에서 $\\lambda$의 적절한 중간의 값을 통해 best performance를 얻습니다. 좀 더 나아가 연구되어야할 점은 adaptive or automatic하도록 estimator parameter $\\gamma,\\lambda$를 조절하는 방법입니다.</p>\n<h3 id=\"8-2-Future-work\"><a href=\"#8-2-Future-work\" class=\"headerlink\" title=\"8.2 Future work\"></a>8.2 Future work</h3><p>Value function estimation error 와 Policy gradient estimation error 사이의 관계를 알아낸다면, 우리는 Value function fitting 에 더 잘 맞는 error metric 을 사용할 수 있습니다. (policy gradient estimation 의 정확성과 더 잘 맞는 value function)</p>\n<p>Policy와 Value function의 파라미터를 공유하는 모델을 만드는 것은 아주 흥미롭고 이점이 많습니다. 하지만 수렴을 보장하도록 적절한 numerical optimization을 제시하여야 합니다.</p>\n<p>DDPG는 별로입니다. TD(0) 는 bias 가 너무 크고, poor performance 로 이끕니다. 특히나 이 paper 에서는 low-dimention 의 쉬운 문제들만 해결했습니다.</p>\n<center> <img width=\"500px\" src=\"https://www.dropbox.com/s/nhc7t9psul5lr3x/Screenshot%202018-07-08%2011.45.15.png?dl=1\"> </center>\n\n<h3 id=\"8-3-FAQ\"><a href=\"#8-3-FAQ\" class=\"headerlink\" title=\"8.3 FAQ\"></a>8.3 FAQ</h3><ul>\n<li>Compatible features 와는 무슨 관계 ?<ul>\n<li>Compatible features 는 value function을 이용하는 policy gradient 알고리즘들과 함께 자주 언급됩니다.</li>\n<li>Actor Critic 의 저자는 policy의 제한된 representation power 때문에, policy gradient 는 단지 advantage function space 의 subspace에만 의존하게 됩니다.</li>\n<li>이 subspace는 compatible features 에 의해 span 됩니다.</li>\n<li>이 이론은 현재 문제 구조를 어떻게 이용해야 advantage function에 대해 더 나은 estimation 을 할 수 있는 지에 대한 지침을 주지 않습니다. GAE paper 의 idea 와 orthogonal 합니다. </li>\n</ul>\n</li>\n<li>왜 Q function을 사용하지 않는가 ?<ul>\n<li>먼저 state-value function이 더 낮은 차원의 input 을 가진다. 그래서 Q function보다 더 배우기 습니다.</li>\n<li>두 번째로 이 페이퍼에서 제안하는 방법으로는 high bias estimator 에서 low bias estimator 로 $\\lambda$를 통해서 부드럽게 interpolate 할 수 있습니다. </li>\n<li>반면에 Q 를 사용하면 단지 high-bias estimator 밖에 사용할 수 없습니다. </li>\n<li>특히나 return에 대한 one-step estimation 은 엄두를 못낼 정도로 bias 가 큽니다.</li>\n</ul>\n</li>\n</ul>\n"},{"title":"pg-travel-guide","date":"2018-07-10T20:34:26.000Z","author":"공민서, 김동민, 양혁렬, 이동민, 이웅원, 장수영, 차금강","subtitle":"피지여행에 관한 개략적 기록","_content":"\n---\n# 1. Policy Gradient의 세계로\n\n반갑습니다! 저희는 PG여행을 위해 모인 PG탐험대입니다. 강화학습하면 보통 Q-learning을 떠올립니다. 그렇지만 오래전부터 Policy Gradient라는 Q-learning 못지 않은 훌륭한 방법론이 연구되어 왔고, 최근에는 강화학습의 최정점의 기술로 자리매김하고 있습니다. 강화학습의 아버지인 Sutton의 논문을 필두로 하여 DQN 연구자들이 제안한 DQN보다 뛰어난 성능을 내는 DPG와 DDPG, 그리고 현재 가장 주목받는 강화학습 연구자인 John Schulmann의 TRPO, GAE, PPO와 이를 이해하기 위해 필요한 Natural Policy Gardient까지 더불어 살펴보고자 합니다.\n\n이 블로그에는 각각의 기술을 제안한 논문을 PG탐험대분들이 자세하게 리뷰한 포스트들이 있습니다. 우리나라에서 PG에 대해서 이렇게 자세하게 리뷰한 포스트들은 없었다고 감히 말씀드리고 싶습니다. 본 글에서는 이 포스트들을 읽기 전에 전체 내용을 개략적으로 소개하고 각각의 포스트들로 안내하고자 합니다. 자, 저희와 함께 PG여행을 즐겨보시겠습니까?\n\n<br><br>\n# 2. \\[Sutton PG\\] Policy gradient methods for reinforcement learning with function approximation\n[Sutton PG 여행하기](../../../06/15/sutton-pg/)\n[Sutton PG Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\n[Sutton PG 여행하기](../../../06/15/sutton-pg/)\n[Sutton PG Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\n<br><br>\n# 3. \\[DPG\\] Deterministic policy gradient algorithms\n[DPG 여행하기](../../../06/16/dpg/)\n[DPG Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\n[DPG 여행하기](../../../06/16/dpg/)\n[DPG Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\n<br><br>\n# 4. \\[DDPG\\] Continuous control with deep reinforcement learning\n[DDPG 여행하기](../../../06/23/ddpg/)\n[DDPG Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\n[DDPG 여행하기](../../../06/23/ddpg/)\n[DDPG Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\n<br><br>\n# 5. \\[NPG\\] A natural policy gradient\n[NPG 여행하기](../../../06/14/2018-06-15-npg/)\n[NPG Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\n[NPG 여행하기](../../../06/14/2018-06-15-npg/)\n[NPG Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\n<br><br>\n# 6. \\[TRPO\\] Trust region policy optimization\n[TRPO 여행하기](blog link)\n[TRPO Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\nPG기법이 각광을 받게 된 계기는 아마도 TRPO 때문이 아닌가 싶습니다. DQN이라는 인공지능이 아타리게임을 사람보다 잘 플레이했던 영상을 보고 충격을 받았던 사람들이 많았던 것처럼 TRPO가 난제로 여겨졌던 서기, 걷기, 뛰기 같은 로봇에 적용할 수 있는 continuous action을 잘 수행하는 것을 보고 많은 사람들이 놀라워했습니다. (솔직히 처음 아타리게임을 봤을 때 만큼은 아니지만...) 이것들은 DQN으로는 달성할 수 없었던 난제였습니다. 그렇지만 거인의 어깨 위에서 세상을 바라보았던 많은 과학자들이 그랬던 것처럼 Schulmann도 TRPO를 갑자기 생각낸 것은 아닙니다. policy를 업데이트할 때 기존의 policy와 너무 많이 달라지면 문제가 생깁니다. 일단, state distribution이 변합니다. state distribution은 간단히 얘기하면 각각의 state를 방문하는 빈도를 의미한다고 할 수 있습니다. policy가 많이 바뀌면 기존의 state distribution을 그대로 이용할 수 없고 이렇게 되면 policy gradient를 구하기 어려워집니다. 그래서 착안한 것이 policy의 변화 정도를 제한하는 것입니다. 여기서 우리는 stochastic policy, 즉, 확률적 policy를 이용한다고 전제하고 있기 때문에, policy의 변화는 policy 확률 분포의 변화를 의미합니다. 현재 policy와 새로운 policy 사이의 확률 분포의 변화를 어떻게 측정할 수 있을까요? Schulmann은 [KL divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)라는 척도를 이용하였습니다. 이 변화량을 특정값 이하로 제한하고 이것을 만족하는 영역을 trust region이라고 불렀습니다. 그렇지만 문제는 풀기에는 복잡했습니다. 이것을 풀기 위해서 여러 번의 근사화를 적용시켰습니다. objective function에는 linear approximation을 적용하고 trust region constraint에는 second-order approximation을 적용하면 문제가 좀 더 풀기 쉬운 형태로 바뀐다는 것을 알아냈습니다. 이것은 사실 natural gradient를 이용하는 것이었습니다. 이러한 이유로 TRPO를 이해하기 위해서 우리는 natural gradient도 살펴보았던 것입니다.\n\n\n[TRPO 여행하기](blog link)\n[TRPO Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\n<br><br>\n# 7. \\[GAE\\] High-Dimensional Continuous Control Using Generalized Advantage Estimation\n[GAE 여행하기](blog link)\n[GAE Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\n[GAE 여행하기](blog link)\n[GAE Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\n<br><br>\n# 8. \\[PPO\\] Proximal policy optimization algorithms\n[PPO 여행하기](blog link)\n[PPO Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\nPPO는 TRPO의 연장선상에 있는 기술이라고 할 수 있습니다. 사실 Schulmann은 TRPO 논문을 쓸 당시 이미 PPO를 구상하고 있었던 것 같습니다. TRPO 논문에도 PPO와 관련있는 내용이 좀 나옵니다. 아이디어 자체는 간단합니다. 그래서그런지 PPO는 arxiv에만 발표되었고 논문도 비교적 짧습니다. TRPO에서는 문제를 단순하게 만들기 위해서 최적화 문제를 여러 번 변형시켰습니다. PPO는 clip이라는 개념을 사용합니다. TRPO에서 이용했던 surrogate objective function을 reward가 특정값 이상이거나 이하가 될 때 더 이상 변화시키지 않는 것입니다.\n\n[PPO 여행하기](blog link)\n[PPO Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\n","source":"_posts/pg-travel-guide.md","raw":"---\ntitle: pg-travel-guide\ndate: 2018-07-11 05:34:26\ntags: [\"프로젝트\", \"피지여행\"]\ncategories: 프로젝트\nauthor: 공민서, 김동민, 양혁렬, 이동민, 이웅원, 장수영, 차금강\nsubtitle: 피지여행에 관한 개략적 기록\n---\n\n---\n# 1. Policy Gradient의 세계로\n\n반갑습니다! 저희는 PG여행을 위해 모인 PG탐험대입니다. 강화학습하면 보통 Q-learning을 떠올립니다. 그렇지만 오래전부터 Policy Gradient라는 Q-learning 못지 않은 훌륭한 방법론이 연구되어 왔고, 최근에는 강화학습의 최정점의 기술로 자리매김하고 있습니다. 강화학습의 아버지인 Sutton의 논문을 필두로 하여 DQN 연구자들이 제안한 DQN보다 뛰어난 성능을 내는 DPG와 DDPG, 그리고 현재 가장 주목받는 강화학습 연구자인 John Schulmann의 TRPO, GAE, PPO와 이를 이해하기 위해 필요한 Natural Policy Gardient까지 더불어 살펴보고자 합니다.\n\n이 블로그에는 각각의 기술을 제안한 논문을 PG탐험대분들이 자세하게 리뷰한 포스트들이 있습니다. 우리나라에서 PG에 대해서 이렇게 자세하게 리뷰한 포스트들은 없었다고 감히 말씀드리고 싶습니다. 본 글에서는 이 포스트들을 읽기 전에 전체 내용을 개략적으로 소개하고 각각의 포스트들로 안내하고자 합니다. 자, 저희와 함께 PG여행을 즐겨보시겠습니까?\n\n<br><br>\n# 2. \\[Sutton PG\\] Policy gradient methods for reinforcement learning with function approximation\n[Sutton PG 여행하기](../../../06/15/sutton-pg/)\n[Sutton PG Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\n[Sutton PG 여행하기](../../../06/15/sutton-pg/)\n[Sutton PG Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\n<br><br>\n# 3. \\[DPG\\] Deterministic policy gradient algorithms\n[DPG 여행하기](../../../06/16/dpg/)\n[DPG Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\n[DPG 여행하기](../../../06/16/dpg/)\n[DPG Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\n<br><br>\n# 4. \\[DDPG\\] Continuous control with deep reinforcement learning\n[DDPG 여행하기](../../../06/23/ddpg/)\n[DDPG Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\n[DDPG 여행하기](../../../06/23/ddpg/)\n[DDPG Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\n<br><br>\n# 5. \\[NPG\\] A natural policy gradient\n[NPG 여행하기](../../../06/14/2018-06-15-npg/)\n[NPG Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\n[NPG 여행하기](../../../06/14/2018-06-15-npg/)\n[NPG Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\n<br><br>\n# 6. \\[TRPO\\] Trust region policy optimization\n[TRPO 여행하기](blog link)\n[TRPO Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\nPG기법이 각광을 받게 된 계기는 아마도 TRPO 때문이 아닌가 싶습니다. DQN이라는 인공지능이 아타리게임을 사람보다 잘 플레이했던 영상을 보고 충격을 받았던 사람들이 많았던 것처럼 TRPO가 난제로 여겨졌던 서기, 걷기, 뛰기 같은 로봇에 적용할 수 있는 continuous action을 잘 수행하는 것을 보고 많은 사람들이 놀라워했습니다. (솔직히 처음 아타리게임을 봤을 때 만큼은 아니지만...) 이것들은 DQN으로는 달성할 수 없었던 난제였습니다. 그렇지만 거인의 어깨 위에서 세상을 바라보았던 많은 과학자들이 그랬던 것처럼 Schulmann도 TRPO를 갑자기 생각낸 것은 아닙니다. policy를 업데이트할 때 기존의 policy와 너무 많이 달라지면 문제가 생깁니다. 일단, state distribution이 변합니다. state distribution은 간단히 얘기하면 각각의 state를 방문하는 빈도를 의미한다고 할 수 있습니다. policy가 많이 바뀌면 기존의 state distribution을 그대로 이용할 수 없고 이렇게 되면 policy gradient를 구하기 어려워집니다. 그래서 착안한 것이 policy의 변화 정도를 제한하는 것입니다. 여기서 우리는 stochastic policy, 즉, 확률적 policy를 이용한다고 전제하고 있기 때문에, policy의 변화는 policy 확률 분포의 변화를 의미합니다. 현재 policy와 새로운 policy 사이의 확률 분포의 변화를 어떻게 측정할 수 있을까요? Schulmann은 [KL divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)라는 척도를 이용하였습니다. 이 변화량을 특정값 이하로 제한하고 이것을 만족하는 영역을 trust region이라고 불렀습니다. 그렇지만 문제는 풀기에는 복잡했습니다. 이것을 풀기 위해서 여러 번의 근사화를 적용시켰습니다. objective function에는 linear approximation을 적용하고 trust region constraint에는 second-order approximation을 적용하면 문제가 좀 더 풀기 쉬운 형태로 바뀐다는 것을 알아냈습니다. 이것은 사실 natural gradient를 이용하는 것이었습니다. 이러한 이유로 TRPO를 이해하기 위해서 우리는 natural gradient도 살펴보았던 것입니다.\n\n\n[TRPO 여행하기](blog link)\n[TRPO Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\n<br><br>\n# 7. \\[GAE\\] High-Dimensional Continuous Control Using Generalized Advantage Estimation\n[GAE 여행하기](blog link)\n[GAE Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\n[GAE 여행하기](blog link)\n[GAE Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\n<br><br>\n# 8. \\[PPO\\] Proximal policy optimization algorithms\n[PPO 여행하기](blog link)\n[PPO Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\nPPO는 TRPO의 연장선상에 있는 기술이라고 할 수 있습니다. 사실 Schulmann은 TRPO 논문을 쓸 당시 이미 PPO를 구상하고 있었던 것 같습니다. TRPO 논문에도 PPO와 관련있는 내용이 좀 나옵니다. 아이디어 자체는 간단합니다. 그래서그런지 PPO는 arxiv에만 발표되었고 논문도 비교적 짧습니다. TRPO에서는 문제를 단순하게 만들기 위해서 최적화 문제를 여러 번 변형시켰습니다. PPO는 clip이라는 개념을 사용합니다. TRPO에서 이용했던 surrogate objective function을 reward가 특정값 이상이거나 이하가 될 때 더 이상 변화시키지 않는 것입니다.\n\n[PPO 여행하기](blog link)\n[PPO Code](https://github.com/reinforcement-learning-kr/pg_travel)\n\n","slug":"pg-travel-guide","published":1,"updated":"2018-07-11T06:34:12.959Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjjgzyg4a0001xo15qjhkwruc","content":"<hr>\n<h1 id=\"1-Policy-Gradient의-세계로\"><a href=\"#1-Policy-Gradient의-세계로\" class=\"headerlink\" title=\"1. Policy Gradient의 세계로\"></a>1. Policy Gradient의 세계로</h1><p>반갑습니다! 저희는 PG여행을 위해 모인 PG탐험대입니다. 강화학습하면 보통 Q-learning을 떠올립니다. 그렇지만 오래전부터 Policy Gradient라는 Q-learning 못지 않은 훌륭한 방법론이 연구되어 왔고, 최근에는 강화학습의 최정점의 기술로 자리매김하고 있습니다. 강화학습의 아버지인 Sutton의 논문을 필두로 하여 DQN 연구자들이 제안한 DQN보다 뛰어난 성능을 내는 DPG와 DDPG, 그리고 현재 가장 주목받는 강화학습 연구자인 John Schulmann의 TRPO, GAE, PPO와 이를 이해하기 위해 필요한 Natural Policy Gardient까지 더불어 살펴보고자 합니다.</p>\n<p>이 블로그에는 각각의 기술을 제안한 논문을 PG탐험대분들이 자세하게 리뷰한 포스트들이 있습니다. 우리나라에서 PG에 대해서 이렇게 자세하게 리뷰한 포스트들은 없었다고 감히 말씀드리고 싶습니다. 본 글에서는 이 포스트들을 읽기 전에 전체 내용을 개략적으로 소개하고 각각의 포스트들로 안내하고자 합니다. 자, 저희와 함께 PG여행을 즐겨보시겠습니까?</p>\n<p><br><br></p>\n<h1 id=\"2-Sutton-PG-Policy-gradient-methods-for-reinforcement-learning-with-function-approximation\"><a href=\"#2-Sutton-PG-Policy-gradient-methods-for-reinforcement-learning-with-function-approximation\" class=\"headerlink\" title=\"2. [Sutton PG] Policy gradient methods for reinforcement learning with function approximation\"></a>2. [Sutton PG] Policy gradient methods for reinforcement learning with function approximation</h1><p><a href=\"../../../06/15/sutton-pg/\">Sutton PG 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">Sutton PG Code</a></p>\n<p><a href=\"../../../06/15/sutton-pg/\">Sutton PG 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">Sutton PG Code</a></p>\n<p><br><br></p>\n<h1 id=\"3-DPG-Deterministic-policy-gradient-algorithms\"><a href=\"#3-DPG-Deterministic-policy-gradient-algorithms\" class=\"headerlink\" title=\"3. [DPG] Deterministic policy gradient algorithms\"></a>3. [DPG] Deterministic policy gradient algorithms</h1><p><a href=\"../../../06/16/dpg/\">DPG 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">DPG Code</a></p>\n<p><a href=\"../../../06/16/dpg/\">DPG 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">DPG Code</a></p>\n<p><br><br></p>\n<h1 id=\"4-DDPG-Continuous-control-with-deep-reinforcement-learning\"><a href=\"#4-DDPG-Continuous-control-with-deep-reinforcement-learning\" class=\"headerlink\" title=\"4. [DDPG] Continuous control with deep reinforcement learning\"></a>4. [DDPG] Continuous control with deep reinforcement learning</h1><p><a href=\"../../../06/23/ddpg/\">DDPG 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">DDPG Code</a></p>\n<p><a href=\"../../../06/23/ddpg/\">DDPG 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">DDPG Code</a></p>\n<p><br><br></p>\n<h1 id=\"5-NPG-A-natural-policy-gradient\"><a href=\"#5-NPG-A-natural-policy-gradient\" class=\"headerlink\" title=\"5. [NPG] A natural policy gradient\"></a>5. [NPG] A natural policy gradient</h1><p><a href=\"../../../06/14/2018-06-15-npg/\">NPG 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">NPG Code</a></p>\n<p><a href=\"../../../06/14/2018-06-15-npg/\">NPG 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">NPG Code</a></p>\n<p><br><br></p>\n<h1 id=\"6-TRPO-Trust-region-policy-optimization\"><a href=\"#6-TRPO-Trust-region-policy-optimization\" class=\"headerlink\" title=\"6. [TRPO] Trust region policy optimization\"></a>6. [TRPO] Trust region policy optimization</h1><p><a href=\"blog link\">TRPO 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">TRPO Code</a></p>\n<p>PG기법이 각광을 받게 된 계기는 아마도 TRPO 때문이 아닌가 싶습니다. DQN이라는 인공지능이 아타리게임을 사람보다 잘 플레이했던 영상을 보고 충격을 받았던 사람들이 많았던 것처럼 TRPO가 난제로 여겨졌던 서기, 걷기, 뛰기 같은 로봇에 적용할 수 있는 continuous action을 잘 수행하는 것을 보고 많은 사람들이 놀라워했습니다. (솔직히 처음 아타리게임을 봤을 때 만큼은 아니지만…) 이것들은 DQN으로는 달성할 수 없었던 난제였습니다. 그렇지만 거인의 어깨 위에서 세상을 바라보았던 많은 과학자들이 그랬던 것처럼 Schulmann도 TRPO를 갑자기 생각낸 것은 아닙니다. policy를 업데이트할 때 기존의 policy와 너무 많이 달라지면 문제가 생깁니다. 일단, state distribution이 변합니다. state distribution은 간단히 얘기하면 각각의 state를 방문하는 빈도를 의미한다고 할 수 있습니다. policy가 많이 바뀌면 기존의 state distribution을 그대로 이용할 수 없고 이렇게 되면 policy gradient를 구하기 어려워집니다. 그래서 착안한 것이 policy의 변화 정도를 제한하는 것입니다. 여기서 우리는 stochastic policy, 즉, 확률적 policy를 이용한다고 전제하고 있기 때문에, policy의 변화는 policy 확률 분포의 변화를 의미합니다. 현재 policy와 새로운 policy 사이의 확률 분포의 변화를 어떻게 측정할 수 있을까요? Schulmann은 <a href=\"https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\" target=\"_blank\" rel=\"noopener\">KL divergence</a>라는 척도를 이용하였습니다. 이 변화량을 특정값 이하로 제한하고 이것을 만족하는 영역을 trust region이라고 불렀습니다. 그렇지만 문제는 풀기에는 복잡했습니다. 이것을 풀기 위해서 여러 번의 근사화를 적용시켰습니다. objective function에는 linear approximation을 적용하고 trust region constraint에는 second-order approximation을 적용하면 문제가 좀 더 풀기 쉬운 형태로 바뀐다는 것을 알아냈습니다. 이것은 사실 natural gradient를 이용하는 것이었습니다. 이러한 이유로 TRPO를 이해하기 위해서 우리는 natural gradient도 살펴보았던 것입니다.</p>\n<p><a href=\"blog link\">TRPO 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">TRPO Code</a></p>\n<p><br><br></p>\n<h1 id=\"7-GAE-High-Dimensional-Continuous-Control-Using-Generalized-Advantage-Estimation\"><a href=\"#7-GAE-High-Dimensional-Continuous-Control-Using-Generalized-Advantage-Estimation\" class=\"headerlink\" title=\"7. [GAE] High-Dimensional Continuous Control Using Generalized Advantage Estimation\"></a>7. [GAE] High-Dimensional Continuous Control Using Generalized Advantage Estimation</h1><p><a href=\"blog link\">GAE 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">GAE Code</a></p>\n<p><a href=\"blog link\">GAE 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">GAE Code</a></p>\n<p><br><br></p>\n<h1 id=\"8-PPO-Proximal-policy-optimization-algorithms\"><a href=\"#8-PPO-Proximal-policy-optimization-algorithms\" class=\"headerlink\" title=\"8. [PPO] Proximal policy optimization algorithms\"></a>8. [PPO] Proximal policy optimization algorithms</h1><p><a href=\"blog link\">PPO 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">PPO Code</a></p>\n<p>PPO는 TRPO의 연장선상에 있는 기술이라고 할 수 있습니다. 사실 Schulmann은 TRPO 논문을 쓸 당시 이미 PPO를 구상하고 있었던 것 같습니다. TRPO 논문에도 PPO와 관련있는 내용이 좀 나옵니다. 아이디어 자체는 간단합니다. 그래서그런지 PPO는 arxiv에만 발표되었고 논문도 비교적 짧습니다. TRPO에서는 문제를 단순하게 만들기 위해서 최적화 문제를 여러 번 변형시켰습니다. PPO는 clip이라는 개념을 사용합니다. TRPO에서 이용했던 surrogate objective function을 reward가 특정값 이상이거나 이하가 될 때 더 이상 변화시키지 않는 것입니다.</p>\n<p><a href=\"blog link\">PPO 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">PPO Code</a></p>\n","site":{"data":{}},"excerpt":"","more":"<hr>\n<h1 id=\"1-Policy-Gradient의-세계로\"><a href=\"#1-Policy-Gradient의-세계로\" class=\"headerlink\" title=\"1. Policy Gradient의 세계로\"></a>1. Policy Gradient의 세계로</h1><p>반갑습니다! 저희는 PG여행을 위해 모인 PG탐험대입니다. 강화학습하면 보통 Q-learning을 떠올립니다. 그렇지만 오래전부터 Policy Gradient라는 Q-learning 못지 않은 훌륭한 방법론이 연구되어 왔고, 최근에는 강화학습의 최정점의 기술로 자리매김하고 있습니다. 강화학습의 아버지인 Sutton의 논문을 필두로 하여 DQN 연구자들이 제안한 DQN보다 뛰어난 성능을 내는 DPG와 DDPG, 그리고 현재 가장 주목받는 강화학습 연구자인 John Schulmann의 TRPO, GAE, PPO와 이를 이해하기 위해 필요한 Natural Policy Gardient까지 더불어 살펴보고자 합니다.</p>\n<p>이 블로그에는 각각의 기술을 제안한 논문을 PG탐험대분들이 자세하게 리뷰한 포스트들이 있습니다. 우리나라에서 PG에 대해서 이렇게 자세하게 리뷰한 포스트들은 없었다고 감히 말씀드리고 싶습니다. 본 글에서는 이 포스트들을 읽기 전에 전체 내용을 개략적으로 소개하고 각각의 포스트들로 안내하고자 합니다. 자, 저희와 함께 PG여행을 즐겨보시겠습니까?</p>\n<p><br><br></p>\n<h1 id=\"2-Sutton-PG-Policy-gradient-methods-for-reinforcement-learning-with-function-approximation\"><a href=\"#2-Sutton-PG-Policy-gradient-methods-for-reinforcement-learning-with-function-approximation\" class=\"headerlink\" title=\"2. [Sutton PG] Policy gradient methods for reinforcement learning with function approximation\"></a>2. [Sutton PG] Policy gradient methods for reinforcement learning with function approximation</h1><p><a href=\"../../../06/15/sutton-pg/\">Sutton PG 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">Sutton PG Code</a></p>\n<p><a href=\"../../../06/15/sutton-pg/\">Sutton PG 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">Sutton PG Code</a></p>\n<p><br><br></p>\n<h1 id=\"3-DPG-Deterministic-policy-gradient-algorithms\"><a href=\"#3-DPG-Deterministic-policy-gradient-algorithms\" class=\"headerlink\" title=\"3. [DPG] Deterministic policy gradient algorithms\"></a>3. [DPG] Deterministic policy gradient algorithms</h1><p><a href=\"../../../06/16/dpg/\">DPG 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">DPG Code</a></p>\n<p><a href=\"../../../06/16/dpg/\">DPG 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">DPG Code</a></p>\n<p><br><br></p>\n<h1 id=\"4-DDPG-Continuous-control-with-deep-reinforcement-learning\"><a href=\"#4-DDPG-Continuous-control-with-deep-reinforcement-learning\" class=\"headerlink\" title=\"4. [DDPG] Continuous control with deep reinforcement learning\"></a>4. [DDPG] Continuous control with deep reinforcement learning</h1><p><a href=\"../../../06/23/ddpg/\">DDPG 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">DDPG Code</a></p>\n<p><a href=\"../../../06/23/ddpg/\">DDPG 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">DDPG Code</a></p>\n<p><br><br></p>\n<h1 id=\"5-NPG-A-natural-policy-gradient\"><a href=\"#5-NPG-A-natural-policy-gradient\" class=\"headerlink\" title=\"5. [NPG] A natural policy gradient\"></a>5. [NPG] A natural policy gradient</h1><p><a href=\"../../../06/14/2018-06-15-npg/\">NPG 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">NPG Code</a></p>\n<p><a href=\"../../../06/14/2018-06-15-npg/\">NPG 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">NPG Code</a></p>\n<p><br><br></p>\n<h1 id=\"6-TRPO-Trust-region-policy-optimization\"><a href=\"#6-TRPO-Trust-region-policy-optimization\" class=\"headerlink\" title=\"6. [TRPO] Trust region policy optimization\"></a>6. [TRPO] Trust region policy optimization</h1><p><a href=\"blog link\">TRPO 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">TRPO Code</a></p>\n<p>PG기법이 각광을 받게 된 계기는 아마도 TRPO 때문이 아닌가 싶습니다. DQN이라는 인공지능이 아타리게임을 사람보다 잘 플레이했던 영상을 보고 충격을 받았던 사람들이 많았던 것처럼 TRPO가 난제로 여겨졌던 서기, 걷기, 뛰기 같은 로봇에 적용할 수 있는 continuous action을 잘 수행하는 것을 보고 많은 사람들이 놀라워했습니다. (솔직히 처음 아타리게임을 봤을 때 만큼은 아니지만…) 이것들은 DQN으로는 달성할 수 없었던 난제였습니다. 그렇지만 거인의 어깨 위에서 세상을 바라보았던 많은 과학자들이 그랬던 것처럼 Schulmann도 TRPO를 갑자기 생각낸 것은 아닙니다. policy를 업데이트할 때 기존의 policy와 너무 많이 달라지면 문제가 생깁니다. 일단, state distribution이 변합니다. state distribution은 간단히 얘기하면 각각의 state를 방문하는 빈도를 의미한다고 할 수 있습니다. policy가 많이 바뀌면 기존의 state distribution을 그대로 이용할 수 없고 이렇게 되면 policy gradient를 구하기 어려워집니다. 그래서 착안한 것이 policy의 변화 정도를 제한하는 것입니다. 여기서 우리는 stochastic policy, 즉, 확률적 policy를 이용한다고 전제하고 있기 때문에, policy의 변화는 policy 확률 분포의 변화를 의미합니다. 현재 policy와 새로운 policy 사이의 확률 분포의 변화를 어떻게 측정할 수 있을까요? Schulmann은 <a href=\"https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\" target=\"_blank\" rel=\"noopener\">KL divergence</a>라는 척도를 이용하였습니다. 이 변화량을 특정값 이하로 제한하고 이것을 만족하는 영역을 trust region이라고 불렀습니다. 그렇지만 문제는 풀기에는 복잡했습니다. 이것을 풀기 위해서 여러 번의 근사화를 적용시켰습니다. objective function에는 linear approximation을 적용하고 trust region constraint에는 second-order approximation을 적용하면 문제가 좀 더 풀기 쉬운 형태로 바뀐다는 것을 알아냈습니다. 이것은 사실 natural gradient를 이용하는 것이었습니다. 이러한 이유로 TRPO를 이해하기 위해서 우리는 natural gradient도 살펴보았던 것입니다.</p>\n<p><a href=\"blog link\">TRPO 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">TRPO Code</a></p>\n<p><br><br></p>\n<h1 id=\"7-GAE-High-Dimensional-Continuous-Control-Using-Generalized-Advantage-Estimation\"><a href=\"#7-GAE-High-Dimensional-Continuous-Control-Using-Generalized-Advantage-Estimation\" class=\"headerlink\" title=\"7. [GAE] High-Dimensional Continuous Control Using Generalized Advantage Estimation\"></a>7. [GAE] High-Dimensional Continuous Control Using Generalized Advantage Estimation</h1><p><a href=\"blog link\">GAE 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">GAE Code</a></p>\n<p><a href=\"blog link\">GAE 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">GAE Code</a></p>\n<p><br><br></p>\n<h1 id=\"8-PPO-Proximal-policy-optimization-algorithms\"><a href=\"#8-PPO-Proximal-policy-optimization-algorithms\" class=\"headerlink\" title=\"8. [PPO] Proximal policy optimization algorithms\"></a>8. [PPO] Proximal policy optimization algorithms</h1><p><a href=\"blog link\">PPO 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">PPO Code</a></p>\n<p>PPO는 TRPO의 연장선상에 있는 기술이라고 할 수 있습니다. 사실 Schulmann은 TRPO 논문을 쓸 당시 이미 PPO를 구상하고 있었던 것 같습니다. TRPO 논문에도 PPO와 관련있는 내용이 좀 나옵니다. 아이디어 자체는 간단합니다. 그래서그런지 PPO는 arxiv에만 발표되었고 논문도 비교적 짧습니다. TRPO에서는 문제를 단순하게 만들기 위해서 최적화 문제를 여러 번 변형시켰습니다. PPO는 clip이라는 개념을 사용합니다. TRPO에서 이용했던 surrogate objective function을 reward가 특정값 이상이거나 이하가 될 때 더 이상 변화시키지 않는 것입니다.</p>\n<p><a href=\"blog link\">PPO 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">PPO Code</a></p>\n"},{"title":"Deep Determinstic Policy Gradient (DDPG)","date":"2018-06-23T02:20:45.000Z","author":"양혁렬","subtitle":"피지여행 3번째 논문","_content":"<br>\n<br>\n# Deep Determinstic Policy Gradient(DDPG)\n<img src=\"https://www.dropbox.com/s/zv8rk0uf87ipiaj/Screenshot%202018-06-23%2010.01.48.png?dl=1\">\n\n- [논문 링크](\"https://arxiv.org/abs/1509.02971\")\n- Deterministic Policy Gradient(DPG)를 기반으로 Deep Neural Network 를 사용해 continuous control 문제를 풀어낸 논문입니다.\n\n<br>\n## 1.Introduction\nDDPG 알고리즘에 대한 개요입니다.\n### 1.1 Success & Limination of DQN  \n-  Success\n    - sensor로 부터 나오는 전처리를 거친 input 대신에 raw pixel input 을 사용 <br>\n     : High dimensional observation space 문제를 풀어냄.\n- Limitation\n    - discrete & low dimensional action space 만 다룰 수 있음 <br>\n     : Continuous action space 를 다루기 위해서는 매 스텝 이를 위한 iterative optimization process 를 거쳐야 함.\n\n### 1.2 Problems of discritization\n<p align=\"center\">\n<img src=\"https://www.dropbox.com/s/nulhzxs8bak2fn6/Screenshot%202018-06-23%2012.22.20.png?dl=1\" width=\"300px\">\n</p>\n\n\n- 만약 7개의 관절을 가진 로봇 팔이 있다면, 가장 간단한 discretization 은 각 관절을 다음과 같이 $a_{i}\\in \\\\{ -k, 0, k \\\\}$ 3 개의 값을 가지도록 하는 것이다.\n- 그렇다면 $3 \\times 3 \\times 3 \\times 3 \\times 3 \\times 3 \\times 3 = 3^{7} = 2187$ 가지의 dimension을 가진 action space가 만들어진다.\n    - Discretization 을 하면 action space 가 exponential 하게 늘어남.\n- 충분히 큰 action space 임에도 discretization으로 인한 정보의 손실도 있다\n    - 섬세한 Control 을 할 수 없다.\n\n### 1.3 New approach for continuous control\n\n- Model-free, Off-policy, Actor-critic algorithm 을 제안.\n- Deep Deterministic Policy(이하 DPG) 를 기반으로 함.\n- Actor-Critic approach 와 DQN의 성공적이었던 부분을 합침\n    - Replay buffer : 샘플들 사이의 상관관계를 줄여줌\n    - target Q Network : Update 동안 target 을 안정적으로 만들어줌.\n\n## 2.Background\n\n### 2.1 Notation\n- Observation : $x_{t}$\n- Action : $a_t \\in {\\rm IR}^N $\n- Reward : $r_t$\n- Discount factor : $\\gamma$\n- Environment : $E$\n- Policy : $\\pi : S \\rightarrow P(A)  $\n- Transition dynamics : $p(s_{t+1} \\vert s_t, a_t) $\n- Reward function : $r(s_t, a_t)$\n- Return : $ \\sum_{ i=t }^{ T } \\gamma^{(i-t)}r(s_i, a_i) $\n- Discounted state visitation distribution for a policy : $\\rho^\\pi $\n\n\n### 2.2 Bellman Equation\n- 상태 $s_t$ 에서 행동 $a_t$를 취했을 때 Expected return\n  $ Q^{\\pi}(s_t, a_t)={\\rm E}_{r_{i \\geqq t},s_{i \\geqq t} \\backsim E, a_{i \\geqq t} \\backsim \\pi } [R_{t} \\vert s_t, a_t  ] $\n  <br>\n- 벨만 방정식을 사용하여 위의 식을 변형\n  $ Q^{\\pi}(s_t, a_t)={\\rm E}_{r_{t},s_{t} \\backsim E } \\[r(s_t,a_t)+\\gamma {\\rm E}_{a_{t+1} \\backsim \\pi } \\[ Q^{\\pi}(s_{t+1}, a_{t+1}) \\] \\] $\n  <br>\n- Determinsitc policy 를 가정\n  $ Q^{\\mu}(s_t, a_t)={\\rm E}_{r_{t},s_{t} \\backsim E } \\[r(s_t,a_t)+\\gamma Q^{\\mu}(s_{t+1}, \\mu (s_{t+1})) \\] $\n  - 위의 식에서 아래 식으로 내려오면서 policy 가 determinstic 하기 때문에 policy 에 dependent 한 Expectation 이 빠진 것을 알 수 있습니다. \n  - Deterministic policy를 가정하기 전의 수식에서는 $a_{t+1}$ 을 골랐던 순간의 policy로 Q에 대한 Expection을 원래 구해야하기 때문에 off-policy가 아니지만, Determinsitic policy를 가정한다면 update 할 당시의 policy로 $a_{t+1}$ 를 구할 수 있기 때문에 off-policy 입니다. \n  <br>\n- Q learning \n$L(\\theta^{Q}) = {\\rm E}_{s_t \\backsim \\rho^\\beta , a_t \\backsim \\beta , r_t \\backsim E} [(Q(s_t, a_t \\vert \\theta^Q)-y_t)^2] $ 참고)  $\\beta$ 는 behavior policy를 의미합니다. \n    - $ y_t = r(s_t, a_t) + \\gamma Q^{\\mu}(s_{t+1},\\mu(s_{t+1})) $  \n    - $\\mu(s) = argmax_{a}Q(s,a)$\n        - Q learning 은 위와 같이 $argmax$ 라는 deterministic policy를 사용하기 때문에 off policy 로 사용할 수 있습니다. \n\n### 2.3 DPG\n<center> $ \\nabla_{\\theta^\\mu} J \\approx  {\\rm E}_{s_t \\backsim \\rho^\\beta} [ \\nabla_{\\theta^\\mu} Q(s, a \\vert \\theta ^ Q) \\vert_{s=s_t, a=\\mu(s_t)}] $ <br> $ = {\\rm E}_{s_t \\backsim \\rho^\\beta} [ \\nabla_{a} Q(s, a \\vert \\theta ^ Q) \\vert_{s=s_t, a=\\mu(s_t)} \\nabla_{\\theta^\\mu} \\mu(s \\vert Q^{\\mu})\\vert_{s=s_t}] $ </center>\n\n- 위의 수식은 피지여행 DPG 글 4-2.Q-learning 을 이용한 off-policy actor-critic 에서 이미 정리 한 바 있습니다.\n\n\n\n## 3.Algorithm\nContinous control 을 위한 새로운 알고리즘을 제안합니다. 제안하는 알고리즘의 특징은 다음과 같습니다. \n\n- Replay buffer 를 사용하였다.\n- \"soft\" target update 를 사용하였다. \n- 각 차원의 scale이 다른 low dimension vector 로 부터 학습할 때 Batch Normalization을 사용하였다. \n- 탐험을 위해 action에 Noise 를 추가하였다. \n\n### 3.1 Replay buffer\n<center>\n<img src=\"https://www.dropbox.com/s/lc61b8nas1clqme/Screenshot%202018-06-23%2016.32.53.png?dl=1\" width=\"600px\"></center>\n\n- 큰 State space를 학습하고 일반화 하기위해서는 Neural Network와 같은 non-linear approximator 가 필수적이지만 수렴한다는 보장이 없음.\n- NFQCA 에서는 수렴의 안정성을 위해서 batch learning을 도입함. 하지만 NFQCA에서는 업데이트시에 policy를 reset 하지 않음.\n- DDPG 는 DQN에서 사용된 Replay buffer를 사용하여 online batch update를 가능하게 했음.\n\n### 3.2 Soft target update\n<center> \n$ \\theta^{Q^{'}} \\leftarrow \\tau \\theta^{Q} + (1-\\tau) \\theta^{Q^{'}}   $<br>\n$ \\theta^{\\mu^{'}} \\leftarrow \\tau \\theta^{\\mu} + (1-\\tau) \\theta^{\\mu^{'}}   $\n</center>\n\n- DQN에서는 일정 주기마다 origin network의 weight 를 target network로 직접 복사해서 사용했음.\n- DDPG 에서는 exponential moving average(지수이동평균) 식으로 대체\n- soft update 가 DQN에서 사용했던 방식에 비해 어떤 장점이 있는지는 명확하게 설명되어있지 않음.\n\n### 3.3 Batch Normalization\n<center> \n<img src=\"https://www.dropbox.com/s/1erxzrgk69x04j8/Screenshot%202018-06-23%2016.51.56.png?dl=1\" width=\"300px\">\n</center>\n\n- 서로 scale 이 다른 feature 를 state 로 사용할 때에 Neural Net이 일반화에서 어려움을 겪는다. \n    - 이걸 해결하기 위해서는 원래 직접 스케일을 조정해주었음.\n- 하지만 각 layer 의 Input 을 Unit Gaussian 이 되도록 강제하는 BatchNormalization을 사용하여 이 문제를 해결하였다.\n\n### 3.4 Noise Process\nDDPG 에서는 Exploration을 위해서 output 으로 나온 행동에 노이즈를 추가해줍니다.\n<center>\nORNSTEIN UHLENBECK PROCESS(이하 OU) : $dx_t = \\theta (\\mu - x_t) dt + \\sigma dW_t$\n</center>\n\n- OU Process는 평균으로 회귀하는 random process 입니다.\n- $\\theta$ 는 얼마나 빨리 평균으로 회귀할 지를 나타내는 파라미터이며  $\\mu$ 는 평균을 의미합니다.\n- $\\sigma$ 는 process 의 변동성을 의미하며 $W_t$ 는 Wiener process 를 의미합니다. <br>\n- 따라서 이전의 noise들과 temporally correlated 입니다.\n- 위와 같은 temporally correlated noise process를 사용하는 이유는 physical control 과 같은 관성이 있는 환경에서 학습 시킬 때 보다 효과적이기 때문입니다.\n\n### 3.5 Diagram & Pseudocode \n\n<img src=\"https://www.dropbox.com/s/0ffb2c9irctjx2n/Screenshot%202018-06-23%2017.58.57.png?dl=1\"> \n\n- DDPG 의 학습 과정을 간단히 도식화 해본 다이어 그램입니다. \n\n<center>\n<img src=\"https://www.dropbox.com/s/fd0nj7goixfnd6z/Screenshot%202018-06-23%2018.02.13.png?dl=1\" width=\"500px\"></center>\n\n- DDPG 의 알고리즘 수도코드입니다. 모든 항목을 위에서 설명했으니 순서대로 보시면 이해에 도움이 될것입니다. \n\n\n## 4. Results\n\n### 4.1 Variants of DPG\n<center>\n<img src=\"https://www.dropbox.com/s/k8q6tih85lgow5l/Screenshot%202018-06-23%2018.08.05.png?dl=1\" width=\"500px\"></center>\n\n- original DPG 에 batchnorm 만 추가 (연한 회색), target network만 추가 (진한 회색), 둘 다 추가 (초록), pixel로만 학습 (파랑) . Target network가 성능을 가장 좌지우지한다.\n\n### 4.2 Q estimation of DDPG\n<center>\n<img src=\"https://www.dropbox.com/s/vvcfoni0lqrisst/Screenshot%202018-06-23%2018.10.23.png?dl=1\" width=\"500px\"></center>\n\n- DQN 은 Q value 를 Over-estimate 하는 경향이 있었지만, DDPG 는 simple task에 대해서는 잘한다. 복잡한 문제에 대해서는 estimation을 잘 못했지만, 여전히 좋은 Policy를 찾아내었다.\n\n### 4.3 Performance Comparison\n<center>\n<img src=\"https://www.dropbox.com/s/u8ibmz9q4kfxh6l/Screenshot%202018-06-23%2018.11.40.png?dl=1\" width=\"500px\"></center>\n\n- Score 는 naive policy 를 0, ILQG (planning algorithm) 의 mean score를 1점으로 놓았을 때의 점수 Torcs 환경에 대해서만 raw reward 를 score로 사용.\n\n## 5. Implementation Details\n\n### 5.1 Hyper parameters\n\n- Optimizer : Adam\n    - actor lr : 0.0001, critic lr : 0.001\n- Weight decay(L2) for critic(Q) = 0.001 \n- discount factor, $\\gamma = 0.99 $\n- soft target updates. $\\tau = 0.001 $\n- Size of replay buffer = 1,000,000\n- Orstein Uhlenbeck Process : $\\theta = 0.15$, $\\sigma = 0.2$\n\n\n### 5.2 Etc.\n\n- Final output layer of actor : tanh (행동의 최소 최대를 맞춰주기 위해서)\n- low-dimentional 문제에서 네트워크는 2개의 hidden layer (1st layer 400 units, 2nd layer 300 units) 를 가진다. \n- 이미지를 통해서 학습시킬 때 : 3 convolutional layers (no pooling) with 32 filters at each layer.\n- actor 와 critic 각각의 final layer(weight, bias 모두 ) 는 다음 범위의 uniform distribution 에서 샘플링한다. [ - 0.003, 0.003], [ - 0.0003 , 0.0003] . 이렇게 하는 이유는 가장 처음의 policy와 value 의 output 이 0에 가깝게 나오도록 하기 위함. \n\n\n<br>\n## 6.Conclusion\n<br>\n\n- 이 연구는 최근 딥러닝의 발전과 강화학습을 엮은 것으로 Continuous action space 를 가지는 문제를 robust 하게 풀어냄.\n- non-linear function approximators 을 쓰는 것은 수렴을 보장하지 않지만, 여러 환경에 대해서 특별한 조작 없이 안정적으로 수렴하는 것을 실험으로 보여냄.\n- Atari 도메인에서 DQN 보다 상당히 적은 step 만에 수렴하는 것을 실험을 통해서 알아냄. \n- model-free 알고리즘은 좋은 solution 을 찾기 위해서는 많은 sample을 필요로 한다는 한계가 있지만 더 큰 시스템에서는 이러한 한계를 물리칠 정도로 중요한 역할을 하게 될 것임.","source":"_posts/ddpg.md","raw":"---\ntitle: Deep Determinstic Policy Gradient (DDPG)\ndate: 2018-06-23 11:20:45\ntags: [\"프로젝트\", \"피지여행\"]\ncategories: 프로젝트\nauthor: 양혁렬\nsubtitle: 피지여행 3번째 논문\n---\n<br>\n<br>\n# Deep Determinstic Policy Gradient(DDPG)\n<img src=\"https://www.dropbox.com/s/zv8rk0uf87ipiaj/Screenshot%202018-06-23%2010.01.48.png?dl=1\">\n\n- [논문 링크](\"https://arxiv.org/abs/1509.02971\")\n- Deterministic Policy Gradient(DPG)를 기반으로 Deep Neural Network 를 사용해 continuous control 문제를 풀어낸 논문입니다.\n\n<br>\n## 1.Introduction\nDDPG 알고리즘에 대한 개요입니다.\n### 1.1 Success & Limination of DQN  \n-  Success\n    - sensor로 부터 나오는 전처리를 거친 input 대신에 raw pixel input 을 사용 <br>\n     : High dimensional observation space 문제를 풀어냄.\n- Limitation\n    - discrete & low dimensional action space 만 다룰 수 있음 <br>\n     : Continuous action space 를 다루기 위해서는 매 스텝 이를 위한 iterative optimization process 를 거쳐야 함.\n\n### 1.2 Problems of discritization\n<p align=\"center\">\n<img src=\"https://www.dropbox.com/s/nulhzxs8bak2fn6/Screenshot%202018-06-23%2012.22.20.png?dl=1\" width=\"300px\">\n</p>\n\n\n- 만약 7개의 관절을 가진 로봇 팔이 있다면, 가장 간단한 discretization 은 각 관절을 다음과 같이 $a_{i}\\in \\\\{ -k, 0, k \\\\}$ 3 개의 값을 가지도록 하는 것이다.\n- 그렇다면 $3 \\times 3 \\times 3 \\times 3 \\times 3 \\times 3 \\times 3 = 3^{7} = 2187$ 가지의 dimension을 가진 action space가 만들어진다.\n    - Discretization 을 하면 action space 가 exponential 하게 늘어남.\n- 충분히 큰 action space 임에도 discretization으로 인한 정보의 손실도 있다\n    - 섬세한 Control 을 할 수 없다.\n\n### 1.3 New approach for continuous control\n\n- Model-free, Off-policy, Actor-critic algorithm 을 제안.\n- Deep Deterministic Policy(이하 DPG) 를 기반으로 함.\n- Actor-Critic approach 와 DQN의 성공적이었던 부분을 합침\n    - Replay buffer : 샘플들 사이의 상관관계를 줄여줌\n    - target Q Network : Update 동안 target 을 안정적으로 만들어줌.\n\n## 2.Background\n\n### 2.1 Notation\n- Observation : $x_{t}$\n- Action : $a_t \\in {\\rm IR}^N $\n- Reward : $r_t$\n- Discount factor : $\\gamma$\n- Environment : $E$\n- Policy : $\\pi : S \\rightarrow P(A)  $\n- Transition dynamics : $p(s_{t+1} \\vert s_t, a_t) $\n- Reward function : $r(s_t, a_t)$\n- Return : $ \\sum_{ i=t }^{ T } \\gamma^{(i-t)}r(s_i, a_i) $\n- Discounted state visitation distribution for a policy : $\\rho^\\pi $\n\n\n### 2.2 Bellman Equation\n- 상태 $s_t$ 에서 행동 $a_t$를 취했을 때 Expected return\n  $ Q^{\\pi}(s_t, a_t)={\\rm E}_{r_{i \\geqq t},s_{i \\geqq t} \\backsim E, a_{i \\geqq t} \\backsim \\pi } [R_{t} \\vert s_t, a_t  ] $\n  <br>\n- 벨만 방정식을 사용하여 위의 식을 변형\n  $ Q^{\\pi}(s_t, a_t)={\\rm E}_{r_{t},s_{t} \\backsim E } \\[r(s_t,a_t)+\\gamma {\\rm E}_{a_{t+1} \\backsim \\pi } \\[ Q^{\\pi}(s_{t+1}, a_{t+1}) \\] \\] $\n  <br>\n- Determinsitc policy 를 가정\n  $ Q^{\\mu}(s_t, a_t)={\\rm E}_{r_{t},s_{t} \\backsim E } \\[r(s_t,a_t)+\\gamma Q^{\\mu}(s_{t+1}, \\mu (s_{t+1})) \\] $\n  - 위의 식에서 아래 식으로 내려오면서 policy 가 determinstic 하기 때문에 policy 에 dependent 한 Expectation 이 빠진 것을 알 수 있습니다. \n  - Deterministic policy를 가정하기 전의 수식에서는 $a_{t+1}$ 을 골랐던 순간의 policy로 Q에 대한 Expection을 원래 구해야하기 때문에 off-policy가 아니지만, Determinsitic policy를 가정한다면 update 할 당시의 policy로 $a_{t+1}$ 를 구할 수 있기 때문에 off-policy 입니다. \n  <br>\n- Q learning \n$L(\\theta^{Q}) = {\\rm E}_{s_t \\backsim \\rho^\\beta , a_t \\backsim \\beta , r_t \\backsim E} [(Q(s_t, a_t \\vert \\theta^Q)-y_t)^2] $ 참고)  $\\beta$ 는 behavior policy를 의미합니다. \n    - $ y_t = r(s_t, a_t) + \\gamma Q^{\\mu}(s_{t+1},\\mu(s_{t+1})) $  \n    - $\\mu(s) = argmax_{a}Q(s,a)$\n        - Q learning 은 위와 같이 $argmax$ 라는 deterministic policy를 사용하기 때문에 off policy 로 사용할 수 있습니다. \n\n### 2.3 DPG\n<center> $ \\nabla_{\\theta^\\mu} J \\approx  {\\rm E}_{s_t \\backsim \\rho^\\beta} [ \\nabla_{\\theta^\\mu} Q(s, a \\vert \\theta ^ Q) \\vert_{s=s_t, a=\\mu(s_t)}] $ <br> $ = {\\rm E}_{s_t \\backsim \\rho^\\beta} [ \\nabla_{a} Q(s, a \\vert \\theta ^ Q) \\vert_{s=s_t, a=\\mu(s_t)} \\nabla_{\\theta^\\mu} \\mu(s \\vert Q^{\\mu})\\vert_{s=s_t}] $ </center>\n\n- 위의 수식은 피지여행 DPG 글 4-2.Q-learning 을 이용한 off-policy actor-critic 에서 이미 정리 한 바 있습니다.\n\n\n\n## 3.Algorithm\nContinous control 을 위한 새로운 알고리즘을 제안합니다. 제안하는 알고리즘의 특징은 다음과 같습니다. \n\n- Replay buffer 를 사용하였다.\n- \"soft\" target update 를 사용하였다. \n- 각 차원의 scale이 다른 low dimension vector 로 부터 학습할 때 Batch Normalization을 사용하였다. \n- 탐험을 위해 action에 Noise 를 추가하였다. \n\n### 3.1 Replay buffer\n<center>\n<img src=\"https://www.dropbox.com/s/lc61b8nas1clqme/Screenshot%202018-06-23%2016.32.53.png?dl=1\" width=\"600px\"></center>\n\n- 큰 State space를 학습하고 일반화 하기위해서는 Neural Network와 같은 non-linear approximator 가 필수적이지만 수렴한다는 보장이 없음.\n- NFQCA 에서는 수렴의 안정성을 위해서 batch learning을 도입함. 하지만 NFQCA에서는 업데이트시에 policy를 reset 하지 않음.\n- DDPG 는 DQN에서 사용된 Replay buffer를 사용하여 online batch update를 가능하게 했음.\n\n### 3.2 Soft target update\n<center> \n$ \\theta^{Q^{'}} \\leftarrow \\tau \\theta^{Q} + (1-\\tau) \\theta^{Q^{'}}   $<br>\n$ \\theta^{\\mu^{'}} \\leftarrow \\tau \\theta^{\\mu} + (1-\\tau) \\theta^{\\mu^{'}}   $\n</center>\n\n- DQN에서는 일정 주기마다 origin network의 weight 를 target network로 직접 복사해서 사용했음.\n- DDPG 에서는 exponential moving average(지수이동평균) 식으로 대체\n- soft update 가 DQN에서 사용했던 방식에 비해 어떤 장점이 있는지는 명확하게 설명되어있지 않음.\n\n### 3.3 Batch Normalization\n<center> \n<img src=\"https://www.dropbox.com/s/1erxzrgk69x04j8/Screenshot%202018-06-23%2016.51.56.png?dl=1\" width=\"300px\">\n</center>\n\n- 서로 scale 이 다른 feature 를 state 로 사용할 때에 Neural Net이 일반화에서 어려움을 겪는다. \n    - 이걸 해결하기 위해서는 원래 직접 스케일을 조정해주었음.\n- 하지만 각 layer 의 Input 을 Unit Gaussian 이 되도록 강제하는 BatchNormalization을 사용하여 이 문제를 해결하였다.\n\n### 3.4 Noise Process\nDDPG 에서는 Exploration을 위해서 output 으로 나온 행동에 노이즈를 추가해줍니다.\n<center>\nORNSTEIN UHLENBECK PROCESS(이하 OU) : $dx_t = \\theta (\\mu - x_t) dt + \\sigma dW_t$\n</center>\n\n- OU Process는 평균으로 회귀하는 random process 입니다.\n- $\\theta$ 는 얼마나 빨리 평균으로 회귀할 지를 나타내는 파라미터이며  $\\mu$ 는 평균을 의미합니다.\n- $\\sigma$ 는 process 의 변동성을 의미하며 $W_t$ 는 Wiener process 를 의미합니다. <br>\n- 따라서 이전의 noise들과 temporally correlated 입니다.\n- 위와 같은 temporally correlated noise process를 사용하는 이유는 physical control 과 같은 관성이 있는 환경에서 학습 시킬 때 보다 효과적이기 때문입니다.\n\n### 3.5 Diagram & Pseudocode \n\n<img src=\"https://www.dropbox.com/s/0ffb2c9irctjx2n/Screenshot%202018-06-23%2017.58.57.png?dl=1\"> \n\n- DDPG 의 학습 과정을 간단히 도식화 해본 다이어 그램입니다. \n\n<center>\n<img src=\"https://www.dropbox.com/s/fd0nj7goixfnd6z/Screenshot%202018-06-23%2018.02.13.png?dl=1\" width=\"500px\"></center>\n\n- DDPG 의 알고리즘 수도코드입니다. 모든 항목을 위에서 설명했으니 순서대로 보시면 이해에 도움이 될것입니다. \n\n\n## 4. Results\n\n### 4.1 Variants of DPG\n<center>\n<img src=\"https://www.dropbox.com/s/k8q6tih85lgow5l/Screenshot%202018-06-23%2018.08.05.png?dl=1\" width=\"500px\"></center>\n\n- original DPG 에 batchnorm 만 추가 (연한 회색), target network만 추가 (진한 회색), 둘 다 추가 (초록), pixel로만 학습 (파랑) . Target network가 성능을 가장 좌지우지한다.\n\n### 4.2 Q estimation of DDPG\n<center>\n<img src=\"https://www.dropbox.com/s/vvcfoni0lqrisst/Screenshot%202018-06-23%2018.10.23.png?dl=1\" width=\"500px\"></center>\n\n- DQN 은 Q value 를 Over-estimate 하는 경향이 있었지만, DDPG 는 simple task에 대해서는 잘한다. 복잡한 문제에 대해서는 estimation을 잘 못했지만, 여전히 좋은 Policy를 찾아내었다.\n\n### 4.3 Performance Comparison\n<center>\n<img src=\"https://www.dropbox.com/s/u8ibmz9q4kfxh6l/Screenshot%202018-06-23%2018.11.40.png?dl=1\" width=\"500px\"></center>\n\n- Score 는 naive policy 를 0, ILQG (planning algorithm) 의 mean score를 1점으로 놓았을 때의 점수 Torcs 환경에 대해서만 raw reward 를 score로 사용.\n\n## 5. Implementation Details\n\n### 5.1 Hyper parameters\n\n- Optimizer : Adam\n    - actor lr : 0.0001, critic lr : 0.001\n- Weight decay(L2) for critic(Q) = 0.001 \n- discount factor, $\\gamma = 0.99 $\n- soft target updates. $\\tau = 0.001 $\n- Size of replay buffer = 1,000,000\n- Orstein Uhlenbeck Process : $\\theta = 0.15$, $\\sigma = 0.2$\n\n\n### 5.2 Etc.\n\n- Final output layer of actor : tanh (행동의 최소 최대를 맞춰주기 위해서)\n- low-dimentional 문제에서 네트워크는 2개의 hidden layer (1st layer 400 units, 2nd layer 300 units) 를 가진다. \n- 이미지를 통해서 학습시킬 때 : 3 convolutional layers (no pooling) with 32 filters at each layer.\n- actor 와 critic 각각의 final layer(weight, bias 모두 ) 는 다음 범위의 uniform distribution 에서 샘플링한다. [ - 0.003, 0.003], [ - 0.0003 , 0.0003] . 이렇게 하는 이유는 가장 처음의 policy와 value 의 output 이 0에 가깝게 나오도록 하기 위함. \n\n\n<br>\n## 6.Conclusion\n<br>\n\n- 이 연구는 최근 딥러닝의 발전과 강화학습을 엮은 것으로 Continuous action space 를 가지는 문제를 robust 하게 풀어냄.\n- non-linear function approximators 을 쓰는 것은 수렴을 보장하지 않지만, 여러 환경에 대해서 특별한 조작 없이 안정적으로 수렴하는 것을 실험으로 보여냄.\n- Atari 도메인에서 DQN 보다 상당히 적은 step 만에 수렴하는 것을 실험을 통해서 알아냄. \n- model-free 알고리즘은 좋은 solution 을 찾기 위해서는 많은 sample을 필요로 한다는 한계가 있지만 더 큰 시스템에서는 이러한 한계를 물리칠 정도로 중요한 역할을 하게 될 것임.","slug":"ddpg","published":1,"updated":"2018-07-11T06:27:58.253Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjjgzyg4h0008xo15m13f942j","content":"<p><br><br><br></p>\n<h1 id=\"Deep-Determinstic-Policy-Gradient-DDPG\"><a href=\"#Deep-Determinstic-Policy-Gradient-DDPG\" class=\"headerlink\" title=\"Deep Determinstic Policy Gradient(DDPG)\"></a>Deep Determinstic Policy Gradient(DDPG)</h1><p><img src=\"https://www.dropbox.com/s/zv8rk0uf87ipiaj/Screenshot%202018-06-23%2010.01.48.png?dl=1\"></p>\n<ul>\n<li><a href=\"&quot;https://arxiv.org/abs/1509.02971&quot;\">논문 링크</a></li>\n<li>Deterministic Policy Gradient(DPG)를 기반으로 Deep Neural Network 를 사용해 continuous control 문제를 풀어낸 논문입니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1.Introduction\"></a>1.Introduction</h2><p>DDPG 알고리즘에 대한 개요입니다.</p>\n<h3 id=\"1-1-Success-amp-Limination-of-DQN\"><a href=\"#1-1-Success-amp-Limination-of-DQN\" class=\"headerlink\" title=\"1.1 Success &amp; Limination of DQN\"></a>1.1 Success &amp; Limination of DQN</h3><ul>\n<li>Success<ul>\n<li>sensor로 부터 나오는 전처리를 거친 input 대신에 raw pixel input 을 사용 <br><br>: High dimensional observation space 문제를 풀어냄.</li>\n</ul>\n</li>\n<li>Limitation<ul>\n<li>discrete &amp; low dimensional action space 만 다룰 수 있음 <br><br>: Continuous action space 를 다루기 위해서는 매 스텝 이를 위한 iterative optimization process 를 거쳐야 함.</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"1-2-Problems-of-discritization\"><a href=\"#1-2-Problems-of-discritization\" class=\"headerlink\" title=\"1.2 Problems of discritization\"></a>1.2 Problems of discritization</h3><p align=\"center\"><br><img src=\"https://www.dropbox.com/s/nulhzxs8bak2fn6/Screenshot%202018-06-23%2012.22.20.png?dl=1\" width=\"300px\"><br></p>\n\n\n<ul>\n<li>만약 7개의 관절을 가진 로봇 팔이 있다면, 가장 간단한 discretization 은 각 관절을 다음과 같이 $a_{i}\\in \\{ -k, 0, k \\}$ 3 개의 값을 가지도록 하는 것이다.</li>\n<li>그렇다면 $3 \\times 3 \\times 3 \\times 3 \\times 3 \\times 3 \\times 3 = 3^{7} = 2187$ 가지의 dimension을 가진 action space가 만들어진다.<ul>\n<li>Discretization 을 하면 action space 가 exponential 하게 늘어남.</li>\n</ul>\n</li>\n<li>충분히 큰 action space 임에도 discretization으로 인한 정보의 손실도 있다<ul>\n<li>섬세한 Control 을 할 수 없다.</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"1-3-New-approach-for-continuous-control\"><a href=\"#1-3-New-approach-for-continuous-control\" class=\"headerlink\" title=\"1.3 New approach for continuous control\"></a>1.3 New approach for continuous control</h3><ul>\n<li>Model-free, Off-policy, Actor-critic algorithm 을 제안.</li>\n<li>Deep Deterministic Policy(이하 DPG) 를 기반으로 함.</li>\n<li>Actor-Critic approach 와 DQN의 성공적이었던 부분을 합침<ul>\n<li>Replay buffer : 샘플들 사이의 상관관계를 줄여줌</li>\n<li>target Q Network : Update 동안 target 을 안정적으로 만들어줌.</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"2-Background\"><a href=\"#2-Background\" class=\"headerlink\" title=\"2.Background\"></a>2.Background</h2><h3 id=\"2-1-Notation\"><a href=\"#2-1-Notation\" class=\"headerlink\" title=\"2.1 Notation\"></a>2.1 Notation</h3><ul>\n<li>Observation : $x_{t}$</li>\n<li>Action : $a_t \\in {\\rm IR}^N $</li>\n<li>Reward : $r_t$</li>\n<li>Discount factor : $\\gamma$</li>\n<li>Environment : $E$</li>\n<li>Policy : $\\pi : S \\rightarrow P(A)  $</li>\n<li>Transition dynamics : $p(s_{t+1} \\vert s_t, a_t) $</li>\n<li>Reward function : $r(s_t, a_t)$</li>\n<li>Return : $ \\sum_{ i=t }^{ T } \\gamma^{(i-t)}r(s_i, a_i) $</li>\n<li>Discounted state visitation distribution for a policy : $\\rho^\\pi $</li>\n</ul>\n<h3 id=\"2-2-Bellman-Equation\"><a href=\"#2-2-Bellman-Equation\" class=\"headerlink\" title=\"2.2 Bellman Equation\"></a>2.2 Bellman Equation</h3><ul>\n<li>상태 $s_t$ 에서 행동 $a_t$를 취했을 때 Expected return<br>$ Q^{\\pi}(s_t, a_t)={\\rm E}_{r_{i \\geqq t},s_{i \\geqq t} \\backsim E, a_{i \\geqq t} \\backsim \\pi } [R_{t} \\vert s_t, a_t  ] $<br><br></li>\n<li>벨만 방정식을 사용하여 위의 식을 변형<br>$ Q^{\\pi}(s_t, a_t)={\\rm E}_{r_{t},s_{t} \\backsim E } [r(s_t,a_t)+\\gamma {\\rm E}_{a_{t+1} \\backsim \\pi } [ Q^{\\pi}(s_{t+1}, a_{t+1}) ] ] $<br><br></li>\n<li>Determinsitc policy 를 가정<br>$ Q^{\\mu}(s_t, a_t)={\\rm E}_{r_{t},s_{t} \\backsim E } [r(s_t,a_t)+\\gamma Q^{\\mu}(s_{t+1}, \\mu (s_{t+1})) ] $<ul>\n<li>위의 식에서 아래 식으로 내려오면서 policy 가 determinstic 하기 때문에 policy 에 dependent 한 Expectation 이 빠진 것을 알 수 있습니다. </li>\n<li>Deterministic policy를 가정하기 전의 수식에서는 $a_{t+1}$ 을 골랐던 순간의 policy로 Q에 대한 Expection을 원래 구해야하기 때문에 off-policy가 아니지만, Determinsitic policy를 가정한다면 update 할 당시의 policy로 $a_{t+1}$ 를 구할 수 있기 때문에 off-policy 입니다.<br><br></li>\n</ul>\n</li>\n<li>Q learning<br>$L(\\theta^{Q}) = {\\rm E}_{s_t \\backsim \\rho^\\beta , a_t \\backsim \\beta , r_t \\backsim E} [(Q(s_t, a_t \\vert \\theta^Q)-y_t)^2] $ 참고)  $\\beta$ 는 behavior policy를 의미합니다. <ul>\n<li>$ y_t = r(s_t, a_t) + \\gamma Q^{\\mu}(s_{t+1},\\mu(s_{t+1})) $  </li>\n<li>$\\mu(s) = argmax_{a}Q(s,a)$<ul>\n<li>Q learning 은 위와 같이 $argmax$ 라는 deterministic policy를 사용하기 때문에 off policy 로 사용할 수 있습니다. </li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"2-3-DPG\"><a href=\"#2-3-DPG\" class=\"headerlink\" title=\"2.3 DPG\"></a>2.3 DPG</h3><center> $ \\nabla_{\\theta^\\mu} J \\approx  {\\rm E}_{s_t \\backsim \\rho^\\beta} [ \\nabla_{\\theta^\\mu} Q(s, a \\vert \\theta ^ Q) \\vert_{s=s_t, a=\\mu(s_t)}] $ <br> $ = {\\rm E}_{s_t \\backsim \\rho^\\beta} [ \\nabla_{a} Q(s, a \\vert \\theta ^ Q) \\vert_{s=s_t, a=\\mu(s_t)} \\nabla_{\\theta^\\mu} \\mu(s \\vert Q^{\\mu})\\vert_{s=s_t}] $ </center>\n\n<ul>\n<li>위의 수식은 피지여행 DPG 글 4-2.Q-learning 을 이용한 off-policy actor-critic 에서 이미 정리 한 바 있습니다.</li>\n</ul>\n<h2 id=\"3-Algorithm\"><a href=\"#3-Algorithm\" class=\"headerlink\" title=\"3.Algorithm\"></a>3.Algorithm</h2><p>Continous control 을 위한 새로운 알고리즘을 제안합니다. 제안하는 알고리즘의 특징은 다음과 같습니다. </p>\n<ul>\n<li>Replay buffer 를 사용하였다.</li>\n<li>“soft” target update 를 사용하였다. </li>\n<li>각 차원의 scale이 다른 low dimension vector 로 부터 학습할 때 Batch Normalization을 사용하였다. </li>\n<li>탐험을 위해 action에 Noise 를 추가하였다. </li>\n</ul>\n<h3 id=\"3-1-Replay-buffer\"><a href=\"#3-1-Replay-buffer\" class=\"headerlink\" title=\"3.1 Replay buffer\"></a>3.1 Replay buffer</h3><center><br><img src=\"https://www.dropbox.com/s/lc61b8nas1clqme/Screenshot%202018-06-23%2016.32.53.png?dl=1\" width=\"600px\"></center>\n\n<ul>\n<li>큰 State space를 학습하고 일반화 하기위해서는 Neural Network와 같은 non-linear approximator 가 필수적이지만 수렴한다는 보장이 없음.</li>\n<li>NFQCA 에서는 수렴의 안정성을 위해서 batch learning을 도입함. 하지만 NFQCA에서는 업데이트시에 policy를 reset 하지 않음.</li>\n<li>DDPG 는 DQN에서 사용된 Replay buffer를 사용하여 online batch update를 가능하게 했음.</li>\n</ul>\n<h3 id=\"3-2-Soft-target-update\"><a href=\"#3-2-Soft-target-update\" class=\"headerlink\" title=\"3.2 Soft target update\"></a>3.2 Soft target update</h3><center><br>$ \\theta^{Q^{‘}} \\leftarrow \\tau \\theta^{Q} + (1-\\tau) \\theta^{Q^{‘}}   $<br><br>$ \\theta^{\\mu^{‘}} \\leftarrow \\tau \\theta^{\\mu} + (1-\\tau) \\theta^{\\mu^{‘}}   $<br></center>\n\n<ul>\n<li>DQN에서는 일정 주기마다 origin network의 weight 를 target network로 직접 복사해서 사용했음.</li>\n<li>DDPG 에서는 exponential moving average(지수이동평균) 식으로 대체</li>\n<li>soft update 가 DQN에서 사용했던 방식에 비해 어떤 장점이 있는지는 명확하게 설명되어있지 않음.</li>\n</ul>\n<h3 id=\"3-3-Batch-Normalization\"><a href=\"#3-3-Batch-Normalization\" class=\"headerlink\" title=\"3.3 Batch Normalization\"></a>3.3 Batch Normalization</h3><center><br><img src=\"https://www.dropbox.com/s/1erxzrgk69x04j8/Screenshot%202018-06-23%2016.51.56.png?dl=1\" width=\"300px\"><br></center>\n\n<ul>\n<li>서로 scale 이 다른 feature 를 state 로 사용할 때에 Neural Net이 일반화에서 어려움을 겪는다. <ul>\n<li>이걸 해결하기 위해서는 원래 직접 스케일을 조정해주었음.</li>\n</ul>\n</li>\n<li>하지만 각 layer 의 Input 을 Unit Gaussian 이 되도록 강제하는 BatchNormalization을 사용하여 이 문제를 해결하였다.</li>\n</ul>\n<h3 id=\"3-4-Noise-Process\"><a href=\"#3-4-Noise-Process\" class=\"headerlink\" title=\"3.4 Noise Process\"></a>3.4 Noise Process</h3><p>DDPG 에서는 Exploration을 위해서 output 으로 나온 행동에 노이즈를 추가해줍니다.</p>\n<center><br>ORNSTEIN UHLENBECK PROCESS(이하 OU) : $dx_t = \\theta (\\mu - x_t) dt + \\sigma dW_t$<br></center>\n\n<ul>\n<li>OU Process는 평균으로 회귀하는 random process 입니다.</li>\n<li>$\\theta$ 는 얼마나 빨리 평균으로 회귀할 지를 나타내는 파라미터이며  $\\mu$ 는 평균을 의미합니다.</li>\n<li>$\\sigma$ 는 process 의 변동성을 의미하며 $W_t$ 는 Wiener process 를 의미합니다. <br></li>\n<li>따라서 이전의 noise들과 temporally correlated 입니다.</li>\n<li>위와 같은 temporally correlated noise process를 사용하는 이유는 physical control 과 같은 관성이 있는 환경에서 학습 시킬 때 보다 효과적이기 때문입니다.</li>\n</ul>\n<h3 id=\"3-5-Diagram-amp-Pseudocode\"><a href=\"#3-5-Diagram-amp-Pseudocode\" class=\"headerlink\" title=\"3.5 Diagram &amp; Pseudocode\"></a>3.5 Diagram &amp; Pseudocode</h3><p><img src=\"https://www.dropbox.com/s/0ffb2c9irctjx2n/Screenshot%202018-06-23%2017.58.57.png?dl=1\"> </p>\n<ul>\n<li>DDPG 의 학습 과정을 간단히 도식화 해본 다이어 그램입니다. </li>\n</ul>\n<center><br><img src=\"https://www.dropbox.com/s/fd0nj7goixfnd6z/Screenshot%202018-06-23%2018.02.13.png?dl=1\" width=\"500px\"></center>\n\n<ul>\n<li>DDPG 의 알고리즘 수도코드입니다. 모든 항목을 위에서 설명했으니 순서대로 보시면 이해에 도움이 될것입니다. </li>\n</ul>\n<h2 id=\"4-Results\"><a href=\"#4-Results\" class=\"headerlink\" title=\"4. Results\"></a>4. Results</h2><h3 id=\"4-1-Variants-of-DPG\"><a href=\"#4-1-Variants-of-DPG\" class=\"headerlink\" title=\"4.1 Variants of DPG\"></a>4.1 Variants of DPG</h3><center><br><img src=\"https://www.dropbox.com/s/k8q6tih85lgow5l/Screenshot%202018-06-23%2018.08.05.png?dl=1\" width=\"500px\"></center>\n\n<ul>\n<li>original DPG 에 batchnorm 만 추가 (연한 회색), target network만 추가 (진한 회색), 둘 다 추가 (초록), pixel로만 학습 (파랑) . Target network가 성능을 가장 좌지우지한다.</li>\n</ul>\n<h3 id=\"4-2-Q-estimation-of-DDPG\"><a href=\"#4-2-Q-estimation-of-DDPG\" class=\"headerlink\" title=\"4.2 Q estimation of DDPG\"></a>4.2 Q estimation of DDPG</h3><center><br><img src=\"https://www.dropbox.com/s/vvcfoni0lqrisst/Screenshot%202018-06-23%2018.10.23.png?dl=1\" width=\"500px\"></center>\n\n<ul>\n<li>DQN 은 Q value 를 Over-estimate 하는 경향이 있었지만, DDPG 는 simple task에 대해서는 잘한다. 복잡한 문제에 대해서는 estimation을 잘 못했지만, 여전히 좋은 Policy를 찾아내었다.</li>\n</ul>\n<h3 id=\"4-3-Performance-Comparison\"><a href=\"#4-3-Performance-Comparison\" class=\"headerlink\" title=\"4.3 Performance Comparison\"></a>4.3 Performance Comparison</h3><center><br><img src=\"https://www.dropbox.com/s/u8ibmz9q4kfxh6l/Screenshot%202018-06-23%2018.11.40.png?dl=1\" width=\"500px\"></center>\n\n<ul>\n<li>Score 는 naive policy 를 0, ILQG (planning algorithm) 의 mean score를 1점으로 놓았을 때의 점수 Torcs 환경에 대해서만 raw reward 를 score로 사용.</li>\n</ul>\n<h2 id=\"5-Implementation-Details\"><a href=\"#5-Implementation-Details\" class=\"headerlink\" title=\"5. Implementation Details\"></a>5. Implementation Details</h2><h3 id=\"5-1-Hyper-parameters\"><a href=\"#5-1-Hyper-parameters\" class=\"headerlink\" title=\"5.1 Hyper parameters\"></a>5.1 Hyper parameters</h3><ul>\n<li>Optimizer : Adam<ul>\n<li>actor lr : 0.0001, critic lr : 0.001</li>\n</ul>\n</li>\n<li>Weight decay(L2) for critic(Q) = 0.001 </li>\n<li>discount factor, $\\gamma = 0.99 $</li>\n<li>soft target updates. $\\tau = 0.001 $</li>\n<li>Size of replay buffer = 1,000,000</li>\n<li>Orstein Uhlenbeck Process : $\\theta = 0.15$, $\\sigma = 0.2$</li>\n</ul>\n<h3 id=\"5-2-Etc\"><a href=\"#5-2-Etc\" class=\"headerlink\" title=\"5.2 Etc.\"></a>5.2 Etc.</h3><ul>\n<li>Final output layer of actor : tanh (행동의 최소 최대를 맞춰주기 위해서)</li>\n<li>low-dimentional 문제에서 네트워크는 2개의 hidden layer (1st layer 400 units, 2nd layer 300 units) 를 가진다. </li>\n<li>이미지를 통해서 학습시킬 때 : 3 convolutional layers (no pooling) with 32 filters at each layer.</li>\n<li>actor 와 critic 각각의 final layer(weight, bias 모두 ) 는 다음 범위의 uniform distribution 에서 샘플링한다. [ - 0.003, 0.003], [ - 0.0003 , 0.0003] . 이렇게 하는 이유는 가장 처음의 policy와 value 의 output 이 0에 가깝게 나오도록 하기 위함. </li>\n</ul>\n<p><br></p>\n<h2 id=\"6-Conclusion\"><a href=\"#6-Conclusion\" class=\"headerlink\" title=\"6.Conclusion\"></a>6.Conclusion</h2><p><br></p>\n<ul>\n<li>이 연구는 최근 딥러닝의 발전과 강화학습을 엮은 것으로 Continuous action space 를 가지는 문제를 robust 하게 풀어냄.</li>\n<li>non-linear function approximators 을 쓰는 것은 수렴을 보장하지 않지만, 여러 환경에 대해서 특별한 조작 없이 안정적으로 수렴하는 것을 실험으로 보여냄.</li>\n<li>Atari 도메인에서 DQN 보다 상당히 적은 step 만에 수렴하는 것을 실험을 통해서 알아냄. </li>\n<li>model-free 알고리즘은 좋은 solution 을 찾기 위해서는 많은 sample을 필요로 한다는 한계가 있지만 더 큰 시스템에서는 이러한 한계를 물리칠 정도로 중요한 역할을 하게 될 것임.</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p><br><br><br></p>\n<h1 id=\"Deep-Determinstic-Policy-Gradient-DDPG\"><a href=\"#Deep-Determinstic-Policy-Gradient-DDPG\" class=\"headerlink\" title=\"Deep Determinstic Policy Gradient(DDPG)\"></a>Deep Determinstic Policy Gradient(DDPG)</h1><p><img src=\"https://www.dropbox.com/s/zv8rk0uf87ipiaj/Screenshot%202018-06-23%2010.01.48.png?dl=1\"></p>\n<ul>\n<li><a href=\"&quot;https://arxiv.org/abs/1509.02971&quot;\">논문 링크</a></li>\n<li>Deterministic Policy Gradient(DPG)를 기반으로 Deep Neural Network 를 사용해 continuous control 문제를 풀어낸 논문입니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1.Introduction\"></a>1.Introduction</h2><p>DDPG 알고리즘에 대한 개요입니다.</p>\n<h3 id=\"1-1-Success-amp-Limination-of-DQN\"><a href=\"#1-1-Success-amp-Limination-of-DQN\" class=\"headerlink\" title=\"1.1 Success &amp; Limination of DQN\"></a>1.1 Success &amp; Limination of DQN</h3><ul>\n<li>Success<ul>\n<li>sensor로 부터 나오는 전처리를 거친 input 대신에 raw pixel input 을 사용 <br><br>: High dimensional observation space 문제를 풀어냄.</li>\n</ul>\n</li>\n<li>Limitation<ul>\n<li>discrete &amp; low dimensional action space 만 다룰 수 있음 <br><br>: Continuous action space 를 다루기 위해서는 매 스텝 이를 위한 iterative optimization process 를 거쳐야 함.</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"1-2-Problems-of-discritization\"><a href=\"#1-2-Problems-of-discritization\" class=\"headerlink\" title=\"1.2 Problems of discritization\"></a>1.2 Problems of discritization</h3><p align=\"center\"><br><img src=\"https://www.dropbox.com/s/nulhzxs8bak2fn6/Screenshot%202018-06-23%2012.22.20.png?dl=1\" width=\"300px\"><br></p>\n\n\n<ul>\n<li>만약 7개의 관절을 가진 로봇 팔이 있다면, 가장 간단한 discretization 은 각 관절을 다음과 같이 $a_{i}\\in \\{ -k, 0, k \\}$ 3 개의 값을 가지도록 하는 것이다.</li>\n<li>그렇다면 $3 \\times 3 \\times 3 \\times 3 \\times 3 \\times 3 \\times 3 = 3^{7} = 2187$ 가지의 dimension을 가진 action space가 만들어진다.<ul>\n<li>Discretization 을 하면 action space 가 exponential 하게 늘어남.</li>\n</ul>\n</li>\n<li>충분히 큰 action space 임에도 discretization으로 인한 정보의 손실도 있다<ul>\n<li>섬세한 Control 을 할 수 없다.</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"1-3-New-approach-for-continuous-control\"><a href=\"#1-3-New-approach-for-continuous-control\" class=\"headerlink\" title=\"1.3 New approach for continuous control\"></a>1.3 New approach for continuous control</h3><ul>\n<li>Model-free, Off-policy, Actor-critic algorithm 을 제안.</li>\n<li>Deep Deterministic Policy(이하 DPG) 를 기반으로 함.</li>\n<li>Actor-Critic approach 와 DQN의 성공적이었던 부분을 합침<ul>\n<li>Replay buffer : 샘플들 사이의 상관관계를 줄여줌</li>\n<li>target Q Network : Update 동안 target 을 안정적으로 만들어줌.</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"2-Background\"><a href=\"#2-Background\" class=\"headerlink\" title=\"2.Background\"></a>2.Background</h2><h3 id=\"2-1-Notation\"><a href=\"#2-1-Notation\" class=\"headerlink\" title=\"2.1 Notation\"></a>2.1 Notation</h3><ul>\n<li>Observation : $x_{t}$</li>\n<li>Action : $a_t \\in {\\rm IR}^N $</li>\n<li>Reward : $r_t$</li>\n<li>Discount factor : $\\gamma$</li>\n<li>Environment : $E$</li>\n<li>Policy : $\\pi : S \\rightarrow P(A)  $</li>\n<li>Transition dynamics : $p(s_{t+1} \\vert s_t, a_t) $</li>\n<li>Reward function : $r(s_t, a_t)$</li>\n<li>Return : $ \\sum_{ i=t }^{ T } \\gamma^{(i-t)}r(s_i, a_i) $</li>\n<li>Discounted state visitation distribution for a policy : $\\rho^\\pi $</li>\n</ul>\n<h3 id=\"2-2-Bellman-Equation\"><a href=\"#2-2-Bellman-Equation\" class=\"headerlink\" title=\"2.2 Bellman Equation\"></a>2.2 Bellman Equation</h3><ul>\n<li>상태 $s_t$ 에서 행동 $a_t$를 취했을 때 Expected return<br>$ Q^{\\pi}(s_t, a_t)={\\rm E}_{r_{i \\geqq t},s_{i \\geqq t} \\backsim E, a_{i \\geqq t} \\backsim \\pi } [R_{t} \\vert s_t, a_t  ] $<br><br></li>\n<li>벨만 방정식을 사용하여 위의 식을 변형<br>$ Q^{\\pi}(s_t, a_t)={\\rm E}_{r_{t},s_{t} \\backsim E } [r(s_t,a_t)+\\gamma {\\rm E}_{a_{t+1} \\backsim \\pi } [ Q^{\\pi}(s_{t+1}, a_{t+1}) ] ] $<br><br></li>\n<li>Determinsitc policy 를 가정<br>$ Q^{\\mu}(s_t, a_t)={\\rm E}_{r_{t},s_{t} \\backsim E } [r(s_t,a_t)+\\gamma Q^{\\mu}(s_{t+1}, \\mu (s_{t+1})) ] $<ul>\n<li>위의 식에서 아래 식으로 내려오면서 policy 가 determinstic 하기 때문에 policy 에 dependent 한 Expectation 이 빠진 것을 알 수 있습니다. </li>\n<li>Deterministic policy를 가정하기 전의 수식에서는 $a_{t+1}$ 을 골랐던 순간의 policy로 Q에 대한 Expection을 원래 구해야하기 때문에 off-policy가 아니지만, Determinsitic policy를 가정한다면 update 할 당시의 policy로 $a_{t+1}$ 를 구할 수 있기 때문에 off-policy 입니다.<br><br></li>\n</ul>\n</li>\n<li>Q learning<br>$L(\\theta^{Q}) = {\\rm E}_{s_t \\backsim \\rho^\\beta , a_t \\backsim \\beta , r_t \\backsim E} [(Q(s_t, a_t \\vert \\theta^Q)-y_t)^2] $ 참고)  $\\beta$ 는 behavior policy를 의미합니다. <ul>\n<li>$ y_t = r(s_t, a_t) + \\gamma Q^{\\mu}(s_{t+1},\\mu(s_{t+1})) $  </li>\n<li>$\\mu(s) = argmax_{a}Q(s,a)$<ul>\n<li>Q learning 은 위와 같이 $argmax$ 라는 deterministic policy를 사용하기 때문에 off policy 로 사용할 수 있습니다. </li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"2-3-DPG\"><a href=\"#2-3-DPG\" class=\"headerlink\" title=\"2.3 DPG\"></a>2.3 DPG</h3><center> $ \\nabla_{\\theta^\\mu} J \\approx  {\\rm E}_{s_t \\backsim \\rho^\\beta} [ \\nabla_{\\theta^\\mu} Q(s, a \\vert \\theta ^ Q) \\vert_{s=s_t, a=\\mu(s_t)}] $ <br> $ = {\\rm E}_{s_t \\backsim \\rho^\\beta} [ \\nabla_{a} Q(s, a \\vert \\theta ^ Q) \\vert_{s=s_t, a=\\mu(s_t)} \\nabla_{\\theta^\\mu} \\mu(s \\vert Q^{\\mu})\\vert_{s=s_t}] $ </center>\n\n<ul>\n<li>위의 수식은 피지여행 DPG 글 4-2.Q-learning 을 이용한 off-policy actor-critic 에서 이미 정리 한 바 있습니다.</li>\n</ul>\n<h2 id=\"3-Algorithm\"><a href=\"#3-Algorithm\" class=\"headerlink\" title=\"3.Algorithm\"></a>3.Algorithm</h2><p>Continous control 을 위한 새로운 알고리즘을 제안합니다. 제안하는 알고리즘의 특징은 다음과 같습니다. </p>\n<ul>\n<li>Replay buffer 를 사용하였다.</li>\n<li>“soft” target update 를 사용하였다. </li>\n<li>각 차원의 scale이 다른 low dimension vector 로 부터 학습할 때 Batch Normalization을 사용하였다. </li>\n<li>탐험을 위해 action에 Noise 를 추가하였다. </li>\n</ul>\n<h3 id=\"3-1-Replay-buffer\"><a href=\"#3-1-Replay-buffer\" class=\"headerlink\" title=\"3.1 Replay buffer\"></a>3.1 Replay buffer</h3><center><br><img src=\"https://www.dropbox.com/s/lc61b8nas1clqme/Screenshot%202018-06-23%2016.32.53.png?dl=1\" width=\"600px\"></center>\n\n<ul>\n<li>큰 State space를 학습하고 일반화 하기위해서는 Neural Network와 같은 non-linear approximator 가 필수적이지만 수렴한다는 보장이 없음.</li>\n<li>NFQCA 에서는 수렴의 안정성을 위해서 batch learning을 도입함. 하지만 NFQCA에서는 업데이트시에 policy를 reset 하지 않음.</li>\n<li>DDPG 는 DQN에서 사용된 Replay buffer를 사용하여 online batch update를 가능하게 했음.</li>\n</ul>\n<h3 id=\"3-2-Soft-target-update\"><a href=\"#3-2-Soft-target-update\" class=\"headerlink\" title=\"3.2 Soft target update\"></a>3.2 Soft target update</h3><center><br>$ \\theta^{Q^{‘}} \\leftarrow \\tau \\theta^{Q} + (1-\\tau) \\theta^{Q^{‘}}   $<br><br>$ \\theta^{\\mu^{‘}} \\leftarrow \\tau \\theta^{\\mu} + (1-\\tau) \\theta^{\\mu^{‘}}   $<br></center>\n\n<ul>\n<li>DQN에서는 일정 주기마다 origin network의 weight 를 target network로 직접 복사해서 사용했음.</li>\n<li>DDPG 에서는 exponential moving average(지수이동평균) 식으로 대체</li>\n<li>soft update 가 DQN에서 사용했던 방식에 비해 어떤 장점이 있는지는 명확하게 설명되어있지 않음.</li>\n</ul>\n<h3 id=\"3-3-Batch-Normalization\"><a href=\"#3-3-Batch-Normalization\" class=\"headerlink\" title=\"3.3 Batch Normalization\"></a>3.3 Batch Normalization</h3><center><br><img src=\"https://www.dropbox.com/s/1erxzrgk69x04j8/Screenshot%202018-06-23%2016.51.56.png?dl=1\" width=\"300px\"><br></center>\n\n<ul>\n<li>서로 scale 이 다른 feature 를 state 로 사용할 때에 Neural Net이 일반화에서 어려움을 겪는다. <ul>\n<li>이걸 해결하기 위해서는 원래 직접 스케일을 조정해주었음.</li>\n</ul>\n</li>\n<li>하지만 각 layer 의 Input 을 Unit Gaussian 이 되도록 강제하는 BatchNormalization을 사용하여 이 문제를 해결하였다.</li>\n</ul>\n<h3 id=\"3-4-Noise-Process\"><a href=\"#3-4-Noise-Process\" class=\"headerlink\" title=\"3.4 Noise Process\"></a>3.4 Noise Process</h3><p>DDPG 에서는 Exploration을 위해서 output 으로 나온 행동에 노이즈를 추가해줍니다.</p>\n<center><br>ORNSTEIN UHLENBECK PROCESS(이하 OU) : $dx_t = \\theta (\\mu - x_t) dt + \\sigma dW_t$<br></center>\n\n<ul>\n<li>OU Process는 평균으로 회귀하는 random process 입니다.</li>\n<li>$\\theta$ 는 얼마나 빨리 평균으로 회귀할 지를 나타내는 파라미터이며  $\\mu$ 는 평균을 의미합니다.</li>\n<li>$\\sigma$ 는 process 의 변동성을 의미하며 $W_t$ 는 Wiener process 를 의미합니다. <br></li>\n<li>따라서 이전의 noise들과 temporally correlated 입니다.</li>\n<li>위와 같은 temporally correlated noise process를 사용하는 이유는 physical control 과 같은 관성이 있는 환경에서 학습 시킬 때 보다 효과적이기 때문입니다.</li>\n</ul>\n<h3 id=\"3-5-Diagram-amp-Pseudocode\"><a href=\"#3-5-Diagram-amp-Pseudocode\" class=\"headerlink\" title=\"3.5 Diagram &amp; Pseudocode\"></a>3.5 Diagram &amp; Pseudocode</h3><p><img src=\"https://www.dropbox.com/s/0ffb2c9irctjx2n/Screenshot%202018-06-23%2017.58.57.png?dl=1\"> </p>\n<ul>\n<li>DDPG 의 학습 과정을 간단히 도식화 해본 다이어 그램입니다. </li>\n</ul>\n<center><br><img src=\"https://www.dropbox.com/s/fd0nj7goixfnd6z/Screenshot%202018-06-23%2018.02.13.png?dl=1\" width=\"500px\"></center>\n\n<ul>\n<li>DDPG 의 알고리즘 수도코드입니다. 모든 항목을 위에서 설명했으니 순서대로 보시면 이해에 도움이 될것입니다. </li>\n</ul>\n<h2 id=\"4-Results\"><a href=\"#4-Results\" class=\"headerlink\" title=\"4. Results\"></a>4. Results</h2><h3 id=\"4-1-Variants-of-DPG\"><a href=\"#4-1-Variants-of-DPG\" class=\"headerlink\" title=\"4.1 Variants of DPG\"></a>4.1 Variants of DPG</h3><center><br><img src=\"https://www.dropbox.com/s/k8q6tih85lgow5l/Screenshot%202018-06-23%2018.08.05.png?dl=1\" width=\"500px\"></center>\n\n<ul>\n<li>original DPG 에 batchnorm 만 추가 (연한 회색), target network만 추가 (진한 회색), 둘 다 추가 (초록), pixel로만 학습 (파랑) . Target network가 성능을 가장 좌지우지한다.</li>\n</ul>\n<h3 id=\"4-2-Q-estimation-of-DDPG\"><a href=\"#4-2-Q-estimation-of-DDPG\" class=\"headerlink\" title=\"4.2 Q estimation of DDPG\"></a>4.2 Q estimation of DDPG</h3><center><br><img src=\"https://www.dropbox.com/s/vvcfoni0lqrisst/Screenshot%202018-06-23%2018.10.23.png?dl=1\" width=\"500px\"></center>\n\n<ul>\n<li>DQN 은 Q value 를 Over-estimate 하는 경향이 있었지만, DDPG 는 simple task에 대해서는 잘한다. 복잡한 문제에 대해서는 estimation을 잘 못했지만, 여전히 좋은 Policy를 찾아내었다.</li>\n</ul>\n<h3 id=\"4-3-Performance-Comparison\"><a href=\"#4-3-Performance-Comparison\" class=\"headerlink\" title=\"4.3 Performance Comparison\"></a>4.3 Performance Comparison</h3><center><br><img src=\"https://www.dropbox.com/s/u8ibmz9q4kfxh6l/Screenshot%202018-06-23%2018.11.40.png?dl=1\" width=\"500px\"></center>\n\n<ul>\n<li>Score 는 naive policy 를 0, ILQG (planning algorithm) 의 mean score를 1점으로 놓았을 때의 점수 Torcs 환경에 대해서만 raw reward 를 score로 사용.</li>\n</ul>\n<h2 id=\"5-Implementation-Details\"><a href=\"#5-Implementation-Details\" class=\"headerlink\" title=\"5. Implementation Details\"></a>5. Implementation Details</h2><h3 id=\"5-1-Hyper-parameters\"><a href=\"#5-1-Hyper-parameters\" class=\"headerlink\" title=\"5.1 Hyper parameters\"></a>5.1 Hyper parameters</h3><ul>\n<li>Optimizer : Adam<ul>\n<li>actor lr : 0.0001, critic lr : 0.001</li>\n</ul>\n</li>\n<li>Weight decay(L2) for critic(Q) = 0.001 </li>\n<li>discount factor, $\\gamma = 0.99 $</li>\n<li>soft target updates. $\\tau = 0.001 $</li>\n<li>Size of replay buffer = 1,000,000</li>\n<li>Orstein Uhlenbeck Process : $\\theta = 0.15$, $\\sigma = 0.2$</li>\n</ul>\n<h3 id=\"5-2-Etc\"><a href=\"#5-2-Etc\" class=\"headerlink\" title=\"5.2 Etc.\"></a>5.2 Etc.</h3><ul>\n<li>Final output layer of actor : tanh (행동의 최소 최대를 맞춰주기 위해서)</li>\n<li>low-dimentional 문제에서 네트워크는 2개의 hidden layer (1st layer 400 units, 2nd layer 300 units) 를 가진다. </li>\n<li>이미지를 통해서 학습시킬 때 : 3 convolutional layers (no pooling) with 32 filters at each layer.</li>\n<li>actor 와 critic 각각의 final layer(weight, bias 모두 ) 는 다음 범위의 uniform distribution 에서 샘플링한다. [ - 0.003, 0.003], [ - 0.0003 , 0.0003] . 이렇게 하는 이유는 가장 처음의 policy와 value 의 output 이 0에 가깝게 나오도록 하기 위함. </li>\n</ul>\n<p><br></p>\n<h2 id=\"6-Conclusion\"><a href=\"#6-Conclusion\" class=\"headerlink\" title=\"6.Conclusion\"></a>6.Conclusion</h2><p><br></p>\n<ul>\n<li>이 연구는 최근 딥러닝의 발전과 강화학습을 엮은 것으로 Continuous action space 를 가지는 문제를 robust 하게 풀어냄.</li>\n<li>non-linear function approximators 을 쓰는 것은 수렴을 보장하지 않지만, 여러 환경에 대해서 특별한 조작 없이 안정적으로 수렴하는 것을 실험으로 보여냄.</li>\n<li>Atari 도메인에서 DQN 보다 상당히 적은 step 만에 수렴하는 것을 실험을 통해서 알아냄. </li>\n<li>model-free 알고리즘은 좋은 solution 을 찾기 위해서는 많은 sample을 필요로 한다는 한계가 있지만 더 큰 시스템에서는 이러한 한계를 물리칠 정도로 중요한 역할을 하게 될 것임.</li>\n</ul>\n"},{"title":"A Natural Policy Gradient","date":"2018-06-14T04:18:45.000Z","author":"이웅원","subtitle":"피지여행 4번째 논문","_content":"\n# A Natural Policy Gradient [2001]\n\n<img src=\"https://www.dropbox.com/s/it82tfhfmhg9uwp/Screenshot%202018-06-10%2010.58.52.png?dl=1\">\n\n- 논문 저자: Sham Kakade\n- 논문 링크: [https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf](https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf)\n- 함께 보면 좋을 논문: \n\t- [Policy Gradient Methods for\nReinforcement Learning with Function\nApproximation (2000)](hhttps://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf)\n\t- [Natural Gradient Works Efficiently in Learning(1998)](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.452.7280&rep=rep1&type=pdf)\n- 논문을 보는 이유: TRPO와 NPG는 관련이 많기 때문에 TRPO를 더 잘 이해하기 위해 봄\n\n## 1. Abstract\n---\n\n- natural gradient method를 policy gradient에 적용\n- natural gradient는 steepest descent direction을 가짐\n- gradient descent는 parameter를 한 번에 많이 update 할 수 없는 반면, natural gradient는 가장 좋은 action을 고르도록 학습이 됌 (sutton 논문에서와 같이 compatible value function을 사용할 경우 policy iteration에서 policy improvement 1 step의 과정에서)\n- simple MDP와 tetris MDP에서 테스트함. 성능이 많이 향상\n\n## 2. Personal Interpretation and Thinking\n---\n\n(개인생각) 뉴럴넷을 사용할 경우 gradient가 steepest direction이 아닌 경우가 많다. 뉴럴넷의 parameter space가 우리가 보통 생각하는 직선으로 쭉쭉 뻗어있는 Euclidean space가 아니다. 좀 더 일반적으로는 구의 표면과 같이 휘어져있는 공간 즉, 리만 공간(Riemannian space)로 표현할 수 있다. 이와 같은 공간에서는 natural gradient가 steepest direction이 된다는 연구가 이뤄지고 있었다. 강화학습의 policy gradient은 objective function의 gradient를 따라 policy를 업데이트한다. 이 때, policy는 parameterized 되는데 이 경우에도 gradient 대신에 natural gradient가 좋다는 것을 실험해보는 논문이다. \n\ngradient가 non-covariant 해서 생기는 문제는 간단히 말하자면 다음과 같다. policy가 parameterized된 상황에서는 같은 policy라도 다른 parameter를 가질 수 있다. 이 때, steepest direction은 두 경우에 같은 방향을 가리켜야하는데 non-covariant한 경우 그렇지 못하다. 이것은 결국 느린 학습으로 연결이 된다. \n\n논문에서 2차미분 방법론들과 짧게 비교를 한다. 하지만 2차미분을 이용한 다른 방법들과의 비교가 생각보다 없는 점이 아쉽다.(Hessian을 이용한다거나 conjugate gradient method를 이용한다거나). 실험을 통해 FIM이 hessian에 수렴안하는 거라던지 Hessian 방법론이 local maxima 부근에서 상당히 느리다던지의 결과를 보여줬었으면 좋았을 것 같다. \n\n또한 natural gradient 만으로 업데이트하면 policy의 improvement보장이 안될 수 있다. policy의 improvement를 보장하기 위해 line search도 써야하는데 line search를 어떻게 쓰는지에 대한 자세한 언급이 없다. 즉, 자세한 algorithm 설명이 없다.\n\nnatural policy gradient 논문은 natural gradient + policy gradient를 처음 적용했다는데 의의가 있다. 하지만 이 논문이 문제 삼은 gradient는 non-covariant하다라는 문제를 natural gradient를 통해 해결하지 못했다(Experiment를 통해 covariant gradient가 되지 못했다는 것이 보인다). NPG의 뒤를 잇는 논문이 \"covariant policy search\"와 \"natural actor-critic\"에서 covariant하지 못하다는 것을 해결하기 위해 Fisher Information Matrix를 sample 하나 하나에 대해서 구하는 것이 아니라 trajectory 전체에 대해서 구한다. \n\n또한 논문은 pg의 두 가지 세팅 중에 average-reward setting(infinite horizon)에서만 NPG를 다룬다. \"covariant policy search\" 논문에서는 average-reward setting과 start-state setting 모두에 대해서 npg를 적용한다. \n\nnatural gradient + policy gradient를 처음 제시했다는 것은 좋지만 npg 학습의 과정을 자세하게 설명하지 않았고 다른 2차 미분 방법들과 비교를 많이 하지 않은 점이 아쉬운 논문이다.\n\n\n## 3. Introduction\n---\n\n- direct policy gradient method는 future reward의 gradient를 따라 policy를 update함\n- 하지만 gradient descent는 non-covariant\n- 이 논문에서는 covarient gradient를 제시함 = natural gradient\n- natural gradient와 policy iteration의 연관성을 설명하겠음: natural policy gradient is moving toward choosing a greedy optimal action (이런 연결점은 아마도 step-size를 덜 신경쓰고 싶어서 그런게 아닌가 싶다)\n\n논문의 Introduction 부분에 다음 멘트가 있다. 이 글만 봐서는 이해가 안갔는데 Mackay 논문에 좀 더 자세히 나와있다.\n \n<img src=\"https://www.dropbox.com/s/41xhhr7lgfk24a1/Screenshot%202018-06-10%2011.45.18.png?dl=1\">\n\n[Mackay](http://www.inference.org.uk/mackay/ica.pdf)논문에서는 다음과 같이 언급하고 있다. Back-propagation을 사용할 경우에 learning rate를 dimension에 1/n로 사용하면 수렴한다는 것이 증명됐다. 하지만 너무 느리다. \n\n<img src=\"https://www.dropbox.com/s/us9ezc7vxgrkez6/Screenshot%202018-06-10%2011.47.21.png?dl=1\">\n\n## 4. A Natural Gradient\n---\n### 4.1 환경에 대한 설정\n이 논문에서 제시하는 학습 환경은 다음과 같다.\n\n- MDP: tuple $(S, s_0, A, R, P)$\n- $S$: a finite set of states\n- $s_0$: a start state\n- $A$: a finite set of actions\n- $R$: reward function $R: S \\times A -> [0, R_{max}]$\n- $\\pi(a;s, \\theta)$: stochastic policy parameterized by $\\theta$\n- 모든 정책 $\\pi$는 ergodic: stationary distribution $\\rho^{\\pi}$이 잘 정의되어있음\n- 이 논문에서는 sutton의 pg 논문의 두 세팅(start-state formulation, average-reward formulation) 중에 두 번째인 average-reward formulation을 가정 \n- performance or average reward: $\\eta(\\pi)=\\sum_{s,a}\\rho^{\\pi}(s)\\pi(a;s)R(s,a)$\n- state-action value: $Q^{\\pi}(s,a)=E_{\\pi}[\\sum_{t=0}^{\\infty}R(s_t, a_t)-\\eta(\\pi)\\vert s_0=s, a_0=a]$\n- 정책이 $\\theta$로 parameterize되어있으므로 performance는 $\\eta(\\pi_{\\theta})$인데 $\\eta(\\theta)$로 쓸거임\n\n### 4.2 Natural Gradient\n#### 4.2.1 Policy gradient Theorem\n서튼 pg 논문의 policy gradient theorem에 따라 exact gradient of the average reward는 다음과 같다. 다음 수식이 어떻게 유도되었는지, 어떤 의미인지 모른다면 서튼 pg 논문을 통해 제대로 이해하는 것이 좋다.\n\n$$\\nabla\\eta(\\theta)=\\sum_{s,a}\\rho^{\\pi}(s)\\nabla\\pi(a;s,\\theta)Q^{\\pi}(s,a)$$\n\nsteepest descent direction of $\\eta(\\theta)$는 $\\eta(\\theta + d\\theta)$를 최소화하는 $d\\theta$로 정의된다. 이 때, $\\vert d\\theta \\vert^2$가 일정 크기 이하인 것으로 제약조건을 준다(held to small constant). Euclidian space에서는 $\\eta(\\theta)$가 steepest direction이지만 Riemannian space에서는 natural gradient가 steepest direction이다. \n\n#### 4.2.2 Natural gradient 증명\nRiemannian space에서 거리는 다음과 같이 정의된다. $G(\\theta)$는 특정한 양수로 이루어진 matrix이다.\n\n$$\\vert d\\theta \\vert^2=\\sum_{ij}(\\theta)d\\theta_id\\theta_i=d\\theta^TG(\\theta)d\\theta$$\n\n이 수식은 Natural Gradient Works Efficiently in Learning 논문에서 증명되어있다. 다음은 natural gradient 증명이다. \n\nsteepest direction을 구할 때 $\\theta$의 크기를 제약조건을 준다. 제약조건은 다음과 같다. \n\n$$\\vert d\\theta \\vert^2 = \\epsilon^2$$\n\n그리고 steepest vector인 $d\\theta$는 다음과 같이 정의할 수 있다. \n\n$$d\\theta = \\epsilon a$$\n\n$$\\vert a \\vert^2=a^TG(\\theta)a = 1$$\n\n이 때, $a$가 steepest direction unit vector이 되려면 다음 수식을 최소로 만들어야 한다. (이 수식은 잘 모르겠지만 $\\theta$에서의 1차근사를 가정하는게 아닌가 싶다.\n\n$$\\eta(\\theta + d\\theta) = \\eta(\\theta) + \\epsilon\\nabla\\eta(\\theta)^Ta$$\n\n위 수식이 제약조건 아래 최소가 되는 $a$를 구하기 위해 Lagrangian method를 사용한다. Lagrangian method를 모른다면 [위키피디아](https://en.wikipedia.org/wiki/Lagrange_multiplier)를 참고하는 것을 추천한다. 위 수식이 최소라는 것은 $\\nabla\\eta(\\theta)^Ta$가 최소라는 것이다. \n\n$$\\frac{\\partial}{\\partial a_i}(\\nabla\\eta(\\theta)^Ta - \\lambda a^TG(\\theta)a)=0$$\n\n따라서 $(\\nabla\\eta(\\theta)^Ta - \\lambda a^TG(\\theta)a)=0$는 상수이다. 상수를 미분하면 0이므로 이 식을 $a$로 미분한다. 그러면 다음과 같다. steepest direction을 구한 것이다.\n\n$$\\nabla\\eta(\\theta) = 2 \\lambda G(\\theta)a$$\n\n$$a=\\frac{1}{2\\lambda}G^{-1}\\nabla\\eta(\\theta)$$\n\n이 때, 다음 식을 natural gradient라고 정의한다.\n\n$$\\bar{\\nabla}\\eta(\\theta) = G^{-1}\\nabla\\eta(\\theta)$$\n\nnatural gradient를 이용한 업데이트는 다음과 같다. \n\n$$\\theta_{t+1}=\\theta_t - \\alpha_tG^{-1}\\nabla\\eta(\\theta)$$\n\n여기까지는 natural gradient의 증명이었다. 이 natural gradient를 policy gradient에 적용한 것이 natural policy gradient이다. natural policy gradient는 다음과 같이 정의된다.\n\n$$\\bar{\\nabla}\\eta(\\theta) = F^{-1}\\nabla\\eta(\\theta)$$\n\n$G$ 대신 $F$를 사용했는데 $F$는 Fisher information matix이다. 수식은 다음과 같다.\n\n$$F(\\theta) = E_{\\rho^\\pi(s)}[F_s(\\theta)]$$\n\n$$F_s(\\theta)=E_{\\pi(a;s,\\theta)}[\\frac{\\partial log \\pi(a;s, \\theta)}{\\partial \\theta_i}\\frac{\\partial log \\pi(a;s, \\theta)}{\\partial\\theta_j}]$$\n\n왜 G가 F가 되는지는 아직 잘 모르겠다. 거리라는 개념을 표현하려면 \n\n## 5. The Natural Gradient and Policy Iteration\n---\n### 5.1 Theorem 1\nsutton pg 논문에 따라 $Q^{\\pi}(s,a)$를 approximation한다. approximate하는 함수 $f^{\\pi}(s,a;w)$는 다음과 같다.(compatible value function)\n\n$$f^{\\pi}(s,a;w)=w^T\\psi^{\\pi}(s,a)$$\n\n$$\\psi^{\\pi}(s,a) = \\nabla log\\pi(a;s,\\theta)$$\n\n$w$는 원래 approximate하는 함수 $Q$와 $f$의 차이를 줄이도록 학습한다(mean square error). 수렴한 local minima의 $w$를 $\\bar{w}$라고 하겠다. 에러는 다음과 같은 수식으로 나타낸다. \n\n$$\\epsilon(w,\\pi)\\equiv\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)(f^{\\pi}(s,a;w)-Q^{\\pi}(s,a))^2$$\n\n위 식이 local minima이면 미분값이 0이다. $w$에 대해서 미분하면 다음과 같다. \n\n$$\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)(\\psi^{\\pi}(s,a)^T\\bar{w}-Q^{\\pi}(s,a))=0$$\n\n$$(\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)\\psi^{\\pi}(s,a)^T)\\bar{w}=\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)Q^{\\pi}(s,a))$$\n\n이 때, 위 식의 우변은 $\\psi$의 정의에 의해 policy gradient가 된다. 또한 왼쪽 항에서는 Fisher information matrix가 나온다.\n\n$$F(\\theta)=\\sum_{s,a}\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)\\psi^{\\pi}(s,a)=E_{\\rho^\\pi(s)}[F_s(\\theta)]$$\n\n따라서 다음과 같다.\n\n$$F(\\theta)\\bar{w}=\\nabla\\eta(\\theta)$$\n\n$$\\bar{w}=F(\\theta)^{-1}\\nabla\\eta(\\theta)$$\n\n이 식은 natural gradient 식과 동일하다. 이 식은 policy가 update 될 때, value function approximator의 parameter 방향으로 이동한다는 것을 의미한다. function approximation이 정확하다면 그 parameter의 natural policy gradient와 inner product가 커야한다. \n\n### 5.2 Theorem 2: Greedy Polict Improvement\nnatural policy gradient가 단순히 더 좋은 행동을 고르도록 학습하는게 아니라 가장 좋은 (greedy) 행동을 고르도록 학습한다는 것을 증명하는 파트이다. 이것을 일반적인 형태의 policy에 대해서 증명하기 전에 exponential 형태의 policy에 대해서 증명하는 것이 Theorem 2이다.\n\npolicy를 다음과 같이 정의한다.\n\n$$\\pi(a;s,\\theta) \\propto exp(\\theta^T\\phi_{sa})$$\n\n$\\bar{\\nabla}\\eta(\\theta)$가 0이 아니고 $\\bar{w}$는 approximation error를 최소화한 $w$라고 가정한다. 이 상태에서 natural gradient update를 생각해보자. policy gradient는 gradient ascent임을 기억하자.\n\n$$\\theta_{t+1}=\\theta_t + \\alpha_t\\bar{\\nabla}\\eta(\\theta)$$\n\n이 때 $\\alpha$가 learning rate로 parameter를 얼마나 업데이트하는지를 결정한다. 이 값을 무한대로 늘렸을 때 policy가 어떻게 업데이트되는지 생각해보자. \n\n$$\\pi_{\\infty}(a;s)=lim_{\\alpha\\rightarrow\\infty}\\pi(a;s,\\theta+\\alpha\\bar{\\nabla}\\eta(\\theta))-(1)$$\n\nfunction approximator는 다음과 같다. \n\n$$f^{\\pi}(s,a;w)=w^T\\psi^{\\pi}(s,a)$$\n\nTheorem 1에 의해 위 식은 아래와 같이 쓸 수 있다.\n\n\n$$f^{\\pi}(s,a;w)=\\bar{\\nabla}\\eta(\\theta)^T\\psi^{\\pi}(s,a)$$\n\n$\\theta$의 정의에 의해 $\\psi$는 다음과 같다.\n\n$$\\psi^{\\pi}(s,a)=\\phi_{sa}-E_{\\pi(a';s,\\theta)}[\\phi_{sa'}]$$\n\nfunction approximator는 다음과 같이 다시 쓸 수 있다.\n\n$$f^{\\pi}(s,a;w)=\\bar{\\nabla}\\eta(\\theta)^T(\\phi_{sa}-E_{\\pi(a';s,\\theta)}[\\phi_{sa'}])$$\n\ngreedy policy improvement가 Q function 값 중 가장 큰 값을 가지는 action을 선택하듯이 여기서도 function approximator의 값이 가장 큰 action을 선택하는 상황을 가정해본다. 이 때 function approximator의 argmax는 다음과 같이 쓸 수 있다.\n\n$$argmax_{a'}f^{\\pi}(s,a)=argmax_{a'}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa'}$$\n\n(1) 식을 다시 살펴보자. policy의 정의에 따라 다음과 같이 쓸 수 있다. \n\n$$\\pi(a;s,\\theta + \\alpha\\bar{\\nabla}\\eta(\\theta)) \\propto exp(\\theta^T\\phi_{sa} + \\alpha\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa})$$\n\n$\\bar{\\nabla}\\eta(\\theta) \\neq 0$이고 $\\alpha\\rightarrow\\infty$이면 exp안의 항 중에서 뒤의 항이 dominate하게 된다. 여러 행동 중에 $\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa}$가 가장 큰 행동이 있다면 이 행동의 policy probability가 1이 되고 나머지는 0이 된다. 따라서 다음이 성립한다.\n\n$$\\pi_{\\infty}=0$$ \n\nif and only if \n\n$$a \\notin argmax_{a'}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa'}$$\n\n이 결과로부터 natural policy gradient는 단지 더 좋은 action이 아니라 best action을 고르도록 학습이 된다. 하지만 non-covariant gradient(1차미분) 에서는 그저 더 좋은 action을 고르도록 학습이 된다. 하지만 이 natural policy gradient에 대한 결과는 infinite learning rate 세팅에서만 성립함. 좀 더 일반적인 경우에 대해서 살펴보자.\n\n#### 4.3 Theorem 3 \nTheorem 2에서와는 달리 일반적인 policy를 가정하자(general parameterized policy). Theorem 3는 이 상황에서 natural gradient를 통한 업데이트가 best action를 고르는 방향으로 학습이 된다는 것을 보여준다. \n\nnatural gradien에 따른 policy parameter의 업데이트는 다음과 같다. $\\bar{w}$는 approximation error를 minimize하는 $w$이다.\n\n$$\\delta\\theta = \\theta' - \\theta = \\alpha\\bar{\\nabla}\\eta(\\theta)=\\alpha\\bar{w}$$\n\npolicy에 대해서 1차근사를 하면 다음과 같다. \n\n$$\\pi(a;s,\\theta')=\\pi(a;s,\\theta)+\\frac{\\partial\\pi(a;s,\\theta)^T}{\\partial\\theta}\\delta\\theta + O(\\delta\\theta^2)$$\n\n$$=\\pi(a;s,\\theta)(1+\\psi(s,a)^T\\delta\\theta) + O(\\delta\\theta^2)$$\n\n$$=\\pi(a;s,\\theta)(1+\\alpha\\psi(s,a)^T\\bar{w}) + O(\\delta\\theta^2)$$\n\n$$=\\pi(a;s,\\theta)(1+\\alpha f^{\\pi}(s,a;\\bar{w}) + O(\\delta\\theta^2)$$\n\npolicy 자체가 function approximator의 크기대로 업데이트가 되므로 local하게 best action의 probability는 커지고 다른 probability의 크기는 작아질 것이다. 하지만 만약 greedy improvement가 된다하더라도 그게 performance의 improvement를 보장하는 것은 아니다. 하지만 line search와 함께 사용할 경우 improvement를 보장할 수 있다. \n\n## 6. Metrics and Curvatures\n---\n다음 식에 해당하는 G는 Fisher Information Matrix만 사용할 수 있는 것이 아니다.\n\n$$\\vert d\\theta \\vert^2=\\sum_{ij}(\\theta)d\\theta_id\\theta_i=d\\theta^TG(\\theta)d\\theta$$\n\n이 파트에서는 FIM과 다른 metric 사이의 관계를 다룬다. \n\n- In the different setting of parameter estimation, the Fisher information converges to the ```Hessian```, so it is [asymptotically efficient](https://en.wikipedia.org/wiki/Efficiency_(statistics)#Asymptotic_efficiency)\n- 이 논문의 경우, 아마리 논문의 'blind separation case'와 유사한데 이 때는 꼭 asymtotically efficient하지 않다. 이 말은 즉 2nd order 수렴이 보장되지 않는다는 것이다.\n- [Mackay](http://www.inference.org.uk/mackay/ica.pdf) 논문에서 hessian에서 data independant한 term을 metric으로 가져오는 방법을 제안했다. 그래서 performance를 2번 미분해보면 다음과 같다. 하지만 다음 식에서는 모든 항이 data dependent하다(Q가 있으니까). 첫 번째 항이 그나마 FIM과의 관련성이 있을 수 있지만 Q 값이 curvature에 weight를 주는 방식 때문에 다르다고 할 수 있다.\n\n$$\n\\nabla^2\\eta(\\theta)=\\sum_{sa}\\rho^{\\pi}(s)(\\nabla^2\\pi(a;s)Q^{\\pi}(s,a)+\\nabla\\pi(a;s)\\nabla Q^{\\pi}(s,a)^T+\\nabla Q^{\\pi}(s,a)\\nabla\\pi(a;s)^T)\n$$\n\n\n- hessian은 보통 positive definite가 아닐수도 있다. 따라서 local maxima가 될 때까지 Hessian이 사용하기 별로 안좋다. 그리고 local maxima에서는 Hessian보다는 Conjugate methods가 더 효율적이다. \n\n이 파트에서는 무엇을 말하고 있는지 알기가 어렵다. FIM과 Hessian이 관련이 있다는 것을 알겠다. 하지만 asymtotically efficient와 같은 내용을 모르므로 내용의 이해가 어려웠다.\n\nMackay 논문에서 해당 부분은 다음과 같다. \n\n<img src=\"https://www.dropbox.com/s/x4n6z6pdyi7xtb9/Screenshot%202018-06-10%2012.04.13.png?dl=1\">\n\n## 7. Experiment\n---\n논문에서는 natural gradient를 simple MDP와 tetris MDP에 대해서 테스트했다. practice에서는 Fisher information matrix는 다음과 같은 식으로 업데이트한다.\n\n$$f\\leftarrow f+\\nabla log \\pi(a_t; s_t, \\theta)\\nabla log \\pi(a_t; s_t, \\theta)^T$$\n\nT length trajectory에 대해서 f/T를 통해 F의 estimate를 구한다.\n\n### 7.1 Linear Quadratic regulator\n에이전트를 테스트할 환경은 다음과 같은 dynamics를 가지고 있다. $u(t)$는 control signal로서 에이전트의 행동이라고 생각하면 된다. $\\epsilon$은 noise distribution으로 환경에 가해지는 노이즈이다. 에이전트의 목표는 적절한 $u(t)$를 통해 \nx(t)를 0으로 유지하는 것이다. 제어분야에서의 LQR controller 문제이다.\n\n$$\nx(t+1) = 0.7x(t)+u(t)+\\epsilon(t)\n$$\n\nx(t)를 0으로 유지하기 위해서 $x(t)^2$를 cost로 잡고 이 cost를 최소화하도록 학습한다. 이 시스템을 linear라고 부르는 것은 아래 그림과 같이 선형의 형태를 띄기 때문이다. 이 논문에서 실험할 때는 이 그림에서의 system에 noise를 더해준 것이다. [그림 출처](https://stanford.edu/class/ee363/lectures/dlqr.pdf)\n\n<img src='https://www.dropbox.com/s/vz0q97lcek4oti5/Screenshot%202018-06-08%2014.21.10.png?dl=1'>\n\n이 실험에서 사용한 parameterized policy는 다음과 같다. parameter가 $\\theta_1$과 $\\theta_2$ 밖에 없는 상당히 간단한 policy이다. \n\n$$\n\\pi(u;x,\\theta) \\propto exp(\\theta_1 s_1 x^2 + \\theta_2 s_2 x)\n$$\n\n이 policy를 간단히 numpy와 matplotlib를 이용해서 그려봤다. $\\theta_1$과 $theta_2$를 (0.5, 0.5), (1, 0), (0, 1)로 하고 $s_1$과 $s_2$는 1로 두었다. x는 -1에서 1까지의 범위로 그렸다. x를 0으로 유지하려면 u(t)가 -와 +가 둘 다 가능해야할 것 같은데 위 식으로만 봐서는 action이 하나이고 그 action일 확률을 표시하는 것처럼 나왔다. 아마 -1과 +1이 u(t)가 될 수 있는데 그 중 +1을 선택할 확률이 위와 같이 되는게 아닌가 싶다.\n<center><img src='https://www.dropbox.com/s/v69qyrwn7zurk8c/Screenshot%202018-06-08%2014.57.07.png?dl=1' width='500px'></center>\n\n다음 그림은 1-d LQR을 학습한 그래프이다. cost가 $x^2$이기 때문에 cost가 0으로 갈수록 agent는 0에서 안정적으로 머무른다고 볼 수 있다. 6개의 선 중에서 오른쪽 세 개가 일반적인 gradient 방법을 사용해서 학습한 결과이다. 그리고 왼쪽의 세 개의 선이 natural policy gradient를 통해 학습한 학습 곡선이다. 일반 gradient 방법보다 natural gradient가 훨씬 빠르게 학습한다(time 축이 log scale인 것을 감안하자).\n\n하지만 문제가 있다. npg를 학습한 세 개의 곡선은 $\\theta$를 rescale 한 것이다. $\\theta$앞에 곱해지는 숫자에 따라 학습의 과정이 다르다. 이 것은 coordinate에 따라 steepest gradient가 다르게 측정된다는 것이다. 즉, covariant gradient가 아니라는 뜻이다. 이 논문에서는 natural gradient를 통해 gradient가 covariant하도록 만들고 싶었는데 실패한 것이다. \n\n\n<center><img src=\"https://www.dropbox.com/s/fhn8cgje0rdws0i/Screenshot%202018-06-08%2023.13.37.png?dl=1\" width=\"300px\"></center>\n\nnatural gradient가 covariant하지 않은 이유는 Fisher Information Matrix가 예상했던 바와는 달리 invariant metric이 아니기 때문이다. 또한 FIM이 invariant metric이 아닌 이유는 FIM을 계산할 때 $\\rho_s$가 곱해지기 때문이다(state distribution에 대한 expectation. $\\rho_s$가 곱해지는 것이 invariant에 미치는 영향이 무엇인지는 모르겠다). 하지만 여전히 의의가 있는 것은 기존 gradient 방법들보다 훨씬 빠르게 학습한다는 것이다.\n\n### 7.2 simple 2-state MDP\n이제 다른 예제에서 NPG를 테스트한다. 2개의 state만 가지는 MDP를 고려해보자. [그림출처](http://repository.cmu.edu/cgi/viewcontent.cgi?article=1080&context=robotics). 그림으로보면 다음과 같다. x=0 상태와 x=1 상태 두 개가 존재한다. 에이전트는 각 상태에서 다시 자신의 상태로 되돌아오는 행동을 하거나 다른 상태로 가는 행동을 할 수 있다. 상태 x=0에서 다시 자기 자신으로 돌아오면 1의 보상을 받고 상태 x=1에서 자기 자신으로 돌아오면 2의 보상을 받는다. 따라서 결국 optimal policy는 상태 x=1에서 계속 자기 자신으로 돌아오는 행동을 취하는 것이다. \n\n<img src=\"https://www.dropbox.com/s/g1x9yknzsrip59i/Screenshot%202018-06-08%2023.06.50.png?dl=1\">\n\n문제를 좀 어렵게 만들기 위해 state distribution을 다음과 같이 설정한다. 즉, 대부분의 경우에 상태 x=0에서 에이전트가 시작하는 것이다. \n\n$$\n\\rho(x=0)=0.8,  \\rho(x=1)=0.2\n$$\n\n일반적인 gradient 방법을 사용하면 다음과 같은 policy gradient 식에 따라서 업데이트를 하게 된다. 이 때, $\\rho(s)$가 gradient에 곱해지므로 상태적으로 상태 0에서의 gradient 값이 커지게 된다. 따라서 에이전트는 상태 0에서의 gradient(상태 0에서 스스로에게 돌아오는 행동을 취하도록 정책을 업데이트하는 gradient)를 따라 parameterized policy를 update한다. 따라서 아래 그림의 첫번째 그림에서처럼 Reward가 1에서 오랜 시간동안 머무른다. 즉, 에이전트가 상태 0에서 self-loop를 계속 돌고 있다는 것이다. $\\rho(x=0)$가 $10^{-7}$까지 떨어진다.\n\n$$\\nabla\\eta(\\theta)=\\sum_{s,a}\\rho^{\\pi}(s)\\nabla\\pi(a;s,\\theta)Q^{\\pi}(s,a)$$\n\n<center><img src=\"https://www.dropbox.com/s/xtb77mfazbppnss/Screenshot%202018-06-08%2023.14.24.png?dl=1\" width=\"300px\"></center>\n\n하지만 NPG를 사용할 경우에는 훨씬 빠르게 average reward가 2에 도달한다. gradient 방법이 $1.7X10^(7)$정도의 시간만에 2에 도달한 반면 NPG는 2만에 도달한다. 또한 $\\rho(x=0)$가 $10^{-5}$이하로 떨어지지 않는다.\n\n한 가지 그래프를 더 살펴보자. 다음 그래프는 parameter $\\theta$가 업데이트 되는 과정을 보여준다. 이 그래프에서는 parameter가 2개 있는 것이다. 일반적인 gradient가 아래 그래프에서 실선에 해당한다. 이 실선의 그래프는 보면 처음부터 중반까지 $\\theta_i$만 거의 업데이트하는 것을 볼 수 있다. 그에 비해 NPG는 두 개의 parameter를 균등하게 업데이트하는 것을 볼 수 있다. \n\n<center><img src=\"https://www.dropbox.com/s/g7pazozw2k6rd7x/Screenshot%202018-06-08%2023.23.25.png?dl=1\" width=\"300px\"></center>\n\npolicy가 $\\pi(a;s,\\theta)\\propto exp(\\theta_{sa})$일 때, 다음과 같이 $F_{-1}$이 gradient 앞에 weight로 곱해지는데 이게 $\\rho$와는 달리 각 parameter에 대해 균등하다. 따라서 위 그래프에서와 같이 각 parameter는 비슷한 비율로 업데이트가 되는 것이다.\n\n$$\\bar{\\nabla}\\eta(\\theta) = F^{-1}\\nabla\\eta(\\theta)$$\n\n### 7.3 Tetris\nNPG를 테스트할 tetris 예제는 Neuro Dynamic Programming 책에 소개되어있다. 다음 그림은 tetris 예제를 보여준다. 보통 그림에서와 같이 state의 feature를 정해준다. [그림 출처](http://slideplayer.com/slide/5215520/)\n\n<img src=\"https://www.dropbox.com/s/y1halso9yermy8s/Screenshot%202018-06-08%2023.44.34.png?dl=1\">\n\n이 예제에서도 exponantial family로 policy를 표현한다. $\\pi(a;s,\\theta) \\propto exp(\\theta^T\\phi_{sa})$ 로 표현한다.\n\ntetris는 linear function approximator와 greedy policy iteration을 사용할 경우 performance가 갑자기 떨어지는 현상이 있다. 밑의 그림에서 A의 spike가 있는 그래프가 이 경우이다. 그 밑에 낮게 누워있는 그래프는 일반적인 policy gradient 방법이다. 하지만 Natural policy gradient를 사용할 경우 B 그림에서 오른쪽 그래프와 같이 성능개선이 뚜렷하다. Policy Iteration 처럼 성능이 뚝 떨어지지 않고 안정적으로 유지한다. 또한 그림 C에서 보는 것처럼 오른쪽 그래프인 일반적인 gradient 방법보다 훨씬 빠르게 학습하는 것을 볼 수 있다.\n\n<img src=\"https://www.dropbox.com/s/pr6s2qrqaic0wyj/Screenshot%202018-06-08%2023.40.16.png?dl=1\">\n\n\n## 8. Discussion\n---\n\n- natural gradient method는 policy iteration에서와 같이 greedy action을 선택하도록 학습됌\n- line search와 함께 쓰면 natural gradient method는 더 policy iteration 같아짐\n- greedy policy iteration에서와는 달리 performance improvement가 보장됌\n- 하지만 F(Fisher information matrix)가 asymtotically Hessian으로 수렴하지 않음. asymtotically conjugate gradient method(Hessian의 inverse를 approx.로 구하는 방법)가 더 좋아 보일 수 있음\n- 하지만 Hessian이 항상 informative하지 않고(hessian이 어떤 정보를 주려면 positive definite와 같은 성질을 가져서 해당 함수가 convex인 것을 알 수 있다든지의 경우를 이야기하는데 hessian이 항상 positive definite가 아닐 수 있다는 것이다) tetris에서 봤듯이 natural gradient method가 더 효율적일 수 있음(pushing the policy toward choosing greedy optimal actions)\n- conjugate gradient method가 좀 더 maximum에 빠르게 수렴하지만, performance는 maximum에서 거의 안변하므로 좋다고 말하기 어려움(?). 이 부분에 대해서 추가적인 연구 필요.\n\namet, consectetur adipisicing elit. Vitae ipsum, voluptatem quis officiis inventore dolor totam deserunt, possimus similique eum, accusantium adipisci doloremque omnis excepturi quasi, suscipit repellendus quibusdam? Veritatis.","source":"_posts/npg.md","raw":"---\ntitle: A Natural Policy Gradient\ndate: 2018-06-14 13:18:45\ntags: [\"프로젝트\", \"피지여행\"]\ncategories: 프로젝트\nauthor: 이웅원\nsubtitle: 피지여행 4번째 논문\n---\n\n# A Natural Policy Gradient [2001]\n\n<img src=\"https://www.dropbox.com/s/it82tfhfmhg9uwp/Screenshot%202018-06-10%2010.58.52.png?dl=1\">\n\n- 논문 저자: Sham Kakade\n- 논문 링크: [https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf](https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf)\n- 함께 보면 좋을 논문: \n\t- [Policy Gradient Methods for\nReinforcement Learning with Function\nApproximation (2000)](hhttps://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf)\n\t- [Natural Gradient Works Efficiently in Learning(1998)](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.452.7280&rep=rep1&type=pdf)\n- 논문을 보는 이유: TRPO와 NPG는 관련이 많기 때문에 TRPO를 더 잘 이해하기 위해 봄\n\n## 1. Abstract\n---\n\n- natural gradient method를 policy gradient에 적용\n- natural gradient는 steepest descent direction을 가짐\n- gradient descent는 parameter를 한 번에 많이 update 할 수 없는 반면, natural gradient는 가장 좋은 action을 고르도록 학습이 됌 (sutton 논문에서와 같이 compatible value function을 사용할 경우 policy iteration에서 policy improvement 1 step의 과정에서)\n- simple MDP와 tetris MDP에서 테스트함. 성능이 많이 향상\n\n## 2. Personal Interpretation and Thinking\n---\n\n(개인생각) 뉴럴넷을 사용할 경우 gradient가 steepest direction이 아닌 경우가 많다. 뉴럴넷의 parameter space가 우리가 보통 생각하는 직선으로 쭉쭉 뻗어있는 Euclidean space가 아니다. 좀 더 일반적으로는 구의 표면과 같이 휘어져있는 공간 즉, 리만 공간(Riemannian space)로 표현할 수 있다. 이와 같은 공간에서는 natural gradient가 steepest direction이 된다는 연구가 이뤄지고 있었다. 강화학습의 policy gradient은 objective function의 gradient를 따라 policy를 업데이트한다. 이 때, policy는 parameterized 되는데 이 경우에도 gradient 대신에 natural gradient가 좋다는 것을 실험해보는 논문이다. \n\ngradient가 non-covariant 해서 생기는 문제는 간단히 말하자면 다음과 같다. policy가 parameterized된 상황에서는 같은 policy라도 다른 parameter를 가질 수 있다. 이 때, steepest direction은 두 경우에 같은 방향을 가리켜야하는데 non-covariant한 경우 그렇지 못하다. 이것은 결국 느린 학습으로 연결이 된다. \n\n논문에서 2차미분 방법론들과 짧게 비교를 한다. 하지만 2차미분을 이용한 다른 방법들과의 비교가 생각보다 없는 점이 아쉽다.(Hessian을 이용한다거나 conjugate gradient method를 이용한다거나). 실험을 통해 FIM이 hessian에 수렴안하는 거라던지 Hessian 방법론이 local maxima 부근에서 상당히 느리다던지의 결과를 보여줬었으면 좋았을 것 같다. \n\n또한 natural gradient 만으로 업데이트하면 policy의 improvement보장이 안될 수 있다. policy의 improvement를 보장하기 위해 line search도 써야하는데 line search를 어떻게 쓰는지에 대한 자세한 언급이 없다. 즉, 자세한 algorithm 설명이 없다.\n\nnatural policy gradient 논문은 natural gradient + policy gradient를 처음 적용했다는데 의의가 있다. 하지만 이 논문이 문제 삼은 gradient는 non-covariant하다라는 문제를 natural gradient를 통해 해결하지 못했다(Experiment를 통해 covariant gradient가 되지 못했다는 것이 보인다). NPG의 뒤를 잇는 논문이 \"covariant policy search\"와 \"natural actor-critic\"에서 covariant하지 못하다는 것을 해결하기 위해 Fisher Information Matrix를 sample 하나 하나에 대해서 구하는 것이 아니라 trajectory 전체에 대해서 구한다. \n\n또한 논문은 pg의 두 가지 세팅 중에 average-reward setting(infinite horizon)에서만 NPG를 다룬다. \"covariant policy search\" 논문에서는 average-reward setting과 start-state setting 모두에 대해서 npg를 적용한다. \n\nnatural gradient + policy gradient를 처음 제시했다는 것은 좋지만 npg 학습의 과정을 자세하게 설명하지 않았고 다른 2차 미분 방법들과 비교를 많이 하지 않은 점이 아쉬운 논문이다.\n\n\n## 3. Introduction\n---\n\n- direct policy gradient method는 future reward의 gradient를 따라 policy를 update함\n- 하지만 gradient descent는 non-covariant\n- 이 논문에서는 covarient gradient를 제시함 = natural gradient\n- natural gradient와 policy iteration의 연관성을 설명하겠음: natural policy gradient is moving toward choosing a greedy optimal action (이런 연결점은 아마도 step-size를 덜 신경쓰고 싶어서 그런게 아닌가 싶다)\n\n논문의 Introduction 부분에 다음 멘트가 있다. 이 글만 봐서는 이해가 안갔는데 Mackay 논문에 좀 더 자세히 나와있다.\n \n<img src=\"https://www.dropbox.com/s/41xhhr7lgfk24a1/Screenshot%202018-06-10%2011.45.18.png?dl=1\">\n\n[Mackay](http://www.inference.org.uk/mackay/ica.pdf)논문에서는 다음과 같이 언급하고 있다. Back-propagation을 사용할 경우에 learning rate를 dimension에 1/n로 사용하면 수렴한다는 것이 증명됐다. 하지만 너무 느리다. \n\n<img src=\"https://www.dropbox.com/s/us9ezc7vxgrkez6/Screenshot%202018-06-10%2011.47.21.png?dl=1\">\n\n## 4. A Natural Gradient\n---\n### 4.1 환경에 대한 설정\n이 논문에서 제시하는 학습 환경은 다음과 같다.\n\n- MDP: tuple $(S, s_0, A, R, P)$\n- $S$: a finite set of states\n- $s_0$: a start state\n- $A$: a finite set of actions\n- $R$: reward function $R: S \\times A -> [0, R_{max}]$\n- $\\pi(a;s, \\theta)$: stochastic policy parameterized by $\\theta$\n- 모든 정책 $\\pi$는 ergodic: stationary distribution $\\rho^{\\pi}$이 잘 정의되어있음\n- 이 논문에서는 sutton의 pg 논문의 두 세팅(start-state formulation, average-reward formulation) 중에 두 번째인 average-reward formulation을 가정 \n- performance or average reward: $\\eta(\\pi)=\\sum_{s,a}\\rho^{\\pi}(s)\\pi(a;s)R(s,a)$\n- state-action value: $Q^{\\pi}(s,a)=E_{\\pi}[\\sum_{t=0}^{\\infty}R(s_t, a_t)-\\eta(\\pi)\\vert s_0=s, a_0=a]$\n- 정책이 $\\theta$로 parameterize되어있으므로 performance는 $\\eta(\\pi_{\\theta})$인데 $\\eta(\\theta)$로 쓸거임\n\n### 4.2 Natural Gradient\n#### 4.2.1 Policy gradient Theorem\n서튼 pg 논문의 policy gradient theorem에 따라 exact gradient of the average reward는 다음과 같다. 다음 수식이 어떻게 유도되었는지, 어떤 의미인지 모른다면 서튼 pg 논문을 통해 제대로 이해하는 것이 좋다.\n\n$$\\nabla\\eta(\\theta)=\\sum_{s,a}\\rho^{\\pi}(s)\\nabla\\pi(a;s,\\theta)Q^{\\pi}(s,a)$$\n\nsteepest descent direction of $\\eta(\\theta)$는 $\\eta(\\theta + d\\theta)$를 최소화하는 $d\\theta$로 정의된다. 이 때, $\\vert d\\theta \\vert^2$가 일정 크기 이하인 것으로 제약조건을 준다(held to small constant). Euclidian space에서는 $\\eta(\\theta)$가 steepest direction이지만 Riemannian space에서는 natural gradient가 steepest direction이다. \n\n#### 4.2.2 Natural gradient 증명\nRiemannian space에서 거리는 다음과 같이 정의된다. $G(\\theta)$는 특정한 양수로 이루어진 matrix이다.\n\n$$\\vert d\\theta \\vert^2=\\sum_{ij}(\\theta)d\\theta_id\\theta_i=d\\theta^TG(\\theta)d\\theta$$\n\n이 수식은 Natural Gradient Works Efficiently in Learning 논문에서 증명되어있다. 다음은 natural gradient 증명이다. \n\nsteepest direction을 구할 때 $\\theta$의 크기를 제약조건을 준다. 제약조건은 다음과 같다. \n\n$$\\vert d\\theta \\vert^2 = \\epsilon^2$$\n\n그리고 steepest vector인 $d\\theta$는 다음과 같이 정의할 수 있다. \n\n$$d\\theta = \\epsilon a$$\n\n$$\\vert a \\vert^2=a^TG(\\theta)a = 1$$\n\n이 때, $a$가 steepest direction unit vector이 되려면 다음 수식을 최소로 만들어야 한다. (이 수식은 잘 모르겠지만 $\\theta$에서의 1차근사를 가정하는게 아닌가 싶다.\n\n$$\\eta(\\theta + d\\theta) = \\eta(\\theta) + \\epsilon\\nabla\\eta(\\theta)^Ta$$\n\n위 수식이 제약조건 아래 최소가 되는 $a$를 구하기 위해 Lagrangian method를 사용한다. Lagrangian method를 모른다면 [위키피디아](https://en.wikipedia.org/wiki/Lagrange_multiplier)를 참고하는 것을 추천한다. 위 수식이 최소라는 것은 $\\nabla\\eta(\\theta)^Ta$가 최소라는 것이다. \n\n$$\\frac{\\partial}{\\partial a_i}(\\nabla\\eta(\\theta)^Ta - \\lambda a^TG(\\theta)a)=0$$\n\n따라서 $(\\nabla\\eta(\\theta)^Ta - \\lambda a^TG(\\theta)a)=0$는 상수이다. 상수를 미분하면 0이므로 이 식을 $a$로 미분한다. 그러면 다음과 같다. steepest direction을 구한 것이다.\n\n$$\\nabla\\eta(\\theta) = 2 \\lambda G(\\theta)a$$\n\n$$a=\\frac{1}{2\\lambda}G^{-1}\\nabla\\eta(\\theta)$$\n\n이 때, 다음 식을 natural gradient라고 정의한다.\n\n$$\\bar{\\nabla}\\eta(\\theta) = G^{-1}\\nabla\\eta(\\theta)$$\n\nnatural gradient를 이용한 업데이트는 다음과 같다. \n\n$$\\theta_{t+1}=\\theta_t - \\alpha_tG^{-1}\\nabla\\eta(\\theta)$$\n\n여기까지는 natural gradient의 증명이었다. 이 natural gradient를 policy gradient에 적용한 것이 natural policy gradient이다. natural policy gradient는 다음과 같이 정의된다.\n\n$$\\bar{\\nabla}\\eta(\\theta) = F^{-1}\\nabla\\eta(\\theta)$$\n\n$G$ 대신 $F$를 사용했는데 $F$는 Fisher information matix이다. 수식은 다음과 같다.\n\n$$F(\\theta) = E_{\\rho^\\pi(s)}[F_s(\\theta)]$$\n\n$$F_s(\\theta)=E_{\\pi(a;s,\\theta)}[\\frac{\\partial log \\pi(a;s, \\theta)}{\\partial \\theta_i}\\frac{\\partial log \\pi(a;s, \\theta)}{\\partial\\theta_j}]$$\n\n왜 G가 F가 되는지는 아직 잘 모르겠다. 거리라는 개념을 표현하려면 \n\n## 5. The Natural Gradient and Policy Iteration\n---\n### 5.1 Theorem 1\nsutton pg 논문에 따라 $Q^{\\pi}(s,a)$를 approximation한다. approximate하는 함수 $f^{\\pi}(s,a;w)$는 다음과 같다.(compatible value function)\n\n$$f^{\\pi}(s,a;w)=w^T\\psi^{\\pi}(s,a)$$\n\n$$\\psi^{\\pi}(s,a) = \\nabla log\\pi(a;s,\\theta)$$\n\n$w$는 원래 approximate하는 함수 $Q$와 $f$의 차이를 줄이도록 학습한다(mean square error). 수렴한 local minima의 $w$를 $\\bar{w}$라고 하겠다. 에러는 다음과 같은 수식으로 나타낸다. \n\n$$\\epsilon(w,\\pi)\\equiv\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)(f^{\\pi}(s,a;w)-Q^{\\pi}(s,a))^2$$\n\n위 식이 local minima이면 미분값이 0이다. $w$에 대해서 미분하면 다음과 같다. \n\n$$\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)(\\psi^{\\pi}(s,a)^T\\bar{w}-Q^{\\pi}(s,a))=0$$\n\n$$(\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)\\psi^{\\pi}(s,a)^T)\\bar{w}=\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)Q^{\\pi}(s,a))$$\n\n이 때, 위 식의 우변은 $\\psi$의 정의에 의해 policy gradient가 된다. 또한 왼쪽 항에서는 Fisher information matrix가 나온다.\n\n$$F(\\theta)=\\sum_{s,a}\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)\\psi^{\\pi}(s,a)=E_{\\rho^\\pi(s)}[F_s(\\theta)]$$\n\n따라서 다음과 같다.\n\n$$F(\\theta)\\bar{w}=\\nabla\\eta(\\theta)$$\n\n$$\\bar{w}=F(\\theta)^{-1}\\nabla\\eta(\\theta)$$\n\n이 식은 natural gradient 식과 동일하다. 이 식은 policy가 update 될 때, value function approximator의 parameter 방향으로 이동한다는 것을 의미한다. function approximation이 정확하다면 그 parameter의 natural policy gradient와 inner product가 커야한다. \n\n### 5.2 Theorem 2: Greedy Polict Improvement\nnatural policy gradient가 단순히 더 좋은 행동을 고르도록 학습하는게 아니라 가장 좋은 (greedy) 행동을 고르도록 학습한다는 것을 증명하는 파트이다. 이것을 일반적인 형태의 policy에 대해서 증명하기 전에 exponential 형태의 policy에 대해서 증명하는 것이 Theorem 2이다.\n\npolicy를 다음과 같이 정의한다.\n\n$$\\pi(a;s,\\theta) \\propto exp(\\theta^T\\phi_{sa})$$\n\n$\\bar{\\nabla}\\eta(\\theta)$가 0이 아니고 $\\bar{w}$는 approximation error를 최소화한 $w$라고 가정한다. 이 상태에서 natural gradient update를 생각해보자. policy gradient는 gradient ascent임을 기억하자.\n\n$$\\theta_{t+1}=\\theta_t + \\alpha_t\\bar{\\nabla}\\eta(\\theta)$$\n\n이 때 $\\alpha$가 learning rate로 parameter를 얼마나 업데이트하는지를 결정한다. 이 값을 무한대로 늘렸을 때 policy가 어떻게 업데이트되는지 생각해보자. \n\n$$\\pi_{\\infty}(a;s)=lim_{\\alpha\\rightarrow\\infty}\\pi(a;s,\\theta+\\alpha\\bar{\\nabla}\\eta(\\theta))-(1)$$\n\nfunction approximator는 다음과 같다. \n\n$$f^{\\pi}(s,a;w)=w^T\\psi^{\\pi}(s,a)$$\n\nTheorem 1에 의해 위 식은 아래와 같이 쓸 수 있다.\n\n\n$$f^{\\pi}(s,a;w)=\\bar{\\nabla}\\eta(\\theta)^T\\psi^{\\pi}(s,a)$$\n\n$\\theta$의 정의에 의해 $\\psi$는 다음과 같다.\n\n$$\\psi^{\\pi}(s,a)=\\phi_{sa}-E_{\\pi(a';s,\\theta)}[\\phi_{sa'}]$$\n\nfunction approximator는 다음과 같이 다시 쓸 수 있다.\n\n$$f^{\\pi}(s,a;w)=\\bar{\\nabla}\\eta(\\theta)^T(\\phi_{sa}-E_{\\pi(a';s,\\theta)}[\\phi_{sa'}])$$\n\ngreedy policy improvement가 Q function 값 중 가장 큰 값을 가지는 action을 선택하듯이 여기서도 function approximator의 값이 가장 큰 action을 선택하는 상황을 가정해본다. 이 때 function approximator의 argmax는 다음과 같이 쓸 수 있다.\n\n$$argmax_{a'}f^{\\pi}(s,a)=argmax_{a'}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa'}$$\n\n(1) 식을 다시 살펴보자. policy의 정의에 따라 다음과 같이 쓸 수 있다. \n\n$$\\pi(a;s,\\theta + \\alpha\\bar{\\nabla}\\eta(\\theta)) \\propto exp(\\theta^T\\phi_{sa} + \\alpha\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa})$$\n\n$\\bar{\\nabla}\\eta(\\theta) \\neq 0$이고 $\\alpha\\rightarrow\\infty$이면 exp안의 항 중에서 뒤의 항이 dominate하게 된다. 여러 행동 중에 $\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa}$가 가장 큰 행동이 있다면 이 행동의 policy probability가 1이 되고 나머지는 0이 된다. 따라서 다음이 성립한다.\n\n$$\\pi_{\\infty}=0$$ \n\nif and only if \n\n$$a \\notin argmax_{a'}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa'}$$\n\n이 결과로부터 natural policy gradient는 단지 더 좋은 action이 아니라 best action을 고르도록 학습이 된다. 하지만 non-covariant gradient(1차미분) 에서는 그저 더 좋은 action을 고르도록 학습이 된다. 하지만 이 natural policy gradient에 대한 결과는 infinite learning rate 세팅에서만 성립함. 좀 더 일반적인 경우에 대해서 살펴보자.\n\n#### 4.3 Theorem 3 \nTheorem 2에서와는 달리 일반적인 policy를 가정하자(general parameterized policy). Theorem 3는 이 상황에서 natural gradient를 통한 업데이트가 best action를 고르는 방향으로 학습이 된다는 것을 보여준다. \n\nnatural gradien에 따른 policy parameter의 업데이트는 다음과 같다. $\\bar{w}$는 approximation error를 minimize하는 $w$이다.\n\n$$\\delta\\theta = \\theta' - \\theta = \\alpha\\bar{\\nabla}\\eta(\\theta)=\\alpha\\bar{w}$$\n\npolicy에 대해서 1차근사를 하면 다음과 같다. \n\n$$\\pi(a;s,\\theta')=\\pi(a;s,\\theta)+\\frac{\\partial\\pi(a;s,\\theta)^T}{\\partial\\theta}\\delta\\theta + O(\\delta\\theta^2)$$\n\n$$=\\pi(a;s,\\theta)(1+\\psi(s,a)^T\\delta\\theta) + O(\\delta\\theta^2)$$\n\n$$=\\pi(a;s,\\theta)(1+\\alpha\\psi(s,a)^T\\bar{w}) + O(\\delta\\theta^2)$$\n\n$$=\\pi(a;s,\\theta)(1+\\alpha f^{\\pi}(s,a;\\bar{w}) + O(\\delta\\theta^2)$$\n\npolicy 자체가 function approximator의 크기대로 업데이트가 되므로 local하게 best action의 probability는 커지고 다른 probability의 크기는 작아질 것이다. 하지만 만약 greedy improvement가 된다하더라도 그게 performance의 improvement를 보장하는 것은 아니다. 하지만 line search와 함께 사용할 경우 improvement를 보장할 수 있다. \n\n## 6. Metrics and Curvatures\n---\n다음 식에 해당하는 G는 Fisher Information Matrix만 사용할 수 있는 것이 아니다.\n\n$$\\vert d\\theta \\vert^2=\\sum_{ij}(\\theta)d\\theta_id\\theta_i=d\\theta^TG(\\theta)d\\theta$$\n\n이 파트에서는 FIM과 다른 metric 사이의 관계를 다룬다. \n\n- In the different setting of parameter estimation, the Fisher information converges to the ```Hessian```, so it is [asymptotically efficient](https://en.wikipedia.org/wiki/Efficiency_(statistics)#Asymptotic_efficiency)\n- 이 논문의 경우, 아마리 논문의 'blind separation case'와 유사한데 이 때는 꼭 asymtotically efficient하지 않다. 이 말은 즉 2nd order 수렴이 보장되지 않는다는 것이다.\n- [Mackay](http://www.inference.org.uk/mackay/ica.pdf) 논문에서 hessian에서 data independant한 term을 metric으로 가져오는 방법을 제안했다. 그래서 performance를 2번 미분해보면 다음과 같다. 하지만 다음 식에서는 모든 항이 data dependent하다(Q가 있으니까). 첫 번째 항이 그나마 FIM과의 관련성이 있을 수 있지만 Q 값이 curvature에 weight를 주는 방식 때문에 다르다고 할 수 있다.\n\n$$\n\\nabla^2\\eta(\\theta)=\\sum_{sa}\\rho^{\\pi}(s)(\\nabla^2\\pi(a;s)Q^{\\pi}(s,a)+\\nabla\\pi(a;s)\\nabla Q^{\\pi}(s,a)^T+\\nabla Q^{\\pi}(s,a)\\nabla\\pi(a;s)^T)\n$$\n\n\n- hessian은 보통 positive definite가 아닐수도 있다. 따라서 local maxima가 될 때까지 Hessian이 사용하기 별로 안좋다. 그리고 local maxima에서는 Hessian보다는 Conjugate methods가 더 효율적이다. \n\n이 파트에서는 무엇을 말하고 있는지 알기가 어렵다. FIM과 Hessian이 관련이 있다는 것을 알겠다. 하지만 asymtotically efficient와 같은 내용을 모르므로 내용의 이해가 어려웠다.\n\nMackay 논문에서 해당 부분은 다음과 같다. \n\n<img src=\"https://www.dropbox.com/s/x4n6z6pdyi7xtb9/Screenshot%202018-06-10%2012.04.13.png?dl=1\">\n\n## 7. Experiment\n---\n논문에서는 natural gradient를 simple MDP와 tetris MDP에 대해서 테스트했다. practice에서는 Fisher information matrix는 다음과 같은 식으로 업데이트한다.\n\n$$f\\leftarrow f+\\nabla log \\pi(a_t; s_t, \\theta)\\nabla log \\pi(a_t; s_t, \\theta)^T$$\n\nT length trajectory에 대해서 f/T를 통해 F의 estimate를 구한다.\n\n### 7.1 Linear Quadratic regulator\n에이전트를 테스트할 환경은 다음과 같은 dynamics를 가지고 있다. $u(t)$는 control signal로서 에이전트의 행동이라고 생각하면 된다. $\\epsilon$은 noise distribution으로 환경에 가해지는 노이즈이다. 에이전트의 목표는 적절한 $u(t)$를 통해 \nx(t)를 0으로 유지하는 것이다. 제어분야에서의 LQR controller 문제이다.\n\n$$\nx(t+1) = 0.7x(t)+u(t)+\\epsilon(t)\n$$\n\nx(t)를 0으로 유지하기 위해서 $x(t)^2$를 cost로 잡고 이 cost를 최소화하도록 학습한다. 이 시스템을 linear라고 부르는 것은 아래 그림과 같이 선형의 형태를 띄기 때문이다. 이 논문에서 실험할 때는 이 그림에서의 system에 noise를 더해준 것이다. [그림 출처](https://stanford.edu/class/ee363/lectures/dlqr.pdf)\n\n<img src='https://www.dropbox.com/s/vz0q97lcek4oti5/Screenshot%202018-06-08%2014.21.10.png?dl=1'>\n\n이 실험에서 사용한 parameterized policy는 다음과 같다. parameter가 $\\theta_1$과 $\\theta_2$ 밖에 없는 상당히 간단한 policy이다. \n\n$$\n\\pi(u;x,\\theta) \\propto exp(\\theta_1 s_1 x^2 + \\theta_2 s_2 x)\n$$\n\n이 policy를 간단히 numpy와 matplotlib를 이용해서 그려봤다. $\\theta_1$과 $theta_2$를 (0.5, 0.5), (1, 0), (0, 1)로 하고 $s_1$과 $s_2$는 1로 두었다. x는 -1에서 1까지의 범위로 그렸다. x를 0으로 유지하려면 u(t)가 -와 +가 둘 다 가능해야할 것 같은데 위 식으로만 봐서는 action이 하나이고 그 action일 확률을 표시하는 것처럼 나왔다. 아마 -1과 +1이 u(t)가 될 수 있는데 그 중 +1을 선택할 확률이 위와 같이 되는게 아닌가 싶다.\n<center><img src='https://www.dropbox.com/s/v69qyrwn7zurk8c/Screenshot%202018-06-08%2014.57.07.png?dl=1' width='500px'></center>\n\n다음 그림은 1-d LQR을 학습한 그래프이다. cost가 $x^2$이기 때문에 cost가 0으로 갈수록 agent는 0에서 안정적으로 머무른다고 볼 수 있다. 6개의 선 중에서 오른쪽 세 개가 일반적인 gradient 방법을 사용해서 학습한 결과이다. 그리고 왼쪽의 세 개의 선이 natural policy gradient를 통해 학습한 학습 곡선이다. 일반 gradient 방법보다 natural gradient가 훨씬 빠르게 학습한다(time 축이 log scale인 것을 감안하자).\n\n하지만 문제가 있다. npg를 학습한 세 개의 곡선은 $\\theta$를 rescale 한 것이다. $\\theta$앞에 곱해지는 숫자에 따라 학습의 과정이 다르다. 이 것은 coordinate에 따라 steepest gradient가 다르게 측정된다는 것이다. 즉, covariant gradient가 아니라는 뜻이다. 이 논문에서는 natural gradient를 통해 gradient가 covariant하도록 만들고 싶었는데 실패한 것이다. \n\n\n<center><img src=\"https://www.dropbox.com/s/fhn8cgje0rdws0i/Screenshot%202018-06-08%2023.13.37.png?dl=1\" width=\"300px\"></center>\n\nnatural gradient가 covariant하지 않은 이유는 Fisher Information Matrix가 예상했던 바와는 달리 invariant metric이 아니기 때문이다. 또한 FIM이 invariant metric이 아닌 이유는 FIM을 계산할 때 $\\rho_s$가 곱해지기 때문이다(state distribution에 대한 expectation. $\\rho_s$가 곱해지는 것이 invariant에 미치는 영향이 무엇인지는 모르겠다). 하지만 여전히 의의가 있는 것은 기존 gradient 방법들보다 훨씬 빠르게 학습한다는 것이다.\n\n### 7.2 simple 2-state MDP\n이제 다른 예제에서 NPG를 테스트한다. 2개의 state만 가지는 MDP를 고려해보자. [그림출처](http://repository.cmu.edu/cgi/viewcontent.cgi?article=1080&context=robotics). 그림으로보면 다음과 같다. x=0 상태와 x=1 상태 두 개가 존재한다. 에이전트는 각 상태에서 다시 자신의 상태로 되돌아오는 행동을 하거나 다른 상태로 가는 행동을 할 수 있다. 상태 x=0에서 다시 자기 자신으로 돌아오면 1의 보상을 받고 상태 x=1에서 자기 자신으로 돌아오면 2의 보상을 받는다. 따라서 결국 optimal policy는 상태 x=1에서 계속 자기 자신으로 돌아오는 행동을 취하는 것이다. \n\n<img src=\"https://www.dropbox.com/s/g1x9yknzsrip59i/Screenshot%202018-06-08%2023.06.50.png?dl=1\">\n\n문제를 좀 어렵게 만들기 위해 state distribution을 다음과 같이 설정한다. 즉, 대부분의 경우에 상태 x=0에서 에이전트가 시작하는 것이다. \n\n$$\n\\rho(x=0)=0.8,  \\rho(x=1)=0.2\n$$\n\n일반적인 gradient 방법을 사용하면 다음과 같은 policy gradient 식에 따라서 업데이트를 하게 된다. 이 때, $\\rho(s)$가 gradient에 곱해지므로 상태적으로 상태 0에서의 gradient 값이 커지게 된다. 따라서 에이전트는 상태 0에서의 gradient(상태 0에서 스스로에게 돌아오는 행동을 취하도록 정책을 업데이트하는 gradient)를 따라 parameterized policy를 update한다. 따라서 아래 그림의 첫번째 그림에서처럼 Reward가 1에서 오랜 시간동안 머무른다. 즉, 에이전트가 상태 0에서 self-loop를 계속 돌고 있다는 것이다. $\\rho(x=0)$가 $10^{-7}$까지 떨어진다.\n\n$$\\nabla\\eta(\\theta)=\\sum_{s,a}\\rho^{\\pi}(s)\\nabla\\pi(a;s,\\theta)Q^{\\pi}(s,a)$$\n\n<center><img src=\"https://www.dropbox.com/s/xtb77mfazbppnss/Screenshot%202018-06-08%2023.14.24.png?dl=1\" width=\"300px\"></center>\n\n하지만 NPG를 사용할 경우에는 훨씬 빠르게 average reward가 2에 도달한다. gradient 방법이 $1.7X10^(7)$정도의 시간만에 2에 도달한 반면 NPG는 2만에 도달한다. 또한 $\\rho(x=0)$가 $10^{-5}$이하로 떨어지지 않는다.\n\n한 가지 그래프를 더 살펴보자. 다음 그래프는 parameter $\\theta$가 업데이트 되는 과정을 보여준다. 이 그래프에서는 parameter가 2개 있는 것이다. 일반적인 gradient가 아래 그래프에서 실선에 해당한다. 이 실선의 그래프는 보면 처음부터 중반까지 $\\theta_i$만 거의 업데이트하는 것을 볼 수 있다. 그에 비해 NPG는 두 개의 parameter를 균등하게 업데이트하는 것을 볼 수 있다. \n\n<center><img src=\"https://www.dropbox.com/s/g7pazozw2k6rd7x/Screenshot%202018-06-08%2023.23.25.png?dl=1\" width=\"300px\"></center>\n\npolicy가 $\\pi(a;s,\\theta)\\propto exp(\\theta_{sa})$일 때, 다음과 같이 $F_{-1}$이 gradient 앞에 weight로 곱해지는데 이게 $\\rho$와는 달리 각 parameter에 대해 균등하다. 따라서 위 그래프에서와 같이 각 parameter는 비슷한 비율로 업데이트가 되는 것이다.\n\n$$\\bar{\\nabla}\\eta(\\theta) = F^{-1}\\nabla\\eta(\\theta)$$\n\n### 7.3 Tetris\nNPG를 테스트할 tetris 예제는 Neuro Dynamic Programming 책에 소개되어있다. 다음 그림은 tetris 예제를 보여준다. 보통 그림에서와 같이 state의 feature를 정해준다. [그림 출처](http://slideplayer.com/slide/5215520/)\n\n<img src=\"https://www.dropbox.com/s/y1halso9yermy8s/Screenshot%202018-06-08%2023.44.34.png?dl=1\">\n\n이 예제에서도 exponantial family로 policy를 표현한다. $\\pi(a;s,\\theta) \\propto exp(\\theta^T\\phi_{sa})$ 로 표현한다.\n\ntetris는 linear function approximator와 greedy policy iteration을 사용할 경우 performance가 갑자기 떨어지는 현상이 있다. 밑의 그림에서 A의 spike가 있는 그래프가 이 경우이다. 그 밑에 낮게 누워있는 그래프는 일반적인 policy gradient 방법이다. 하지만 Natural policy gradient를 사용할 경우 B 그림에서 오른쪽 그래프와 같이 성능개선이 뚜렷하다. Policy Iteration 처럼 성능이 뚝 떨어지지 않고 안정적으로 유지한다. 또한 그림 C에서 보는 것처럼 오른쪽 그래프인 일반적인 gradient 방법보다 훨씬 빠르게 학습하는 것을 볼 수 있다.\n\n<img src=\"https://www.dropbox.com/s/pr6s2qrqaic0wyj/Screenshot%202018-06-08%2023.40.16.png?dl=1\">\n\n\n## 8. Discussion\n---\n\n- natural gradient method는 policy iteration에서와 같이 greedy action을 선택하도록 학습됌\n- line search와 함께 쓰면 natural gradient method는 더 policy iteration 같아짐\n- greedy policy iteration에서와는 달리 performance improvement가 보장됌\n- 하지만 F(Fisher information matrix)가 asymtotically Hessian으로 수렴하지 않음. asymtotically conjugate gradient method(Hessian의 inverse를 approx.로 구하는 방법)가 더 좋아 보일 수 있음\n- 하지만 Hessian이 항상 informative하지 않고(hessian이 어떤 정보를 주려면 positive definite와 같은 성질을 가져서 해당 함수가 convex인 것을 알 수 있다든지의 경우를 이야기하는데 hessian이 항상 positive definite가 아닐 수 있다는 것이다) tetris에서 봤듯이 natural gradient method가 더 효율적일 수 있음(pushing the policy toward choosing greedy optimal actions)\n- conjugate gradient method가 좀 더 maximum에 빠르게 수렴하지만, performance는 maximum에서 거의 안변하므로 좋다고 말하기 어려움(?). 이 부분에 대해서 추가적인 연구 필요.\n\namet, consectetur adipisicing elit. Vitae ipsum, voluptatem quis officiis inventore dolor totam deserunt, possimus similique eum, accusantium adipisci doloremque omnis excepturi quasi, suscipit repellendus quibusdam? Veritatis.","slug":"npg","published":1,"updated":"2018-07-11T06:27:58.252Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjjgzyg4q000cxo15a2yfnmeh","content":"<h1 id=\"A-Natural-Policy-Gradient-2001\"><a href=\"#A-Natural-Policy-Gradient-2001\" class=\"headerlink\" title=\"A Natural Policy Gradient [2001]\"></a>A Natural Policy Gradient [2001]</h1><p><img src=\"https://www.dropbox.com/s/it82tfhfmhg9uwp/Screenshot%202018-06-10%2010.58.52.png?dl=1\"></p>\n<ul>\n<li>논문 저자: Sham Kakade</li>\n<li>논문 링크: <a href=\"https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf\" target=\"_blank\" rel=\"noopener\">https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf</a></li>\n<li>함께 보면 좋을 논문: <ul>\n<li><a href=\"hhttps://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf\" target=\"_blank\" rel=\"noopener\">Policy Gradient Methods for<br>Reinforcement Learning with Function<br>Approximation (2000)</a></li>\n<li><a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.452.7280&amp;rep=rep1&amp;type=pdf\" target=\"_blank\" rel=\"noopener\">Natural Gradient Works Efficiently in Learning(1998)</a></li>\n</ul>\n</li>\n<li>논문을 보는 이유: TRPO와 NPG는 관련이 많기 때문에 TRPO를 더 잘 이해하기 위해 봄</li>\n</ul>\n<h2 id=\"1-Abstract\"><a href=\"#1-Abstract\" class=\"headerlink\" title=\"1. Abstract\"></a>1. Abstract</h2><hr>\n<ul>\n<li>natural gradient method를 policy gradient에 적용</li>\n<li>natural gradient는 steepest descent direction을 가짐</li>\n<li>gradient descent는 parameter를 한 번에 많이 update 할 수 없는 반면, natural gradient는 가장 좋은 action을 고르도록 학습이 됌 (sutton 논문에서와 같이 compatible value function을 사용할 경우 policy iteration에서 policy improvement 1 step의 과정에서)</li>\n<li>simple MDP와 tetris MDP에서 테스트함. 성능이 많이 향상</li>\n</ul>\n<h2 id=\"2-Personal-Interpretation-and-Thinking\"><a href=\"#2-Personal-Interpretation-and-Thinking\" class=\"headerlink\" title=\"2. Personal Interpretation and Thinking\"></a>2. Personal Interpretation and Thinking</h2><hr>\n<p>(개인생각) 뉴럴넷을 사용할 경우 gradient가 steepest direction이 아닌 경우가 많다. 뉴럴넷의 parameter space가 우리가 보통 생각하는 직선으로 쭉쭉 뻗어있는 Euclidean space가 아니다. 좀 더 일반적으로는 구의 표면과 같이 휘어져있는 공간 즉, 리만 공간(Riemannian space)로 표현할 수 있다. 이와 같은 공간에서는 natural gradient가 steepest direction이 된다는 연구가 이뤄지고 있었다. 강화학습의 policy gradient은 objective function의 gradient를 따라 policy를 업데이트한다. 이 때, policy는 parameterized 되는데 이 경우에도 gradient 대신에 natural gradient가 좋다는 것을 실험해보는 논문이다. </p>\n<p>gradient가 non-covariant 해서 생기는 문제는 간단히 말하자면 다음과 같다. policy가 parameterized된 상황에서는 같은 policy라도 다른 parameter를 가질 수 있다. 이 때, steepest direction은 두 경우에 같은 방향을 가리켜야하는데 non-covariant한 경우 그렇지 못하다. 이것은 결국 느린 학습으로 연결이 된다. </p>\n<p>논문에서 2차미분 방법론들과 짧게 비교를 한다. 하지만 2차미분을 이용한 다른 방법들과의 비교가 생각보다 없는 점이 아쉽다.(Hessian을 이용한다거나 conjugate gradient method를 이용한다거나). 실험을 통해 FIM이 hessian에 수렴안하는 거라던지 Hessian 방법론이 local maxima 부근에서 상당히 느리다던지의 결과를 보여줬었으면 좋았을 것 같다. </p>\n<p>또한 natural gradient 만으로 업데이트하면 policy의 improvement보장이 안될 수 있다. policy의 improvement를 보장하기 위해 line search도 써야하는데 line search를 어떻게 쓰는지에 대한 자세한 언급이 없다. 즉, 자세한 algorithm 설명이 없다.</p>\n<p>natural policy gradient 논문은 natural gradient + policy gradient를 처음 적용했다는데 의의가 있다. 하지만 이 논문이 문제 삼은 gradient는 non-covariant하다라는 문제를 natural gradient를 통해 해결하지 못했다(Experiment를 통해 covariant gradient가 되지 못했다는 것이 보인다). NPG의 뒤를 잇는 논문이 “covariant policy search”와 “natural actor-critic”에서 covariant하지 못하다는 것을 해결하기 위해 Fisher Information Matrix를 sample 하나 하나에 대해서 구하는 것이 아니라 trajectory 전체에 대해서 구한다. </p>\n<p>또한 논문은 pg의 두 가지 세팅 중에 average-reward setting(infinite horizon)에서만 NPG를 다룬다. “covariant policy search” 논문에서는 average-reward setting과 start-state setting 모두에 대해서 npg를 적용한다. </p>\n<p>natural gradient + policy gradient를 처음 제시했다는 것은 좋지만 npg 학습의 과정을 자세하게 설명하지 않았고 다른 2차 미분 방법들과 비교를 많이 하지 않은 점이 아쉬운 논문이다.</p>\n<h2 id=\"3-Introduction\"><a href=\"#3-Introduction\" class=\"headerlink\" title=\"3. Introduction\"></a>3. Introduction</h2><hr>\n<ul>\n<li>direct policy gradient method는 future reward의 gradient를 따라 policy를 update함</li>\n<li>하지만 gradient descent는 non-covariant</li>\n<li>이 논문에서는 covarient gradient를 제시함 = natural gradient</li>\n<li>natural gradient와 policy iteration의 연관성을 설명하겠음: natural policy gradient is moving toward choosing a greedy optimal action (이런 연결점은 아마도 step-size를 덜 신경쓰고 싶어서 그런게 아닌가 싶다)</li>\n</ul>\n<p>논문의 Introduction 부분에 다음 멘트가 있다. 이 글만 봐서는 이해가 안갔는데 Mackay 논문에 좀 더 자세히 나와있다.</p>\n<p><img src=\"https://www.dropbox.com/s/41xhhr7lgfk24a1/Screenshot%202018-06-10%2011.45.18.png?dl=1\"></p>\n<p><a href=\"http://www.inference.org.uk/mackay/ica.pdf\" target=\"_blank\" rel=\"noopener\">Mackay</a>논문에서는 다음과 같이 언급하고 있다. Back-propagation을 사용할 경우에 learning rate를 dimension에 1/n로 사용하면 수렴한다는 것이 증명됐다. 하지만 너무 느리다. </p>\n<p><img src=\"https://www.dropbox.com/s/us9ezc7vxgrkez6/Screenshot%202018-06-10%2011.47.21.png?dl=1\"></p>\n<h2 id=\"4-A-Natural-Gradient\"><a href=\"#4-A-Natural-Gradient\" class=\"headerlink\" title=\"4. A Natural Gradient\"></a>4. A Natural Gradient</h2><hr>\n<h3 id=\"4-1-환경에-대한-설정\"><a href=\"#4-1-환경에-대한-설정\" class=\"headerlink\" title=\"4.1 환경에 대한 설정\"></a>4.1 환경에 대한 설정</h3><p>이 논문에서 제시하는 학습 환경은 다음과 같다.</p>\n<ul>\n<li>MDP: tuple $(S, s_0, A, R, P)$</li>\n<li>$S$: a finite set of states</li>\n<li>$s_0$: a start state</li>\n<li>$A$: a finite set of actions</li>\n<li>$R$: reward function $R: S \\times A -&gt; [0, R_{max}]$</li>\n<li>$\\pi(a;s, \\theta)$: stochastic policy parameterized by $\\theta$</li>\n<li>모든 정책 $\\pi$는 ergodic: stationary distribution $\\rho^{\\pi}$이 잘 정의되어있음</li>\n<li>이 논문에서는 sutton의 pg 논문의 두 세팅(start-state formulation, average-reward formulation) 중에 두 번째인 average-reward formulation을 가정 </li>\n<li>performance or average reward: $\\eta(\\pi)=\\sum_{s,a}\\rho^{\\pi}(s)\\pi(a;s)R(s,a)$</li>\n<li>state-action value: $Q^{\\pi}(s,a)=E_{\\pi}[\\sum_{t=0}^{\\infty}R(s_t, a_t)-\\eta(\\pi)\\vert s_0=s, a_0=a]$</li>\n<li>정책이 $\\theta$로 parameterize되어있으므로 performance는 $\\eta(\\pi_{\\theta})$인데 $\\eta(\\theta)$로 쓸거임</li>\n</ul>\n<h3 id=\"4-2-Natural-Gradient\"><a href=\"#4-2-Natural-Gradient\" class=\"headerlink\" title=\"4.2 Natural Gradient\"></a>4.2 Natural Gradient</h3><h4 id=\"4-2-1-Policy-gradient-Theorem\"><a href=\"#4-2-1-Policy-gradient-Theorem\" class=\"headerlink\" title=\"4.2.1 Policy gradient Theorem\"></a>4.2.1 Policy gradient Theorem</h4><p>서튼 pg 논문의 policy gradient theorem에 따라 exact gradient of the average reward는 다음과 같다. 다음 수식이 어떻게 유도되었는지, 어떤 의미인지 모른다면 서튼 pg 논문을 통해 제대로 이해하는 것이 좋다.</p>\n<p>$$\\nabla\\eta(\\theta)=\\sum_{s,a}\\rho^{\\pi}(s)\\nabla\\pi(a;s,\\theta)Q^{\\pi}(s,a)$$</p>\n<p>steepest descent direction of $\\eta(\\theta)$는 $\\eta(\\theta + d\\theta)$를 최소화하는 $d\\theta$로 정의된다. 이 때, $\\vert d\\theta \\vert^2$가 일정 크기 이하인 것으로 제약조건을 준다(held to small constant). Euclidian space에서는 $\\eta(\\theta)$가 steepest direction이지만 Riemannian space에서는 natural gradient가 steepest direction이다. </p>\n<h4 id=\"4-2-2-Natural-gradient-증명\"><a href=\"#4-2-2-Natural-gradient-증명\" class=\"headerlink\" title=\"4.2.2 Natural gradient 증명\"></a>4.2.2 Natural gradient 증명</h4><p>Riemannian space에서 거리는 다음과 같이 정의된다. $G(\\theta)$는 특정한 양수로 이루어진 matrix이다.</p>\n<p>$$\\vert d\\theta \\vert^2=\\sum_{ij}(\\theta)d\\theta_id\\theta_i=d\\theta^TG(\\theta)d\\theta$$</p>\n<p>이 수식은 Natural Gradient Works Efficiently in Learning 논문에서 증명되어있다. 다음은 natural gradient 증명이다. </p>\n<p>steepest direction을 구할 때 $\\theta$의 크기를 제약조건을 준다. 제약조건은 다음과 같다. </p>\n<p>$$\\vert d\\theta \\vert^2 = \\epsilon^2$$</p>\n<p>그리고 steepest vector인 $d\\theta$는 다음과 같이 정의할 수 있다. </p>\n<p>$$d\\theta = \\epsilon a$$</p>\n<p>$$\\vert a \\vert^2=a^TG(\\theta)a = 1$$</p>\n<p>이 때, $a$가 steepest direction unit vector이 되려면 다음 수식을 최소로 만들어야 한다. (이 수식은 잘 모르겠지만 $\\theta$에서의 1차근사를 가정하는게 아닌가 싶다.</p>\n<p>$$\\eta(\\theta + d\\theta) = \\eta(\\theta) + \\epsilon\\nabla\\eta(\\theta)^Ta$$</p>\n<p>위 수식이 제약조건 아래 최소가 되는 $a$를 구하기 위해 Lagrangian method를 사용한다. Lagrangian method를 모른다면 <a href=\"https://en.wikipedia.org/wiki/Lagrange_multiplier\" target=\"_blank\" rel=\"noopener\">위키피디아</a>를 참고하는 것을 추천한다. 위 수식이 최소라는 것은 $\\nabla\\eta(\\theta)^Ta$가 최소라는 것이다. </p>\n<p>$$\\frac{\\partial}{\\partial a_i}(\\nabla\\eta(\\theta)^Ta - \\lambda a^TG(\\theta)a)=0$$</p>\n<p>따라서 $(\\nabla\\eta(\\theta)^Ta - \\lambda a^TG(\\theta)a)=0$는 상수이다. 상수를 미분하면 0이므로 이 식을 $a$로 미분한다. 그러면 다음과 같다. steepest direction을 구한 것이다.</p>\n<p>$$\\nabla\\eta(\\theta) = 2 \\lambda G(\\theta)a$$</p>\n<p>$$a=\\frac{1}{2\\lambda}G^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>이 때, 다음 식을 natural gradient라고 정의한다.</p>\n<p>$$\\bar{\\nabla}\\eta(\\theta) = G^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>natural gradient를 이용한 업데이트는 다음과 같다. </p>\n<p>$$\\theta_{t+1}=\\theta_t - \\alpha_tG^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>여기까지는 natural gradient의 증명이었다. 이 natural gradient를 policy gradient에 적용한 것이 natural policy gradient이다. natural policy gradient는 다음과 같이 정의된다.</p>\n<p>$$\\bar{\\nabla}\\eta(\\theta) = F^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>$G$ 대신 $F$를 사용했는데 $F$는 Fisher information matix이다. 수식은 다음과 같다.</p>\n<p>$$F(\\theta) = E_{\\rho^\\pi(s)}[F_s(\\theta)]$$</p>\n<p>$$F_s(\\theta)=E_{\\pi(a;s,\\theta)}[\\frac{\\partial log \\pi(a;s, \\theta)}{\\partial \\theta_i}\\frac{\\partial log \\pi(a;s, \\theta)}{\\partial\\theta_j}]$$</p>\n<p>왜 G가 F가 되는지는 아직 잘 모르겠다. 거리라는 개념을 표현하려면 </p>\n<h2 id=\"5-The-Natural-Gradient-and-Policy-Iteration\"><a href=\"#5-The-Natural-Gradient-and-Policy-Iteration\" class=\"headerlink\" title=\"5. The Natural Gradient and Policy Iteration\"></a>5. The Natural Gradient and Policy Iteration</h2><hr>\n<h3 id=\"5-1-Theorem-1\"><a href=\"#5-1-Theorem-1\" class=\"headerlink\" title=\"5.1 Theorem 1\"></a>5.1 Theorem 1</h3><p>sutton pg 논문에 따라 $Q^{\\pi}(s,a)$를 approximation한다. approximate하는 함수 $f^{\\pi}(s,a;w)$는 다음과 같다.(compatible value function)</p>\n<p>$$f^{\\pi}(s,a;w)=w^T\\psi^{\\pi}(s,a)$$</p>\n<p>$$\\psi^{\\pi}(s,a) = \\nabla log\\pi(a;s,\\theta)$$</p>\n<p>$w$는 원래 approximate하는 함수 $Q$와 $f$의 차이를 줄이도록 학습한다(mean square error). 수렴한 local minima의 $w$를 $\\bar{w}$라고 하겠다. 에러는 다음과 같은 수식으로 나타낸다. </p>\n<p>$$\\epsilon(w,\\pi)\\equiv\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)(f^{\\pi}(s,a;w)-Q^{\\pi}(s,a))^2$$</p>\n<p>위 식이 local minima이면 미분값이 0이다. $w$에 대해서 미분하면 다음과 같다. </p>\n<p>$$\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)(\\psi^{\\pi}(s,a)^T\\bar{w}-Q^{\\pi}(s,a))=0$$</p>\n<p>$$(\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)\\psi^{\\pi}(s,a)^T)\\bar{w}=\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)Q^{\\pi}(s,a))$$</p>\n<p>이 때, 위 식의 우변은 $\\psi$의 정의에 의해 policy gradient가 된다. 또한 왼쪽 항에서는 Fisher information matrix가 나온다.</p>\n<p>$$F(\\theta)=\\sum_{s,a}\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)\\psi^{\\pi}(s,a)=E_{\\rho^\\pi(s)}[F_s(\\theta)]$$</p>\n<p>따라서 다음과 같다.</p>\n<p>$$F(\\theta)\\bar{w}=\\nabla\\eta(\\theta)$$</p>\n<p>$$\\bar{w}=F(\\theta)^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>이 식은 natural gradient 식과 동일하다. 이 식은 policy가 update 될 때, value function approximator의 parameter 방향으로 이동한다는 것을 의미한다. function approximation이 정확하다면 그 parameter의 natural policy gradient와 inner product가 커야한다. </p>\n<h3 id=\"5-2-Theorem-2-Greedy-Polict-Improvement\"><a href=\"#5-2-Theorem-2-Greedy-Polict-Improvement\" class=\"headerlink\" title=\"5.2 Theorem 2: Greedy Polict Improvement\"></a>5.2 Theorem 2: Greedy Polict Improvement</h3><p>natural policy gradient가 단순히 더 좋은 행동을 고르도록 학습하는게 아니라 가장 좋은 (greedy) 행동을 고르도록 학습한다는 것을 증명하는 파트이다. 이것을 일반적인 형태의 policy에 대해서 증명하기 전에 exponential 형태의 policy에 대해서 증명하는 것이 Theorem 2이다.</p>\n<p>policy를 다음과 같이 정의한다.</p>\n<p>$$\\pi(a;s,\\theta) \\propto exp(\\theta^T\\phi_{sa})$$</p>\n<p>$\\bar{\\nabla}\\eta(\\theta)$가 0이 아니고 $\\bar{w}$는 approximation error를 최소화한 $w$라고 가정한다. 이 상태에서 natural gradient update를 생각해보자. policy gradient는 gradient ascent임을 기억하자.</p>\n<p>$$\\theta_{t+1}=\\theta_t + \\alpha_t\\bar{\\nabla}\\eta(\\theta)$$</p>\n<p>이 때 $\\alpha$가 learning rate로 parameter를 얼마나 업데이트하는지를 결정한다. 이 값을 무한대로 늘렸을 때 policy가 어떻게 업데이트되는지 생각해보자. </p>\n<p>$$\\pi_{\\infty}(a;s)=lim_{\\alpha\\rightarrow\\infty}\\pi(a;s,\\theta+\\alpha\\bar{\\nabla}\\eta(\\theta))-(1)$$</p>\n<p>function approximator는 다음과 같다. </p>\n<p>$$f^{\\pi}(s,a;w)=w^T\\psi^{\\pi}(s,a)$$</p>\n<p>Theorem 1에 의해 위 식은 아래와 같이 쓸 수 있다.</p>\n<p>$$f^{\\pi}(s,a;w)=\\bar{\\nabla}\\eta(\\theta)^T\\psi^{\\pi}(s,a)$$</p>\n<p>$\\theta$의 정의에 의해 $\\psi$는 다음과 같다.</p>\n<p>$$\\psi^{\\pi}(s,a)=\\phi_{sa}-E_{\\pi(a’;s,\\theta)}[\\phi_{sa’}]$$</p>\n<p>function approximator는 다음과 같이 다시 쓸 수 있다.</p>\n<p>$$f^{\\pi}(s,a;w)=\\bar{\\nabla}\\eta(\\theta)^T(\\phi_{sa}-E_{\\pi(a’;s,\\theta)}[\\phi_{sa’}])$$</p>\n<p>greedy policy improvement가 Q function 값 중 가장 큰 값을 가지는 action을 선택하듯이 여기서도 function approximator의 값이 가장 큰 action을 선택하는 상황을 가정해본다. 이 때 function approximator의 argmax는 다음과 같이 쓸 수 있다.</p>\n<p>$$argmax_{a’}f^{\\pi}(s,a)=argmax_{a’}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa’}$$</p>\n<p>(1) 식을 다시 살펴보자. policy의 정의에 따라 다음과 같이 쓸 수 있다. </p>\n<p>$$\\pi(a;s,\\theta + \\alpha\\bar{\\nabla}\\eta(\\theta)) \\propto exp(\\theta^T\\phi_{sa} + \\alpha\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa})$$</p>\n<p>$\\bar{\\nabla}\\eta(\\theta) \\neq 0$이고 $\\alpha\\rightarrow\\infty$이면 exp안의 항 중에서 뒤의 항이 dominate하게 된다. 여러 행동 중에 $\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa}$가 가장 큰 행동이 있다면 이 행동의 policy probability가 1이 되고 나머지는 0이 된다. 따라서 다음이 성립한다.</p>\n<p>$$\\pi_{\\infty}=0$$ </p>\n<p>if and only if </p>\n<p>$$a \\notin argmax_{a’}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa’}$$</p>\n<p>이 결과로부터 natural policy gradient는 단지 더 좋은 action이 아니라 best action을 고르도록 학습이 된다. 하지만 non-covariant gradient(1차미분) 에서는 그저 더 좋은 action을 고르도록 학습이 된다. 하지만 이 natural policy gradient에 대한 결과는 infinite learning rate 세팅에서만 성립함. 좀 더 일반적인 경우에 대해서 살펴보자.</p>\n<h4 id=\"4-3-Theorem-3\"><a href=\"#4-3-Theorem-3\" class=\"headerlink\" title=\"4.3 Theorem 3\"></a>4.3 Theorem 3</h4><p>Theorem 2에서와는 달리 일반적인 policy를 가정하자(general parameterized policy). Theorem 3는 이 상황에서 natural gradient를 통한 업데이트가 best action를 고르는 방향으로 학습이 된다는 것을 보여준다. </p>\n<p>natural gradien에 따른 policy parameter의 업데이트는 다음과 같다. $\\bar{w}$는 approximation error를 minimize하는 $w$이다.</p>\n<p>$$\\delta\\theta = \\theta’ - \\theta = \\alpha\\bar{\\nabla}\\eta(\\theta)=\\alpha\\bar{w}$$</p>\n<p>policy에 대해서 1차근사를 하면 다음과 같다. </p>\n<p>$$\\pi(a;s,\\theta’)=\\pi(a;s,\\theta)+\\frac{\\partial\\pi(a;s,\\theta)^T}{\\partial\\theta}\\delta\\theta + O(\\delta\\theta^2)$$</p>\n<p>$$=\\pi(a;s,\\theta)(1+\\psi(s,a)^T\\delta\\theta) + O(\\delta\\theta^2)$$</p>\n<p>$$=\\pi(a;s,\\theta)(1+\\alpha\\psi(s,a)^T\\bar{w}) + O(\\delta\\theta^2)$$</p>\n<p>$$=\\pi(a;s,\\theta)(1+\\alpha f^{\\pi}(s,a;\\bar{w}) + O(\\delta\\theta^2)$$</p>\n<p>policy 자체가 function approximator의 크기대로 업데이트가 되므로 local하게 best action의 probability는 커지고 다른 probability의 크기는 작아질 것이다. 하지만 만약 greedy improvement가 된다하더라도 그게 performance의 improvement를 보장하는 것은 아니다. 하지만 line search와 함께 사용할 경우 improvement를 보장할 수 있다. </p>\n<h2 id=\"6-Metrics-and-Curvatures\"><a href=\"#6-Metrics-and-Curvatures\" class=\"headerlink\" title=\"6. Metrics and Curvatures\"></a>6. Metrics and Curvatures</h2><hr>\n<p>다음 식에 해당하는 G는 Fisher Information Matrix만 사용할 수 있는 것이 아니다.</p>\n<p>$$\\vert d\\theta \\vert^2=\\sum_{ij}(\\theta)d\\theta_id\\theta_i=d\\theta^TG(\\theta)d\\theta$$</p>\n<p>이 파트에서는 FIM과 다른 metric 사이의 관계를 다룬다. </p>\n<ul>\n<li>In the different setting of parameter estimation, the Fisher information converges to the <code>Hessian</code>, so it is <a href=\"https://en.wikipedia.org/wiki/Efficiency_(statistics\" target=\"_blank\" rel=\"noopener\">asymptotically efficient</a>#Asymptotic_efficiency)</li>\n<li>이 논문의 경우, 아마리 논문의 ‘blind separation case’와 유사한데 이 때는 꼭 asymtotically efficient하지 않다. 이 말은 즉 2nd order 수렴이 보장되지 않는다는 것이다.</li>\n<li><a href=\"http://www.inference.org.uk/mackay/ica.pdf\" target=\"_blank\" rel=\"noopener\">Mackay</a> 논문에서 hessian에서 data independant한 term을 metric으로 가져오는 방법을 제안했다. 그래서 performance를 2번 미분해보면 다음과 같다. 하지만 다음 식에서는 모든 항이 data dependent하다(Q가 있으니까). 첫 번째 항이 그나마 FIM과의 관련성이 있을 수 있지만 Q 값이 curvature에 weight를 주는 방식 때문에 다르다고 할 수 있다.</li>\n</ul>\n<p>$$<br>\\nabla^2\\eta(\\theta)=\\sum_{sa}\\rho^{\\pi}(s)(\\nabla^2\\pi(a;s)Q^{\\pi}(s,a)+\\nabla\\pi(a;s)\\nabla Q^{\\pi}(s,a)^T+\\nabla Q^{\\pi}(s,a)\\nabla\\pi(a;s)^T)<br>$$</p>\n<ul>\n<li>hessian은 보통 positive definite가 아닐수도 있다. 따라서 local maxima가 될 때까지 Hessian이 사용하기 별로 안좋다. 그리고 local maxima에서는 Hessian보다는 Conjugate methods가 더 효율적이다. </li>\n</ul>\n<p>이 파트에서는 무엇을 말하고 있는지 알기가 어렵다. FIM과 Hessian이 관련이 있다는 것을 알겠다. 하지만 asymtotically efficient와 같은 내용을 모르므로 내용의 이해가 어려웠다.</p>\n<p>Mackay 논문에서 해당 부분은 다음과 같다. </p>\n<p><img src=\"https://www.dropbox.com/s/x4n6z6pdyi7xtb9/Screenshot%202018-06-10%2012.04.13.png?dl=1\"></p>\n<h2 id=\"7-Experiment\"><a href=\"#7-Experiment\" class=\"headerlink\" title=\"7. Experiment\"></a>7. Experiment</h2><hr>\n<p>논문에서는 natural gradient를 simple MDP와 tetris MDP에 대해서 테스트했다. practice에서는 Fisher information matrix는 다음과 같은 식으로 업데이트한다.</p>\n<p>$$f\\leftarrow f+\\nabla log \\pi(a_t; s_t, \\theta)\\nabla log \\pi(a_t; s_t, \\theta)^T$$</p>\n<p>T length trajectory에 대해서 f/T를 통해 F의 estimate를 구한다.</p>\n<h3 id=\"7-1-Linear-Quadratic-regulator\"><a href=\"#7-1-Linear-Quadratic-regulator\" class=\"headerlink\" title=\"7.1 Linear Quadratic regulator\"></a>7.1 Linear Quadratic regulator</h3><p>에이전트를 테스트할 환경은 다음과 같은 dynamics를 가지고 있다. $u(t)$는 control signal로서 에이전트의 행동이라고 생각하면 된다. $\\epsilon$은 noise distribution으로 환경에 가해지는 노이즈이다. 에이전트의 목표는 적절한 $u(t)$를 통해<br>x(t)를 0으로 유지하는 것이다. 제어분야에서의 LQR controller 문제이다.</p>\n<p>$$<br>x(t+1) = 0.7x(t)+u(t)+\\epsilon(t)<br>$$</p>\n<p>x(t)를 0으로 유지하기 위해서 $x(t)^2$를 cost로 잡고 이 cost를 최소화하도록 학습한다. 이 시스템을 linear라고 부르는 것은 아래 그림과 같이 선형의 형태를 띄기 때문이다. 이 논문에서 실험할 때는 이 그림에서의 system에 noise를 더해준 것이다. <a href=\"https://stanford.edu/class/ee363/lectures/dlqr.pdf\" target=\"_blank\" rel=\"noopener\">그림 출처</a></p>\n<p><img src=\"https://www.dropbox.com/s/vz0q97lcek4oti5/Screenshot%202018-06-08%2014.21.10.png?dl=1\"></p>\n<p>이 실험에서 사용한 parameterized policy는 다음과 같다. parameter가 $\\theta_1$과 $\\theta_2$ 밖에 없는 상당히 간단한 policy이다. </p>\n<p>$$<br>\\pi(u;x,\\theta) \\propto exp(\\theta_1 s_1 x^2 + \\theta_2 s_2 x)<br>$$</p>\n<p>이 policy를 간단히 numpy와 matplotlib를 이용해서 그려봤다. $\\theta_1$과 $theta_2$를 (0.5, 0.5), (1, 0), (0, 1)로 하고 $s_1$과 $s_2$는 1로 두었다. x는 -1에서 1까지의 범위로 그렸다. x를 0으로 유지하려면 u(t)가 -와 +가 둘 다 가능해야할 것 같은데 위 식으로만 봐서는 action이 하나이고 그 action일 확률을 표시하는 것처럼 나왔다. 아마 -1과 +1이 u(t)가 될 수 있는데 그 중 +1을 선택할 확률이 위와 같이 되는게 아닌가 싶다.</p>\n<center><img src=\"https://www.dropbox.com/s/v69qyrwn7zurk8c/Screenshot%202018-06-08%2014.57.07.png?dl=1\" width=\"500px\"></center>\n\n<p>다음 그림은 1-d LQR을 학습한 그래프이다. cost가 $x^2$이기 때문에 cost가 0으로 갈수록 agent는 0에서 안정적으로 머무른다고 볼 수 있다. 6개의 선 중에서 오른쪽 세 개가 일반적인 gradient 방법을 사용해서 학습한 결과이다. 그리고 왼쪽의 세 개의 선이 natural policy gradient를 통해 학습한 학습 곡선이다. 일반 gradient 방법보다 natural gradient가 훨씬 빠르게 학습한다(time 축이 log scale인 것을 감안하자).</p>\n<p>하지만 문제가 있다. npg를 학습한 세 개의 곡선은 $\\theta$를 rescale 한 것이다. $\\theta$앞에 곱해지는 숫자에 따라 학습의 과정이 다르다. 이 것은 coordinate에 따라 steepest gradient가 다르게 측정된다는 것이다. 즉, covariant gradient가 아니라는 뜻이다. 이 논문에서는 natural gradient를 통해 gradient가 covariant하도록 만들고 싶었는데 실패한 것이다. </p>\n<center><img src=\"https://www.dropbox.com/s/fhn8cgje0rdws0i/Screenshot%202018-06-08%2023.13.37.png?dl=1\" width=\"300px\"></center>\n\n<p>natural gradient가 covariant하지 않은 이유는 Fisher Information Matrix가 예상했던 바와는 달리 invariant metric이 아니기 때문이다. 또한 FIM이 invariant metric이 아닌 이유는 FIM을 계산할 때 $\\rho_s$가 곱해지기 때문이다(state distribution에 대한 expectation. $\\rho_s$가 곱해지는 것이 invariant에 미치는 영향이 무엇인지는 모르겠다). 하지만 여전히 의의가 있는 것은 기존 gradient 방법들보다 훨씬 빠르게 학습한다는 것이다.</p>\n<h3 id=\"7-2-simple-2-state-MDP\"><a href=\"#7-2-simple-2-state-MDP\" class=\"headerlink\" title=\"7.2 simple 2-state MDP\"></a>7.2 simple 2-state MDP</h3><p>이제 다른 예제에서 NPG를 테스트한다. 2개의 state만 가지는 MDP를 고려해보자. <a href=\"http://repository.cmu.edu/cgi/viewcontent.cgi?article=1080&amp;context=robotics\" target=\"_blank\" rel=\"noopener\">그림출처</a>. 그림으로보면 다음과 같다. x=0 상태와 x=1 상태 두 개가 존재한다. 에이전트는 각 상태에서 다시 자신의 상태로 되돌아오는 행동을 하거나 다른 상태로 가는 행동을 할 수 있다. 상태 x=0에서 다시 자기 자신으로 돌아오면 1의 보상을 받고 상태 x=1에서 자기 자신으로 돌아오면 2의 보상을 받는다. 따라서 결국 optimal policy는 상태 x=1에서 계속 자기 자신으로 돌아오는 행동을 취하는 것이다. </p>\n<p><img src=\"https://www.dropbox.com/s/g1x9yknzsrip59i/Screenshot%202018-06-08%2023.06.50.png?dl=1\"></p>\n<p>문제를 좀 어렵게 만들기 위해 state distribution을 다음과 같이 설정한다. 즉, 대부분의 경우에 상태 x=0에서 에이전트가 시작하는 것이다. </p>\n<p>$$<br>\\rho(x=0)=0.8,  \\rho(x=1)=0.2<br>$$</p>\n<p>일반적인 gradient 방법을 사용하면 다음과 같은 policy gradient 식에 따라서 업데이트를 하게 된다. 이 때, $\\rho(s)$가 gradient에 곱해지므로 상태적으로 상태 0에서의 gradient 값이 커지게 된다. 따라서 에이전트는 상태 0에서의 gradient(상태 0에서 스스로에게 돌아오는 행동을 취하도록 정책을 업데이트하는 gradient)를 따라 parameterized policy를 update한다. 따라서 아래 그림의 첫번째 그림에서처럼 Reward가 1에서 오랜 시간동안 머무른다. 즉, 에이전트가 상태 0에서 self-loop를 계속 돌고 있다는 것이다. $\\rho(x=0)$가 $10^{-7}$까지 떨어진다.</p>\n<p>$$\\nabla\\eta(\\theta)=\\sum_{s,a}\\rho^{\\pi}(s)\\nabla\\pi(a;s,\\theta)Q^{\\pi}(s,a)$$</p>\n<center><img src=\"https://www.dropbox.com/s/xtb77mfazbppnss/Screenshot%202018-06-08%2023.14.24.png?dl=1\" width=\"300px\"></center>\n\n<p>하지만 NPG를 사용할 경우에는 훨씬 빠르게 average reward가 2에 도달한다. gradient 방법이 $1.7X10^(7)$정도의 시간만에 2에 도달한 반면 NPG는 2만에 도달한다. 또한 $\\rho(x=0)$가 $10^{-5}$이하로 떨어지지 않는다.</p>\n<p>한 가지 그래프를 더 살펴보자. 다음 그래프는 parameter $\\theta$가 업데이트 되는 과정을 보여준다. 이 그래프에서는 parameter가 2개 있는 것이다. 일반적인 gradient가 아래 그래프에서 실선에 해당한다. 이 실선의 그래프는 보면 처음부터 중반까지 $\\theta_i$만 거의 업데이트하는 것을 볼 수 있다. 그에 비해 NPG는 두 개의 parameter를 균등하게 업데이트하는 것을 볼 수 있다. </p>\n<center><img src=\"https://www.dropbox.com/s/g7pazozw2k6rd7x/Screenshot%202018-06-08%2023.23.25.png?dl=1\" width=\"300px\"></center>\n\n<p>policy가 $\\pi(a;s,\\theta)\\propto exp(\\theta_{sa})$일 때, 다음과 같이 $F_{-1}$이 gradient 앞에 weight로 곱해지는데 이게 $\\rho$와는 달리 각 parameter에 대해 균등하다. 따라서 위 그래프에서와 같이 각 parameter는 비슷한 비율로 업데이트가 되는 것이다.</p>\n<p>$$\\bar{\\nabla}\\eta(\\theta) = F^{-1}\\nabla\\eta(\\theta)$$</p>\n<h3 id=\"7-3-Tetris\"><a href=\"#7-3-Tetris\" class=\"headerlink\" title=\"7.3 Tetris\"></a>7.3 Tetris</h3><p>NPG를 테스트할 tetris 예제는 Neuro Dynamic Programming 책에 소개되어있다. 다음 그림은 tetris 예제를 보여준다. 보통 그림에서와 같이 state의 feature를 정해준다. <a href=\"http://slideplayer.com/slide/5215520/\" target=\"_blank\" rel=\"noopener\">그림 출처</a></p>\n<p><img src=\"https://www.dropbox.com/s/y1halso9yermy8s/Screenshot%202018-06-08%2023.44.34.png?dl=1\"></p>\n<p>이 예제에서도 exponantial family로 policy를 표현한다. $\\pi(a;s,\\theta) \\propto exp(\\theta^T\\phi_{sa})$ 로 표현한다.</p>\n<p>tetris는 linear function approximator와 greedy policy iteration을 사용할 경우 performance가 갑자기 떨어지는 현상이 있다. 밑의 그림에서 A의 spike가 있는 그래프가 이 경우이다. 그 밑에 낮게 누워있는 그래프는 일반적인 policy gradient 방법이다. 하지만 Natural policy gradient를 사용할 경우 B 그림에서 오른쪽 그래프와 같이 성능개선이 뚜렷하다. Policy Iteration 처럼 성능이 뚝 떨어지지 않고 안정적으로 유지한다. 또한 그림 C에서 보는 것처럼 오른쪽 그래프인 일반적인 gradient 방법보다 훨씬 빠르게 학습하는 것을 볼 수 있다.</p>\n<p><img src=\"https://www.dropbox.com/s/pr6s2qrqaic0wyj/Screenshot%202018-06-08%2023.40.16.png?dl=1\"></p>\n<h2 id=\"8-Discussion\"><a href=\"#8-Discussion\" class=\"headerlink\" title=\"8. Discussion\"></a>8. Discussion</h2><hr>\n<ul>\n<li>natural gradient method는 policy iteration에서와 같이 greedy action을 선택하도록 학습됌</li>\n<li>line search와 함께 쓰면 natural gradient method는 더 policy iteration 같아짐</li>\n<li>greedy policy iteration에서와는 달리 performance improvement가 보장됌</li>\n<li>하지만 F(Fisher information matrix)가 asymtotically Hessian으로 수렴하지 않음. asymtotically conjugate gradient method(Hessian의 inverse를 approx.로 구하는 방법)가 더 좋아 보일 수 있음</li>\n<li>하지만 Hessian이 항상 informative하지 않고(hessian이 어떤 정보를 주려면 positive definite와 같은 성질을 가져서 해당 함수가 convex인 것을 알 수 있다든지의 경우를 이야기하는데 hessian이 항상 positive definite가 아닐 수 있다는 것이다) tetris에서 봤듯이 natural gradient method가 더 효율적일 수 있음(pushing the policy toward choosing greedy optimal actions)</li>\n<li>conjugate gradient method가 좀 더 maximum에 빠르게 수렴하지만, performance는 maximum에서 거의 안변하므로 좋다고 말하기 어려움(?). 이 부분에 대해서 추가적인 연구 필요.</li>\n</ul>\n<p>amet, consectetur adipisicing elit. Vitae ipsum, voluptatem quis officiis inventore dolor totam deserunt, possimus similique eum, accusantium adipisci doloremque omnis excepturi quasi, suscipit repellendus quibusdam? Veritatis.</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"A-Natural-Policy-Gradient-2001\"><a href=\"#A-Natural-Policy-Gradient-2001\" class=\"headerlink\" title=\"A Natural Policy Gradient [2001]\"></a>A Natural Policy Gradient [2001]</h1><p><img src=\"https://www.dropbox.com/s/it82tfhfmhg9uwp/Screenshot%202018-06-10%2010.58.52.png?dl=1\"></p>\n<ul>\n<li>논문 저자: Sham Kakade</li>\n<li>논문 링크: <a href=\"https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf\" target=\"_blank\" rel=\"noopener\">https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf</a></li>\n<li>함께 보면 좋을 논문: <ul>\n<li><a href=\"hhttps://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf\" target=\"_blank\" rel=\"noopener\">Policy Gradient Methods for<br>Reinforcement Learning with Function<br>Approximation (2000)</a></li>\n<li><a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.452.7280&amp;rep=rep1&amp;type=pdf\" target=\"_blank\" rel=\"noopener\">Natural Gradient Works Efficiently in Learning(1998)</a></li>\n</ul>\n</li>\n<li>논문을 보는 이유: TRPO와 NPG는 관련이 많기 때문에 TRPO를 더 잘 이해하기 위해 봄</li>\n</ul>\n<h2 id=\"1-Abstract\"><a href=\"#1-Abstract\" class=\"headerlink\" title=\"1. Abstract\"></a>1. Abstract</h2><hr>\n<ul>\n<li>natural gradient method를 policy gradient에 적용</li>\n<li>natural gradient는 steepest descent direction을 가짐</li>\n<li>gradient descent는 parameter를 한 번에 많이 update 할 수 없는 반면, natural gradient는 가장 좋은 action을 고르도록 학습이 됌 (sutton 논문에서와 같이 compatible value function을 사용할 경우 policy iteration에서 policy improvement 1 step의 과정에서)</li>\n<li>simple MDP와 tetris MDP에서 테스트함. 성능이 많이 향상</li>\n</ul>\n<h2 id=\"2-Personal-Interpretation-and-Thinking\"><a href=\"#2-Personal-Interpretation-and-Thinking\" class=\"headerlink\" title=\"2. Personal Interpretation and Thinking\"></a>2. Personal Interpretation and Thinking</h2><hr>\n<p>(개인생각) 뉴럴넷을 사용할 경우 gradient가 steepest direction이 아닌 경우가 많다. 뉴럴넷의 parameter space가 우리가 보통 생각하는 직선으로 쭉쭉 뻗어있는 Euclidean space가 아니다. 좀 더 일반적으로는 구의 표면과 같이 휘어져있는 공간 즉, 리만 공간(Riemannian space)로 표현할 수 있다. 이와 같은 공간에서는 natural gradient가 steepest direction이 된다는 연구가 이뤄지고 있었다. 강화학습의 policy gradient은 objective function의 gradient를 따라 policy를 업데이트한다. 이 때, policy는 parameterized 되는데 이 경우에도 gradient 대신에 natural gradient가 좋다는 것을 실험해보는 논문이다. </p>\n<p>gradient가 non-covariant 해서 생기는 문제는 간단히 말하자면 다음과 같다. policy가 parameterized된 상황에서는 같은 policy라도 다른 parameter를 가질 수 있다. 이 때, steepest direction은 두 경우에 같은 방향을 가리켜야하는데 non-covariant한 경우 그렇지 못하다. 이것은 결국 느린 학습으로 연결이 된다. </p>\n<p>논문에서 2차미분 방법론들과 짧게 비교를 한다. 하지만 2차미분을 이용한 다른 방법들과의 비교가 생각보다 없는 점이 아쉽다.(Hessian을 이용한다거나 conjugate gradient method를 이용한다거나). 실험을 통해 FIM이 hessian에 수렴안하는 거라던지 Hessian 방법론이 local maxima 부근에서 상당히 느리다던지의 결과를 보여줬었으면 좋았을 것 같다. </p>\n<p>또한 natural gradient 만으로 업데이트하면 policy의 improvement보장이 안될 수 있다. policy의 improvement를 보장하기 위해 line search도 써야하는데 line search를 어떻게 쓰는지에 대한 자세한 언급이 없다. 즉, 자세한 algorithm 설명이 없다.</p>\n<p>natural policy gradient 논문은 natural gradient + policy gradient를 처음 적용했다는데 의의가 있다. 하지만 이 논문이 문제 삼은 gradient는 non-covariant하다라는 문제를 natural gradient를 통해 해결하지 못했다(Experiment를 통해 covariant gradient가 되지 못했다는 것이 보인다). NPG의 뒤를 잇는 논문이 “covariant policy search”와 “natural actor-critic”에서 covariant하지 못하다는 것을 해결하기 위해 Fisher Information Matrix를 sample 하나 하나에 대해서 구하는 것이 아니라 trajectory 전체에 대해서 구한다. </p>\n<p>또한 논문은 pg의 두 가지 세팅 중에 average-reward setting(infinite horizon)에서만 NPG를 다룬다. “covariant policy search” 논문에서는 average-reward setting과 start-state setting 모두에 대해서 npg를 적용한다. </p>\n<p>natural gradient + policy gradient를 처음 제시했다는 것은 좋지만 npg 학습의 과정을 자세하게 설명하지 않았고 다른 2차 미분 방법들과 비교를 많이 하지 않은 점이 아쉬운 논문이다.</p>\n<h2 id=\"3-Introduction\"><a href=\"#3-Introduction\" class=\"headerlink\" title=\"3. Introduction\"></a>3. Introduction</h2><hr>\n<ul>\n<li>direct policy gradient method는 future reward의 gradient를 따라 policy를 update함</li>\n<li>하지만 gradient descent는 non-covariant</li>\n<li>이 논문에서는 covarient gradient를 제시함 = natural gradient</li>\n<li>natural gradient와 policy iteration의 연관성을 설명하겠음: natural policy gradient is moving toward choosing a greedy optimal action (이런 연결점은 아마도 step-size를 덜 신경쓰고 싶어서 그런게 아닌가 싶다)</li>\n</ul>\n<p>논문의 Introduction 부분에 다음 멘트가 있다. 이 글만 봐서는 이해가 안갔는데 Mackay 논문에 좀 더 자세히 나와있다.</p>\n<p><img src=\"https://www.dropbox.com/s/41xhhr7lgfk24a1/Screenshot%202018-06-10%2011.45.18.png?dl=1\"></p>\n<p><a href=\"http://www.inference.org.uk/mackay/ica.pdf\" target=\"_blank\" rel=\"noopener\">Mackay</a>논문에서는 다음과 같이 언급하고 있다. Back-propagation을 사용할 경우에 learning rate를 dimension에 1/n로 사용하면 수렴한다는 것이 증명됐다. 하지만 너무 느리다. </p>\n<p><img src=\"https://www.dropbox.com/s/us9ezc7vxgrkez6/Screenshot%202018-06-10%2011.47.21.png?dl=1\"></p>\n<h2 id=\"4-A-Natural-Gradient\"><a href=\"#4-A-Natural-Gradient\" class=\"headerlink\" title=\"4. A Natural Gradient\"></a>4. A Natural Gradient</h2><hr>\n<h3 id=\"4-1-환경에-대한-설정\"><a href=\"#4-1-환경에-대한-설정\" class=\"headerlink\" title=\"4.1 환경에 대한 설정\"></a>4.1 환경에 대한 설정</h3><p>이 논문에서 제시하는 학습 환경은 다음과 같다.</p>\n<ul>\n<li>MDP: tuple $(S, s_0, A, R, P)$</li>\n<li>$S$: a finite set of states</li>\n<li>$s_0$: a start state</li>\n<li>$A$: a finite set of actions</li>\n<li>$R$: reward function $R: S \\times A -&gt; [0, R_{max}]$</li>\n<li>$\\pi(a;s, \\theta)$: stochastic policy parameterized by $\\theta$</li>\n<li>모든 정책 $\\pi$는 ergodic: stationary distribution $\\rho^{\\pi}$이 잘 정의되어있음</li>\n<li>이 논문에서는 sutton의 pg 논문의 두 세팅(start-state formulation, average-reward formulation) 중에 두 번째인 average-reward formulation을 가정 </li>\n<li>performance or average reward: $\\eta(\\pi)=\\sum_{s,a}\\rho^{\\pi}(s)\\pi(a;s)R(s,a)$</li>\n<li>state-action value: $Q^{\\pi}(s,a)=E_{\\pi}[\\sum_{t=0}^{\\infty}R(s_t, a_t)-\\eta(\\pi)\\vert s_0=s, a_0=a]$</li>\n<li>정책이 $\\theta$로 parameterize되어있으므로 performance는 $\\eta(\\pi_{\\theta})$인데 $\\eta(\\theta)$로 쓸거임</li>\n</ul>\n<h3 id=\"4-2-Natural-Gradient\"><a href=\"#4-2-Natural-Gradient\" class=\"headerlink\" title=\"4.2 Natural Gradient\"></a>4.2 Natural Gradient</h3><h4 id=\"4-2-1-Policy-gradient-Theorem\"><a href=\"#4-2-1-Policy-gradient-Theorem\" class=\"headerlink\" title=\"4.2.1 Policy gradient Theorem\"></a>4.2.1 Policy gradient Theorem</h4><p>서튼 pg 논문의 policy gradient theorem에 따라 exact gradient of the average reward는 다음과 같다. 다음 수식이 어떻게 유도되었는지, 어떤 의미인지 모른다면 서튼 pg 논문을 통해 제대로 이해하는 것이 좋다.</p>\n<p>$$\\nabla\\eta(\\theta)=\\sum_{s,a}\\rho^{\\pi}(s)\\nabla\\pi(a;s,\\theta)Q^{\\pi}(s,a)$$</p>\n<p>steepest descent direction of $\\eta(\\theta)$는 $\\eta(\\theta + d\\theta)$를 최소화하는 $d\\theta$로 정의된다. 이 때, $\\vert d\\theta \\vert^2$가 일정 크기 이하인 것으로 제약조건을 준다(held to small constant). Euclidian space에서는 $\\eta(\\theta)$가 steepest direction이지만 Riemannian space에서는 natural gradient가 steepest direction이다. </p>\n<h4 id=\"4-2-2-Natural-gradient-증명\"><a href=\"#4-2-2-Natural-gradient-증명\" class=\"headerlink\" title=\"4.2.2 Natural gradient 증명\"></a>4.2.2 Natural gradient 증명</h4><p>Riemannian space에서 거리는 다음과 같이 정의된다. $G(\\theta)$는 특정한 양수로 이루어진 matrix이다.</p>\n<p>$$\\vert d\\theta \\vert^2=\\sum_{ij}(\\theta)d\\theta_id\\theta_i=d\\theta^TG(\\theta)d\\theta$$</p>\n<p>이 수식은 Natural Gradient Works Efficiently in Learning 논문에서 증명되어있다. 다음은 natural gradient 증명이다. </p>\n<p>steepest direction을 구할 때 $\\theta$의 크기를 제약조건을 준다. 제약조건은 다음과 같다. </p>\n<p>$$\\vert d\\theta \\vert^2 = \\epsilon^2$$</p>\n<p>그리고 steepest vector인 $d\\theta$는 다음과 같이 정의할 수 있다. </p>\n<p>$$d\\theta = \\epsilon a$$</p>\n<p>$$\\vert a \\vert^2=a^TG(\\theta)a = 1$$</p>\n<p>이 때, $a$가 steepest direction unit vector이 되려면 다음 수식을 최소로 만들어야 한다. (이 수식은 잘 모르겠지만 $\\theta$에서의 1차근사를 가정하는게 아닌가 싶다.</p>\n<p>$$\\eta(\\theta + d\\theta) = \\eta(\\theta) + \\epsilon\\nabla\\eta(\\theta)^Ta$$</p>\n<p>위 수식이 제약조건 아래 최소가 되는 $a$를 구하기 위해 Lagrangian method를 사용한다. Lagrangian method를 모른다면 <a href=\"https://en.wikipedia.org/wiki/Lagrange_multiplier\" target=\"_blank\" rel=\"noopener\">위키피디아</a>를 참고하는 것을 추천한다. 위 수식이 최소라는 것은 $\\nabla\\eta(\\theta)^Ta$가 최소라는 것이다. </p>\n<p>$$\\frac{\\partial}{\\partial a_i}(\\nabla\\eta(\\theta)^Ta - \\lambda a^TG(\\theta)a)=0$$</p>\n<p>따라서 $(\\nabla\\eta(\\theta)^Ta - \\lambda a^TG(\\theta)a)=0$는 상수이다. 상수를 미분하면 0이므로 이 식을 $a$로 미분한다. 그러면 다음과 같다. steepest direction을 구한 것이다.</p>\n<p>$$\\nabla\\eta(\\theta) = 2 \\lambda G(\\theta)a$$</p>\n<p>$$a=\\frac{1}{2\\lambda}G^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>이 때, 다음 식을 natural gradient라고 정의한다.</p>\n<p>$$\\bar{\\nabla}\\eta(\\theta) = G^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>natural gradient를 이용한 업데이트는 다음과 같다. </p>\n<p>$$\\theta_{t+1}=\\theta_t - \\alpha_tG^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>여기까지는 natural gradient의 증명이었다. 이 natural gradient를 policy gradient에 적용한 것이 natural policy gradient이다. natural policy gradient는 다음과 같이 정의된다.</p>\n<p>$$\\bar{\\nabla}\\eta(\\theta) = F^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>$G$ 대신 $F$를 사용했는데 $F$는 Fisher information matix이다. 수식은 다음과 같다.</p>\n<p>$$F(\\theta) = E_{\\rho^\\pi(s)}[F_s(\\theta)]$$</p>\n<p>$$F_s(\\theta)=E_{\\pi(a;s,\\theta)}[\\frac{\\partial log \\pi(a;s, \\theta)}{\\partial \\theta_i}\\frac{\\partial log \\pi(a;s, \\theta)}{\\partial\\theta_j}]$$</p>\n<p>왜 G가 F가 되는지는 아직 잘 모르겠다. 거리라는 개념을 표현하려면 </p>\n<h2 id=\"5-The-Natural-Gradient-and-Policy-Iteration\"><a href=\"#5-The-Natural-Gradient-and-Policy-Iteration\" class=\"headerlink\" title=\"5. The Natural Gradient and Policy Iteration\"></a>5. The Natural Gradient and Policy Iteration</h2><hr>\n<h3 id=\"5-1-Theorem-1\"><a href=\"#5-1-Theorem-1\" class=\"headerlink\" title=\"5.1 Theorem 1\"></a>5.1 Theorem 1</h3><p>sutton pg 논문에 따라 $Q^{\\pi}(s,a)$를 approximation한다. approximate하는 함수 $f^{\\pi}(s,a;w)$는 다음과 같다.(compatible value function)</p>\n<p>$$f^{\\pi}(s,a;w)=w^T\\psi^{\\pi}(s,a)$$</p>\n<p>$$\\psi^{\\pi}(s,a) = \\nabla log\\pi(a;s,\\theta)$$</p>\n<p>$w$는 원래 approximate하는 함수 $Q$와 $f$의 차이를 줄이도록 학습한다(mean square error). 수렴한 local minima의 $w$를 $\\bar{w}$라고 하겠다. 에러는 다음과 같은 수식으로 나타낸다. </p>\n<p>$$\\epsilon(w,\\pi)\\equiv\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)(f^{\\pi}(s,a;w)-Q^{\\pi}(s,a))^2$$</p>\n<p>위 식이 local minima이면 미분값이 0이다. $w$에 대해서 미분하면 다음과 같다. </p>\n<p>$$\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)(\\psi^{\\pi}(s,a)^T\\bar{w}-Q^{\\pi}(s,a))=0$$</p>\n<p>$$(\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)\\psi^{\\pi}(s,a)^T)\\bar{w}=\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)Q^{\\pi}(s,a))$$</p>\n<p>이 때, 위 식의 우변은 $\\psi$의 정의에 의해 policy gradient가 된다. 또한 왼쪽 항에서는 Fisher information matrix가 나온다.</p>\n<p>$$F(\\theta)=\\sum_{s,a}\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)\\psi^{\\pi}(s,a)=E_{\\rho^\\pi(s)}[F_s(\\theta)]$$</p>\n<p>따라서 다음과 같다.</p>\n<p>$$F(\\theta)\\bar{w}=\\nabla\\eta(\\theta)$$</p>\n<p>$$\\bar{w}=F(\\theta)^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>이 식은 natural gradient 식과 동일하다. 이 식은 policy가 update 될 때, value function approximator의 parameter 방향으로 이동한다는 것을 의미한다. function approximation이 정확하다면 그 parameter의 natural policy gradient와 inner product가 커야한다. </p>\n<h3 id=\"5-2-Theorem-2-Greedy-Polict-Improvement\"><a href=\"#5-2-Theorem-2-Greedy-Polict-Improvement\" class=\"headerlink\" title=\"5.2 Theorem 2: Greedy Polict Improvement\"></a>5.2 Theorem 2: Greedy Polict Improvement</h3><p>natural policy gradient가 단순히 더 좋은 행동을 고르도록 학습하는게 아니라 가장 좋은 (greedy) 행동을 고르도록 학습한다는 것을 증명하는 파트이다. 이것을 일반적인 형태의 policy에 대해서 증명하기 전에 exponential 형태의 policy에 대해서 증명하는 것이 Theorem 2이다.</p>\n<p>policy를 다음과 같이 정의한다.</p>\n<p>$$\\pi(a;s,\\theta) \\propto exp(\\theta^T\\phi_{sa})$$</p>\n<p>$\\bar{\\nabla}\\eta(\\theta)$가 0이 아니고 $\\bar{w}$는 approximation error를 최소화한 $w$라고 가정한다. 이 상태에서 natural gradient update를 생각해보자. policy gradient는 gradient ascent임을 기억하자.</p>\n<p>$$\\theta_{t+1}=\\theta_t + \\alpha_t\\bar{\\nabla}\\eta(\\theta)$$</p>\n<p>이 때 $\\alpha$가 learning rate로 parameter를 얼마나 업데이트하는지를 결정한다. 이 값을 무한대로 늘렸을 때 policy가 어떻게 업데이트되는지 생각해보자. </p>\n<p>$$\\pi_{\\infty}(a;s)=lim_{\\alpha\\rightarrow\\infty}\\pi(a;s,\\theta+\\alpha\\bar{\\nabla}\\eta(\\theta))-(1)$$</p>\n<p>function approximator는 다음과 같다. </p>\n<p>$$f^{\\pi}(s,a;w)=w^T\\psi^{\\pi}(s,a)$$</p>\n<p>Theorem 1에 의해 위 식은 아래와 같이 쓸 수 있다.</p>\n<p>$$f^{\\pi}(s,a;w)=\\bar{\\nabla}\\eta(\\theta)^T\\psi^{\\pi}(s,a)$$</p>\n<p>$\\theta$의 정의에 의해 $\\psi$는 다음과 같다.</p>\n<p>$$\\psi^{\\pi}(s,a)=\\phi_{sa}-E_{\\pi(a’;s,\\theta)}[\\phi_{sa’}]$$</p>\n<p>function approximator는 다음과 같이 다시 쓸 수 있다.</p>\n<p>$$f^{\\pi}(s,a;w)=\\bar{\\nabla}\\eta(\\theta)^T(\\phi_{sa}-E_{\\pi(a’;s,\\theta)}[\\phi_{sa’}])$$</p>\n<p>greedy policy improvement가 Q function 값 중 가장 큰 값을 가지는 action을 선택하듯이 여기서도 function approximator의 값이 가장 큰 action을 선택하는 상황을 가정해본다. 이 때 function approximator의 argmax는 다음과 같이 쓸 수 있다.</p>\n<p>$$argmax_{a’}f^{\\pi}(s,a)=argmax_{a’}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa’}$$</p>\n<p>(1) 식을 다시 살펴보자. policy의 정의에 따라 다음과 같이 쓸 수 있다. </p>\n<p>$$\\pi(a;s,\\theta + \\alpha\\bar{\\nabla}\\eta(\\theta)) \\propto exp(\\theta^T\\phi_{sa} + \\alpha\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa})$$</p>\n<p>$\\bar{\\nabla}\\eta(\\theta) \\neq 0$이고 $\\alpha\\rightarrow\\infty$이면 exp안의 항 중에서 뒤의 항이 dominate하게 된다. 여러 행동 중에 $\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa}$가 가장 큰 행동이 있다면 이 행동의 policy probability가 1이 되고 나머지는 0이 된다. 따라서 다음이 성립한다.</p>\n<p>$$\\pi_{\\infty}=0$$ </p>\n<p>if and only if </p>\n<p>$$a \\notin argmax_{a’}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa’}$$</p>\n<p>이 결과로부터 natural policy gradient는 단지 더 좋은 action이 아니라 best action을 고르도록 학습이 된다. 하지만 non-covariant gradient(1차미분) 에서는 그저 더 좋은 action을 고르도록 학습이 된다. 하지만 이 natural policy gradient에 대한 결과는 infinite learning rate 세팅에서만 성립함. 좀 더 일반적인 경우에 대해서 살펴보자.</p>\n<h4 id=\"4-3-Theorem-3\"><a href=\"#4-3-Theorem-3\" class=\"headerlink\" title=\"4.3 Theorem 3\"></a>4.3 Theorem 3</h4><p>Theorem 2에서와는 달리 일반적인 policy를 가정하자(general parameterized policy). Theorem 3는 이 상황에서 natural gradient를 통한 업데이트가 best action를 고르는 방향으로 학습이 된다는 것을 보여준다. </p>\n<p>natural gradien에 따른 policy parameter의 업데이트는 다음과 같다. $\\bar{w}$는 approximation error를 minimize하는 $w$이다.</p>\n<p>$$\\delta\\theta = \\theta’ - \\theta = \\alpha\\bar{\\nabla}\\eta(\\theta)=\\alpha\\bar{w}$$</p>\n<p>policy에 대해서 1차근사를 하면 다음과 같다. </p>\n<p>$$\\pi(a;s,\\theta’)=\\pi(a;s,\\theta)+\\frac{\\partial\\pi(a;s,\\theta)^T}{\\partial\\theta}\\delta\\theta + O(\\delta\\theta^2)$$</p>\n<p>$$=\\pi(a;s,\\theta)(1+\\psi(s,a)^T\\delta\\theta) + O(\\delta\\theta^2)$$</p>\n<p>$$=\\pi(a;s,\\theta)(1+\\alpha\\psi(s,a)^T\\bar{w}) + O(\\delta\\theta^2)$$</p>\n<p>$$=\\pi(a;s,\\theta)(1+\\alpha f^{\\pi}(s,a;\\bar{w}) + O(\\delta\\theta^2)$$</p>\n<p>policy 자체가 function approximator의 크기대로 업데이트가 되므로 local하게 best action의 probability는 커지고 다른 probability의 크기는 작아질 것이다. 하지만 만약 greedy improvement가 된다하더라도 그게 performance의 improvement를 보장하는 것은 아니다. 하지만 line search와 함께 사용할 경우 improvement를 보장할 수 있다. </p>\n<h2 id=\"6-Metrics-and-Curvatures\"><a href=\"#6-Metrics-and-Curvatures\" class=\"headerlink\" title=\"6. Metrics and Curvatures\"></a>6. Metrics and Curvatures</h2><hr>\n<p>다음 식에 해당하는 G는 Fisher Information Matrix만 사용할 수 있는 것이 아니다.</p>\n<p>$$\\vert d\\theta \\vert^2=\\sum_{ij}(\\theta)d\\theta_id\\theta_i=d\\theta^TG(\\theta)d\\theta$$</p>\n<p>이 파트에서는 FIM과 다른 metric 사이의 관계를 다룬다. </p>\n<ul>\n<li>In the different setting of parameter estimation, the Fisher information converges to the <code>Hessian</code>, so it is <a href=\"https://en.wikipedia.org/wiki/Efficiency_(statistics\" target=\"_blank\" rel=\"noopener\">asymptotically efficient</a>#Asymptotic_efficiency)</li>\n<li>이 논문의 경우, 아마리 논문의 ‘blind separation case’와 유사한데 이 때는 꼭 asymtotically efficient하지 않다. 이 말은 즉 2nd order 수렴이 보장되지 않는다는 것이다.</li>\n<li><a href=\"http://www.inference.org.uk/mackay/ica.pdf\" target=\"_blank\" rel=\"noopener\">Mackay</a> 논문에서 hessian에서 data independant한 term을 metric으로 가져오는 방법을 제안했다. 그래서 performance를 2번 미분해보면 다음과 같다. 하지만 다음 식에서는 모든 항이 data dependent하다(Q가 있으니까). 첫 번째 항이 그나마 FIM과의 관련성이 있을 수 있지만 Q 값이 curvature에 weight를 주는 방식 때문에 다르다고 할 수 있다.</li>\n</ul>\n<p>$$<br>\\nabla^2\\eta(\\theta)=\\sum_{sa}\\rho^{\\pi}(s)(\\nabla^2\\pi(a;s)Q^{\\pi}(s,a)+\\nabla\\pi(a;s)\\nabla Q^{\\pi}(s,a)^T+\\nabla Q^{\\pi}(s,a)\\nabla\\pi(a;s)^T)<br>$$</p>\n<ul>\n<li>hessian은 보통 positive definite가 아닐수도 있다. 따라서 local maxima가 될 때까지 Hessian이 사용하기 별로 안좋다. 그리고 local maxima에서는 Hessian보다는 Conjugate methods가 더 효율적이다. </li>\n</ul>\n<p>이 파트에서는 무엇을 말하고 있는지 알기가 어렵다. FIM과 Hessian이 관련이 있다는 것을 알겠다. 하지만 asymtotically efficient와 같은 내용을 모르므로 내용의 이해가 어려웠다.</p>\n<p>Mackay 논문에서 해당 부분은 다음과 같다. </p>\n<p><img src=\"https://www.dropbox.com/s/x4n6z6pdyi7xtb9/Screenshot%202018-06-10%2012.04.13.png?dl=1\"></p>\n<h2 id=\"7-Experiment\"><a href=\"#7-Experiment\" class=\"headerlink\" title=\"7. Experiment\"></a>7. Experiment</h2><hr>\n<p>논문에서는 natural gradient를 simple MDP와 tetris MDP에 대해서 테스트했다. practice에서는 Fisher information matrix는 다음과 같은 식으로 업데이트한다.</p>\n<p>$$f\\leftarrow f+\\nabla log \\pi(a_t; s_t, \\theta)\\nabla log \\pi(a_t; s_t, \\theta)^T$$</p>\n<p>T length trajectory에 대해서 f/T를 통해 F의 estimate를 구한다.</p>\n<h3 id=\"7-1-Linear-Quadratic-regulator\"><a href=\"#7-1-Linear-Quadratic-regulator\" class=\"headerlink\" title=\"7.1 Linear Quadratic regulator\"></a>7.1 Linear Quadratic regulator</h3><p>에이전트를 테스트할 환경은 다음과 같은 dynamics를 가지고 있다. $u(t)$는 control signal로서 에이전트의 행동이라고 생각하면 된다. $\\epsilon$은 noise distribution으로 환경에 가해지는 노이즈이다. 에이전트의 목표는 적절한 $u(t)$를 통해<br>x(t)를 0으로 유지하는 것이다. 제어분야에서의 LQR controller 문제이다.</p>\n<p>$$<br>x(t+1) = 0.7x(t)+u(t)+\\epsilon(t)<br>$$</p>\n<p>x(t)를 0으로 유지하기 위해서 $x(t)^2$를 cost로 잡고 이 cost를 최소화하도록 학습한다. 이 시스템을 linear라고 부르는 것은 아래 그림과 같이 선형의 형태를 띄기 때문이다. 이 논문에서 실험할 때는 이 그림에서의 system에 noise를 더해준 것이다. <a href=\"https://stanford.edu/class/ee363/lectures/dlqr.pdf\" target=\"_blank\" rel=\"noopener\">그림 출처</a></p>\n<p><img src=\"https://www.dropbox.com/s/vz0q97lcek4oti5/Screenshot%202018-06-08%2014.21.10.png?dl=1\"></p>\n<p>이 실험에서 사용한 parameterized policy는 다음과 같다. parameter가 $\\theta_1$과 $\\theta_2$ 밖에 없는 상당히 간단한 policy이다. </p>\n<p>$$<br>\\pi(u;x,\\theta) \\propto exp(\\theta_1 s_1 x^2 + \\theta_2 s_2 x)<br>$$</p>\n<p>이 policy를 간단히 numpy와 matplotlib를 이용해서 그려봤다. $\\theta_1$과 $theta_2$를 (0.5, 0.5), (1, 0), (0, 1)로 하고 $s_1$과 $s_2$는 1로 두었다. x는 -1에서 1까지의 범위로 그렸다. x를 0으로 유지하려면 u(t)가 -와 +가 둘 다 가능해야할 것 같은데 위 식으로만 봐서는 action이 하나이고 그 action일 확률을 표시하는 것처럼 나왔다. 아마 -1과 +1이 u(t)가 될 수 있는데 그 중 +1을 선택할 확률이 위와 같이 되는게 아닌가 싶다.</p>\n<center><img src=\"https://www.dropbox.com/s/v69qyrwn7zurk8c/Screenshot%202018-06-08%2014.57.07.png?dl=1\" width=\"500px\"></center>\n\n<p>다음 그림은 1-d LQR을 학습한 그래프이다. cost가 $x^2$이기 때문에 cost가 0으로 갈수록 agent는 0에서 안정적으로 머무른다고 볼 수 있다. 6개의 선 중에서 오른쪽 세 개가 일반적인 gradient 방법을 사용해서 학습한 결과이다. 그리고 왼쪽의 세 개의 선이 natural policy gradient를 통해 학습한 학습 곡선이다. 일반 gradient 방법보다 natural gradient가 훨씬 빠르게 학습한다(time 축이 log scale인 것을 감안하자).</p>\n<p>하지만 문제가 있다. npg를 학습한 세 개의 곡선은 $\\theta$를 rescale 한 것이다. $\\theta$앞에 곱해지는 숫자에 따라 학습의 과정이 다르다. 이 것은 coordinate에 따라 steepest gradient가 다르게 측정된다는 것이다. 즉, covariant gradient가 아니라는 뜻이다. 이 논문에서는 natural gradient를 통해 gradient가 covariant하도록 만들고 싶었는데 실패한 것이다. </p>\n<center><img src=\"https://www.dropbox.com/s/fhn8cgje0rdws0i/Screenshot%202018-06-08%2023.13.37.png?dl=1\" width=\"300px\"></center>\n\n<p>natural gradient가 covariant하지 않은 이유는 Fisher Information Matrix가 예상했던 바와는 달리 invariant metric이 아니기 때문이다. 또한 FIM이 invariant metric이 아닌 이유는 FIM을 계산할 때 $\\rho_s$가 곱해지기 때문이다(state distribution에 대한 expectation. $\\rho_s$가 곱해지는 것이 invariant에 미치는 영향이 무엇인지는 모르겠다). 하지만 여전히 의의가 있는 것은 기존 gradient 방법들보다 훨씬 빠르게 학습한다는 것이다.</p>\n<h3 id=\"7-2-simple-2-state-MDP\"><a href=\"#7-2-simple-2-state-MDP\" class=\"headerlink\" title=\"7.2 simple 2-state MDP\"></a>7.2 simple 2-state MDP</h3><p>이제 다른 예제에서 NPG를 테스트한다. 2개의 state만 가지는 MDP를 고려해보자. <a href=\"http://repository.cmu.edu/cgi/viewcontent.cgi?article=1080&amp;context=robotics\" target=\"_blank\" rel=\"noopener\">그림출처</a>. 그림으로보면 다음과 같다. x=0 상태와 x=1 상태 두 개가 존재한다. 에이전트는 각 상태에서 다시 자신의 상태로 되돌아오는 행동을 하거나 다른 상태로 가는 행동을 할 수 있다. 상태 x=0에서 다시 자기 자신으로 돌아오면 1의 보상을 받고 상태 x=1에서 자기 자신으로 돌아오면 2의 보상을 받는다. 따라서 결국 optimal policy는 상태 x=1에서 계속 자기 자신으로 돌아오는 행동을 취하는 것이다. </p>\n<p><img src=\"https://www.dropbox.com/s/g1x9yknzsrip59i/Screenshot%202018-06-08%2023.06.50.png?dl=1\"></p>\n<p>문제를 좀 어렵게 만들기 위해 state distribution을 다음과 같이 설정한다. 즉, 대부분의 경우에 상태 x=0에서 에이전트가 시작하는 것이다. </p>\n<p>$$<br>\\rho(x=0)=0.8,  \\rho(x=1)=0.2<br>$$</p>\n<p>일반적인 gradient 방법을 사용하면 다음과 같은 policy gradient 식에 따라서 업데이트를 하게 된다. 이 때, $\\rho(s)$가 gradient에 곱해지므로 상태적으로 상태 0에서의 gradient 값이 커지게 된다. 따라서 에이전트는 상태 0에서의 gradient(상태 0에서 스스로에게 돌아오는 행동을 취하도록 정책을 업데이트하는 gradient)를 따라 parameterized policy를 update한다. 따라서 아래 그림의 첫번째 그림에서처럼 Reward가 1에서 오랜 시간동안 머무른다. 즉, 에이전트가 상태 0에서 self-loop를 계속 돌고 있다는 것이다. $\\rho(x=0)$가 $10^{-7}$까지 떨어진다.</p>\n<p>$$\\nabla\\eta(\\theta)=\\sum_{s,a}\\rho^{\\pi}(s)\\nabla\\pi(a;s,\\theta)Q^{\\pi}(s,a)$$</p>\n<center><img src=\"https://www.dropbox.com/s/xtb77mfazbppnss/Screenshot%202018-06-08%2023.14.24.png?dl=1\" width=\"300px\"></center>\n\n<p>하지만 NPG를 사용할 경우에는 훨씬 빠르게 average reward가 2에 도달한다. gradient 방법이 $1.7X10^(7)$정도의 시간만에 2에 도달한 반면 NPG는 2만에 도달한다. 또한 $\\rho(x=0)$가 $10^{-5}$이하로 떨어지지 않는다.</p>\n<p>한 가지 그래프를 더 살펴보자. 다음 그래프는 parameter $\\theta$가 업데이트 되는 과정을 보여준다. 이 그래프에서는 parameter가 2개 있는 것이다. 일반적인 gradient가 아래 그래프에서 실선에 해당한다. 이 실선의 그래프는 보면 처음부터 중반까지 $\\theta_i$만 거의 업데이트하는 것을 볼 수 있다. 그에 비해 NPG는 두 개의 parameter를 균등하게 업데이트하는 것을 볼 수 있다. </p>\n<center><img src=\"https://www.dropbox.com/s/g7pazozw2k6rd7x/Screenshot%202018-06-08%2023.23.25.png?dl=1\" width=\"300px\"></center>\n\n<p>policy가 $\\pi(a;s,\\theta)\\propto exp(\\theta_{sa})$일 때, 다음과 같이 $F_{-1}$이 gradient 앞에 weight로 곱해지는데 이게 $\\rho$와는 달리 각 parameter에 대해 균등하다. 따라서 위 그래프에서와 같이 각 parameter는 비슷한 비율로 업데이트가 되는 것이다.</p>\n<p>$$\\bar{\\nabla}\\eta(\\theta) = F^{-1}\\nabla\\eta(\\theta)$$</p>\n<h3 id=\"7-3-Tetris\"><a href=\"#7-3-Tetris\" class=\"headerlink\" title=\"7.3 Tetris\"></a>7.3 Tetris</h3><p>NPG를 테스트할 tetris 예제는 Neuro Dynamic Programming 책에 소개되어있다. 다음 그림은 tetris 예제를 보여준다. 보통 그림에서와 같이 state의 feature를 정해준다. <a href=\"http://slideplayer.com/slide/5215520/\" target=\"_blank\" rel=\"noopener\">그림 출처</a></p>\n<p><img src=\"https://www.dropbox.com/s/y1halso9yermy8s/Screenshot%202018-06-08%2023.44.34.png?dl=1\"></p>\n<p>이 예제에서도 exponantial family로 policy를 표현한다. $\\pi(a;s,\\theta) \\propto exp(\\theta^T\\phi_{sa})$ 로 표현한다.</p>\n<p>tetris는 linear function approximator와 greedy policy iteration을 사용할 경우 performance가 갑자기 떨어지는 현상이 있다. 밑의 그림에서 A의 spike가 있는 그래프가 이 경우이다. 그 밑에 낮게 누워있는 그래프는 일반적인 policy gradient 방법이다. 하지만 Natural policy gradient를 사용할 경우 B 그림에서 오른쪽 그래프와 같이 성능개선이 뚜렷하다. Policy Iteration 처럼 성능이 뚝 떨어지지 않고 안정적으로 유지한다. 또한 그림 C에서 보는 것처럼 오른쪽 그래프인 일반적인 gradient 방법보다 훨씬 빠르게 학습하는 것을 볼 수 있다.</p>\n<p><img src=\"https://www.dropbox.com/s/pr6s2qrqaic0wyj/Screenshot%202018-06-08%2023.40.16.png?dl=1\"></p>\n<h2 id=\"8-Discussion\"><a href=\"#8-Discussion\" class=\"headerlink\" title=\"8. Discussion\"></a>8. Discussion</h2><hr>\n<ul>\n<li>natural gradient method는 policy iteration에서와 같이 greedy action을 선택하도록 학습됌</li>\n<li>line search와 함께 쓰면 natural gradient method는 더 policy iteration 같아짐</li>\n<li>greedy policy iteration에서와는 달리 performance improvement가 보장됌</li>\n<li>하지만 F(Fisher information matrix)가 asymtotically Hessian으로 수렴하지 않음. asymtotically conjugate gradient method(Hessian의 inverse를 approx.로 구하는 방법)가 더 좋아 보일 수 있음</li>\n<li>하지만 Hessian이 항상 informative하지 않고(hessian이 어떤 정보를 주려면 positive definite와 같은 성질을 가져서 해당 함수가 convex인 것을 알 수 있다든지의 경우를 이야기하는데 hessian이 항상 positive definite가 아닐 수 있다는 것이다) tetris에서 봤듯이 natural gradient method가 더 효율적일 수 있음(pushing the policy toward choosing greedy optimal actions)</li>\n<li>conjugate gradient method가 좀 더 maximum에 빠르게 수렴하지만, performance는 maximum에서 거의 안변하므로 좋다고 말하기 어려움(?). 이 부분에 대해서 추가적인 연구 필요.</li>\n</ul>\n<p>amet, consectetur adipisicing elit. Vitae ipsum, voluptatem quis officiis inventore dolor totam deserunt, possimus similique eum, accusantium adipisci doloremque omnis excepturi quasi, suscipit repellendus quibusdam? Veritatis.</p>\n"},{"title":"trpo","date":"2018-07-12T07:53:12.000Z","author":"공민서, 김동민","subtitle":"TRPO 여행하기","_content":"Trust Region Policy Optimization\n========================\nAuthors: John Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, Pieter Abbeel    \nProceeding: International Conference on Machine Learning (ICML) 2015\n\n정리 by 공민서, 김동민\n\n\n# 1. 들어가며...\n\nTrust region policy optimization (TRPO)는 상당히 우수한 성능을 보여주는 policy gradient 기법으로 알려져 있습니다. 높은 차원의 action space를 가진 robot locomotion부터 action은 적지만 화면을 그대로 처리하여 플레이하기 때문에 control parameter가 매우 많은 Atari game까지 각 application에 세부적으로 hyperparameter들을 특화시키지 않아도 두루두루 좋은 성능을 나타내기 때문에 일반화성능이 매우 좋은 기법입니다. 이 TRPO에 대해서 알아보겠습니다.\n\n※TRPO를 매우 재밌게 설명한 Crazymuse AI의 [Youtube video](https://www.youtube.com/watch?v=CKaN5PgkSBc&t=90s)에서 일부 그림을 차용했습니다. 이 비디오를 시청하시는 것을 강력하게 추천합니다!\n\n<br><br>\n## 1.1. TRPO 흐름 잡기\n\nTRPO 논문은 많은 수식이 등장하여 이 수식들을 따라가다보면 큰 그림을 놓칠 수도 있습니다. 세부적인 내용을 살펴보기 전에 기존 연구에서 출발해서 TRPO로 어떻게 발전해나가는지 간략하게 살펴보겠습니다.\n\n### 1.1.1. Original Problem\n\n$$\\max\\_\\pi \\eta(\\pi)$$\n\n모든 강화학습이 그렇듯이 expected discounted reward를 최대화하는 policy를 찾는 문제로부터 출발합니다.\n\n### 1.1.2. Conservative Policy Iteration\n\n$$\\max L\\_\\pi(\\tilde\\pi) = \\eta(\\pi) + \\sum\\_s \\rho_\\pi(s)\\sum\\_a\\tilde\\pi(a\\vert s)A\\_\\pi(s,a)$$\n\n$\\eta(\\pi)$를 바로 최대화하는 것은 많은 경우 어렵습니다. $\\eta(\\pi)$의 성능향상을 보장하면서 policy를 update하는 conservative policy iteration 기법이 [Kakade와 Langford](http://www.cs.cmu.edu/~./jcl/papers/aoarl/Final.pdf)에 의하여 제안되었습니다. 이 기법을 이용하면 policy update가 성능을 악화시키지는 않는다는 것이 이론적으로 보장됩니다.\n\n### 1.1.3. Theorem 1 of TRPO\n\n$$\\max L\\_{\\pi\\_\\mathrm{old} }\\left(\\pi\\_\\mathrm{new}\\right) - \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2,\\quad\\left(\\alpha=D\\_\\mathrm{TV}^\\max\\left(\\pi\\_\\mathrm{old},\\pi\\_\\mathrm{new}\\right)\\right)$$\n\n기존의 conservative policy iteration은 과거 policy와 새로운 policy를 섞어서 사용해서 실용적이지 않다는 단점이 있었는데 이것을 보완하여 온전히 새로운 policy만으로 update할 수 있는 기법을 제안합니다. \n\n### 1.1.4. KL divergence version of Theorem 1\n\n$$\\max L\\_{\\pi}\\left(\\tilde\\pi\\right) - C\\cdot D_\\mathrm{KL}^\\max\\left(\\pi,\\tilde\\pi\\right),\\quad\\left(C = \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\right)$$\n\ndistance metric을 KL divergence로 바꿀 수 있습니다.\n\n### 1.1.5. Using parameterized policy\n\n$$\\max\\_\\theta L\\_{\\theta\\_\\mathrm{old} }(\\theta) - C\\cdot D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right)$$\n\n최적화문제를 더욱 편리하게 풀 수 있도록 낮은 dimension을 가지는 parameter들로 parameterized된 policy를 사용할 수 있습니다.\n\n### 1.1.6. Trust region constraint\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ }&D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right) \\leq \\delta\n\\end{align}$$\n\n5까지는 아직 conservative policy iteration을 약간 변형시킨 것입니다. policy를 update할 때 지나치게 많이 변하는 것을 방지하기 위하여 [trust region](https://en.wikipedia.org/wiki/Trust_region)을 constraint로 설정할 수 있습니다. 이 아이디어로 인해서 TRPO라는 명칭을 가지게 됩니다.\n\n### 1.1.7. Heuristic approximation\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ }&\\overline{D}\\_\\mathrm{KL}^{\\rho\\_\\mathrm{old} }\\left(\\theta\\_\\mathrm{old}, \\theta\\right) \\leq \\delta\n\\end{align}$$\n\n사실 6의 constraint는 모든 state에 대해서 성립해야 하기 때문에 문제를 매우 어렵게 만듭니다. 이것을 좀 더 다루기 쉽게 state distribution에 대한 평균을 취한 것으로 변형합니다.\n\n### 1.1.8. Monte Carlo Simulation\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi\\_\\theta(a\\vert s)}{q(a\\vert s)}Q\\_{\\theta\\_\\mathrm{old} }(s,a)\\right] \\\\\\\\\n\\mathrm{s.t.\\ }&E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} }}\\left[D\\_\\mathrm{KL}\\left(\\pi\\_{\\theta\\_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi\\_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta\n\\end{align}$$\n\nSampling을 통한 계산이 가능하도록 식을 다시 표현할 수 있습니다. \n\n### 1.1.9. Efficiently solving TRPO\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &\\nabla\\_\\theta \\left. L\\_{\\theta\\_\\mathrm{old} }(\\theta)\\right\\vert \\_{\\theta=\\theta\\_\\mathrm{old} } \\left(\\theta - \\theta\\_\\mathrm{old}\\right) \\\\\\\\\n\\mathrm{s.t.\\ }&\\frac{1}{2}\\left(\\theta\\_\\mathrm{old} - \\theta \\right)^T A\\left(\\theta\\_\\mathrm{old}\\right)\\left(\\theta\\_\\mathrm{old} - \\theta \\right) \\leq \\delta \\\\\\\\\n&A\\_{ij} = \\frac{\\partial}{\\partial \\theta\\_i}\\frac{\\partial}{\\partial \\theta\\_j}\\overline{D}\\_\\mathrm{KL}^{\\rho\\_\\mathrm{old} }\\left(\\theta\\_\\mathrm{old}, \\theta\\right)\n\\end{align}$$\n\n문제를 효율적으로 풀기 위하여 approximation을 적용할 수 있습니다. objective function은 first order approximation, constraint는 quadratic approximation을 취하면 효율적으로 문제를 풀 수 있는 형태로 바뀌는데 이것을 natural gradient로 풀 수도 있고 conjugate gradient로 풀 수도 있습니다.\n\nTRPO는 이렇게 다양한 방법으로 문제를 변형한 것입니다! 이제 좀 더 자세히 살펴보겠습니다.\n\n<br><br>\n# 2. Preliminaries\n\n다음과 같은 파라미터를 가지는 infinite-horizon discounted Markov decision process (MDP)를 고려합니다. \n\n* $\\mathcal{S}$: finite set of states\n* $\\\\mathcal{A}$: finite set of actions\n* $P: \\mathcal{S}\\times\\mathcal{A}\\rightarrow\\mathbb{R}$: transition probability distribution\n* $r: \\mathcal{S}\\rightarrow\\mathbb{R}$: reward function\n* $\\rho\\_0:\\mathcal{S}\\rightarrow\\mathbb{R}$: distribution of the initial state $s\\_0$\n* $\\gamma\\in(0,1)$: discount factor\n* $\\eta(\\pi)=E\\_{s\\_0,a_0,\\ldots}\\left[\\sum\\_{t=0}^\\infty\\gamma^t r\\left(s\\_t\\right)\\right]$, where $s\\_0\\sim\\rho\\_0\\left(s\\_0\\right), a\\_t\\sim\\pi\\left(\\left.a\\_t \\right\\vert s\\_t\\right), s\\_{t+1}\\sim P\\left(\\left.s\\_{t+1}\\right\\vert s\\_t,a\\_t\\right)$\n* $Q\\_\\pi\\left(s\\_t,a\\_t\\right)=E\\_{\\color{red}s\\_{t+1},a\\_{t+1},\\ldots}\\left[\\sum\\_{l=0}^\\infty \\gamma^l r\\left(s\\_{t+l}\\right)\\right]$: action value function\n* $V\\_\\pi\\left(s\\_t\\right)=E\\_{\\color{red}a\\_t, s\\_{t+1},a\\_{t+1},\\ldots}\\left[\\sum\\_{l=0}^\\infty \\gamma^l r\\left(s\\_{t+l}\\right)\\right]$: value function (action value function과 expectation의 첨자가 다른 부분을 유의하세요.)\n* $A\\_\\pi(s,a) = Q\\_\\pi(s,a) - V\\_\\pi(s)$: advantage function\n\n## 2.1 Useful identity [Kakade & Langford 2002]\n\n우리의 목표는 $\\eta\\(\\pi\\)$가 최대화되도록 만드는 것입니다. 하지만 $\\pi$의 변화에 따라 $\\eta$가 어떻게 변하는지 알아내는 것도 쉽지 않습니다. $\\pi$는 기존의 policy이고 $\\tilde\\pi$는 새로운 policy를 나타낸다고 할 때 $\\eta$와 policy update 사이의 관계에 대해서 다음과 같은 관계가 있다는 것이 밝혀졌습니다.\n\n**Lemma 1.** $\\eta\\left(\\tilde\\pi\\right) = \\eta(\\pi) + E\\_{s\\_{0},a\\_{0},\\ldots\\sim\\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t A\\_\\pi\\left(s\\_t,a\\_t\\right)\\right]$\n\n*Proof.* $A\\_\\pi(s,a)=E\\_{s'\\sim P\\left(s'\\vert s,a\\right)}\\left[r(s)+\\gamma V\\_\\pi\\left(s'\\right)-V\\_\\pi(s)\\right]$로 다시 표현할 수 있습니다. 표기의 편의를 위하여 $\\tau:=\\left(s\\_0,a\\_0,s\\_1,a\\_1,\\ldots\\right)$를 정의하겠습니다. 다음과 같은 수식 전개가 가능합니다.\n$$\n\\begin{align}\n&E\\_{\\tau\\vert \\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t A\\_\\pi\\left(s\\_t,a\\_t\\right)\\right] \\\\\\\\\n &= E\\_{\\tau\\vert \\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t \\left(r(s\\_t)+\\gamma V\\_\\pi\\left(s\\_{t+1}\\right)-V\\_\\pi(s\\_t)\\right)\\right] \\\\\\\\\n&= E\\_{\\tau\\vert \\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t r(s\\_t)+\\gamma V\\_\\pi\\left(s\\_{1}\\right)-V\\_\\pi(s\\_0)+\\gamma^2 V\\_\\pi\\left(s\\_{2}\\right)-\\gamma V\\_\\pi(s\\_1)+\\gamma^3 V\\_\\pi\\left(s\\_{3}\\right)-\\gamma^2 V\\_\\pi(s\\_2)+\\cdots\\right] \\\\\\\\\n&= E\\_{\\tau\\vert \\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t r(s\\_t)+\\cancel{\\gamma V\\_\\pi\\left(s\\_{1}\\right)}-V\\_\\pi(s\\_0)+\\cancel{\\gamma^2 V\\_\\pi\\left(s\\_{2}\\right)}-\\cancel{\\gamma V\\_\\pi(s\\_1)}+\\cdots\\right] \\\\\\\\\n&= E\\_{\\tau\\vert \\tilde\\pi}\\left[-V\\_\\pi(s\\_0)+\\sum\\_{t=0}^\\infty\\gamma^t r(s\\_t)\\right] \\\\\\\\\n&\\overset{\\underset{\\mathrm{(a)} }{} }{=} -E\\_{\\color{red}s\\_0}\\left[V\\_\\pi(s\\_0)\\right]+E\\_{\\tau\\vert \\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t r(s\\_t)\\right] \\\\\\\\\n&=-\\eta(\\pi) + \\eta\\left(\\tilde\\pi\\right)\\\\\\\\\n&\\quad\\therefore \\eta\\left(\\tilde\\pi\\right) = \\eta(\\pi) + E\\_{s\\_{0},a\\_{0},\\ldots\\sim\\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t A\\_\\pi\\left(s\\_t,a\\_t\\right)\\right]\n\\end{align}\n$$\n위의 전개의 (a) 부분은 $V\\_\\pi\\left(s\\_0\\right)=E\\_{a\\_0,s\\_0,a\\_1,\\ldots}\\left[\\sum\\_{t=0}^\\infty\\gamma^l r\\left(s\\_l\\right)\\right]$이므로 $\\tau$ 중 많은 부분들이 이미 expectation이 취해진 값이므로 무시됩니다. 오직 $s\\_0$만 expectation이 취해지지 않았기 때문에 남아있습니다. $\\square$\n\n**Lemma 1**은 새로운 policy와 기존의 policy 사이의 관계를 규정합니다. 다음을 정의합니다. \n\n* $\\rho\\_\\pi(s) = P\\left(s\\_0=s\\right) + \\gamma P\\left(s\\_1=s\\right)\\ + \\gamma^2 P\\left(s\\_2=s\\right) + \\cdots $: (unnormalized) discounted visitation frequencies\n\n이것을 이용해서 **Lemma 1**을 변형시켜 봅시다.\n\n$$\n\\begin{align}\n\\eta\\left(\\tilde\\pi\\right) &= \\eta(\\pi) + \\sum\\_{t=0}^\\infty\\sum\\_{s}P\\left(\\left.s\\_t=s\\right\\vert \\tilde\\pi\\right)\\sum\\_a\\tilde\\pi(a\\vert s)\\gamma^t A\\_\\pi(s,a) \\\\\\\\\n&= \\eta(\\pi) + \\sum\\_{s}{\\color{red}\\sum\\_{t=0}^\\infty \\gamma^t P\\left(\\left.s\\_t=s\\right\\vert \\tilde\\pi\\right)}\\sum\\_a\\tilde\\pi(a\\vert s) A\\_\\pi(s,a) \\\\\\\\\n&= \\eta(\\pi) + \\sum\\_{s}{\\color{red}\\rho\\_\\tilde\\pi(s)}\\sum\\_a\\tilde\\pi(a\\vert s) A\\_\\pi(s,a) \\\\\\\\\n\\end{align}\n$$\n\n이 수식의 의미가 무엇일까요? 만약 $\\sum\\_a\\tilde\\pi(a\\vert s) A\\_\\pi(s,a) \\geq 0$이라면 $\\eta(\\tilde\\pi)$는 항상 $\\eta(\\pi)$보다 큽니다. 즉, policy를 업데이트함으로써 항상 개선됩니다. 즉, 이 수식을 통해서 항상 더 좋은 성능을 내는 policy update가 가져야 할 특징을 알 수 있습니다. 다음과 같은 deterministic policy가 있다고 합시다.\n\n$$\n\\tilde\\pi(s) = \\arg\\max_a A\\_\\pi(s,a)\n$$\n\n이 policy는 적어도 하나의 state-action pair에서 0보다 큰 값을 가지는 advantage가 있고 그 때의 확률이 0이 아니라면 항상 성능을 개선시킵니다. 어려운 점은 policy를 바꾸면 $\\rho$도 바뀐다는 점입니다. \n\n다음 그림을 봅시다. Starting Point에서 여러가지 경로를 거쳐서 Destination으로 갈 수 있습니다. 다른 policy를 이용하는 것은 다른 경로를 이용한 것입니다.\n\n![policy_change](../img/policy_change.png)\n\n이 때 다른 policy를 이용함에 따라 state visitation frequency도 변하게 됩니다.\n\n![state_visitation_change](../img/state_visitation_change.png)\n\n이로 인해서 policy를 최적화하는 것은 어려워집니다. 그래서 이러한 변화를 무시하는 다음과 같은 approximation을 취합니다.\n\n$$\n\\begin{align}\nL_\\pi\\left(\\tilde\\pi\\right) &= \\eta(\\pi) + \\sum\\_{s}{\\color{red}\\rho\\_\\pi(s)}\\sum\\_a\\tilde\\pi(a\\vert s) A\\_\\pi(s,a)\n\\end{align}\n$$\n\npolicy가 바뀌었음에도 이전의 state distribution을 게속 이용하는 것입니다. 이것은 policy의 변화가 크지 않다면 어느 정도 허용될 수 있을 것입니다. 그렇지만 얼마나 많은 변화가 허용될까요? 이것을 정하기 위해서 이용하는 것이 trust region입니다.\n\n<br>\n## 2.1. Conservative Policy Iteration\n\npolicy의 변화를 다루기 용이하게 하기 위해서 policy를 다음과 같이 파라미터를 이용해서 표현합시다.\n\n* $\\pi\\_\\theta(a\\vert s)$: parameterized policy\n\n$\\pi\\_\\theta$는 $\\theta$에 대하여 미분가능한 함수입니다. $L\\_\\pi\\left(\\tilde\\pi\\right)$은 $\\theta\\_0$에서 $\\eta(\\pi)$에 대한 first order approximation입니다.\n\n$$\n\\begin{align}\nL\\_{\\pi\\_{\\theta\\_0} }\\left(\\pi\\_{\\theta_0}\\right) &= \\eta\\left(\\pi\\_{\\theta\\_0}\\right) \\\\\\\\\n\\nabla\\_\\theta \\left.L\\_{\\pi\\_{\\theta\\_0} }\\left(\\pi\\_{\\theta\\_0}\\right)\\right\\vert \\_{\\theta=\\theta\\_0} &= \\nabla\\_\\theta\\left.\\eta(\\pi\\_{\\theta\\_0})\\right\\vert \\_\\theta=\\theta\\_0\n\\end{align}\n$$\n\n이것의 의미는 $\\pi\\_{\\theta\\_0}$가 매우 작게 변한다면 $L\\_{\\pi\\_{\\theta\\_0} }$를 개선시키는 것이 $\\eta$를 개선시키는 것이라는 것입니다. 그러나 지금까지의 설명만으로는 $\\pi\\_{\\theta\\_0}$를 얼마나 작게 변화시켜야 할지에 대해서는 알 수 없습니다.\n\nKakade & Langford가 2002년 발표한 논문에서 이것을 고려하였습니다. 그 논문에서 *conservative policy iteration*이라는 기법을 제안합니다. 이 논문의 contribution은 다음과 같습니다.\n\n* $\\eta$ 개선의 lower bound를 제공\n* 기존의 policy를 $\\pi\\_\\mathrm{old}$라고 하고 $\\pi'$를 $\\pi'=\\arg\\max\\_\\{\\pi'}L\\_{\\pi\\_\\mathrm{old} }\\left(\\pi'\\right)$과 같이 정의할 때, 새로운 mixture policy $\\pi\\_\\mathrm{new}$를 다음과 같이 제안\n$$\n\\pi\\_\\mathrm{new}(a\\vert s) = (1-\\alpha)\\pi\\_\\mathrm{old}(a\\vert s) + \\alpha \\pi'(a\\vert s)\n$$\n\n그림으로 표현하면 다음과 같습니다.\n\n![mixure_policy](../img/mixure_policy.png)\n\n* 다음과 같은 lower bound를 정의\n$$\n\\eta\\left(\\pi\\_\\mathrm{new}\\right) \\geq L\\_{\\pi\\_{\\theta\\_\\mathrm{old} }}\\left(\\pi\\_\\mathrm{new}\\right) - \\frac{2\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2,\\\\\\\\\n\\mathrm{where\\ }\\epsilon = \\max_s\\left\\vert E\\_{a\\sim\\pi'(a\\vert s)}\\left[A\\_\\pi(s,a)\\right]\\right\\vert \n$$\n\n하지만 mixture된 policy라는 것은 실용적이지 않습니다. \n\n<br><br>\n# 3. Monotonic Improvement Guarantee for General Stochastic Policies\n\n이전 장에서 설명한 lower bound는 오직 mixture policy에 대해서만 성립하고 보다 더 많이 사용되는 stochastic policy에는 적용되지 않습니다. 따라서 이러한 방향으로 개선이 필요합니다. 기존 수식의 두 가지를 바꿈으로써 이것을 달성할 수 있습니다.\n\n* $\\alpha\\rightarrow$ distance measure between $\\pi$ and $\\tilde\\pi$\n* constant $\\epsilon\\rightarrow\\max\\_{s,a}\\left\\vert A\\_\\pi(s,a)\\right\\vert $\n\n여기서 distance measure로 total variation divergence를 이용합니다. discrete porbability distribution $p$와 $q$에 대하여 다음과 같이 정의됩니다.\n$$\nD\\_\\mathrm{TV}(p\\parallel q) = \\frac{1}{2}\\sum\\_i\\left\\vert p\\_i - q\\_i\\right\\vert \n$$\n\n이것을 이용하여 $D\\_\\mathrm{TV}^\\max$를 다음과 같이 정의하겠습니다.\n$$\nD\\_\\mathrm{TV}^\\max(\\pi\\parallel \\tilde\\pi) = \\max_s D\\_\\mathrm{TV}\\left(\\pi(\\cdot\\vert s)\\parallel\\tilde\\pi(\\cdot\\vert s)\\right)\n$$\n\n그림으로 표현하면 다음과 같습니다.\n\n![tvd](../img/tvd.png)\n\n이것을 이용하여 다음과 같은 관계식을 얻을 수 있습니다.\n\n**Theorem 1.** Let $\\alpha=D\\_\\mathrm{TV}^\\max(\\pi\\parallel \\tilde\\pi)$. Then the following bound holds:\n$$\n\\begin{align}\n\\eta\\left(\\pi\\_\\mathrm{new}\\right) &\\geq L\\_{\\pi\\_\\mathrm{old} }\\left(\\pi\\_\\mathrm{new}\\right) - \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2, \\\\\\\\\n\\mathrm{where\\ } \\epsilon&= \\max\\_{s,a}\\left\\vert A\\_\\pi(s,a)\\right\\vert \n\\end{align}\n$$\n*Proof.* TBD.\n\n또다른 distance metric으로 아래 그림과 같은 KL divergence가 있습니다.\n(그런데 왜 하필 KL divergence로 바꿀까요? 논문의 뒤쪽에서 계산효율을 위해서 conjugate gradient method를 이용하는데 이를 위해서 바꾼게 아닐까 싶습니다. Wasserstein distance 같은 다른 방향으로 발전시킬 수도 있을 것 같습니다. Schulmann이 아마 해봤겠죠?^^)\n\n![kld](../img/kld.png)\n\ntotal variation divergence와 KL divergence 사이에는 다음과 같은 관계가 있습니다.\n$$\nD\\_\\mathrm{TV}(p\\parallel q)^2 \\leq D\\_\\mathrm{KL}(p\\parallel q)\n$$\n\n다음 수식을 정의합니다.\n$$\nD\\_\\mathrm{KL}^\\max(\\pi\\parallel \\tilde\\pi) = \\max_s D\\_\\mathrm{KL}\\left(\\pi(\\cdot\\vert s)\\parallel\\tilde\\pi(\\cdot\\vert s)\\right)\n$$\n\n**Theorem 1**을 이용하여 다음과 같은 수식이 성립함을 알 수 있습니다.\n$$\n\\begin{align}\n\\eta\\left(\\tilde\\pi\\right) &\\geq L\\_{\\pi}\\left(\\tilde\\pi\\right) - C\\cdot D\\_\\mathrm{KL}^\\max(\\pi, \\tilde\\pi), \\\\\\\\\n\\mathrm{where\\ } C&= \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\n\\end{align}\n$$\n\n이러한 policy imporvement bound를 기반으로 다음과 같은 approximate policy iteration 알고리듬을 고안해낼 수 있습니다.\n\n**Algorithm 1** Policy iteration algorithm guaranteeing non-decreasing expected return $\\eta$\n> Initialize $\\pi\\_0$   \n> **for** $i=0,1,2,\\ldots$ until convergence **do**   \n> $\\quad$Compute all advantage values $A\\_{\\pi\\_i}(s,a)$    \n> $\\quad$Solve the contstrained optimization problem    \n> $\\quad\\pi\\_{i+1}=\\arg\\max\\_\\pi\\left[L\\_{\\pi\\_i} - CD\\_\\mathrm{KL}^\\max\\left(\\pi\\_i,\\pi\\right)\\right]$     \n> $\\quad\\quad$where $C = 4\\epsilon\\gamma / (1-\\gamma)^2$    \n> $\\quad\\quad$and $L\\_{\\pi\\_i}\\left(\\pi\\right) = \\eta(\\pi\\_i) + \\sum\\_{s}\\rho\\_{\\pi\\_i}(s)\\sum\\_a\\pi(a\\vert s) A\\_{\\pi\\_i}(s,a)$    \n> **end for**\n\n**Algorithm 1**은 advantage를 정확하게 계산할 수 있다고 가정하고 있습니다. 이 알고리듬은  monotonical한 성능 증가($\\eta(\\pi\\_0)\\leq\\eta(\\pi\\_1)\\leq\\cdots$)를 한다는 것을 다음과 같이 보일 수 있습니다. $M\\_i(\\pi)=L\\_{\\pi\\_i}(\\pi) - CD\\_\\mathrm{KL}^\\max\\left(\\pi\\_i,\\pi\\right)$라고 합시다.\n$$\\begin{align}\n\\eta\\_\\left(\\pi\\_{i+1}\\right) &\\geq M\\_i\\left(\\pi\\_{i+1}\\right)\\\\\\\\\n\\eta\\_\\left(\\pi\\_{i}\\right) &= M\\_i\\left(\\pi\\_{i}\\right) {\\color{blue}\\mathrm{\\ (why\\ same?)} }\\\\\\\\\n\\eta\\_\\left(\\pi\\_{i+1}\\right) - \\eta\\_\\left(\\pi\\_{i}\\right) &\\geq M\\_i\\left(\\pi\\_{i+1}\\right) - M\\_i\\left(\\pi\\_{i}\\right)\n\\end{align}$$\n\n위 수식과 같이 매 iteration 마다 $M\\_i$를 최대화함으로써, $\\eta$가 감소하지 않는다는 것을 보장할 수 있습니다. 이와 같은 타입의 알고리듬을 minorization-maximization (MM) algorithm이라고 합니다. EM 알고리듬도 이 타입의 알고리듬 중 하나입니다.\n\n**Algorithm 1**은 아래 그림과 같이 동작합니다.\n\n![surrogate](../img/surrogate.png)\n\n\n$M\\_i$는 $\\pi\\_i$일 때 equality가 되는 $\\eta$에 대한  surrogate function입니다. TRPO는 이 surrogate function을 최대화하고 KL divergence를 penalty가 아닌 constraint로 두는 알고리듬입니다.\n\n<br><br>\n# 4. Optimization of Parameterized Policies\n\n표기의 편의를 위해 다음과 같이 notation들을 정의합니다.\n\n* $\\eta(\\theta):=\\eta\\left(\\pi\\_\\theta\\right)$\n* $L\\_\\theta\\left(\\tilde\\theta\\right):=L\\_{\\pi\\_\\theta}\\left(\\pi\\_{\\tilde\\theta}\\right)$\n* $D\\_\\mathrm{KL}\\left(\\theta\\parallel\\tilde\\theta\\right):=D\\_\\mathrm{KL}\\left(\\pi\\_\\theta\\parallel\\pi_{\\tilde\\theta}\\right)$\n* $\\theta\\_\\mathrm{old}$: previous policy parameter\n\n## 4.1. Trust Region Policy Optimization\n\n이전 장의 중요 결과를 위의 notation으로 다시 표기하면 아래와 같습니다.\n\n$$\n\\eta\\left(\\theta\\right) \\geq L\\_{\\theta\\_\\mathrm{old} }\\left(\\theta\\right) - C\\cdot D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right)\n$$\n\n$\\eta$의 성능 향상을 보장하기 위해서 그것의 lower bound를 최대화할 수 있습니다.\n\n$$\n\\max\\_\\theta \\left[L\\_{\\theta\\_\\mathrm{old} }\\left(\\theta\\right) - C D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right)\\right]\n$$\n\n이 최적화 문제는 step size를 매우 작게 해야 올바른 동작을 합니다. 위에서 살펴봤듯이 first order approximation이기 때문입니다. 좀 더 큰 step size를 가질 수 있도록 이 최적화 문제를 trust region constraint를 도입하여 다음과 같이 바꿉니다.\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ }&D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right) \\leq \\delta\n\\end{align}$$\n\n이 최적화문제의 constraint는 모든 state space에 대해서 성립해야 합니다. 또한 maximum값을 매번 찾아야 합니다. state가 많은 경우 constraint의 수가 매우 많아지고 이것은 문제를 풀기 어렵게 만듭니다. constraint의 수를 줄이기 위하여 다음과 같은 avergae KL divergence를 이용하는 heuristic approximation을 취합니다. 이것이 최선의 방법은 아닐 수 있지만 실용적인 방법입니다.\n\n$$\n\\overline D\\_\\mathrm{KL}^{\\rho}\\left(\\theta\\_1, \\theta\\_2\\right):=E\\_{s\\sim\\rho}\\left[D\\_\\mathrm{KL}\\left(\\pi\\_{\\theta_1}(\\cdot\\vert s)\\parallel\\pi\\_{\\theta\\_2}(\\cdot\\vert s)\\right)\\right]\n$$\n\n이것을 기반으로 다음과 같은 최적화 문제를 풀 수 있습니다.\n\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ }&\\overline{D}\\_\\mathrm{KL}^{\\rho\\_\\mathrm{old} }\\left(\\theta\\_\\mathrm{old}, \\theta\\right) \\leq \\delta\n\\end{align}$$\n\n\n아래 그림처럼 step size에 대해서 고려할 수 있습니다.\n\n![heuristic_approx](../img/heuristic_approx.png)\n\n<br><br>\n# 5. Sample-Based Estimation of the Objective and Constraint\n\n실용적인 알고리듬을 만들려고 하는 노력은 아직 끝나지 않았습니다. 이제 앞의 알고리듬을 sample-based estimation 즉, Monte Carlo estimation을 할 수 있도록 바꿔보겠습니다. sampling을 편하게 할 수 있도록 아래와 같이 바꿔줍니다.\n\n* $\\sum\\_s \\rho\\_{\\theta\\_\\mathrm{old} }(s)[\\ldots]\\rightarrow\\frac{1}{1-\\gamma}E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} }}[\\ldots]$\n* $A\\_{\\theta\\_\\mathrm{old} }\\rightarrow Q\\_{\\theta\\_\\mathrm{old} }$\n* $\\sum\\_a\\pi\\_{\\theta\\_\\mathrm{old} }(a\\vert s)A\\_{\\theta\\_\\mathrm{old} }\\rightarrowE\\_{a\\sim q}\\left[\\frac{\\pi\\_\\theta(a\\vert s)}{q(a\\vert s)}A\\_{\\theta\\_\\mathrm{old} }\\right]$\n\n이러한 변화를 그림처럼 도식화할 수 있습니다.\n\n![sample-based](../img/sample-based.png)\n\n한 가지 짚고 넘어가야 할 점은 action sampling을 할 때 importance sampling을 사용한다는 것입니다.\n\n![importance_sampling](../img/importance_sampling.png)\n\n바뀐 최적화문제는 아래와 같습니다.\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi\\_\\theta(a\\vert s)}{q(a\\vert s)}Q\\_{\\theta\\_\\mathrm{old} }(s,a)\\right] \\\\\\\\\n\\mathrm{s.t.\\ }&E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} }}\\left[D\\_\\mathrm{KL}\\left(\\pi\\_{\\theta\\_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi\\_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta\n\\end{align}$$\n\n이 때 sampling하는 두 가지 방법이 있습니다.\n\n<br>\n## 5.1. Single Path\n\n*single path*는 개별 trajectory들을 이용하는 방법입니다.\n\n![single](../img/single.png)\n\n<br>\n## 5.2. Vine\n\n*vine*은 한 state에서 rollout을 이용하여 여러 action을 수행하는 방법입니다.\n\n![vine1](../img/vine1.png)\n\n![vine2](../img/vine2.png)\n\nestimation의 variance를 낮출 수 있지만 계산량이 많고 한 state에서 여러 action을 수행할 수 있어야 하기 때문에 현실적인 문제에 적용하기에는 어려움이 있습니다.\n\n\n# Trust Region Policy Optimization\n\n# Conjugate Gradient Method\n![](https://datascienceschool.net/upfiles/a5ba6251b6f144249cca6eb8cc523682.png)\n\n테일러 급수에 대하여 : [naver](https://terms.naver.com/entry.nhn?docId=3572065&cid=58944&categoryId=58970) , [wikipedia](https://ko.wikipedia.org/wiki/%ED%85%8C%EC%9D%BC%EB%9F%AC_%EC%A0%95%EB%A6%AC)\n\n: 근사 다항식을 구하기위해.\n\n$\\large f(x) = \\frac{f(a)}{0!} + \\frac{f'(a)}{1!}(x-a)^1 + \\frac{f''(a)}{2!}(x-a)^2 + ... + R_{n+1}(x)$\n\n$\\large = \\sum_{k=0}^{n} \\frac{f^{(k)}(a)}{k!}(x-a)^k + R_{n+1}(x)$\n\n## 5. Sample-based Estimation of the Objective and Constraint\n\n이전 장에서, 각 업데이트 시에 정책 변화를 제약하면서 기대누적보상 $\\large \\eta$ 의 추정치를 최적화하는\n\n정책 파라미터의 제약조건이 있는 최적화 문제를 제안했다. - eq.(12)\n\n#### Equation 12\n\n$\\LARGE \\underset{\\theta}{maximize} \\ L_{\\theta_{old} }(\\theta)$\n\n$\\LARGE subject \\ to \\ \\bar{D}_{KL}^{\\rho_{\\theta_{old} }} (\\theta_{old}, \\theta) \\leq \\delta$\n\n<br/>\n\n이번 5장에선, **어떻게 목적함수와 제약식이 몬테카를로 시뮬레이션을 통해 근사되는지** 설명한다.\n\n## 5. Sample-based Estimation of the Objective and Constraint\n\neq. (12)의 $\\large L_{\\theta_{old}(\\theta)}$ 를 확장하면 다음의 식을 얻을 수 있다.\n\n**Equation 13**\n\n$\\LARGE \\underset{\\theta}{maximize} \\ \\sum_s \\rho_{\\theta_{old} }(s) \\sum_a \\pi_{\\theta}(a\\vert s) A_{\\theta_{old} }(s, a)$\n\n$\\LARGE subject \\ to \\ \\bar{D}_{KL}^{\\rho_{\\theta_{old} }} (\\theta_{old}, \\theta) \\leq \\delta$\n\n\n여기서...\n\n1) $\\LARGE \\sum_s \\rho_{\\theta_{old} }(s)$  를 $\\LARGE \\frac{1}{1-\\gamma} E_{s \\sim \\rho_{\\theta_{old} }} $ 로 대체.\n\n** 어디서 $\\large \\frac{1}{1-\\gamma}$ 가 나왔을까? Kakade & LangFord 2002 **\n\n\n-\\vert  -\n:---------------:\\vert :---------------:\n![](https://i.imgur.com/uo0zOqd.png) \\vert  ![](https://i.imgur.com/PeG5gNp.png)\\vert \n\n\n2) $\\large A_{\\theta_{old} }$ 을 $\\large Q_{\\theta_{old} }$ 로 대체.\n\n\n3) 액션의 확률합을 importance sampling으로 대체\n\n$\\LARGE \\sum_a \\pi_{\\theta}(a\\vert s_n)A_{\\theta_{old} }(s_n, a) = \\color{Red}{E_{a \\sim q} [\\frac{\\pi_{\\theta}(a\\vert s_n)}{q(a\\vert s_n)} } \\ A_{\\theta_{old} }(s_n, a)]$\n\n\n대체한 결과는\n\n**Equation 14**\n\n$\\LARGE \\underset{\\theta}{maximize} E_{s \\sim \\rho_{\\theta_{old} } \\ , \\ a \\sim q} [\\frac{\\pi_{\\theta}(a\\vert s_n)}{q(a\\vert s_n)}Q_{\\theta_{old} }(s, a)]$\n\n$\\LARGE subject \\ to \\ E_{s \\sim \\rho_{\\theta_{old} }}[D_{KL}(\\pi_{\\theta_{old} })(\\cdot \\vert  s) \\ \\vert \\vert  \\ \\pi_{\\theta}(\\cdot \\vert  s)] \\leq \\delta$\n\n## 5. Sample-based Estimation of the Objective and Constraint\n\nEq. 14에서 Expectation은 샘플 평균으로 구하고, Q value는 실행했을때의 추정값으로 활용한다.\n\n아래는 추정을 수행하는 방식이 다른 2가지 에 대해 설명한다.\n\n### 5.1 Single Path\n\nsingle path 샘플링 방식은 일반적으로 policy gradient estimation에 사용하고\n\n개별적인 trajectory를 샘플링 한다.\n\n여기서 $\\large s_0 \\sim \\rho_{\\theta} \\leftarrow$ (discounted visitation distribution)\n\n상태들의 시퀀스를 모으고, trajectory를 생성하기위해 임의의 타임스텝만큼 policy인 $\\large \\pi_{\\theta_{old} }$ 을 시뮬레이션한다.\n\n$\\large (s_0 \\ , a_0 \\ , s_1 \\ , a_1 ... \\ s_{t-1} \\ , a_{t-1} \\ , s_t  )$\n\n따라서, $\\LARGE q(a\\vert s) = \\pi_{\\theta_{old} }$ 이다.\n\n$\\large Q_{\\theta_{old} }(s, a)$ 는 trajectory의 각 state-action 쌍인 $\\large (s_t, a_t)$ 의 $\\large \\sum \\gamma r(s,a)$ 를 계산한다.\n\n### 5.2 Vine\n\nvine 방식은 roll out set이라는 것을 만들고 roll out set의 각 상태마다 여러 액션을 수행해본다.\n\n추정 과정에서, 처음엔 $\\large s_0 \\sim \\rho_{\\theta}$ 샘플링하고 $\\large \\pi_{\\theta}$ 로 시뮬레이션을 반복해서 몇개의 trajectory들을 생성한다.\n\n다음엔 이 trajectory들 에서 N개의 상태를 부분집합으로 고른다. $\\large s_1, s_2, ... , s_N$ 이걸 roll out set이라 부른다.\n\nroll out set 각 상태 $\\large s_n$에서 $\\large a_{n , k} \\sim q(\\cdot \\vert  s_n)$ 에 대해 K개의 액션을 샘플링한다.\n\n이와 같은 방식은 exploration 측면에 있어 좋은 성능을 보였고, Robotic locomotion 과 Atari game, 즉 Continuous / discrete task 모두 잘 되었다.\n\n각 상태 $\\large s_n$ 에서 샘플링된 각 액션 $\\large a_{n , k}$에서 roll out을 수행하면서 (짧은 trajectory) $\\large \\overset{\\wedge}{Q}_{\\theta_i}(s_n, a_n, k)$ 을 추정한다.\n\n각 K개의 roll out에서 노이즈를 발생하기 위해 random number sequence를 사용한 roll out과 Q value의 차이의 분산을 매우 감소시켰다.\n\n작고 유한한 액션공간에서는 주어진 상태에서 모든 가능한 액션을 rollout을 통해 생성할 수 있었다.\n\n한 상태 $\\large s_n$ 에서 $\\large L_{\\theta_{old} }$ 에 기여하는 식은 다음과 같다.\n\n**Equation 15**\n\n$\\LARGE L_n(\\theta) = \\sum_{k=1}^K \\pi_{\\theta}(a_k\\vert s_n) \\overset{\\wedge}{Q}(s_n, a_k)$\n\n<br/>\n\n상태공간이 크고 continuous 한 상황에서, importance sampling을 활용해 surrogate objective를 추정할 수 있었다.\n\n한 상태 $\\large s_n$ 에서 $\\large L_{\\theta_{old} }$ 을 얻는 self normalized estimator[(Owen, 2013)](http://statweb.stanford.edu/~owen/mc/Ch-var-is.pdf)는 다음과 같다.\n\n**Equation 16**\n\n$\\LARGE L_n(\\theta) = \\frac{\\sum_{k=1}^{K} \\frac{\\pi_{\\theta}(a_{n,k} \\ \\vert  \\ s_n)}{\\pi_{\\theta_{old} }(a_{n,k} \\ \\vert  \\ s_n)} \\overset{\\wedge}{Q}(s_n, a_{n,k})}{\\sum_{k=1}^{K} \\frac{\\pi_{\\theta}(a_{n,k} \\ \\vert  \\ s_n)}{\\pi_{\\theta_{old} }(a_{n,k} \\ \\vert  \\ s_n)} }$\n\n여기서 상태 $\\large s_n$ 에서 K개의 액션을 수행한다고 가정한다. $\\large a_{n,1}, a_{n,2} \\ ... \\ a_{n,k} $\n\n이 self normalized estimator는 Q value를 구하기위해 베이스라인을 사용할 필요성을 없애준다. (Q value에 상수를 더함으로서 그라디언트가 바뀌지않음???)\n\n$\\large s_n \\sim \\rho(\\pi)$ 을 평균을 내줌으로서, $\\large L_{\\theta_{old} }$ estimator와 그라디언트를 얻을 수 있다.\n\n\n\n### 5.2 Vine\n\nvine은 샘플링을 통한 trajectory들이 다양한 포인트(rollout set)에서 여러갈래로 짧은 가지를 치고 있는 것이 마치 덩굴식물 줄기같아서 이름붙였다.\n\nsingle-path 보다 vine의 이점은 surrogate objective에서 같은 숫자의 Q-value 샘플들이 주어졌을때, 목적함수의 추정치가 낮은 분산을 갖는 것이다.\n\n즉, vine 방법은 advantage value의 아주 좋은 추정치를 갖게 해준다.\n\nvine의 불리한 부분은 반드시 이 advantage 추정 각각을 위해 시뮬레이터가 매우 많은 요청을 수행해야하는 것이다.\n\n게다가, vine은  rollout set 내부의 각 상태에서 다수의 trajectory들을 생성해야하는데 이것은 시스템이 임의의 상태로 초기화 될수 있는 환경으로 제한해버린다.\n\n반면, single path는 상태 초기화도 없고, 실세계로 직접 구현 가능하다.\n\n<br>\n## 6. Practical Algorithm\n\n앞서 Single-path, Vine 샘플링을 사용하는 두가지 방식의 policy optimization 알고리즘을 선보였다.\n\n반복적으로 아래의 과정을 수행한다.\n\n1) Q-values의 몬테카를로 추정과 state-action 쌍 집합을 single path / vine 과정을 통해 모은다.\n\n2) 샘플 평균으로, 목적함수와 eq.(14)의 제약식을 추정한다.  \neq.(14) : <br/>\n\n$\\LARGE \\underset{\\theta}{maximize} E_{s \\ \\sim \\ \\rho_{\\theta_{old} } \\ \\text{,} \\ a \\ \\sim \\ q} [\\frac{\\pi_{\\theta}(a\\vert s)}{q(a\\vert s)}Q_{\\theta_{old} }(s, a)]$\n\n$\\large subject \\ to \\ E_{s \\ \\sim \\ \\rho_{\\theta_{old} }} [D_{KL}(\\pi_{\\theta_{old} }(\\cdot \\vert  s) \\ \\vert \\vert  \\ \\pi_{\\theta}(\\cdot\\vert s))] \\leq \\delta$\n\n3) policy parameter vector인 $\\theta$를 업데이트 하면서 제약조건이 있는 최적화 문제를 근사적으로 푼다.\n\n본 논문에서는 1차미분보다는 좀 계산량이 있는 line search와 Conjugate Gradient algorithm을 사용했다.\n\n3)에 대해서,  그라디언트의 covariance matrix를 사용하지않고 KL Divergence의 헤시안을 해석적으로 계산해서  Fisher Information Matrix를 구성했다.\n\n다시말해서\n\n$\\LARGE \\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial}{\\partial \\theta_i} log \\pi_{\\theta}(a_n\\vert s_n) \\frac{\\partial}{\\partial \\theta_j} log \\pi_{\\theta}(a_n\\vert s_n)$\n\n$\\ \\ \\ \\ \\ \\ \\ \\ \\downarrow$\n\n$\\LARGE \\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_j} D_{KL}(\\pi_{\\theta_{old} }(\\cdot \\vert  s_n) \\vert \\vert  \\pi_{\\theta}(\\cdot \\vert  s_n))$\n\n**TODO KL divergence 미분과정**\n\n**\\TODO # 왜 KL divergence의 헤시안을 계산할때 covariance matrix 얘기가 나왔을까?**\n\nhttps://timvieira.github.io/blog/post/2014/10/06/kl-divergence-as-an-objective-function/\n\n이 analytic estimator는 헤시안이나 trajectories의 모든 그라디언트를 저장하지 않아도 되기 때문에 대규모 환경에서 계산 이점이 있다.\n\n**TODO Appendix C**\n\n- 3장에서 surrogate objective $M(\\pi)$ 과 KL divergence penalty를 활용한 최적화를 증명했으나 <br/>\n$C$가 크면 엄청나게 step size가 작아지게 된다. 그래서 $C$를 감소하려하는데, 강건하게 정하기가 어려워서 <br/>\npenalty대신, 강한 제약조건으로 KL divergence의 bound $\\delta $ 를 사용했다. -> (Trust Region)\n\n- $\\large D_{KL}^{max}(\\theta_{old}, \\theta)$ 제약조건은 추정이나 최적화를 하기 어려운 부분이 있어서 <br/>\n대신에 $\\large \\bar{D}_{KL}(\\theta_{old}, \\theta)$ 을 제약했다.\n\n- 본 논문은 advantage function의 추정에러를 무시한다. Kakade&Langford(2002) 에서도 이런 에러를 고려했고 상황도 비슷하지만 <br/>\n단순함을 위해 생략했다. (매우 작은 차이라?)\n\n<br>\n## 6. Practical Algorithm\n\n### Appendix C. Efficiently Solving the Trust-Region Constrained Optimization Problem\n\n풀어야 하는 식\n\n$\\LARGE maximize \\ L(\\theta) \\  subject \\ to \\ \\bar{D}_{KL}(\\theta_{old}, \\theta) \\leq \\delta$\n\n크게 두가지 과정으로 진행\n\n1) 탐색 방향을 계산, 목적함수의 선형근사 와 제약식의 2차 근사\n\n2) 계산한 방향으로 line search 수행, 비선형 제약식을 만족시키면서 비선형 목적함수를 개선\n\n\n탐색 방향은 $\\large Ax = g$ 수식을 근사적으로 풀면 계산된다. 여기서 $\\large A$ 는 Fisher Information Matrix이고,\n\nKL divergence 제약식인 $\\large \\bar{D}_{KL} (\\theta_{old}, \\theta) \\approx \\frac{1}{2}(\\theta - \\theta_{old})^T A(\\theta - \\theta_{old})$ 의 2차 근사이다.\n\n여기서 $\\large A_{ij} = \\frac{\\partial}{\\partial \\theta_i} \\frac{\\partial}{\\partial \\theta_j} \\bar{D}_{KL}(\\theta_{old}, \\theta)$\n\n대규모 문제에서 $A$ 혹은 $A^{-1}$ 을 계산하는 것은 엄청난 계산량이 필요.\n\n하지만 Conjugate Gradient Algorithm이 전체 $A$행렬(FIM)을 계산하지 않아도 $\\large Ax=g$ 를 근사적으로 해결할 수 있게 해준다.\n\n탐색 방향 $\\large s \\approx A^{-1}g$ 을 계산하면서, 최대스텝길이(step size) $\\large \\beta$를 계산할 필요가 있다.\n\n이 계산을 위해서 $\\large \\delta = \\bar{D}_{KL} \\approx \\frac{1}{2}(\\beta s)^T A(\\beta s) = \\frac{1}{2}\\beta^2s^TAs$ 이고\n\n여기서 $\\large \\beta = \\sqrt{2\\delta / s^T As}$ , 이때 $\\large \\delta$는 KL divergence의 boundary tgerm 이다.\n\n$\\large s^T As$ 텀은 헤시안 - 벡터 곱으로 계산될 수 있고, CG의 과정에서 계산된다.\n\n마지막으로, surrogate objective와 KL divergence 제약식을 만족하는 개선을 위해 line search 를 사용할 것이며, surrogate objective와 KL divergence 제약식은 둘 다 $\\large \\theta$ 에 대해 비선형이다.\n\n목적함수인 $\\large L_{\\theta_{old} }(\\theta) - \\chi [\\bar{D}_{KL}(\\theta_{old}, \\theta) \\leq \\delta] = 0$ 0이 맞으면 True고 무한으로 발산하면 False\n\n위에서 계산한 $\\large \\beta$ 의 최대 값부터 시작해서 목적함수가 개선될때까지 점점 줄여갈텐데, line search가 없었다면\n\n이 알고리즘은 매우 큰 스텝으로 계산될 것이고 성능의 심각한 저하가 발생될 것이다.\n\n### C.1 Computing the Fisher-Vector Product\n\n\n임의의 벡터와 평균한 Fisher Information Matrix를 어떻게 계산할지 서술하려한다.\n\n이 행렬-벡터 곱은 CG를 수행가능하게 한다. 파라미터화 한 policy가 입력 $\\large x$를 분포 파라미터 벡터인 $\\large \\mu_{\\theta}(x)$ 로 사상시킨다고 할때, $\\large \\pi(a \\vert x)$.\n\n주어진 입력 $\\large x$ 의 KL divergence는 아래 식과 같이 쓸 수 있다.\n\n$\\LARGE D_{KL} (\\pi_{\\theta_{old} }(\\cdot \\vert x)  \\ \\vert\\vert \\ \\pi_{\\theta}(\\cdot \\vert x)) = kl(\\mu_{\\theta}(x), \\mu_{old})$\n\n여기서 kl 은 두 mean 파라미터 벡터에 대응하는 분포 사이의 KL divergence이다. kl 을 $\\large \\theta$에 대해 두번 미분하면\n\n$\\LARGE \\frac{\\partial \\mu_{a}(x)}{\\partial \\theta_i} \\frac{\\partial \\mu_b(x)}{\\partial \\theta_j} kl^{''}_{ab}(\\mu_{\\theta}(x), \\mu_{old}) + \\frac{\\partial^2 \\mu_a(x)}{\\partial \\theta_i \\partial \\theta_j}kl^{'}_a(\\mu_{\\theta}(x), \\mu_{old})$\n\n여기서 두번째 term은 0이 된다. (두번미분하면 사라짐?)\n\n$\\LARGE J^T M J$\n\n## 7\n\n## 7. Connections with Prior Work - (1)\n\nNPG는 L의 선형 근사와 $\\large \\bar D_{KL}$ 제약식을 2차근사 하는 아래의 식의 special case.\n\n**equation 17. **\n\n$\\LARGE L_{\\pi}(\\tilde\\pi) = \\eta(\\pi) + \\underset{s}{\\sum}\\rho_{\\pi}(s) \\underset{a}{\\sum}\\tilde\\pi(a\\vert s)A_\\pi(s,a) $ - eq.(3)\n\n$\\LARGE \\underset{\\theta}{maximize} \\ [\\triangledown_{\\theta}L_{\\theta_{old} }(\\theta) \\ \\rvert_{\\theta=\\theta_{old} } \\cdot (\\theta - \\theta_{old})]$\n\n$\\LARGE subject \\ to \\ \\frac{1}{2}(\\theta_{old} - \\theta)^{T} A(\\theta_{old})(\\theta_{old}-\\theta) \\leq \\delta$\n\n$\\LARGE where \\ A(\\theta_{old})_{ij} = $\n\n$\\huge \\frac{\\partial}{\\partial \\theta_{i} } \\frac{\\partial}{\\partial \\theta_{j} }\nE_{s \\sim \\rho_{\\pi} }[D_{KL}(\\pi(\\cdot\\rvert s, \\theta_{old})  \\rvert\\rvert \\pi(\\cdot \\rvert s, \\theta))] \\rvert_{\\theta=\\theta_{old} }$\n\n## 7. Connections with Prior Work - (2)\n업데이트 식\n\n$\\LARGE \\theta_{new} = \\theta_{old} +  \\color{Red}{\\frac{1}{\\lambda} } A(\\theta_{old})^{-1}\\triangledown_{\\theta}L(\\theta) \\rvert_{\\theta=\\theta_{old} }$\n\n여기서 step size인 $\\LARGE \\frac{1}{\\lambda}$ 일반적으로 알고리즘의 파라미터로 취급된다.\n\n이 부분에서 TRPO는 각 업데이트 마다 제약식을 강제한다는 점이 다르다.\n\n좀 사소한 차이처럼 보이지만, 큰 차원을 다루는 실험에서 알고리즘의 성능을 크게 향상시켰다.\n\n또한 $\\LARGE \\mathcal{l}^2$ 제약식(혹은 페널티) 를 사용한 표준 Policy Gradient 업데이트 식을 얻었다.\n\n**Equation 18**\n\n\n$\\LARGE \\underset{\\theta}{maximize}[\\triangledown_{\\theta} L_{\\theta_{old} }(\\theta) \\rvert_{\\theta=\\theta_{old} } \\cdot (\\theta - \\theta_{old})]$\n\n$\\LARGE subject \\ to \\ \\frac{1}{2} \\vert\\vert \\theta - \\theta_{old} \\vert\\vert^2 \\leq \\delta$\n\n비제약 문제인 $\\underset{\\pi}{maximize} \\ L_{\\pi_{old} }(\\pi)$을\n\n$\\LARGE L_{\\pi}(\\tilde\\pi) = \\eta(\\pi) + \\underset{s}{\\sum}\\rho_{\\pi}(s) \\underset{a}{\\sum}\\tilde\\pi(a\\vert s)A_\\pi(s,a) $  - eq.(3)\n\n을 푸는 것으로 Policy Iteration update를 행한다.\n\n## 7. Connections with Prior Work - (3)\n\n$\\LARGE \\underset{\\pi}{maximize} \\ L_{\\pi_{old} }(\\pi)$  식과 유사하게 업데이트를 수행하는 다양한 방법론들이 있다.\n\n[REPS(Relative Entropy Policy Search)](https://pdfs.semanticscholar.org/ff47/526838ce85d77a50197a0c5f6ee5095156aa.pdf) 는\n\n[state-action marginal](http://rll.berkeley.edu/deeprlcourse/f17docs/lecture_3_rl_intro.pdf)인 $p(s,a)$ 를 제약하고 TRPO는 조건부인 $p(a\\vert s)$ 를 제약한다.\n\nREPS constraint \\vert  TRPO constraint\n:--------------:\\vert :---------------:\\vert \n![](https://i.imgur.com/U5Xgci1.png) \\vert  ![](https://i.imgur.com/12RPFh3.png)\n\nREPS랑은 달리, TRPO는 내부 루프 계산에서 비선형 최적화를 하지않는다.(REPS는 BFGS수행)\n\n**\\# REPS algorithm**\n\n![](https://i.imgur.com/F2m1yMG.png)\n\n<br>\n## 7. Connections with Prior Work - (4)\n\n[Levine and Abbeel(2014)](https://people.eecs.berkeley.edu/~svlevine/papers/mfcgps.pdf) 또한 KL Divergence 제약식을 사용했다.\n\n하지만 policy가 추정한 dynamics model이 유효한 영역에서 벗어나지않게 하는데에 목적이 있는 반면,\n\nTRPO는 system dynamics를 추정하지 않는다.\n\n[Pirotta et al. (2013)](https://pdfs.semanticscholar.org/f2c5/a91f35004aebc120360679f76488ac2613ee.pdf) 또한 [Kakade & Langford](https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/KakadeLangford-icml2002.pdf)의 결과를 일반화했으나 이들은 TRPO와는 다른 알고리즘을 도출했다.\n\n<br>\n## 8. Experiments\n\n- 3 가지 궁금증을 위한 실험을 설계함\n\n    1. Single-path 와 vine 의 성능면에서 특징을 알고싶음.\n\n    2. Fixed penalty coefficient(NPG) 보다 fixed KL divergence를 사용하는 것(TRPO)으로    \n    변경한 것이 얼마나 뚜렷한 차이를 보일지, 성능 면에서 어떤 영향을 미치는지 알고싶음\n\n    3. TRPO가 큰 스케일의 문제를 해결할 수 있는지, 기존에 연구/적용되어왔던 방법들과\n    성능, 계산시간, 샘플 복잡도 면에서 비교하고 싶음\n\n\n- 1, 2의 궁금증에 대해서 실험 환경에 single path, vine을 비롯한 여러 사전방법들을 함께 실험.\n\n- 3에 대해선, robotic locomotion과 Atari game에 TRPO를 실험함.\n\n\n## 8.1\n\n### 8.1 Simulated Robotic Locomotion\n\n- MuJoCo 시뮬레이터를 활용해 로봇 locomotion 실험 진행\n- Robot State: positions and velocities, joint torques\n\n![](https://i.imgur.com/bs5ATS3.png)\n\n- 수영, 점프, 걷기 행동을 학습시키고자 함.\n\n    - 수영: 10 dimensional state space , reward: $\\LARGE r(x,u)=v_x - 10^{-5}\\vert \\vert u\\vert \\vert ^2$, quadratic penalty\n\n    - 점프: 12 dimensional state space, episode의 끝은 높이와 각도 threshol로 점프를 판단, non-terminal state에 대해 +1 보상 추가\n\n    - 걷기: 18 dimensional state space, 뜀뛰는 듯한 보법(점프에이전트)보다 부드러운 보법을 유도하기위해 페널티를 추가했다함\n\n### 8.1 Simulated Robotic Locomotion\n#### Detailed Experiment Setup and used parameters, used network model\nused parameter \\vert  network model\n:----:\\vert :----:\n![](https://i.imgur.com/zgnsbw6.png)\\vert  ![](https://i.imgur.com/FqdWC53.png)\n\n### 8.1 Simulated Robotic Locomotion\n\nequation (12) : $\\huge maximize L_{\\theta_{old}(\\theta)} \\ , \\ \\ subject \\ to \\bar{D}_{KL}^{\\rho_{\\theta_{old} }}(\\theta_{old}, \\theta) \\leq \\delta$\n\n이 실험에서 $\\LARGE \\delta = 0.01$\n\n- 비교에 사용된 모델들\n\n    - TRPO Single-path\n    - TRPO vine\n    - CEM(cross-entropy method)\n    - CMA(covariance matrix adaptation)\n    - NPG (Lagrange Multiplier penalty coefficien를 사용하는 것이 차이점)\n    - Empirical FIM\n    - maximum KL(cartpole)\n\n### 8.1 Simulated Robotic Locomotion\n\n![](https://i.imgur.com/wSqolvS.png)\n\n- 제안한 single path, vine 모두 task를 잘 학습하는 것을 확인할 수 있음\n\n- 페널티를 고정하는 것보다, KL divergenc를 제약하는 것이 step size를 선택하는 부분에서 Robust 하다.\n\n- CEM/CMA는 derivative free 알고리즘으로, 샘플 복잡도가 네트워크 파라미터\u000b수 만큼 확장되어 high dimension 문제에서는 속도와 성능이 약한 모습을 보인다.\n\n- maxKL은 제안방법보다 느린 학습\n\n\n\n### 8.1 Simulated Robotic Locomotion\n\n\nTRPO video link: https://www.youtube.com/watch?v=jeid0wIrSn4\n\n- 사전지식이 적은 상태에서 여러 액션을 학습하는 실험으로 TRPO의 성능을 보임\n\n- 지금까지의 균형이나 걸음 같은 개념을 명시적으로 인코딩한 robotic locomotion에서의 사전 연구들과 차이를 보임\n\n\n## 8.2\n\n### 8.2 Playing Games from Images\n\n- 부분관찰가능하며 복잡한 observation인 환경에서 TRPO를 평가하기위해 raw image를 입력으로 하는 아타리게임 환경에서 실험\n\n* Challenging point :\n    - High dimensional space\n    - delayed reward\n    - non-stationary image (Enduro는 배경 이미지가 바뀌고, 반전처리가 됨)\n    - complex action sequence (Q*bert는 21개의 다른 발판에서 점프 해야함)\n\n- ALE(Arcade Learning Environment) 에서 테스트\n- DQN과 전처리 방식은 동일\n    210 x 160 pixel -> gray-scaling & down-sampling -> 110 x 84  -> crop -> 84 x 84\n\n\n\n### 8.2 Playing Games from Images\n\n- 2 convolution layer ( 16 channels) , stride 2, FC layer with 20 units\n\nparameter setup \\vert  network model\n:--------------:\\vert :-------------:\n![](https://i.imgur.com/NJBC69d.png) \\vert  ![](https://i.imgur.com/wTe1OEW.png)\n\n### 8.2 Playing Games from Images\n\n![](https://i.imgur.com/j22VtNQ.png)\n\n- 30시간 동안 500 iteration 의 학습\n\n- DQN과 MCST를 사용한 모델(UCC-I), 사람이 플레이한 것(Human) 과의 비교\n\n- 이전 방법들에 비해 압도적인 점수를 기록하진 못했으나, 다양한 게임에서 적절한 게임 기록.\n\n- 이전 방법들과 다른 점은, 특정 task에 초점을 맞춰 설계하지않았는데\n<br/>\nrobotics나 game playing Task에도 적절한 성능을 내는 점에서 **TRPO의 일반화 성능**을 보임\n\n<br>\n## 9. Discussion\n\n- Trust Region Policy Optimization 을 제안\n\n- KL Divergence 페널티로 $J(\\pi)$를 최적화하는 알고리즘이 monotonically improve 함을 증명\n\n- Locomotion 도메인에서 여러 행동(수영, 걷기, 점프)을 제어하는 controlle를 학습\n\n- TRPO가 향후 연구의 도약점이 되었으면 좋겠음\n\n- Robotics와 게임 실험을 결합해, 시각정보와 센서데이터를 사용하는 로봇제어 정책을 학습시키는 가능성을 봄\n\n- 샘플의 복잡도를 상당히 줄일수있어 실상황 적용가능성을 봄\n\n<br><br>\n# END\n","source":"_posts/trpo.md","raw":"---\ntitle: trpo\ndate: 2018-07-12 16:53:12\ntags: [\"프로젝트\", \"피지여행\"]\ncategories: 프로젝트\nauthor: 공민서, 김동민\nsubtitle: TRPO 여행하기\n---\nTrust Region Policy Optimization\n========================\nAuthors: John Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, Pieter Abbeel    \nProceeding: International Conference on Machine Learning (ICML) 2015\n\n정리 by 공민서, 김동민\n\n\n# 1. 들어가며...\n\nTrust region policy optimization (TRPO)는 상당히 우수한 성능을 보여주는 policy gradient 기법으로 알려져 있습니다. 높은 차원의 action space를 가진 robot locomotion부터 action은 적지만 화면을 그대로 처리하여 플레이하기 때문에 control parameter가 매우 많은 Atari game까지 각 application에 세부적으로 hyperparameter들을 특화시키지 않아도 두루두루 좋은 성능을 나타내기 때문에 일반화성능이 매우 좋은 기법입니다. 이 TRPO에 대해서 알아보겠습니다.\n\n※TRPO를 매우 재밌게 설명한 Crazymuse AI의 [Youtube video](https://www.youtube.com/watch?v=CKaN5PgkSBc&t=90s)에서 일부 그림을 차용했습니다. 이 비디오를 시청하시는 것을 강력하게 추천합니다!\n\n<br><br>\n## 1.1. TRPO 흐름 잡기\n\nTRPO 논문은 많은 수식이 등장하여 이 수식들을 따라가다보면 큰 그림을 놓칠 수도 있습니다. 세부적인 내용을 살펴보기 전에 기존 연구에서 출발해서 TRPO로 어떻게 발전해나가는지 간략하게 살펴보겠습니다.\n\n### 1.1.1. Original Problem\n\n$$\\max\\_\\pi \\eta(\\pi)$$\n\n모든 강화학습이 그렇듯이 expected discounted reward를 최대화하는 policy를 찾는 문제로부터 출발합니다.\n\n### 1.1.2. Conservative Policy Iteration\n\n$$\\max L\\_\\pi(\\tilde\\pi) = \\eta(\\pi) + \\sum\\_s \\rho_\\pi(s)\\sum\\_a\\tilde\\pi(a\\vert s)A\\_\\pi(s,a)$$\n\n$\\eta(\\pi)$를 바로 최대화하는 것은 많은 경우 어렵습니다. $\\eta(\\pi)$의 성능향상을 보장하면서 policy를 update하는 conservative policy iteration 기법이 [Kakade와 Langford](http://www.cs.cmu.edu/~./jcl/papers/aoarl/Final.pdf)에 의하여 제안되었습니다. 이 기법을 이용하면 policy update가 성능을 악화시키지는 않는다는 것이 이론적으로 보장됩니다.\n\n### 1.1.3. Theorem 1 of TRPO\n\n$$\\max L\\_{\\pi\\_\\mathrm{old} }\\left(\\pi\\_\\mathrm{new}\\right) - \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2,\\quad\\left(\\alpha=D\\_\\mathrm{TV}^\\max\\left(\\pi\\_\\mathrm{old},\\pi\\_\\mathrm{new}\\right)\\right)$$\n\n기존의 conservative policy iteration은 과거 policy와 새로운 policy를 섞어서 사용해서 실용적이지 않다는 단점이 있었는데 이것을 보완하여 온전히 새로운 policy만으로 update할 수 있는 기법을 제안합니다. \n\n### 1.1.4. KL divergence version of Theorem 1\n\n$$\\max L\\_{\\pi}\\left(\\tilde\\pi\\right) - C\\cdot D_\\mathrm{KL}^\\max\\left(\\pi,\\tilde\\pi\\right),\\quad\\left(C = \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\right)$$\n\ndistance metric을 KL divergence로 바꿀 수 있습니다.\n\n### 1.1.5. Using parameterized policy\n\n$$\\max\\_\\theta L\\_{\\theta\\_\\mathrm{old} }(\\theta) - C\\cdot D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right)$$\n\n최적화문제를 더욱 편리하게 풀 수 있도록 낮은 dimension을 가지는 parameter들로 parameterized된 policy를 사용할 수 있습니다.\n\n### 1.1.6. Trust region constraint\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ }&D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right) \\leq \\delta\n\\end{align}$$\n\n5까지는 아직 conservative policy iteration을 약간 변형시킨 것입니다. policy를 update할 때 지나치게 많이 변하는 것을 방지하기 위하여 [trust region](https://en.wikipedia.org/wiki/Trust_region)을 constraint로 설정할 수 있습니다. 이 아이디어로 인해서 TRPO라는 명칭을 가지게 됩니다.\n\n### 1.1.7. Heuristic approximation\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ }&\\overline{D}\\_\\mathrm{KL}^{\\rho\\_\\mathrm{old} }\\left(\\theta\\_\\mathrm{old}, \\theta\\right) \\leq \\delta\n\\end{align}$$\n\n사실 6의 constraint는 모든 state에 대해서 성립해야 하기 때문에 문제를 매우 어렵게 만듭니다. 이것을 좀 더 다루기 쉽게 state distribution에 대한 평균을 취한 것으로 변형합니다.\n\n### 1.1.8. Monte Carlo Simulation\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi\\_\\theta(a\\vert s)}{q(a\\vert s)}Q\\_{\\theta\\_\\mathrm{old} }(s,a)\\right] \\\\\\\\\n\\mathrm{s.t.\\ }&E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} }}\\left[D\\_\\mathrm{KL}\\left(\\pi\\_{\\theta\\_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi\\_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta\n\\end{align}$$\n\nSampling을 통한 계산이 가능하도록 식을 다시 표현할 수 있습니다. \n\n### 1.1.9. Efficiently solving TRPO\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &\\nabla\\_\\theta \\left. L\\_{\\theta\\_\\mathrm{old} }(\\theta)\\right\\vert \\_{\\theta=\\theta\\_\\mathrm{old} } \\left(\\theta - \\theta\\_\\mathrm{old}\\right) \\\\\\\\\n\\mathrm{s.t.\\ }&\\frac{1}{2}\\left(\\theta\\_\\mathrm{old} - \\theta \\right)^T A\\left(\\theta\\_\\mathrm{old}\\right)\\left(\\theta\\_\\mathrm{old} - \\theta \\right) \\leq \\delta \\\\\\\\\n&A\\_{ij} = \\frac{\\partial}{\\partial \\theta\\_i}\\frac{\\partial}{\\partial \\theta\\_j}\\overline{D}\\_\\mathrm{KL}^{\\rho\\_\\mathrm{old} }\\left(\\theta\\_\\mathrm{old}, \\theta\\right)\n\\end{align}$$\n\n문제를 효율적으로 풀기 위하여 approximation을 적용할 수 있습니다. objective function은 first order approximation, constraint는 quadratic approximation을 취하면 효율적으로 문제를 풀 수 있는 형태로 바뀌는데 이것을 natural gradient로 풀 수도 있고 conjugate gradient로 풀 수도 있습니다.\n\nTRPO는 이렇게 다양한 방법으로 문제를 변형한 것입니다! 이제 좀 더 자세히 살펴보겠습니다.\n\n<br><br>\n# 2. Preliminaries\n\n다음과 같은 파라미터를 가지는 infinite-horizon discounted Markov decision process (MDP)를 고려합니다. \n\n* $\\mathcal{S}$: finite set of states\n* $\\\\mathcal{A}$: finite set of actions\n* $P: \\mathcal{S}\\times\\mathcal{A}\\rightarrow\\mathbb{R}$: transition probability distribution\n* $r: \\mathcal{S}\\rightarrow\\mathbb{R}$: reward function\n* $\\rho\\_0:\\mathcal{S}\\rightarrow\\mathbb{R}$: distribution of the initial state $s\\_0$\n* $\\gamma\\in(0,1)$: discount factor\n* $\\eta(\\pi)=E\\_{s\\_0,a_0,\\ldots}\\left[\\sum\\_{t=0}^\\infty\\gamma^t r\\left(s\\_t\\right)\\right]$, where $s\\_0\\sim\\rho\\_0\\left(s\\_0\\right), a\\_t\\sim\\pi\\left(\\left.a\\_t \\right\\vert s\\_t\\right), s\\_{t+1}\\sim P\\left(\\left.s\\_{t+1}\\right\\vert s\\_t,a\\_t\\right)$\n* $Q\\_\\pi\\left(s\\_t,a\\_t\\right)=E\\_{\\color{red}s\\_{t+1},a\\_{t+1},\\ldots}\\left[\\sum\\_{l=0}^\\infty \\gamma^l r\\left(s\\_{t+l}\\right)\\right]$: action value function\n* $V\\_\\pi\\left(s\\_t\\right)=E\\_{\\color{red}a\\_t, s\\_{t+1},a\\_{t+1},\\ldots}\\left[\\sum\\_{l=0}^\\infty \\gamma^l r\\left(s\\_{t+l}\\right)\\right]$: value function (action value function과 expectation의 첨자가 다른 부분을 유의하세요.)\n* $A\\_\\pi(s,a) = Q\\_\\pi(s,a) - V\\_\\pi(s)$: advantage function\n\n## 2.1 Useful identity [Kakade & Langford 2002]\n\n우리의 목표는 $\\eta\\(\\pi\\)$가 최대화되도록 만드는 것입니다. 하지만 $\\pi$의 변화에 따라 $\\eta$가 어떻게 변하는지 알아내는 것도 쉽지 않습니다. $\\pi$는 기존의 policy이고 $\\tilde\\pi$는 새로운 policy를 나타낸다고 할 때 $\\eta$와 policy update 사이의 관계에 대해서 다음과 같은 관계가 있다는 것이 밝혀졌습니다.\n\n**Lemma 1.** $\\eta\\left(\\tilde\\pi\\right) = \\eta(\\pi) + E\\_{s\\_{0},a\\_{0},\\ldots\\sim\\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t A\\_\\pi\\left(s\\_t,a\\_t\\right)\\right]$\n\n*Proof.* $A\\_\\pi(s,a)=E\\_{s'\\sim P\\left(s'\\vert s,a\\right)}\\left[r(s)+\\gamma V\\_\\pi\\left(s'\\right)-V\\_\\pi(s)\\right]$로 다시 표현할 수 있습니다. 표기의 편의를 위하여 $\\tau:=\\left(s\\_0,a\\_0,s\\_1,a\\_1,\\ldots\\right)$를 정의하겠습니다. 다음과 같은 수식 전개가 가능합니다.\n$$\n\\begin{align}\n&E\\_{\\tau\\vert \\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t A\\_\\pi\\left(s\\_t,a\\_t\\right)\\right] \\\\\\\\\n &= E\\_{\\tau\\vert \\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t \\left(r(s\\_t)+\\gamma V\\_\\pi\\left(s\\_{t+1}\\right)-V\\_\\pi(s\\_t)\\right)\\right] \\\\\\\\\n&= E\\_{\\tau\\vert \\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t r(s\\_t)+\\gamma V\\_\\pi\\left(s\\_{1}\\right)-V\\_\\pi(s\\_0)+\\gamma^2 V\\_\\pi\\left(s\\_{2}\\right)-\\gamma V\\_\\pi(s\\_1)+\\gamma^3 V\\_\\pi\\left(s\\_{3}\\right)-\\gamma^2 V\\_\\pi(s\\_2)+\\cdots\\right] \\\\\\\\\n&= E\\_{\\tau\\vert \\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t r(s\\_t)+\\cancel{\\gamma V\\_\\pi\\left(s\\_{1}\\right)}-V\\_\\pi(s\\_0)+\\cancel{\\gamma^2 V\\_\\pi\\left(s\\_{2}\\right)}-\\cancel{\\gamma V\\_\\pi(s\\_1)}+\\cdots\\right] \\\\\\\\\n&= E\\_{\\tau\\vert \\tilde\\pi}\\left[-V\\_\\pi(s\\_0)+\\sum\\_{t=0}^\\infty\\gamma^t r(s\\_t)\\right] \\\\\\\\\n&\\overset{\\underset{\\mathrm{(a)} }{} }{=} -E\\_{\\color{red}s\\_0}\\left[V\\_\\pi(s\\_0)\\right]+E\\_{\\tau\\vert \\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t r(s\\_t)\\right] \\\\\\\\\n&=-\\eta(\\pi) + \\eta\\left(\\tilde\\pi\\right)\\\\\\\\\n&\\quad\\therefore \\eta\\left(\\tilde\\pi\\right) = \\eta(\\pi) + E\\_{s\\_{0},a\\_{0},\\ldots\\sim\\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t A\\_\\pi\\left(s\\_t,a\\_t\\right)\\right]\n\\end{align}\n$$\n위의 전개의 (a) 부분은 $V\\_\\pi\\left(s\\_0\\right)=E\\_{a\\_0,s\\_0,a\\_1,\\ldots}\\left[\\sum\\_{t=0}^\\infty\\gamma^l r\\left(s\\_l\\right)\\right]$이므로 $\\tau$ 중 많은 부분들이 이미 expectation이 취해진 값이므로 무시됩니다. 오직 $s\\_0$만 expectation이 취해지지 않았기 때문에 남아있습니다. $\\square$\n\n**Lemma 1**은 새로운 policy와 기존의 policy 사이의 관계를 규정합니다. 다음을 정의합니다. \n\n* $\\rho\\_\\pi(s) = P\\left(s\\_0=s\\right) + \\gamma P\\left(s\\_1=s\\right)\\ + \\gamma^2 P\\left(s\\_2=s\\right) + \\cdots $: (unnormalized) discounted visitation frequencies\n\n이것을 이용해서 **Lemma 1**을 변형시켜 봅시다.\n\n$$\n\\begin{align}\n\\eta\\left(\\tilde\\pi\\right) &= \\eta(\\pi) + \\sum\\_{t=0}^\\infty\\sum\\_{s}P\\left(\\left.s\\_t=s\\right\\vert \\tilde\\pi\\right)\\sum\\_a\\tilde\\pi(a\\vert s)\\gamma^t A\\_\\pi(s,a) \\\\\\\\\n&= \\eta(\\pi) + \\sum\\_{s}{\\color{red}\\sum\\_{t=0}^\\infty \\gamma^t P\\left(\\left.s\\_t=s\\right\\vert \\tilde\\pi\\right)}\\sum\\_a\\tilde\\pi(a\\vert s) A\\_\\pi(s,a) \\\\\\\\\n&= \\eta(\\pi) + \\sum\\_{s}{\\color{red}\\rho\\_\\tilde\\pi(s)}\\sum\\_a\\tilde\\pi(a\\vert s) A\\_\\pi(s,a) \\\\\\\\\n\\end{align}\n$$\n\n이 수식의 의미가 무엇일까요? 만약 $\\sum\\_a\\tilde\\pi(a\\vert s) A\\_\\pi(s,a) \\geq 0$이라면 $\\eta(\\tilde\\pi)$는 항상 $\\eta(\\pi)$보다 큽니다. 즉, policy를 업데이트함으로써 항상 개선됩니다. 즉, 이 수식을 통해서 항상 더 좋은 성능을 내는 policy update가 가져야 할 특징을 알 수 있습니다. 다음과 같은 deterministic policy가 있다고 합시다.\n\n$$\n\\tilde\\pi(s) = \\arg\\max_a A\\_\\pi(s,a)\n$$\n\n이 policy는 적어도 하나의 state-action pair에서 0보다 큰 값을 가지는 advantage가 있고 그 때의 확률이 0이 아니라면 항상 성능을 개선시킵니다. 어려운 점은 policy를 바꾸면 $\\rho$도 바뀐다는 점입니다. \n\n다음 그림을 봅시다. Starting Point에서 여러가지 경로를 거쳐서 Destination으로 갈 수 있습니다. 다른 policy를 이용하는 것은 다른 경로를 이용한 것입니다.\n\n![policy_change](../img/policy_change.png)\n\n이 때 다른 policy를 이용함에 따라 state visitation frequency도 변하게 됩니다.\n\n![state_visitation_change](../img/state_visitation_change.png)\n\n이로 인해서 policy를 최적화하는 것은 어려워집니다. 그래서 이러한 변화를 무시하는 다음과 같은 approximation을 취합니다.\n\n$$\n\\begin{align}\nL_\\pi\\left(\\tilde\\pi\\right) &= \\eta(\\pi) + \\sum\\_{s}{\\color{red}\\rho\\_\\pi(s)}\\sum\\_a\\tilde\\pi(a\\vert s) A\\_\\pi(s,a)\n\\end{align}\n$$\n\npolicy가 바뀌었음에도 이전의 state distribution을 게속 이용하는 것입니다. 이것은 policy의 변화가 크지 않다면 어느 정도 허용될 수 있을 것입니다. 그렇지만 얼마나 많은 변화가 허용될까요? 이것을 정하기 위해서 이용하는 것이 trust region입니다.\n\n<br>\n## 2.1. Conservative Policy Iteration\n\npolicy의 변화를 다루기 용이하게 하기 위해서 policy를 다음과 같이 파라미터를 이용해서 표현합시다.\n\n* $\\pi\\_\\theta(a\\vert s)$: parameterized policy\n\n$\\pi\\_\\theta$는 $\\theta$에 대하여 미분가능한 함수입니다. $L\\_\\pi\\left(\\tilde\\pi\\right)$은 $\\theta\\_0$에서 $\\eta(\\pi)$에 대한 first order approximation입니다.\n\n$$\n\\begin{align}\nL\\_{\\pi\\_{\\theta\\_0} }\\left(\\pi\\_{\\theta_0}\\right) &= \\eta\\left(\\pi\\_{\\theta\\_0}\\right) \\\\\\\\\n\\nabla\\_\\theta \\left.L\\_{\\pi\\_{\\theta\\_0} }\\left(\\pi\\_{\\theta\\_0}\\right)\\right\\vert \\_{\\theta=\\theta\\_0} &= \\nabla\\_\\theta\\left.\\eta(\\pi\\_{\\theta\\_0})\\right\\vert \\_\\theta=\\theta\\_0\n\\end{align}\n$$\n\n이것의 의미는 $\\pi\\_{\\theta\\_0}$가 매우 작게 변한다면 $L\\_{\\pi\\_{\\theta\\_0} }$를 개선시키는 것이 $\\eta$를 개선시키는 것이라는 것입니다. 그러나 지금까지의 설명만으로는 $\\pi\\_{\\theta\\_0}$를 얼마나 작게 변화시켜야 할지에 대해서는 알 수 없습니다.\n\nKakade & Langford가 2002년 발표한 논문에서 이것을 고려하였습니다. 그 논문에서 *conservative policy iteration*이라는 기법을 제안합니다. 이 논문의 contribution은 다음과 같습니다.\n\n* $\\eta$ 개선의 lower bound를 제공\n* 기존의 policy를 $\\pi\\_\\mathrm{old}$라고 하고 $\\pi'$를 $\\pi'=\\arg\\max\\_\\{\\pi'}L\\_{\\pi\\_\\mathrm{old} }\\left(\\pi'\\right)$과 같이 정의할 때, 새로운 mixture policy $\\pi\\_\\mathrm{new}$를 다음과 같이 제안\n$$\n\\pi\\_\\mathrm{new}(a\\vert s) = (1-\\alpha)\\pi\\_\\mathrm{old}(a\\vert s) + \\alpha \\pi'(a\\vert s)\n$$\n\n그림으로 표현하면 다음과 같습니다.\n\n![mixure_policy](../img/mixure_policy.png)\n\n* 다음과 같은 lower bound를 정의\n$$\n\\eta\\left(\\pi\\_\\mathrm{new}\\right) \\geq L\\_{\\pi\\_{\\theta\\_\\mathrm{old} }}\\left(\\pi\\_\\mathrm{new}\\right) - \\frac{2\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2,\\\\\\\\\n\\mathrm{where\\ }\\epsilon = \\max_s\\left\\vert E\\_{a\\sim\\pi'(a\\vert s)}\\left[A\\_\\pi(s,a)\\right]\\right\\vert \n$$\n\n하지만 mixture된 policy라는 것은 실용적이지 않습니다. \n\n<br><br>\n# 3. Monotonic Improvement Guarantee for General Stochastic Policies\n\n이전 장에서 설명한 lower bound는 오직 mixture policy에 대해서만 성립하고 보다 더 많이 사용되는 stochastic policy에는 적용되지 않습니다. 따라서 이러한 방향으로 개선이 필요합니다. 기존 수식의 두 가지를 바꿈으로써 이것을 달성할 수 있습니다.\n\n* $\\alpha\\rightarrow$ distance measure between $\\pi$ and $\\tilde\\pi$\n* constant $\\epsilon\\rightarrow\\max\\_{s,a}\\left\\vert A\\_\\pi(s,a)\\right\\vert $\n\n여기서 distance measure로 total variation divergence를 이용합니다. discrete porbability distribution $p$와 $q$에 대하여 다음과 같이 정의됩니다.\n$$\nD\\_\\mathrm{TV}(p\\parallel q) = \\frac{1}{2}\\sum\\_i\\left\\vert p\\_i - q\\_i\\right\\vert \n$$\n\n이것을 이용하여 $D\\_\\mathrm{TV}^\\max$를 다음과 같이 정의하겠습니다.\n$$\nD\\_\\mathrm{TV}^\\max(\\pi\\parallel \\tilde\\pi) = \\max_s D\\_\\mathrm{TV}\\left(\\pi(\\cdot\\vert s)\\parallel\\tilde\\pi(\\cdot\\vert s)\\right)\n$$\n\n그림으로 표현하면 다음과 같습니다.\n\n![tvd](../img/tvd.png)\n\n이것을 이용하여 다음과 같은 관계식을 얻을 수 있습니다.\n\n**Theorem 1.** Let $\\alpha=D\\_\\mathrm{TV}^\\max(\\pi\\parallel \\tilde\\pi)$. Then the following bound holds:\n$$\n\\begin{align}\n\\eta\\left(\\pi\\_\\mathrm{new}\\right) &\\geq L\\_{\\pi\\_\\mathrm{old} }\\left(\\pi\\_\\mathrm{new}\\right) - \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2, \\\\\\\\\n\\mathrm{where\\ } \\epsilon&= \\max\\_{s,a}\\left\\vert A\\_\\pi(s,a)\\right\\vert \n\\end{align}\n$$\n*Proof.* TBD.\n\n또다른 distance metric으로 아래 그림과 같은 KL divergence가 있습니다.\n(그런데 왜 하필 KL divergence로 바꿀까요? 논문의 뒤쪽에서 계산효율을 위해서 conjugate gradient method를 이용하는데 이를 위해서 바꾼게 아닐까 싶습니다. Wasserstein distance 같은 다른 방향으로 발전시킬 수도 있을 것 같습니다. Schulmann이 아마 해봤겠죠?^^)\n\n![kld](../img/kld.png)\n\ntotal variation divergence와 KL divergence 사이에는 다음과 같은 관계가 있습니다.\n$$\nD\\_\\mathrm{TV}(p\\parallel q)^2 \\leq D\\_\\mathrm{KL}(p\\parallel q)\n$$\n\n다음 수식을 정의합니다.\n$$\nD\\_\\mathrm{KL}^\\max(\\pi\\parallel \\tilde\\pi) = \\max_s D\\_\\mathrm{KL}\\left(\\pi(\\cdot\\vert s)\\parallel\\tilde\\pi(\\cdot\\vert s)\\right)\n$$\n\n**Theorem 1**을 이용하여 다음과 같은 수식이 성립함을 알 수 있습니다.\n$$\n\\begin{align}\n\\eta\\left(\\tilde\\pi\\right) &\\geq L\\_{\\pi}\\left(\\tilde\\pi\\right) - C\\cdot D\\_\\mathrm{KL}^\\max(\\pi, \\tilde\\pi), \\\\\\\\\n\\mathrm{where\\ } C&= \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\n\\end{align}\n$$\n\n이러한 policy imporvement bound를 기반으로 다음과 같은 approximate policy iteration 알고리듬을 고안해낼 수 있습니다.\n\n**Algorithm 1** Policy iteration algorithm guaranteeing non-decreasing expected return $\\eta$\n> Initialize $\\pi\\_0$   \n> **for** $i=0,1,2,\\ldots$ until convergence **do**   \n> $\\quad$Compute all advantage values $A\\_{\\pi\\_i}(s,a)$    \n> $\\quad$Solve the contstrained optimization problem    \n> $\\quad\\pi\\_{i+1}=\\arg\\max\\_\\pi\\left[L\\_{\\pi\\_i} - CD\\_\\mathrm{KL}^\\max\\left(\\pi\\_i,\\pi\\right)\\right]$     \n> $\\quad\\quad$where $C = 4\\epsilon\\gamma / (1-\\gamma)^2$    \n> $\\quad\\quad$and $L\\_{\\pi\\_i}\\left(\\pi\\right) = \\eta(\\pi\\_i) + \\sum\\_{s}\\rho\\_{\\pi\\_i}(s)\\sum\\_a\\pi(a\\vert s) A\\_{\\pi\\_i}(s,a)$    \n> **end for**\n\n**Algorithm 1**은 advantage를 정확하게 계산할 수 있다고 가정하고 있습니다. 이 알고리듬은  monotonical한 성능 증가($\\eta(\\pi\\_0)\\leq\\eta(\\pi\\_1)\\leq\\cdots$)를 한다는 것을 다음과 같이 보일 수 있습니다. $M\\_i(\\pi)=L\\_{\\pi\\_i}(\\pi) - CD\\_\\mathrm{KL}^\\max\\left(\\pi\\_i,\\pi\\right)$라고 합시다.\n$$\\begin{align}\n\\eta\\_\\left(\\pi\\_{i+1}\\right) &\\geq M\\_i\\left(\\pi\\_{i+1}\\right)\\\\\\\\\n\\eta\\_\\left(\\pi\\_{i}\\right) &= M\\_i\\left(\\pi\\_{i}\\right) {\\color{blue}\\mathrm{\\ (why\\ same?)} }\\\\\\\\\n\\eta\\_\\left(\\pi\\_{i+1}\\right) - \\eta\\_\\left(\\pi\\_{i}\\right) &\\geq M\\_i\\left(\\pi\\_{i+1}\\right) - M\\_i\\left(\\pi\\_{i}\\right)\n\\end{align}$$\n\n위 수식과 같이 매 iteration 마다 $M\\_i$를 최대화함으로써, $\\eta$가 감소하지 않는다는 것을 보장할 수 있습니다. 이와 같은 타입의 알고리듬을 minorization-maximization (MM) algorithm이라고 합니다. EM 알고리듬도 이 타입의 알고리듬 중 하나입니다.\n\n**Algorithm 1**은 아래 그림과 같이 동작합니다.\n\n![surrogate](../img/surrogate.png)\n\n\n$M\\_i$는 $\\pi\\_i$일 때 equality가 되는 $\\eta$에 대한  surrogate function입니다. TRPO는 이 surrogate function을 최대화하고 KL divergence를 penalty가 아닌 constraint로 두는 알고리듬입니다.\n\n<br><br>\n# 4. Optimization of Parameterized Policies\n\n표기의 편의를 위해 다음과 같이 notation들을 정의합니다.\n\n* $\\eta(\\theta):=\\eta\\left(\\pi\\_\\theta\\right)$\n* $L\\_\\theta\\left(\\tilde\\theta\\right):=L\\_{\\pi\\_\\theta}\\left(\\pi\\_{\\tilde\\theta}\\right)$\n* $D\\_\\mathrm{KL}\\left(\\theta\\parallel\\tilde\\theta\\right):=D\\_\\mathrm{KL}\\left(\\pi\\_\\theta\\parallel\\pi_{\\tilde\\theta}\\right)$\n* $\\theta\\_\\mathrm{old}$: previous policy parameter\n\n## 4.1. Trust Region Policy Optimization\n\n이전 장의 중요 결과를 위의 notation으로 다시 표기하면 아래와 같습니다.\n\n$$\n\\eta\\left(\\theta\\right) \\geq L\\_{\\theta\\_\\mathrm{old} }\\left(\\theta\\right) - C\\cdot D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right)\n$$\n\n$\\eta$의 성능 향상을 보장하기 위해서 그것의 lower bound를 최대화할 수 있습니다.\n\n$$\n\\max\\_\\theta \\left[L\\_{\\theta\\_\\mathrm{old} }\\left(\\theta\\right) - C D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right)\\right]\n$$\n\n이 최적화 문제는 step size를 매우 작게 해야 올바른 동작을 합니다. 위에서 살펴봤듯이 first order approximation이기 때문입니다. 좀 더 큰 step size를 가질 수 있도록 이 최적화 문제를 trust region constraint를 도입하여 다음과 같이 바꿉니다.\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ }&D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right) \\leq \\delta\n\\end{align}$$\n\n이 최적화문제의 constraint는 모든 state space에 대해서 성립해야 합니다. 또한 maximum값을 매번 찾아야 합니다. state가 많은 경우 constraint의 수가 매우 많아지고 이것은 문제를 풀기 어렵게 만듭니다. constraint의 수를 줄이기 위하여 다음과 같은 avergae KL divergence를 이용하는 heuristic approximation을 취합니다. 이것이 최선의 방법은 아닐 수 있지만 실용적인 방법입니다.\n\n$$\n\\overline D\\_\\mathrm{KL}^{\\rho}\\left(\\theta\\_1, \\theta\\_2\\right):=E\\_{s\\sim\\rho}\\left[D\\_\\mathrm{KL}\\left(\\pi\\_{\\theta_1}(\\cdot\\vert s)\\parallel\\pi\\_{\\theta\\_2}(\\cdot\\vert s)\\right)\\right]\n$$\n\n이것을 기반으로 다음과 같은 최적화 문제를 풀 수 있습니다.\n\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ }&\\overline{D}\\_\\mathrm{KL}^{\\rho\\_\\mathrm{old} }\\left(\\theta\\_\\mathrm{old}, \\theta\\right) \\leq \\delta\n\\end{align}$$\n\n\n아래 그림처럼 step size에 대해서 고려할 수 있습니다.\n\n![heuristic_approx](../img/heuristic_approx.png)\n\n<br><br>\n# 5. Sample-Based Estimation of the Objective and Constraint\n\n실용적인 알고리듬을 만들려고 하는 노력은 아직 끝나지 않았습니다. 이제 앞의 알고리듬을 sample-based estimation 즉, Monte Carlo estimation을 할 수 있도록 바꿔보겠습니다. sampling을 편하게 할 수 있도록 아래와 같이 바꿔줍니다.\n\n* $\\sum\\_s \\rho\\_{\\theta\\_\\mathrm{old} }(s)[\\ldots]\\rightarrow\\frac{1}{1-\\gamma}E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} }}[\\ldots]$\n* $A\\_{\\theta\\_\\mathrm{old} }\\rightarrow Q\\_{\\theta\\_\\mathrm{old} }$\n* $\\sum\\_a\\pi\\_{\\theta\\_\\mathrm{old} }(a\\vert s)A\\_{\\theta\\_\\mathrm{old} }\\rightarrowE\\_{a\\sim q}\\left[\\frac{\\pi\\_\\theta(a\\vert s)}{q(a\\vert s)}A\\_{\\theta\\_\\mathrm{old} }\\right]$\n\n이러한 변화를 그림처럼 도식화할 수 있습니다.\n\n![sample-based](../img/sample-based.png)\n\n한 가지 짚고 넘어가야 할 점은 action sampling을 할 때 importance sampling을 사용한다는 것입니다.\n\n![importance_sampling](../img/importance_sampling.png)\n\n바뀐 최적화문제는 아래와 같습니다.\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi\\_\\theta(a\\vert s)}{q(a\\vert s)}Q\\_{\\theta\\_\\mathrm{old} }(s,a)\\right] \\\\\\\\\n\\mathrm{s.t.\\ }&E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} }}\\left[D\\_\\mathrm{KL}\\left(\\pi\\_{\\theta\\_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi\\_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta\n\\end{align}$$\n\n이 때 sampling하는 두 가지 방법이 있습니다.\n\n<br>\n## 5.1. Single Path\n\n*single path*는 개별 trajectory들을 이용하는 방법입니다.\n\n![single](../img/single.png)\n\n<br>\n## 5.2. Vine\n\n*vine*은 한 state에서 rollout을 이용하여 여러 action을 수행하는 방법입니다.\n\n![vine1](../img/vine1.png)\n\n![vine2](../img/vine2.png)\n\nestimation의 variance를 낮출 수 있지만 계산량이 많고 한 state에서 여러 action을 수행할 수 있어야 하기 때문에 현실적인 문제에 적용하기에는 어려움이 있습니다.\n\n\n# Trust Region Policy Optimization\n\n# Conjugate Gradient Method\n![](https://datascienceschool.net/upfiles/a5ba6251b6f144249cca6eb8cc523682.png)\n\n테일러 급수에 대하여 : [naver](https://terms.naver.com/entry.nhn?docId=3572065&cid=58944&categoryId=58970) , [wikipedia](https://ko.wikipedia.org/wiki/%ED%85%8C%EC%9D%BC%EB%9F%AC_%EC%A0%95%EB%A6%AC)\n\n: 근사 다항식을 구하기위해.\n\n$\\large f(x) = \\frac{f(a)}{0!} + \\frac{f'(a)}{1!}(x-a)^1 + \\frac{f''(a)}{2!}(x-a)^2 + ... + R_{n+1}(x)$\n\n$\\large = \\sum_{k=0}^{n} \\frac{f^{(k)}(a)}{k!}(x-a)^k + R_{n+1}(x)$\n\n## 5. Sample-based Estimation of the Objective and Constraint\n\n이전 장에서, 각 업데이트 시에 정책 변화를 제약하면서 기대누적보상 $\\large \\eta$ 의 추정치를 최적화하는\n\n정책 파라미터의 제약조건이 있는 최적화 문제를 제안했다. - eq.(12)\n\n#### Equation 12\n\n$\\LARGE \\underset{\\theta}{maximize} \\ L_{\\theta_{old} }(\\theta)$\n\n$\\LARGE subject \\ to \\ \\bar{D}_{KL}^{\\rho_{\\theta_{old} }} (\\theta_{old}, \\theta) \\leq \\delta$\n\n<br/>\n\n이번 5장에선, **어떻게 목적함수와 제약식이 몬테카를로 시뮬레이션을 통해 근사되는지** 설명한다.\n\n## 5. Sample-based Estimation of the Objective and Constraint\n\neq. (12)의 $\\large L_{\\theta_{old}(\\theta)}$ 를 확장하면 다음의 식을 얻을 수 있다.\n\n**Equation 13**\n\n$\\LARGE \\underset{\\theta}{maximize} \\ \\sum_s \\rho_{\\theta_{old} }(s) \\sum_a \\pi_{\\theta}(a\\vert s) A_{\\theta_{old} }(s, a)$\n\n$\\LARGE subject \\ to \\ \\bar{D}_{KL}^{\\rho_{\\theta_{old} }} (\\theta_{old}, \\theta) \\leq \\delta$\n\n\n여기서...\n\n1) $\\LARGE \\sum_s \\rho_{\\theta_{old} }(s)$  를 $\\LARGE \\frac{1}{1-\\gamma} E_{s \\sim \\rho_{\\theta_{old} }} $ 로 대체.\n\n** 어디서 $\\large \\frac{1}{1-\\gamma}$ 가 나왔을까? Kakade & LangFord 2002 **\n\n\n-\\vert  -\n:---------------:\\vert :---------------:\n![](https://i.imgur.com/uo0zOqd.png) \\vert  ![](https://i.imgur.com/PeG5gNp.png)\\vert \n\n\n2) $\\large A_{\\theta_{old} }$ 을 $\\large Q_{\\theta_{old} }$ 로 대체.\n\n\n3) 액션의 확률합을 importance sampling으로 대체\n\n$\\LARGE \\sum_a \\pi_{\\theta}(a\\vert s_n)A_{\\theta_{old} }(s_n, a) = \\color{Red}{E_{a \\sim q} [\\frac{\\pi_{\\theta}(a\\vert s_n)}{q(a\\vert s_n)} } \\ A_{\\theta_{old} }(s_n, a)]$\n\n\n대체한 결과는\n\n**Equation 14**\n\n$\\LARGE \\underset{\\theta}{maximize} E_{s \\sim \\rho_{\\theta_{old} } \\ , \\ a \\sim q} [\\frac{\\pi_{\\theta}(a\\vert s_n)}{q(a\\vert s_n)}Q_{\\theta_{old} }(s, a)]$\n\n$\\LARGE subject \\ to \\ E_{s \\sim \\rho_{\\theta_{old} }}[D_{KL}(\\pi_{\\theta_{old} })(\\cdot \\vert  s) \\ \\vert \\vert  \\ \\pi_{\\theta}(\\cdot \\vert  s)] \\leq \\delta$\n\n## 5. Sample-based Estimation of the Objective and Constraint\n\nEq. 14에서 Expectation은 샘플 평균으로 구하고, Q value는 실행했을때의 추정값으로 활용한다.\n\n아래는 추정을 수행하는 방식이 다른 2가지 에 대해 설명한다.\n\n### 5.1 Single Path\n\nsingle path 샘플링 방식은 일반적으로 policy gradient estimation에 사용하고\n\n개별적인 trajectory를 샘플링 한다.\n\n여기서 $\\large s_0 \\sim \\rho_{\\theta} \\leftarrow$ (discounted visitation distribution)\n\n상태들의 시퀀스를 모으고, trajectory를 생성하기위해 임의의 타임스텝만큼 policy인 $\\large \\pi_{\\theta_{old} }$ 을 시뮬레이션한다.\n\n$\\large (s_0 \\ , a_0 \\ , s_1 \\ , a_1 ... \\ s_{t-1} \\ , a_{t-1} \\ , s_t  )$\n\n따라서, $\\LARGE q(a\\vert s) = \\pi_{\\theta_{old} }$ 이다.\n\n$\\large Q_{\\theta_{old} }(s, a)$ 는 trajectory의 각 state-action 쌍인 $\\large (s_t, a_t)$ 의 $\\large \\sum \\gamma r(s,a)$ 를 계산한다.\n\n### 5.2 Vine\n\nvine 방식은 roll out set이라는 것을 만들고 roll out set의 각 상태마다 여러 액션을 수행해본다.\n\n추정 과정에서, 처음엔 $\\large s_0 \\sim \\rho_{\\theta}$ 샘플링하고 $\\large \\pi_{\\theta}$ 로 시뮬레이션을 반복해서 몇개의 trajectory들을 생성한다.\n\n다음엔 이 trajectory들 에서 N개의 상태를 부분집합으로 고른다. $\\large s_1, s_2, ... , s_N$ 이걸 roll out set이라 부른다.\n\nroll out set 각 상태 $\\large s_n$에서 $\\large a_{n , k} \\sim q(\\cdot \\vert  s_n)$ 에 대해 K개의 액션을 샘플링한다.\n\n이와 같은 방식은 exploration 측면에 있어 좋은 성능을 보였고, Robotic locomotion 과 Atari game, 즉 Continuous / discrete task 모두 잘 되었다.\n\n각 상태 $\\large s_n$ 에서 샘플링된 각 액션 $\\large a_{n , k}$에서 roll out을 수행하면서 (짧은 trajectory) $\\large \\overset{\\wedge}{Q}_{\\theta_i}(s_n, a_n, k)$ 을 추정한다.\n\n각 K개의 roll out에서 노이즈를 발생하기 위해 random number sequence를 사용한 roll out과 Q value의 차이의 분산을 매우 감소시켰다.\n\n작고 유한한 액션공간에서는 주어진 상태에서 모든 가능한 액션을 rollout을 통해 생성할 수 있었다.\n\n한 상태 $\\large s_n$ 에서 $\\large L_{\\theta_{old} }$ 에 기여하는 식은 다음과 같다.\n\n**Equation 15**\n\n$\\LARGE L_n(\\theta) = \\sum_{k=1}^K \\pi_{\\theta}(a_k\\vert s_n) \\overset{\\wedge}{Q}(s_n, a_k)$\n\n<br/>\n\n상태공간이 크고 continuous 한 상황에서, importance sampling을 활용해 surrogate objective를 추정할 수 있었다.\n\n한 상태 $\\large s_n$ 에서 $\\large L_{\\theta_{old} }$ 을 얻는 self normalized estimator[(Owen, 2013)](http://statweb.stanford.edu/~owen/mc/Ch-var-is.pdf)는 다음과 같다.\n\n**Equation 16**\n\n$\\LARGE L_n(\\theta) = \\frac{\\sum_{k=1}^{K} \\frac{\\pi_{\\theta}(a_{n,k} \\ \\vert  \\ s_n)}{\\pi_{\\theta_{old} }(a_{n,k} \\ \\vert  \\ s_n)} \\overset{\\wedge}{Q}(s_n, a_{n,k})}{\\sum_{k=1}^{K} \\frac{\\pi_{\\theta}(a_{n,k} \\ \\vert  \\ s_n)}{\\pi_{\\theta_{old} }(a_{n,k} \\ \\vert  \\ s_n)} }$\n\n여기서 상태 $\\large s_n$ 에서 K개의 액션을 수행한다고 가정한다. $\\large a_{n,1}, a_{n,2} \\ ... \\ a_{n,k} $\n\n이 self normalized estimator는 Q value를 구하기위해 베이스라인을 사용할 필요성을 없애준다. (Q value에 상수를 더함으로서 그라디언트가 바뀌지않음???)\n\n$\\large s_n \\sim \\rho(\\pi)$ 을 평균을 내줌으로서, $\\large L_{\\theta_{old} }$ estimator와 그라디언트를 얻을 수 있다.\n\n\n\n### 5.2 Vine\n\nvine은 샘플링을 통한 trajectory들이 다양한 포인트(rollout set)에서 여러갈래로 짧은 가지를 치고 있는 것이 마치 덩굴식물 줄기같아서 이름붙였다.\n\nsingle-path 보다 vine의 이점은 surrogate objective에서 같은 숫자의 Q-value 샘플들이 주어졌을때, 목적함수의 추정치가 낮은 분산을 갖는 것이다.\n\n즉, vine 방법은 advantage value의 아주 좋은 추정치를 갖게 해준다.\n\nvine의 불리한 부분은 반드시 이 advantage 추정 각각을 위해 시뮬레이터가 매우 많은 요청을 수행해야하는 것이다.\n\n게다가, vine은  rollout set 내부의 각 상태에서 다수의 trajectory들을 생성해야하는데 이것은 시스템이 임의의 상태로 초기화 될수 있는 환경으로 제한해버린다.\n\n반면, single path는 상태 초기화도 없고, 실세계로 직접 구현 가능하다.\n\n<br>\n## 6. Practical Algorithm\n\n앞서 Single-path, Vine 샘플링을 사용하는 두가지 방식의 policy optimization 알고리즘을 선보였다.\n\n반복적으로 아래의 과정을 수행한다.\n\n1) Q-values의 몬테카를로 추정과 state-action 쌍 집합을 single path / vine 과정을 통해 모은다.\n\n2) 샘플 평균으로, 목적함수와 eq.(14)의 제약식을 추정한다.  \neq.(14) : <br/>\n\n$\\LARGE \\underset{\\theta}{maximize} E_{s \\ \\sim \\ \\rho_{\\theta_{old} } \\ \\text{,} \\ a \\ \\sim \\ q} [\\frac{\\pi_{\\theta}(a\\vert s)}{q(a\\vert s)}Q_{\\theta_{old} }(s, a)]$\n\n$\\large subject \\ to \\ E_{s \\ \\sim \\ \\rho_{\\theta_{old} }} [D_{KL}(\\pi_{\\theta_{old} }(\\cdot \\vert  s) \\ \\vert \\vert  \\ \\pi_{\\theta}(\\cdot\\vert s))] \\leq \\delta$\n\n3) policy parameter vector인 $\\theta$를 업데이트 하면서 제약조건이 있는 최적화 문제를 근사적으로 푼다.\n\n본 논문에서는 1차미분보다는 좀 계산량이 있는 line search와 Conjugate Gradient algorithm을 사용했다.\n\n3)에 대해서,  그라디언트의 covariance matrix를 사용하지않고 KL Divergence의 헤시안을 해석적으로 계산해서  Fisher Information Matrix를 구성했다.\n\n다시말해서\n\n$\\LARGE \\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial}{\\partial \\theta_i} log \\pi_{\\theta}(a_n\\vert s_n) \\frac{\\partial}{\\partial \\theta_j} log \\pi_{\\theta}(a_n\\vert s_n)$\n\n$\\ \\ \\ \\ \\ \\ \\ \\ \\downarrow$\n\n$\\LARGE \\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_j} D_{KL}(\\pi_{\\theta_{old} }(\\cdot \\vert  s_n) \\vert \\vert  \\pi_{\\theta}(\\cdot \\vert  s_n))$\n\n**TODO KL divergence 미분과정**\n\n**\\TODO # 왜 KL divergence의 헤시안을 계산할때 covariance matrix 얘기가 나왔을까?**\n\nhttps://timvieira.github.io/blog/post/2014/10/06/kl-divergence-as-an-objective-function/\n\n이 analytic estimator는 헤시안이나 trajectories의 모든 그라디언트를 저장하지 않아도 되기 때문에 대규모 환경에서 계산 이점이 있다.\n\n**TODO Appendix C**\n\n- 3장에서 surrogate objective $M(\\pi)$ 과 KL divergence penalty를 활용한 최적화를 증명했으나 <br/>\n$C$가 크면 엄청나게 step size가 작아지게 된다. 그래서 $C$를 감소하려하는데, 강건하게 정하기가 어려워서 <br/>\npenalty대신, 강한 제약조건으로 KL divergence의 bound $\\delta $ 를 사용했다. -> (Trust Region)\n\n- $\\large D_{KL}^{max}(\\theta_{old}, \\theta)$ 제약조건은 추정이나 최적화를 하기 어려운 부분이 있어서 <br/>\n대신에 $\\large \\bar{D}_{KL}(\\theta_{old}, \\theta)$ 을 제약했다.\n\n- 본 논문은 advantage function의 추정에러를 무시한다. Kakade&Langford(2002) 에서도 이런 에러를 고려했고 상황도 비슷하지만 <br/>\n단순함을 위해 생략했다. (매우 작은 차이라?)\n\n<br>\n## 6. Practical Algorithm\n\n### Appendix C. Efficiently Solving the Trust-Region Constrained Optimization Problem\n\n풀어야 하는 식\n\n$\\LARGE maximize \\ L(\\theta) \\  subject \\ to \\ \\bar{D}_{KL}(\\theta_{old}, \\theta) \\leq \\delta$\n\n크게 두가지 과정으로 진행\n\n1) 탐색 방향을 계산, 목적함수의 선형근사 와 제약식의 2차 근사\n\n2) 계산한 방향으로 line search 수행, 비선형 제약식을 만족시키면서 비선형 목적함수를 개선\n\n\n탐색 방향은 $\\large Ax = g$ 수식을 근사적으로 풀면 계산된다. 여기서 $\\large A$ 는 Fisher Information Matrix이고,\n\nKL divergence 제약식인 $\\large \\bar{D}_{KL} (\\theta_{old}, \\theta) \\approx \\frac{1}{2}(\\theta - \\theta_{old})^T A(\\theta - \\theta_{old})$ 의 2차 근사이다.\n\n여기서 $\\large A_{ij} = \\frac{\\partial}{\\partial \\theta_i} \\frac{\\partial}{\\partial \\theta_j} \\bar{D}_{KL}(\\theta_{old}, \\theta)$\n\n대규모 문제에서 $A$ 혹은 $A^{-1}$ 을 계산하는 것은 엄청난 계산량이 필요.\n\n하지만 Conjugate Gradient Algorithm이 전체 $A$행렬(FIM)을 계산하지 않아도 $\\large Ax=g$ 를 근사적으로 해결할 수 있게 해준다.\n\n탐색 방향 $\\large s \\approx A^{-1}g$ 을 계산하면서, 최대스텝길이(step size) $\\large \\beta$를 계산할 필요가 있다.\n\n이 계산을 위해서 $\\large \\delta = \\bar{D}_{KL} \\approx \\frac{1}{2}(\\beta s)^T A(\\beta s) = \\frac{1}{2}\\beta^2s^TAs$ 이고\n\n여기서 $\\large \\beta = \\sqrt{2\\delta / s^T As}$ , 이때 $\\large \\delta$는 KL divergence의 boundary tgerm 이다.\n\n$\\large s^T As$ 텀은 헤시안 - 벡터 곱으로 계산될 수 있고, CG의 과정에서 계산된다.\n\n마지막으로, surrogate objective와 KL divergence 제약식을 만족하는 개선을 위해 line search 를 사용할 것이며, surrogate objective와 KL divergence 제약식은 둘 다 $\\large \\theta$ 에 대해 비선형이다.\n\n목적함수인 $\\large L_{\\theta_{old} }(\\theta) - \\chi [\\bar{D}_{KL}(\\theta_{old}, \\theta) \\leq \\delta] = 0$ 0이 맞으면 True고 무한으로 발산하면 False\n\n위에서 계산한 $\\large \\beta$ 의 최대 값부터 시작해서 목적함수가 개선될때까지 점점 줄여갈텐데, line search가 없었다면\n\n이 알고리즘은 매우 큰 스텝으로 계산될 것이고 성능의 심각한 저하가 발생될 것이다.\n\n### C.1 Computing the Fisher-Vector Product\n\n\n임의의 벡터와 평균한 Fisher Information Matrix를 어떻게 계산할지 서술하려한다.\n\n이 행렬-벡터 곱은 CG를 수행가능하게 한다. 파라미터화 한 policy가 입력 $\\large x$를 분포 파라미터 벡터인 $\\large \\mu_{\\theta}(x)$ 로 사상시킨다고 할때, $\\large \\pi(a \\vert x)$.\n\n주어진 입력 $\\large x$ 의 KL divergence는 아래 식과 같이 쓸 수 있다.\n\n$\\LARGE D_{KL} (\\pi_{\\theta_{old} }(\\cdot \\vert x)  \\ \\vert\\vert \\ \\pi_{\\theta}(\\cdot \\vert x)) = kl(\\mu_{\\theta}(x), \\mu_{old})$\n\n여기서 kl 은 두 mean 파라미터 벡터에 대응하는 분포 사이의 KL divergence이다. kl 을 $\\large \\theta$에 대해 두번 미분하면\n\n$\\LARGE \\frac{\\partial \\mu_{a}(x)}{\\partial \\theta_i} \\frac{\\partial \\mu_b(x)}{\\partial \\theta_j} kl^{''}_{ab}(\\mu_{\\theta}(x), \\mu_{old}) + \\frac{\\partial^2 \\mu_a(x)}{\\partial \\theta_i \\partial \\theta_j}kl^{'}_a(\\mu_{\\theta}(x), \\mu_{old})$\n\n여기서 두번째 term은 0이 된다. (두번미분하면 사라짐?)\n\n$\\LARGE J^T M J$\n\n## 7\n\n## 7. Connections with Prior Work - (1)\n\nNPG는 L의 선형 근사와 $\\large \\bar D_{KL}$ 제약식을 2차근사 하는 아래의 식의 special case.\n\n**equation 17. **\n\n$\\LARGE L_{\\pi}(\\tilde\\pi) = \\eta(\\pi) + \\underset{s}{\\sum}\\rho_{\\pi}(s) \\underset{a}{\\sum}\\tilde\\pi(a\\vert s)A_\\pi(s,a) $ - eq.(3)\n\n$\\LARGE \\underset{\\theta}{maximize} \\ [\\triangledown_{\\theta}L_{\\theta_{old} }(\\theta) \\ \\rvert_{\\theta=\\theta_{old} } \\cdot (\\theta - \\theta_{old})]$\n\n$\\LARGE subject \\ to \\ \\frac{1}{2}(\\theta_{old} - \\theta)^{T} A(\\theta_{old})(\\theta_{old}-\\theta) \\leq \\delta$\n\n$\\LARGE where \\ A(\\theta_{old})_{ij} = $\n\n$\\huge \\frac{\\partial}{\\partial \\theta_{i} } \\frac{\\partial}{\\partial \\theta_{j} }\nE_{s \\sim \\rho_{\\pi} }[D_{KL}(\\pi(\\cdot\\rvert s, \\theta_{old})  \\rvert\\rvert \\pi(\\cdot \\rvert s, \\theta))] \\rvert_{\\theta=\\theta_{old} }$\n\n## 7. Connections with Prior Work - (2)\n업데이트 식\n\n$\\LARGE \\theta_{new} = \\theta_{old} +  \\color{Red}{\\frac{1}{\\lambda} } A(\\theta_{old})^{-1}\\triangledown_{\\theta}L(\\theta) \\rvert_{\\theta=\\theta_{old} }$\n\n여기서 step size인 $\\LARGE \\frac{1}{\\lambda}$ 일반적으로 알고리즘의 파라미터로 취급된다.\n\n이 부분에서 TRPO는 각 업데이트 마다 제약식을 강제한다는 점이 다르다.\n\n좀 사소한 차이처럼 보이지만, 큰 차원을 다루는 실험에서 알고리즘의 성능을 크게 향상시켰다.\n\n또한 $\\LARGE \\mathcal{l}^2$ 제약식(혹은 페널티) 를 사용한 표준 Policy Gradient 업데이트 식을 얻었다.\n\n**Equation 18**\n\n\n$\\LARGE \\underset{\\theta}{maximize}[\\triangledown_{\\theta} L_{\\theta_{old} }(\\theta) \\rvert_{\\theta=\\theta_{old} } \\cdot (\\theta - \\theta_{old})]$\n\n$\\LARGE subject \\ to \\ \\frac{1}{2} \\vert\\vert \\theta - \\theta_{old} \\vert\\vert^2 \\leq \\delta$\n\n비제약 문제인 $\\underset{\\pi}{maximize} \\ L_{\\pi_{old} }(\\pi)$을\n\n$\\LARGE L_{\\pi}(\\tilde\\pi) = \\eta(\\pi) + \\underset{s}{\\sum}\\rho_{\\pi}(s) \\underset{a}{\\sum}\\tilde\\pi(a\\vert s)A_\\pi(s,a) $  - eq.(3)\n\n을 푸는 것으로 Policy Iteration update를 행한다.\n\n## 7. Connections with Prior Work - (3)\n\n$\\LARGE \\underset{\\pi}{maximize} \\ L_{\\pi_{old} }(\\pi)$  식과 유사하게 업데이트를 수행하는 다양한 방법론들이 있다.\n\n[REPS(Relative Entropy Policy Search)](https://pdfs.semanticscholar.org/ff47/526838ce85d77a50197a0c5f6ee5095156aa.pdf) 는\n\n[state-action marginal](http://rll.berkeley.edu/deeprlcourse/f17docs/lecture_3_rl_intro.pdf)인 $p(s,a)$ 를 제약하고 TRPO는 조건부인 $p(a\\vert s)$ 를 제약한다.\n\nREPS constraint \\vert  TRPO constraint\n:--------------:\\vert :---------------:\\vert \n![](https://i.imgur.com/U5Xgci1.png) \\vert  ![](https://i.imgur.com/12RPFh3.png)\n\nREPS랑은 달리, TRPO는 내부 루프 계산에서 비선형 최적화를 하지않는다.(REPS는 BFGS수행)\n\n**\\# REPS algorithm**\n\n![](https://i.imgur.com/F2m1yMG.png)\n\n<br>\n## 7. Connections with Prior Work - (4)\n\n[Levine and Abbeel(2014)](https://people.eecs.berkeley.edu/~svlevine/papers/mfcgps.pdf) 또한 KL Divergence 제약식을 사용했다.\n\n하지만 policy가 추정한 dynamics model이 유효한 영역에서 벗어나지않게 하는데에 목적이 있는 반면,\n\nTRPO는 system dynamics를 추정하지 않는다.\n\n[Pirotta et al. (2013)](https://pdfs.semanticscholar.org/f2c5/a91f35004aebc120360679f76488ac2613ee.pdf) 또한 [Kakade & Langford](https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/KakadeLangford-icml2002.pdf)의 결과를 일반화했으나 이들은 TRPO와는 다른 알고리즘을 도출했다.\n\n<br>\n## 8. Experiments\n\n- 3 가지 궁금증을 위한 실험을 설계함\n\n    1. Single-path 와 vine 의 성능면에서 특징을 알고싶음.\n\n    2. Fixed penalty coefficient(NPG) 보다 fixed KL divergence를 사용하는 것(TRPO)으로    \n    변경한 것이 얼마나 뚜렷한 차이를 보일지, 성능 면에서 어떤 영향을 미치는지 알고싶음\n\n    3. TRPO가 큰 스케일의 문제를 해결할 수 있는지, 기존에 연구/적용되어왔던 방법들과\n    성능, 계산시간, 샘플 복잡도 면에서 비교하고 싶음\n\n\n- 1, 2의 궁금증에 대해서 실험 환경에 single path, vine을 비롯한 여러 사전방법들을 함께 실험.\n\n- 3에 대해선, robotic locomotion과 Atari game에 TRPO를 실험함.\n\n\n## 8.1\n\n### 8.1 Simulated Robotic Locomotion\n\n- MuJoCo 시뮬레이터를 활용해 로봇 locomotion 실험 진행\n- Robot State: positions and velocities, joint torques\n\n![](https://i.imgur.com/bs5ATS3.png)\n\n- 수영, 점프, 걷기 행동을 학습시키고자 함.\n\n    - 수영: 10 dimensional state space , reward: $\\LARGE r(x,u)=v_x - 10^{-5}\\vert \\vert u\\vert \\vert ^2$, quadratic penalty\n\n    - 점프: 12 dimensional state space, episode의 끝은 높이와 각도 threshol로 점프를 판단, non-terminal state에 대해 +1 보상 추가\n\n    - 걷기: 18 dimensional state space, 뜀뛰는 듯한 보법(점프에이전트)보다 부드러운 보법을 유도하기위해 페널티를 추가했다함\n\n### 8.1 Simulated Robotic Locomotion\n#### Detailed Experiment Setup and used parameters, used network model\nused parameter \\vert  network model\n:----:\\vert :----:\n![](https://i.imgur.com/zgnsbw6.png)\\vert  ![](https://i.imgur.com/FqdWC53.png)\n\n### 8.1 Simulated Robotic Locomotion\n\nequation (12) : $\\huge maximize L_{\\theta_{old}(\\theta)} \\ , \\ \\ subject \\ to \\bar{D}_{KL}^{\\rho_{\\theta_{old} }}(\\theta_{old}, \\theta) \\leq \\delta$\n\n이 실험에서 $\\LARGE \\delta = 0.01$\n\n- 비교에 사용된 모델들\n\n    - TRPO Single-path\n    - TRPO vine\n    - CEM(cross-entropy method)\n    - CMA(covariance matrix adaptation)\n    - NPG (Lagrange Multiplier penalty coefficien를 사용하는 것이 차이점)\n    - Empirical FIM\n    - maximum KL(cartpole)\n\n### 8.1 Simulated Robotic Locomotion\n\n![](https://i.imgur.com/wSqolvS.png)\n\n- 제안한 single path, vine 모두 task를 잘 학습하는 것을 확인할 수 있음\n\n- 페널티를 고정하는 것보다, KL divergenc를 제약하는 것이 step size를 선택하는 부분에서 Robust 하다.\n\n- CEM/CMA는 derivative free 알고리즘으로, 샘플 복잡도가 네트워크 파라미터\u000b수 만큼 확장되어 high dimension 문제에서는 속도와 성능이 약한 모습을 보인다.\n\n- maxKL은 제안방법보다 느린 학습\n\n\n\n### 8.1 Simulated Robotic Locomotion\n\n\nTRPO video link: https://www.youtube.com/watch?v=jeid0wIrSn4\n\n- 사전지식이 적은 상태에서 여러 액션을 학습하는 실험으로 TRPO의 성능을 보임\n\n- 지금까지의 균형이나 걸음 같은 개념을 명시적으로 인코딩한 robotic locomotion에서의 사전 연구들과 차이를 보임\n\n\n## 8.2\n\n### 8.2 Playing Games from Images\n\n- 부분관찰가능하며 복잡한 observation인 환경에서 TRPO를 평가하기위해 raw image를 입력으로 하는 아타리게임 환경에서 실험\n\n* Challenging point :\n    - High dimensional space\n    - delayed reward\n    - non-stationary image (Enduro는 배경 이미지가 바뀌고, 반전처리가 됨)\n    - complex action sequence (Q*bert는 21개의 다른 발판에서 점프 해야함)\n\n- ALE(Arcade Learning Environment) 에서 테스트\n- DQN과 전처리 방식은 동일\n    210 x 160 pixel -> gray-scaling & down-sampling -> 110 x 84  -> crop -> 84 x 84\n\n\n\n### 8.2 Playing Games from Images\n\n- 2 convolution layer ( 16 channels) , stride 2, FC layer with 20 units\n\nparameter setup \\vert  network model\n:--------------:\\vert :-------------:\n![](https://i.imgur.com/NJBC69d.png) \\vert  ![](https://i.imgur.com/wTe1OEW.png)\n\n### 8.2 Playing Games from Images\n\n![](https://i.imgur.com/j22VtNQ.png)\n\n- 30시간 동안 500 iteration 의 학습\n\n- DQN과 MCST를 사용한 모델(UCC-I), 사람이 플레이한 것(Human) 과의 비교\n\n- 이전 방법들에 비해 압도적인 점수를 기록하진 못했으나, 다양한 게임에서 적절한 게임 기록.\n\n- 이전 방법들과 다른 점은, 특정 task에 초점을 맞춰 설계하지않았는데\n<br/>\nrobotics나 game playing Task에도 적절한 성능을 내는 점에서 **TRPO의 일반화 성능**을 보임\n\n<br>\n## 9. Discussion\n\n- Trust Region Policy Optimization 을 제안\n\n- KL Divergence 페널티로 $J(\\pi)$를 최적화하는 알고리즘이 monotonically improve 함을 증명\n\n- Locomotion 도메인에서 여러 행동(수영, 걷기, 점프)을 제어하는 controlle를 학습\n\n- TRPO가 향후 연구의 도약점이 되었으면 좋겠음\n\n- Robotics와 게임 실험을 결합해, 시각정보와 센서데이터를 사용하는 로봇제어 정책을 학습시키는 가능성을 봄\n\n- 샘플의 복잡도를 상당히 줄일수있어 실상황 적용가능성을 봄\n\n<br><br>\n# END\n","slug":"trpo","published":1,"updated":"2018-07-14T08:04:20.384Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjjl633g90000hz15gd4bxk4d","content":"<h1 id=\"Trust-Region-Policy-Optimization\"><a href=\"#Trust-Region-Policy-Optimization\" class=\"headerlink\" title=\"Trust Region Policy Optimization\"></a>Trust Region Policy Optimization</h1><p>Authors: John Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, Pieter Abbeel<br>Proceeding: International Conference on Machine Learning (ICML) 2015</p>\n<p>정리 by 공민서, 김동민</p>\n<h1 id=\"1-들어가며…\"><a href=\"#1-들어가며…\" class=\"headerlink\" title=\"1. 들어가며…\"></a>1. 들어가며…</h1><p>Trust region policy optimization (TRPO)는 상당히 우수한 성능을 보여주는 policy gradient 기법으로 알려져 있습니다. 높은 차원의 action space를 가진 robot locomotion부터 action은 적지만 화면을 그대로 처리하여 플레이하기 때문에 control parameter가 매우 많은 Atari game까지 각 application에 세부적으로 hyperparameter들을 특화시키지 않아도 두루두루 좋은 성능을 나타내기 때문에 일반화성능이 매우 좋은 기법입니다. 이 TRPO에 대해서 알아보겠습니다.</p>\n<p>※TRPO를 매우 재밌게 설명한 Crazymuse AI의 <a href=\"https://www.youtube.com/watch?v=CKaN5PgkSBc&amp;t=90s\" target=\"_blank\" rel=\"noopener\">Youtube video</a>에서 일부 그림을 차용했습니다. 이 비디오를 시청하시는 것을 강력하게 추천합니다!</p>\n<p><br><br></p>\n<h2 id=\"1-1-TRPO-흐름-잡기\"><a href=\"#1-1-TRPO-흐름-잡기\" class=\"headerlink\" title=\"1.1. TRPO 흐름 잡기\"></a>1.1. TRPO 흐름 잡기</h2><p>TRPO 논문은 많은 수식이 등장하여 이 수식들을 따라가다보면 큰 그림을 놓칠 수도 있습니다. 세부적인 내용을 살펴보기 전에 기존 연구에서 출발해서 TRPO로 어떻게 발전해나가는지 간략하게 살펴보겠습니다.</p>\n<h3 id=\"1-1-1-Original-Problem\"><a href=\"#1-1-1-Original-Problem\" class=\"headerlink\" title=\"1.1.1. Original Problem\"></a>1.1.1. Original Problem</h3><p>$$\\max_\\pi \\eta(\\pi)$$</p>\n<p>모든 강화학습이 그렇듯이 expected discounted reward를 최대화하는 policy를 찾는 문제로부터 출발합니다.</p>\n<h3 id=\"1-1-2-Conservative-Policy-Iteration\"><a href=\"#1-1-2-Conservative-Policy-Iteration\" class=\"headerlink\" title=\"1.1.2. Conservative Policy Iteration\"></a>1.1.2. Conservative Policy Iteration</h3><p>$$\\max L_\\pi(\\tilde\\pi) = \\eta(\\pi) + \\sum_s \\rho_\\pi(s)\\sum_a\\tilde\\pi(a\\vert s)A_\\pi(s,a)$$</p>\n<p>$\\eta(\\pi)$를 바로 최대화하는 것은 많은 경우 어렵습니다. $\\eta(\\pi)$의 성능향상을 보장하면서 policy를 update하는 conservative policy iteration 기법이 <a href=\"http://www.cs.cmu.edu/~./jcl/papers/aoarl/Final.pdf\" target=\"_blank\" rel=\"noopener\">Kakade와 Langford</a>에 의하여 제안되었습니다. 이 기법을 이용하면 policy update가 성능을 악화시키지는 않는다는 것이 이론적으로 보장됩니다.</p>\n<h3 id=\"1-1-3-Theorem-1-of-TRPO\"><a href=\"#1-1-3-Theorem-1-of-TRPO\" class=\"headerlink\" title=\"1.1.3. Theorem 1 of TRPO\"></a>1.1.3. Theorem 1 of TRPO</h3><p>$$\\max L_{\\pi_\\mathrm{old} }\\left(\\pi_\\mathrm{new}\\right) - \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2,\\quad\\left(\\alpha=D_\\mathrm{TV}^\\max\\left(\\pi_\\mathrm{old},\\pi_\\mathrm{new}\\right)\\right)$$</p>\n<p>기존의 conservative policy iteration은 과거 policy와 새로운 policy를 섞어서 사용해서 실용적이지 않다는 단점이 있었는데 이것을 보완하여 온전히 새로운 policy만으로 update할 수 있는 기법을 제안합니다. </p>\n<h3 id=\"1-1-4-KL-divergence-version-of-Theorem-1\"><a href=\"#1-1-4-KL-divergence-version-of-Theorem-1\" class=\"headerlink\" title=\"1.1.4. KL divergence version of Theorem 1\"></a>1.1.4. KL divergence version of Theorem 1</h3><p>$$\\max L_{\\pi}\\left(\\tilde\\pi\\right) - C\\cdot D_\\mathrm{KL}^\\max\\left(\\pi,\\tilde\\pi\\right),\\quad\\left(C = \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\right)$$</p>\n<p>distance metric을 KL divergence로 바꿀 수 있습니다.</p>\n<h3 id=\"1-1-5-Using-parameterized-policy\"><a href=\"#1-1-5-Using-parameterized-policy\" class=\"headerlink\" title=\"1.1.5. Using parameterized policy\"></a>1.1.5. Using parameterized policy</h3><p>$$\\max_\\theta L_{\\theta_\\mathrm{old} }(\\theta) - C\\cdot D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right)$$</p>\n<p>최적화문제를 더욱 편리하게 풀 수 있도록 낮은 dimension을 가지는 parameter들로 parameterized된 policy를 사용할 수 있습니다.</p>\n<h3 id=\"1-1-6-Trust-region-constraint\"><a href=\"#1-1-6-Trust-region-constraint\" class=\"headerlink\" title=\"1.1.6. Trust region constraint\"></a>1.1.6. Trust region constraint</h3><p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;L_{\\theta_\\mathrm{old} }(\\theta) \\\\<br>\\mathrm{s.t.\\ }&amp;D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right) \\leq \\delta<br>\\end{align}$$</p>\n<p>5까지는 아직 conservative policy iteration을 약간 변형시킨 것입니다. policy를 update할 때 지나치게 많이 변하는 것을 방지하기 위하여 <a href=\"https://en.wikipedia.org/wiki/Trust_region\" target=\"_blank\" rel=\"noopener\">trust region</a>을 constraint로 설정할 수 있습니다. 이 아이디어로 인해서 TRPO라는 명칭을 가지게 됩니다.</p>\n<h3 id=\"1-1-7-Heuristic-approximation\"><a href=\"#1-1-7-Heuristic-approximation\" class=\"headerlink\" title=\"1.1.7. Heuristic approximation\"></a>1.1.7. Heuristic approximation</h3><p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;L_{\\theta_\\mathrm{old} }(\\theta) \\\\<br>\\mathrm{s.t.\\ }&amp;\\overline{D}_\\mathrm{KL}^{\\rho_\\mathrm{old} }\\left(\\theta_\\mathrm{old}, \\theta\\right) \\leq \\delta<br>\\end{align}$$</p>\n<p>사실 6의 constraint는 모든 state에 대해서 성립해야 하기 때문에 문제를 매우 어렵게 만듭니다. 이것을 좀 더 다루기 쉽게 state distribution에 대한 평균을 취한 것으로 변형합니다.</p>\n<h3 id=\"1-1-8-Monte-Carlo-Simulation\"><a href=\"#1-1-8-Monte-Carlo-Simulation\" class=\"headerlink\" title=\"1.1.8. Monte Carlo Simulation\"></a>1.1.8. Monte Carlo Simulation</h3><p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}Q_{\\theta_\\mathrm{old} }(s,a)\\right] \\\\<br>\\mathrm{s.t.\\ }&amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} }}\\left[D_\\mathrm{KL}\\left(\\pi_{\\theta_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta<br>\\end{align}$$</p>\n<p>Sampling을 통한 계산이 가능하도록 식을 다시 표현할 수 있습니다. </p>\n<h3 id=\"1-1-9-Efficiently-solving-TRPO\"><a href=\"#1-1-9-Efficiently-solving-TRPO\" class=\"headerlink\" title=\"1.1.9. Efficiently solving TRPO\"></a>1.1.9. Efficiently solving TRPO</h3><p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;\\nabla_\\theta \\left. L_{\\theta_\\mathrm{old} }(\\theta)\\right\\vert _{\\theta=\\theta_\\mathrm{old} } \\left(\\theta - \\theta_\\mathrm{old}\\right) \\\\<br>\\mathrm{s.t.\\ }&amp;\\frac{1}{2}\\left(\\theta_\\mathrm{old} - \\theta \\right)^T A\\left(\\theta_\\mathrm{old}\\right)\\left(\\theta_\\mathrm{old} - \\theta \\right) \\leq \\delta \\\\<br>&amp;A_{ij} = \\frac{\\partial}{\\partial \\theta_i}\\frac{\\partial}{\\partial \\theta_j}\\overline{D}_\\mathrm{KL}^{\\rho_\\mathrm{old} }\\left(\\theta_\\mathrm{old}, \\theta\\right)<br>\\end{align}$$</p>\n<p>문제를 효율적으로 풀기 위하여 approximation을 적용할 수 있습니다. objective function은 first order approximation, constraint는 quadratic approximation을 취하면 효율적으로 문제를 풀 수 있는 형태로 바뀌는데 이것을 natural gradient로 풀 수도 있고 conjugate gradient로 풀 수도 있습니다.</p>\n<p>TRPO는 이렇게 다양한 방법으로 문제를 변형한 것입니다! 이제 좀 더 자세히 살펴보겠습니다.</p>\n<p><br><br></p>\n<h1 id=\"2-Preliminaries\"><a href=\"#2-Preliminaries\" class=\"headerlink\" title=\"2. Preliminaries\"></a>2. Preliminaries</h1><p>다음과 같은 파라미터를 가지는 infinite-horizon discounted Markov decision process (MDP)를 고려합니다. </p>\n<ul>\n<li>$\\mathcal{S}$: finite set of states</li>\n<li>$\\mathcal{A}$: finite set of actions</li>\n<li>$P: \\mathcal{S}\\times\\mathcal{A}\\rightarrow\\mathbb{R}$: transition probability distribution</li>\n<li>$r: \\mathcal{S}\\rightarrow\\mathbb{R}$: reward function</li>\n<li>$\\rho_0:\\mathcal{S}\\rightarrow\\mathbb{R}$: distribution of the initial state $s_0$</li>\n<li>$\\gamma\\in(0,1)$: discount factor</li>\n<li>$\\eta(\\pi)=E_{s_0,a_0,\\ldots}\\left[\\sum_{t=0}^\\infty\\gamma^t r\\left(s_t\\right)\\right]$, where $s_0\\sim\\rho_0\\left(s_0\\right), a_t\\sim\\pi\\left(\\left.a_t \\right\\vert s_t\\right), s_{t+1}\\sim P\\left(\\left.s_{t+1}\\right\\vert s_t,a_t\\right)$</li>\n<li>$Q_\\pi\\left(s_t,a_t\\right)=E_{\\color{red}s_{t+1},a_{t+1},\\ldots}\\left[\\sum_{l=0}^\\infty \\gamma^l r\\left(s_{t+l}\\right)\\right]$: action value function</li>\n<li>$V_\\pi\\left(s_t\\right)=E_{\\color{red}a_t, s_{t+1},a_{t+1},\\ldots}\\left[\\sum_{l=0}^\\infty \\gamma^l r\\left(s_{t+l}\\right)\\right]$: value function (action value function과 expectation의 첨자가 다른 부분을 유의하세요.)</li>\n<li>$A_\\pi(s,a) = Q_\\pi(s,a) - V_\\pi(s)$: advantage function</li>\n</ul>\n<h2 id=\"2-1-Useful-identity-Kakade-amp-Langford-2002\"><a href=\"#2-1-Useful-identity-Kakade-amp-Langford-2002\" class=\"headerlink\" title=\"2.1 Useful identity [Kakade &amp; Langford 2002]\"></a>2.1 Useful identity [Kakade &amp; Langford 2002]</h2><p>우리의 목표는 $\\eta(\\pi)$가 최대화되도록 만드는 것입니다. 하지만 $\\pi$의 변화에 따라 $\\eta$가 어떻게 변하는지 알아내는 것도 쉽지 않습니다. $\\pi$는 기존의 policy이고 $\\tilde\\pi$는 새로운 policy를 나타낸다고 할 때 $\\eta$와 policy update 사이의 관계에 대해서 다음과 같은 관계가 있다는 것이 밝혀졌습니다.</p>\n<p><strong>Lemma 1.</strong> $\\eta\\left(\\tilde\\pi\\right) = \\eta(\\pi) + E_{s_{0},a_{0},\\ldots\\sim\\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t A_\\pi\\left(s_t,a_t\\right)\\right]$</p>\n<p><em>Proof.</em> $A_\\pi(s,a)=E_{s’\\sim P\\left(s’\\vert s,a\\right)}\\left[r(s)+\\gamma V_\\pi\\left(s’\\right)-V_\\pi(s)\\right]$로 다시 표현할 수 있습니다. 표기의 편의를 위하여 $\\tau:=\\left(s_0,a_0,s_1,a_1,\\ldots\\right)$를 정의하겠습니다. 다음과 같은 수식 전개가 가능합니다.<br>$$<br>\\begin{align}<br>&amp;E_{\\tau\\vert \\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t A_\\pi\\left(s_t,a_t\\right)\\right] \\\\<br> &amp;= E_{\\tau\\vert \\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t \\left(r(s_t)+\\gamma V_\\pi\\left(s_{t+1}\\right)-V_\\pi(s_t)\\right)\\right] \\\\<br>&amp;= E_{\\tau\\vert \\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t r(s_t)+\\gamma V_\\pi\\left(s_{1}\\right)-V_\\pi(s_0)+\\gamma^2 V_\\pi\\left(s_{2}\\right)-\\gamma V_\\pi(s_1)+\\gamma^3 V_\\pi\\left(s_{3}\\right)-\\gamma^2 V_\\pi(s_2)+\\cdots\\right] \\\\<br>&amp;= E_{\\tau\\vert \\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t r(s_t)+\\cancel{\\gamma V_\\pi\\left(s_{1}\\right)}-V_\\pi(s_0)+\\cancel{\\gamma^2 V_\\pi\\left(s_{2}\\right)}-\\cancel{\\gamma V_\\pi(s_1)}+\\cdots\\right] \\\\<br>&amp;= E_{\\tau\\vert \\tilde\\pi}\\left[-V_\\pi(s_0)+\\sum_{t=0}^\\infty\\gamma^t r(s_t)\\right] \\\\<br>&amp;\\overset{\\underset{\\mathrm{(a)} }{} }{=} -E_{\\color{red}s_0}\\left[V_\\pi(s_0)\\right]+E_{\\tau\\vert \\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t r(s_t)\\right] \\\\<br>&amp;=-\\eta(\\pi) + \\eta\\left(\\tilde\\pi\\right)\\\\<br>&amp;\\quad\\therefore \\eta\\left(\\tilde\\pi\\right) = \\eta(\\pi) + E_{s_{0},a_{0},\\ldots\\sim\\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t A_\\pi\\left(s_t,a_t\\right)\\right]<br>\\end{align}<br>$$<br>위의 전개의 (a) 부분은 $V_\\pi\\left(s_0\\right)=E_{a_0,s_0,a_1,\\ldots}\\left[\\sum_{t=0}^\\infty\\gamma^l r\\left(s_l\\right)\\right]$이므로 $\\tau$ 중 많은 부분들이 이미 expectation이 취해진 값이므로 무시됩니다. 오직 $s_0$만 expectation이 취해지지 않았기 때문에 남아있습니다. $\\square$</p>\n<p><strong>Lemma 1</strong>은 새로운 policy와 기존의 policy 사이의 관계를 규정합니다. 다음을 정의합니다. </p>\n<ul>\n<li>$\\rho_\\pi(s) = P\\left(s_0=s\\right) + \\gamma P\\left(s_1=s\\right)\\ + \\gamma^2 P\\left(s_2=s\\right) + \\cdots $: (unnormalized) discounted visitation frequencies</li>\n</ul>\n<p>이것을 이용해서 <strong>Lemma 1</strong>을 변형시켜 봅시다.</p>\n<p>$$<br>\\begin{align}<br>\\eta\\left(\\tilde\\pi\\right) &amp;= \\eta(\\pi) + \\sum_{t=0}^\\infty\\sum_{s}P\\left(\\left.s_t=s\\right\\vert \\tilde\\pi\\right)\\sum_a\\tilde\\pi(a\\vert s)\\gamma^t A_\\pi(s,a) \\\\<br>&amp;= \\eta(\\pi) + \\sum_{s}{\\color{red}\\sum_{t=0}^\\infty \\gamma^t P\\left(\\left.s_t=s\\right\\vert \\tilde\\pi\\right)}\\sum_a\\tilde\\pi(a\\vert s) A_\\pi(s,a) \\\\<br>&amp;= \\eta(\\pi) + \\sum_{s}{\\color{red}\\rho_\\tilde\\pi(s)}\\sum_a\\tilde\\pi(a\\vert s) A_\\pi(s,a) \\\\<br>\\end{align}<br>$$</p>\n<p>이 수식의 의미가 무엇일까요? 만약 $\\sum_a\\tilde\\pi(a\\vert s) A_\\pi(s,a) \\geq 0$이라면 $\\eta(\\tilde\\pi)$는 항상 $\\eta(\\pi)$보다 큽니다. 즉, policy를 업데이트함으로써 항상 개선됩니다. 즉, 이 수식을 통해서 항상 더 좋은 성능을 내는 policy update가 가져야 할 특징을 알 수 있습니다. 다음과 같은 deterministic policy가 있다고 합시다.</p>\n<p>$$<br>\\tilde\\pi(s) = \\arg\\max_a A_\\pi(s,a)<br>$$</p>\n<p>이 policy는 적어도 하나의 state-action pair에서 0보다 큰 값을 가지는 advantage가 있고 그 때의 확률이 0이 아니라면 항상 성능을 개선시킵니다. 어려운 점은 policy를 바꾸면 $\\rho$도 바뀐다는 점입니다. </p>\n<p>다음 그림을 봅시다. Starting Point에서 여러가지 경로를 거쳐서 Destination으로 갈 수 있습니다. 다른 policy를 이용하는 것은 다른 경로를 이용한 것입니다.</p>\n<p><img src=\"../img/policy_change.png\" alt=\"policy_change\"></p>\n<p>이 때 다른 policy를 이용함에 따라 state visitation frequency도 변하게 됩니다.</p>\n<p><img src=\"../img/state_visitation_change.png\" alt=\"state_visitation_change\"></p>\n<p>이로 인해서 policy를 최적화하는 것은 어려워집니다. 그래서 이러한 변화를 무시하는 다음과 같은 approximation을 취합니다.</p>\n<p>$$<br>\\begin{align}<br>L_\\pi\\left(\\tilde\\pi\\right) &amp;= \\eta(\\pi) + \\sum_{s}{\\color{red}\\rho_\\pi(s)}\\sum_a\\tilde\\pi(a\\vert s) A_\\pi(s,a)<br>\\end{align}<br>$$</p>\n<p>policy가 바뀌었음에도 이전의 state distribution을 게속 이용하는 것입니다. 이것은 policy의 변화가 크지 않다면 어느 정도 허용될 수 있을 것입니다. 그렇지만 얼마나 많은 변화가 허용될까요? 이것을 정하기 위해서 이용하는 것이 trust region입니다.</p>\n<p><br></p>\n<h2 id=\"2-1-Conservative-Policy-Iteration\"><a href=\"#2-1-Conservative-Policy-Iteration\" class=\"headerlink\" title=\"2.1. Conservative Policy Iteration\"></a>2.1. Conservative Policy Iteration</h2><p>policy의 변화를 다루기 용이하게 하기 위해서 policy를 다음과 같이 파라미터를 이용해서 표현합시다.</p>\n<ul>\n<li>$\\pi_\\theta(a\\vert s)$: parameterized policy</li>\n</ul>\n<p>$\\pi_\\theta$는 $\\theta$에 대하여 미분가능한 함수입니다. $L_\\pi\\left(\\tilde\\pi\\right)$은 $\\theta_0$에서 $\\eta(\\pi)$에 대한 first order approximation입니다.</p>\n<p>$$<br>\\begin{align}<br>L_{\\pi_{\\theta_0} }\\left(\\pi_{\\theta_0}\\right) &amp;= \\eta\\left(\\pi_{\\theta_0}\\right) \\\\<br>\\nabla_\\theta \\left.L_{\\pi_{\\theta_0} }\\left(\\pi_{\\theta_0}\\right)\\right\\vert _{\\theta=\\theta_0} &amp;= \\nabla_\\theta\\left.\\eta(\\pi_{\\theta_0})\\right\\vert _\\theta=\\theta_0<br>\\end{align}<br>$$</p>\n<p>이것의 의미는 $\\pi_{\\theta_0}$가 매우 작게 변한다면 $L_{\\pi_{\\theta_0} }$를 개선시키는 것이 $\\eta$를 개선시키는 것이라는 것입니다. 그러나 지금까지의 설명만으로는 $\\pi_{\\theta_0}$를 얼마나 작게 변화시켜야 할지에 대해서는 알 수 없습니다.</p>\n<p>Kakade &amp; Langford가 2002년 발표한 논문에서 이것을 고려하였습니다. 그 논문에서 <em>conservative policy iteration</em>이라는 기법을 제안합니다. 이 논문의 contribution은 다음과 같습니다.</p>\n<ul>\n<li>$\\eta$ 개선의 lower bound를 제공</li>\n<li>기존의 policy를 $\\pi_\\mathrm{old}$라고 하고 $\\pi’$를 $\\pi’=\\arg\\max_{\\pi’}L_{\\pi_\\mathrm{old} }\\left(\\pi’\\right)$과 같이 정의할 때, 새로운 mixture policy $\\pi_\\mathrm{new}$를 다음과 같이 제안<br>$$<br>\\pi_\\mathrm{new}(a\\vert s) = (1-\\alpha)\\pi_\\mathrm{old}(a\\vert s) + \\alpha \\pi’(a\\vert s)<br>$$</li>\n</ul>\n<p>그림으로 표현하면 다음과 같습니다.</p>\n<p><img src=\"../img/mixure_policy.png\" alt=\"mixure_policy\"></p>\n<ul>\n<li>다음과 같은 lower bound를 정의<br>$$<br>\\eta\\left(\\pi_\\mathrm{new}\\right) \\geq L_{\\pi_{\\theta_\\mathrm{old} }}\\left(\\pi_\\mathrm{new}\\right) - \\frac{2\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2,\\\\<br>\\mathrm{where\\ }\\epsilon = \\max_s\\left\\vert E_{a\\sim\\pi’(a\\vert s)}\\left[A_\\pi(s,a)\\right]\\right\\vert<br>$$</li>\n</ul>\n<p>하지만 mixture된 policy라는 것은 실용적이지 않습니다. </p>\n<p><br><br></p>\n<h1 id=\"3-Monotonic-Improvement-Guarantee-for-General-Stochastic-Policies\"><a href=\"#3-Monotonic-Improvement-Guarantee-for-General-Stochastic-Policies\" class=\"headerlink\" title=\"3. Monotonic Improvement Guarantee for General Stochastic Policies\"></a>3. Monotonic Improvement Guarantee for General Stochastic Policies</h1><p>이전 장에서 설명한 lower bound는 오직 mixture policy에 대해서만 성립하고 보다 더 많이 사용되는 stochastic policy에는 적용되지 않습니다. 따라서 이러한 방향으로 개선이 필요합니다. 기존 수식의 두 가지를 바꿈으로써 이것을 달성할 수 있습니다.</p>\n<ul>\n<li>$\\alpha\\rightarrow$ distance measure between $\\pi$ and $\\tilde\\pi$</li>\n<li>constant $\\epsilon\\rightarrow\\max_{s,a}\\left\\vert A_\\pi(s,a)\\right\\vert $</li>\n</ul>\n<p>여기서 distance measure로 total variation divergence를 이용합니다. discrete porbability distribution $p$와 $q$에 대하여 다음과 같이 정의됩니다.<br>$$<br>D_\\mathrm{TV}(p\\parallel q) = \\frac{1}{2}\\sum_i\\left\\vert p_i - q_i\\right\\vert<br>$$</p>\n<p>이것을 이용하여 $D_\\mathrm{TV}^\\max$를 다음과 같이 정의하겠습니다.<br>$$<br>D_\\mathrm{TV}^\\max(\\pi\\parallel \\tilde\\pi) = \\max_s D_\\mathrm{TV}\\left(\\pi(\\cdot\\vert s)\\parallel\\tilde\\pi(\\cdot\\vert s)\\right)<br>$$</p>\n<p>그림으로 표현하면 다음과 같습니다.</p>\n<p><img src=\"../img/tvd.png\" alt=\"tvd\"></p>\n<p>이것을 이용하여 다음과 같은 관계식을 얻을 수 있습니다.</p>\n<p><strong>Theorem 1.</strong> Let $\\alpha=D_\\mathrm{TV}^\\max(\\pi\\parallel \\tilde\\pi)$. Then the following bound holds:<br>$$<br>\\begin{align}<br>\\eta\\left(\\pi_\\mathrm{new}\\right) &amp;\\geq L_{\\pi_\\mathrm{old} }\\left(\\pi_\\mathrm{new}\\right) - \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2, \\\\<br>\\mathrm{where\\ } \\epsilon&amp;= \\max_{s,a}\\left\\vert A_\\pi(s,a)\\right\\vert<br>\\end{align}<br>$$<br><em>Proof.</em> TBD.</p>\n<p>또다른 distance metric으로 아래 그림과 같은 KL divergence가 있습니다.<br>(그런데 왜 하필 KL divergence로 바꿀까요? 논문의 뒤쪽에서 계산효율을 위해서 conjugate gradient method를 이용하는데 이를 위해서 바꾼게 아닐까 싶습니다. Wasserstein distance 같은 다른 방향으로 발전시킬 수도 있을 것 같습니다. Schulmann이 아마 해봤겠죠?^^)</p>\n<p><img src=\"../img/kld.png\" alt=\"kld\"></p>\n<p>total variation divergence와 KL divergence 사이에는 다음과 같은 관계가 있습니다.<br>$$<br>D_\\mathrm{TV}(p\\parallel q)^2 \\leq D_\\mathrm{KL}(p\\parallel q)<br>$$</p>\n<p>다음 수식을 정의합니다.<br>$$<br>D_\\mathrm{KL}^\\max(\\pi\\parallel \\tilde\\pi) = \\max_s D_\\mathrm{KL}\\left(\\pi(\\cdot\\vert s)\\parallel\\tilde\\pi(\\cdot\\vert s)\\right)<br>$$</p>\n<p><strong>Theorem 1</strong>을 이용하여 다음과 같은 수식이 성립함을 알 수 있습니다.<br>$$<br>\\begin{align}<br>\\eta\\left(\\tilde\\pi\\right) &amp;\\geq L_{\\pi}\\left(\\tilde\\pi\\right) - C\\cdot D_\\mathrm{KL}^\\max(\\pi, \\tilde\\pi), \\\\<br>\\mathrm{where\\ } C&amp;= \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}<br>\\end{align}<br>$$</p>\n<p>이러한 policy imporvement bound를 기반으로 다음과 같은 approximate policy iteration 알고리듬을 고안해낼 수 있습니다.</p>\n<p><strong>Algorithm 1</strong> Policy iteration algorithm guaranteeing non-decreasing expected return $\\eta$</p>\n<blockquote>\n<p>Initialize $\\pi_0$<br><strong>for</strong> $i=0,1,2,\\ldots$ until convergence <strong>do</strong><br>$\\quad$Compute all advantage values $A_{\\pi_i}(s,a)$<br>$\\quad$Solve the contstrained optimization problem<br>$\\quad\\pi_{i+1}=\\arg\\max_\\pi\\left[L_{\\pi_i} - CD_\\mathrm{KL}^\\max\\left(\\pi_i,\\pi\\right)\\right]$<br>$\\quad\\quad$where $C = 4\\epsilon\\gamma / (1-\\gamma)^2$<br>$\\quad\\quad$and $L_{\\pi_i}\\left(\\pi\\right) = \\eta(\\pi_i) + \\sum_{s}\\rho_{\\pi_i}(s)\\sum_a\\pi(a\\vert s) A_{\\pi_i}(s,a)$<br><strong>end for</strong></p>\n</blockquote>\n<p><strong>Algorithm 1</strong>은 advantage를 정확하게 계산할 수 있다고 가정하고 있습니다. 이 알고리듬은  monotonical한 성능 증가($\\eta(\\pi_0)\\leq\\eta(\\pi_1)\\leq\\cdots$)를 한다는 것을 다음과 같이 보일 수 있습니다. $M_i(\\pi)=L_{\\pi_i}(\\pi) - CD_\\mathrm{KL}^\\max\\left(\\pi_i,\\pi\\right)$라고 합시다.<br>$$\\begin{align}<br>\\eta_\\left(\\pi_{i+1}\\right) &amp;\\geq M_i\\left(\\pi_{i+1}\\right)\\\\<br>\\eta_\\left(\\pi_{i}\\right) &amp;= M_i\\left(\\pi_{i}\\right) {\\color{blue}\\mathrm{\\ (why\\ same?)} }\\\\<br>\\eta_\\left(\\pi_{i+1}\\right) - \\eta_\\left(\\pi_{i}\\right) &amp;\\geq M_i\\left(\\pi_{i+1}\\right) - M_i\\left(\\pi_{i}\\right)<br>\\end{align}$$</p>\n<p>위 수식과 같이 매 iteration 마다 $M_i$를 최대화함으로써, $\\eta$가 감소하지 않는다는 것을 보장할 수 있습니다. 이와 같은 타입의 알고리듬을 minorization-maximization (MM) algorithm이라고 합니다. EM 알고리듬도 이 타입의 알고리듬 중 하나입니다.</p>\n<p><strong>Algorithm 1</strong>은 아래 그림과 같이 동작합니다.</p>\n<p><img src=\"../img/surrogate.png\" alt=\"surrogate\"></p>\n<p>$M_i$는 $\\pi_i$일 때 equality가 되는 $\\eta$에 대한  surrogate function입니다. TRPO는 이 surrogate function을 최대화하고 KL divergence를 penalty가 아닌 constraint로 두는 알고리듬입니다.</p>\n<p><br><br></p>\n<h1 id=\"4-Optimization-of-Parameterized-Policies\"><a href=\"#4-Optimization-of-Parameterized-Policies\" class=\"headerlink\" title=\"4. Optimization of Parameterized Policies\"></a>4. Optimization of Parameterized Policies</h1><p>표기의 편의를 위해 다음과 같이 notation들을 정의합니다.</p>\n<ul>\n<li>$\\eta(\\theta):=\\eta\\left(\\pi_\\theta\\right)$</li>\n<li>$L_\\theta\\left(\\tilde\\theta\\right):=L_{\\pi_\\theta}\\left(\\pi_{\\tilde\\theta}\\right)$</li>\n<li>$D_\\mathrm{KL}\\left(\\theta\\parallel\\tilde\\theta\\right):=D_\\mathrm{KL}\\left(\\pi_\\theta\\parallel\\pi_{\\tilde\\theta}\\right)$</li>\n<li>$\\theta_\\mathrm{old}$: previous policy parameter</li>\n</ul>\n<h2 id=\"4-1-Trust-Region-Policy-Optimization\"><a href=\"#4-1-Trust-Region-Policy-Optimization\" class=\"headerlink\" title=\"4.1. Trust Region Policy Optimization\"></a>4.1. Trust Region Policy Optimization</h2><p>이전 장의 중요 결과를 위의 notation으로 다시 표기하면 아래와 같습니다.</p>\n<p>$$<br>\\eta\\left(\\theta\\right) \\geq L_{\\theta_\\mathrm{old} }\\left(\\theta\\right) - C\\cdot D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right)<br>$$</p>\n<p>$\\eta$의 성능 향상을 보장하기 위해서 그것의 lower bound를 최대화할 수 있습니다.</p>\n<p>$$<br>\\max_\\theta \\left[L_{\\theta_\\mathrm{old} }\\left(\\theta\\right) - C D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right)\\right]<br>$$</p>\n<p>이 최적화 문제는 step size를 매우 작게 해야 올바른 동작을 합니다. 위에서 살펴봤듯이 first order approximation이기 때문입니다. 좀 더 큰 step size를 가질 수 있도록 이 최적화 문제를 trust region constraint를 도입하여 다음과 같이 바꿉니다.</p>\n<p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;L_{\\theta_\\mathrm{old} }(\\theta) \\\\<br>\\mathrm{s.t.\\ }&amp;D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right) \\leq \\delta<br>\\end{align}$$</p>\n<p>이 최적화문제의 constraint는 모든 state space에 대해서 성립해야 합니다. 또한 maximum값을 매번 찾아야 합니다. state가 많은 경우 constraint의 수가 매우 많아지고 이것은 문제를 풀기 어렵게 만듭니다. constraint의 수를 줄이기 위하여 다음과 같은 avergae KL divergence를 이용하는 heuristic approximation을 취합니다. 이것이 최선의 방법은 아닐 수 있지만 실용적인 방법입니다.</p>\n<p>$$<br>\\overline D_\\mathrm{KL}^{\\rho}\\left(\\theta_1, \\theta_2\\right):=E_{s\\sim\\rho}\\left[D_\\mathrm{KL}\\left(\\pi_{\\theta_1}(\\cdot\\vert s)\\parallel\\pi_{\\theta_2}(\\cdot\\vert s)\\right)\\right]<br>$$</p>\n<p>이것을 기반으로 다음과 같은 최적화 문제를 풀 수 있습니다.</p>\n<p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;L_{\\theta_\\mathrm{old} }(\\theta) \\\\<br>\\mathrm{s.t.\\ }&amp;\\overline{D}_\\mathrm{KL}^{\\rho_\\mathrm{old} }\\left(\\theta_\\mathrm{old}, \\theta\\right) \\leq \\delta<br>\\end{align}$$</p>\n<p>아래 그림처럼 step size에 대해서 고려할 수 있습니다.</p>\n<p><img src=\"../img/heuristic_approx.png\" alt=\"heuristic_approx\"></p>\n<p><br><br></p>\n<h1 id=\"5-Sample-Based-Estimation-of-the-Objective-and-Constraint\"><a href=\"#5-Sample-Based-Estimation-of-the-Objective-and-Constraint\" class=\"headerlink\" title=\"5. Sample-Based Estimation of the Objective and Constraint\"></a>5. Sample-Based Estimation of the Objective and Constraint</h1><p>실용적인 알고리듬을 만들려고 하는 노력은 아직 끝나지 않았습니다. 이제 앞의 알고리듬을 sample-based estimation 즉, Monte Carlo estimation을 할 수 있도록 바꿔보겠습니다. sampling을 편하게 할 수 있도록 아래와 같이 바꿔줍니다.</p>\n<ul>\n<li>$\\sum_s \\rho_{\\theta_\\mathrm{old} }(s)[\\ldots]\\rightarrow\\frac{1}{1-\\gamma}E_{s\\sim\\rho_{\\theta_\\mathrm{old} }}[\\ldots]$</li>\n<li>$A_{\\theta_\\mathrm{old} }\\rightarrow Q_{\\theta_\\mathrm{old} }$</li>\n<li>$\\sum_a\\pi_{\\theta_\\mathrm{old} }(a\\vert s)A_{\\theta_\\mathrm{old} }\\rightarrowE_{a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}A_{\\theta_\\mathrm{old} }\\right]$</li>\n</ul>\n<p>이러한 변화를 그림처럼 도식화할 수 있습니다.</p>\n<p><img src=\"../img/sample-based.png\" alt=\"sample-based\"></p>\n<p>한 가지 짚고 넘어가야 할 점은 action sampling을 할 때 importance sampling을 사용한다는 것입니다.</p>\n<p><img src=\"../img/importance_sampling.png\" alt=\"importance_sampling\"></p>\n<p>바뀐 최적화문제는 아래와 같습니다.</p>\n<p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}Q_{\\theta_\\mathrm{old} }(s,a)\\right] \\\\<br>\\mathrm{s.t.\\ }&amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} }}\\left[D_\\mathrm{KL}\\left(\\pi_{\\theta_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta<br>\\end{align}$$</p>\n<p>이 때 sampling하는 두 가지 방법이 있습니다.</p>\n<p><br></p>\n<h2 id=\"5-1-Single-Path\"><a href=\"#5-1-Single-Path\" class=\"headerlink\" title=\"5.1. Single Path\"></a>5.1. Single Path</h2><p><em>single path</em>는 개별 trajectory들을 이용하는 방법입니다.</p>\n<p><img src=\"../img/single.png\" alt=\"single\"></p>\n<p><br></p>\n<h2 id=\"5-2-Vine\"><a href=\"#5-2-Vine\" class=\"headerlink\" title=\"5.2. Vine\"></a>5.2. Vine</h2><p><em>vine</em>은 한 state에서 rollout을 이용하여 여러 action을 수행하는 방법입니다.</p>\n<p><img src=\"../img/vine1.png\" alt=\"vine1\"></p>\n<p><img src=\"../img/vine2.png\" alt=\"vine2\"></p>\n<p>estimation의 variance를 낮출 수 있지만 계산량이 많고 한 state에서 여러 action을 수행할 수 있어야 하기 때문에 현실적인 문제에 적용하기에는 어려움이 있습니다.</p>\n<h1 id=\"Trust-Region-Policy-Optimization-1\"><a href=\"#Trust-Region-Policy-Optimization-1\" class=\"headerlink\" title=\"Trust Region Policy Optimization\"></a>Trust Region Policy Optimization</h1><h1 id=\"Conjugate-Gradient-Method\"><a href=\"#Conjugate-Gradient-Method\" class=\"headerlink\" title=\"Conjugate Gradient Method\"></a>Conjugate Gradient Method</h1><p><img src=\"https://datascienceschool.net/upfiles/a5ba6251b6f144249cca6eb8cc523682.png\" alt=\"\"></p>\n<p>테일러 급수에 대하여 : <a href=\"https://terms.naver.com/entry.nhn?docId=3572065&amp;cid=58944&amp;categoryId=58970\" target=\"_blank\" rel=\"noopener\">naver</a> , <a href=\"https://ko.wikipedia.org/wiki/%ED%85%8C%EC%9D%BC%EB%9F%AC_%EC%A0%95%EB%A6%AC\" target=\"_blank\" rel=\"noopener\">wikipedia</a></p>\n<p>: 근사 다항식을 구하기위해.</p>\n<p>$\\large f(x) = \\frac{f(a)}{0!} + \\frac{f’(a)}{1!}(x-a)^1 + \\frac{f’’(a)}{2!}(x-a)^2 + … + R_{n+1}(x)$</p>\n<p>$\\large = \\sum_{k=0}^{n} \\frac{f^{(k)}(a)}{k!}(x-a)^k + R_{n+1}(x)$</p>\n<h2 id=\"5-Sample-based-Estimation-of-the-Objective-and-Constraint\"><a href=\"#5-Sample-based-Estimation-of-the-Objective-and-Constraint\" class=\"headerlink\" title=\"5. Sample-based Estimation of the Objective and Constraint\"></a>5. Sample-based Estimation of the Objective and Constraint</h2><p>이전 장에서, 각 업데이트 시에 정책 변화를 제약하면서 기대누적보상 $\\large \\eta$ 의 추정치를 최적화하는</p>\n<p>정책 파라미터의 제약조건이 있는 최적화 문제를 제안했다. - eq.(12)</p>\n<h4 id=\"Equation-12\"><a href=\"#Equation-12\" class=\"headerlink\" title=\"Equation 12\"></a>Equation 12</h4><p>$\\LARGE \\underset{\\theta}{maximize} \\ L_{\\theta_{old} }(\\theta)$</p>\n<p>$\\LARGE subject \\ to \\ \\bar{D}<em>{KL}^{\\rho</em>{\\theta_{old} }} (\\theta_{old}, \\theta) \\leq \\delta$</p>\n<p><br></p>\n<p>이번 5장에선, <strong>어떻게 목적함수와 제약식이 몬테카를로 시뮬레이션을 통해 근사되는지</strong> 설명한다.</p>\n<h2 id=\"5-Sample-based-Estimation-of-the-Objective-and-Constraint-1\"><a href=\"#5-Sample-based-Estimation-of-the-Objective-and-Constraint-1\" class=\"headerlink\" title=\"5. Sample-based Estimation of the Objective and Constraint\"></a>5. Sample-based Estimation of the Objective and Constraint</h2><p>eq. (12)의 $\\large L_{\\theta_{old}(\\theta)}$ 를 확장하면 다음의 식을 얻을 수 있다.</p>\n<p><strong>Equation 13</strong></p>\n<p>$\\LARGE \\underset{\\theta}{maximize} \\ \\sum_s \\rho_{\\theta_{old} }(s) \\sum_a \\pi_{\\theta}(a\\vert s) A_{\\theta_{old} }(s, a)$</p>\n<p>$\\LARGE subject \\ to \\ \\bar{D}<em>{KL}^{\\rho</em>{\\theta_{old} }} (\\theta_{old}, \\theta) \\leq \\delta$</p>\n<p>여기서…</p>\n<p>1) $\\LARGE \\sum_s \\rho_{\\theta_{old} }(s)$  를 $\\LARGE \\frac{1}{1-\\gamma} E_{s \\sim \\rho_{\\theta_{old} }} $ 로 대체.</p>\n<p><strong> 어디서 $\\large \\frac{1}{1-\\gamma}$ 가 나왔을까? Kakade &amp; LangFord 2002 </strong></p>\n<p>-\\vert  -<br>:—————:\\vert :—————:<br><img src=\"https://i.imgur.com/uo0zOqd.png\" alt=\"\"> \\vert  <img src=\"https://i.imgur.com/PeG5gNp.png\" alt=\"\">\\vert </p>\n<p>2) $\\large A_{\\theta_{old} }$ 을 $\\large Q_{\\theta_{old} }$ 로 대체.</p>\n<p>3) 액션의 확률합을 importance sampling으로 대체</p>\n<p>$\\LARGE \\sum_a \\pi_{\\theta}(a\\vert s_n)A_{\\theta_{old} }(s_n, a) = \\color{Red}{E_{a \\sim q} [\\frac{\\pi_{\\theta}(a\\vert s_n)}{q(a\\vert s_n)} } \\ A_{\\theta_{old} }(s_n, a)]$</p>\n<p>대체한 결과는</p>\n<p><strong>Equation 14</strong></p>\n<p>$\\LARGE \\underset{\\theta}{maximize} E_{s \\sim \\rho_{\\theta_{old} } \\ , \\ a \\sim q} [\\frac{\\pi_{\\theta}(a\\vert s_n)}{q(a\\vert s_n)}Q_{\\theta_{old} }(s, a)]$</p>\n<p>$\\LARGE subject \\ to \\ E_{s \\sim \\rho_{\\theta_{old} }}[D_{KL}(\\pi_{\\theta_{old} })(\\cdot \\vert  s) \\ \\vert \\vert  \\ \\pi_{\\theta}(\\cdot \\vert  s)] \\leq \\delta$</p>\n<h2 id=\"5-Sample-based-Estimation-of-the-Objective-and-Constraint-2\"><a href=\"#5-Sample-based-Estimation-of-the-Objective-and-Constraint-2\" class=\"headerlink\" title=\"5. Sample-based Estimation of the Objective and Constraint\"></a>5. Sample-based Estimation of the Objective and Constraint</h2><p>Eq. 14에서 Expectation은 샘플 평균으로 구하고, Q value는 실행했을때의 추정값으로 활용한다.</p>\n<p>아래는 추정을 수행하는 방식이 다른 2가지 에 대해 설명한다.</p>\n<h3 id=\"5-1-Single-Path-1\"><a href=\"#5-1-Single-Path-1\" class=\"headerlink\" title=\"5.1 Single Path\"></a>5.1 Single Path</h3><p>single path 샘플링 방식은 일반적으로 policy gradient estimation에 사용하고</p>\n<p>개별적인 trajectory를 샘플링 한다.</p>\n<p>여기서 $\\large s_0 \\sim \\rho_{\\theta} \\leftarrow$ (discounted visitation distribution)</p>\n<p>상태들의 시퀀스를 모으고, trajectory를 생성하기위해 임의의 타임스텝만큼 policy인 $\\large \\pi_{\\theta_{old} }$ 을 시뮬레이션한다.</p>\n<p>$\\large (s_0 \\ , a_0 \\ , s_1 \\ , a_1 … \\ s_{t-1} \\ , a_{t-1} \\ , s_t  )$</p>\n<p>따라서, $\\LARGE q(a\\vert s) = \\pi_{\\theta_{old} }$ 이다.</p>\n<p>$\\large Q_{\\theta_{old} }(s, a)$ 는 trajectory의 각 state-action 쌍인 $\\large (s_t, a_t)$ 의 $\\large \\sum \\gamma r(s,a)$ 를 계산한다.</p>\n<h3 id=\"5-2-Vine-1\"><a href=\"#5-2-Vine-1\" class=\"headerlink\" title=\"5.2 Vine\"></a>5.2 Vine</h3><p>vine 방식은 roll out set이라는 것을 만들고 roll out set의 각 상태마다 여러 액션을 수행해본다.</p>\n<p>추정 과정에서, 처음엔 $\\large s_0 \\sim \\rho_{\\theta}$ 샘플링하고 $\\large \\pi_{\\theta}$ 로 시뮬레이션을 반복해서 몇개의 trajectory들을 생성한다.</p>\n<p>다음엔 이 trajectory들 에서 N개의 상태를 부분집합으로 고른다. $\\large s_1, s_2, … , s_N$ 이걸 roll out set이라 부른다.</p>\n<p>roll out set 각 상태 $\\large s_n$에서 $\\large a_{n , k} \\sim q(\\cdot \\vert  s_n)$ 에 대해 K개의 액션을 샘플링한다.</p>\n<p>이와 같은 방식은 exploration 측면에 있어 좋은 성능을 보였고, Robotic locomotion 과 Atari game, 즉 Continuous / discrete task 모두 잘 되었다.</p>\n<p>각 상태 $\\large s_n$ 에서 샘플링된 각 액션 $\\large a_{n , k}$에서 roll out을 수행하면서 (짧은 trajectory) $\\large \\overset{\\wedge}{Q}_{\\theta_i}(s_n, a_n, k)$ 을 추정한다.</p>\n<p>각 K개의 roll out에서 노이즈를 발생하기 위해 random number sequence를 사용한 roll out과 Q value의 차이의 분산을 매우 감소시켰다.</p>\n<p>작고 유한한 액션공간에서는 주어진 상태에서 모든 가능한 액션을 rollout을 통해 생성할 수 있었다.</p>\n<p>한 상태 $\\large s_n$ 에서 $\\large L_{\\theta_{old} }$ 에 기여하는 식은 다음과 같다.</p>\n<p><strong>Equation 15</strong></p>\n<p>$\\LARGE L_n(\\theta) = \\sum_{k=1}^K \\pi_{\\theta}(a_k\\vert s_n) \\overset{\\wedge}{Q}(s_n, a_k)$</p>\n<p><br></p>\n<p>상태공간이 크고 continuous 한 상황에서, importance sampling을 활용해 surrogate objective를 추정할 수 있었다.</p>\n<p>한 상태 $\\large s_n$ 에서 $\\large L_{\\theta_{old} }$ 을 얻는 self normalized estimator<a href=\"http://statweb.stanford.edu/~owen/mc/Ch-var-is.pdf\" target=\"_blank\" rel=\"noopener\">(Owen, 2013)</a>는 다음과 같다.</p>\n<p><strong>Equation 16</strong></p>\n<p>$\\LARGE L_n(\\theta) = \\frac{\\sum_{k=1}^{K} \\frac{\\pi_{\\theta}(a_{n,k} \\ \\vert  \\ s_n)}{\\pi_{\\theta_{old} }(a_{n,k} \\ \\vert  \\ s_n)} \\overset{\\wedge}{Q}(s_n, a_{n,k})}{\\sum_{k=1}^{K} \\frac{\\pi_{\\theta}(a_{n,k} \\ \\vert  \\ s_n)}{\\pi_{\\theta_{old} }(a_{n,k} \\ \\vert  \\ s_n)} }$</p>\n<p>여기서 상태 $\\large s_n$ 에서 K개의 액션을 수행한다고 가정한다. $\\large a_{n,1}, a_{n,2} \\ … \\ a_{n,k} $</p>\n<p>이 self normalized estimator는 Q value를 구하기위해 베이스라인을 사용할 필요성을 없애준다. (Q value에 상수를 더함으로서 그라디언트가 바뀌지않음???)</p>\n<p>$\\large s_n \\sim \\rho(\\pi)$ 을 평균을 내줌으로서, $\\large L_{\\theta_{old} }$ estimator와 그라디언트를 얻을 수 있다.</p>\n<h3 id=\"5-2-Vine-2\"><a href=\"#5-2-Vine-2\" class=\"headerlink\" title=\"5.2 Vine\"></a>5.2 Vine</h3><p>vine은 샘플링을 통한 trajectory들이 다양한 포인트(rollout set)에서 여러갈래로 짧은 가지를 치고 있는 것이 마치 덩굴식물 줄기같아서 이름붙였다.</p>\n<p>single-path 보다 vine의 이점은 surrogate objective에서 같은 숫자의 Q-value 샘플들이 주어졌을때, 목적함수의 추정치가 낮은 분산을 갖는 것이다.</p>\n<p>즉, vine 방법은 advantage value의 아주 좋은 추정치를 갖게 해준다.</p>\n<p>vine의 불리한 부분은 반드시 이 advantage 추정 각각을 위해 시뮬레이터가 매우 많은 요청을 수행해야하는 것이다.</p>\n<p>게다가, vine은  rollout set 내부의 각 상태에서 다수의 trajectory들을 생성해야하는데 이것은 시스템이 임의의 상태로 초기화 될수 있는 환경으로 제한해버린다.</p>\n<p>반면, single path는 상태 초기화도 없고, 실세계로 직접 구현 가능하다.</p>\n<p><br></p>\n<h2 id=\"6-Practical-Algorithm\"><a href=\"#6-Practical-Algorithm\" class=\"headerlink\" title=\"6. Practical Algorithm\"></a>6. Practical Algorithm</h2><p>앞서 Single-path, Vine 샘플링을 사용하는 두가지 방식의 policy optimization 알고리즘을 선보였다.</p>\n<p>반복적으로 아래의 과정을 수행한다.</p>\n<p>1) Q-values의 몬테카를로 추정과 state-action 쌍 집합을 single path / vine 과정을 통해 모은다.</p>\n<p>2) 샘플 평균으로, 목적함수와 eq.(14)의 제약식을 추정한다.<br>eq.(14) : <br></p>\n<p>$\\LARGE \\underset{\\theta}{maximize} E_{s \\ \\sim \\ \\rho_{\\theta_{old} } \\ \\text{,} \\ a \\ \\sim \\ q} [\\frac{\\pi_{\\theta}(a\\vert s)}{q(a\\vert s)}Q_{\\theta_{old} }(s, a)]$</p>\n<p>$\\large subject \\ to \\ E_{s \\ \\sim \\ \\rho_{\\theta_{old} }} [D_{KL}(\\pi_{\\theta_{old} }(\\cdot \\vert  s) \\ \\vert \\vert  \\ \\pi_{\\theta}(\\cdot\\vert s))] \\leq \\delta$</p>\n<p>3) policy parameter vector인 $\\theta$를 업데이트 하면서 제약조건이 있는 최적화 문제를 근사적으로 푼다.</p>\n<p>본 논문에서는 1차미분보다는 좀 계산량이 있는 line search와 Conjugate Gradient algorithm을 사용했다.</p>\n<p>3)에 대해서,  그라디언트의 covariance matrix를 사용하지않고 KL Divergence의 헤시안을 해석적으로 계산해서  Fisher Information Matrix를 구성했다.</p>\n<p>다시말해서</p>\n<p>$\\LARGE \\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial}{\\partial \\theta_i} log \\pi_{\\theta}(a_n\\vert s_n) \\frac{\\partial}{\\partial \\theta_j} log \\pi_{\\theta}(a_n\\vert s_n)$</p>\n<p>$\\ \\ \\ \\ \\ \\ \\ \\ \\downarrow$</p>\n<p>$\\LARGE \\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_j} D_{KL}(\\pi_{\\theta_{old} }(\\cdot \\vert  s_n) \\vert \\vert  \\pi_{\\theta}(\\cdot \\vert  s_n))$</p>\n<p><strong>TODO KL divergence 미분과정</strong></p>\n<p><strong>\\TODO # 왜 KL divergence의 헤시안을 계산할때 covariance matrix 얘기가 나왔을까?</strong></p>\n<p><a href=\"https://timvieira.github.io/blog/post/2014/10/06/kl-divergence-as-an-objective-function/\" target=\"_blank\" rel=\"noopener\">https://timvieira.github.io/blog/post/2014/10/06/kl-divergence-as-an-objective-function/</a></p>\n<p>이 analytic estimator는 헤시안이나 trajectories의 모든 그라디언트를 저장하지 않아도 되기 때문에 대규모 환경에서 계산 이점이 있다.</p>\n<p><strong>TODO Appendix C</strong></p>\n<ul>\n<li><p>3장에서 surrogate objective $M(\\pi)$ 과 KL divergence penalty를 활용한 최적화를 증명했으나 <br><br>$C$가 크면 엄청나게 step size가 작아지게 된다. 그래서 $C$를 감소하려하는데, 강건하게 정하기가 어려워서 <br><br>penalty대신, 강한 제약조건으로 KL divergence의 bound $\\delta $ 를 사용했다. -&gt; (Trust Region)</p>\n</li>\n<li><p>$\\large D_{KL}^{max}(\\theta_{old}, \\theta)$ 제약조건은 추정이나 최적화를 하기 어려운 부분이 있어서 <br><br>대신에 $\\large \\bar{D}<em>{KL}(\\theta</em>{old}, \\theta)$ 을 제약했다.</p>\n</li>\n<li><p>본 논문은 advantage function의 추정에러를 무시한다. Kakade&amp;Langford(2002) 에서도 이런 에러를 고려했고 상황도 비슷하지만 <br><br>단순함을 위해 생략했다. (매우 작은 차이라?)</p>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"6-Practical-Algorithm-1\"><a href=\"#6-Practical-Algorithm-1\" class=\"headerlink\" title=\"6. Practical Algorithm\"></a>6. Practical Algorithm</h2><h3 id=\"Appendix-C-Efficiently-Solving-the-Trust-Region-Constrained-Optimization-Problem\"><a href=\"#Appendix-C-Efficiently-Solving-the-Trust-Region-Constrained-Optimization-Problem\" class=\"headerlink\" title=\"Appendix C. Efficiently Solving the Trust-Region Constrained Optimization Problem\"></a>Appendix C. Efficiently Solving the Trust-Region Constrained Optimization Problem</h3><p>풀어야 하는 식</p>\n<p>$\\LARGE maximize \\ L(\\theta) \\  subject \\ to \\ \\bar{D}<em>{KL}(\\theta</em>{old}, \\theta) \\leq \\delta$</p>\n<p>크게 두가지 과정으로 진행</p>\n<p>1) 탐색 방향을 계산, 목적함수의 선형근사 와 제약식의 2차 근사</p>\n<p>2) 계산한 방향으로 line search 수행, 비선형 제약식을 만족시키면서 비선형 목적함수를 개선</p>\n<p>탐색 방향은 $\\large Ax = g$ 수식을 근사적으로 풀면 계산된다. 여기서 $\\large A$ 는 Fisher Information Matrix이고,</p>\n<p>KL divergence 제약식인 $\\large \\bar{D}<em>{KL} (\\theta</em>{old}, \\theta) \\approx \\frac{1}{2}(\\theta - \\theta_{old})^T A(\\theta - \\theta_{old})$ 의 2차 근사이다.</p>\n<p>여기서 $\\large A_{ij} = \\frac{\\partial}{\\partial \\theta_i} \\frac{\\partial}{\\partial \\theta_j} \\bar{D}<em>{KL}(\\theta</em>{old}, \\theta)$</p>\n<p>대규모 문제에서 $A$ 혹은 $A^{-1}$ 을 계산하는 것은 엄청난 계산량이 필요.</p>\n<p>하지만 Conjugate Gradient Algorithm이 전체 $A$행렬(FIM)을 계산하지 않아도 $\\large Ax=g$ 를 근사적으로 해결할 수 있게 해준다.</p>\n<p>탐색 방향 $\\large s \\approx A^{-1}g$ 을 계산하면서, 최대스텝길이(step size) $\\large \\beta$를 계산할 필요가 있다.</p>\n<p>이 계산을 위해서 $\\large \\delta = \\bar{D}_{KL} \\approx \\frac{1}{2}(\\beta s)^T A(\\beta s) = \\frac{1}{2}\\beta^2s^TAs$ 이고</p>\n<p>여기서 $\\large \\beta = \\sqrt{2\\delta / s^T As}$ , 이때 $\\large \\delta$는 KL divergence의 boundary tgerm 이다.</p>\n<p>$\\large s^T As$ 텀은 헤시안 - 벡터 곱으로 계산될 수 있고, CG의 과정에서 계산된다.</p>\n<p>마지막으로, surrogate objective와 KL divergence 제약식을 만족하는 개선을 위해 line search 를 사용할 것이며, surrogate objective와 KL divergence 제약식은 둘 다 $\\large \\theta$ 에 대해 비선형이다.</p>\n<p>목적함수인 $\\large L_{\\theta_{old} }(\\theta) - \\chi [\\bar{D}<em>{KL}(\\theta</em>{old}, \\theta) \\leq \\delta] = 0$ 0이 맞으면 True고 무한으로 발산하면 False</p>\n<p>위에서 계산한 $\\large \\beta$ 의 최대 값부터 시작해서 목적함수가 개선될때까지 점점 줄여갈텐데, line search가 없었다면</p>\n<p>이 알고리즘은 매우 큰 스텝으로 계산될 것이고 성능의 심각한 저하가 발생될 것이다.</p>\n<h3 id=\"C-1-Computing-the-Fisher-Vector-Product\"><a href=\"#C-1-Computing-the-Fisher-Vector-Product\" class=\"headerlink\" title=\"C.1 Computing the Fisher-Vector Product\"></a>C.1 Computing the Fisher-Vector Product</h3><p>임의의 벡터와 평균한 Fisher Information Matrix를 어떻게 계산할지 서술하려한다.</p>\n<p>이 행렬-벡터 곱은 CG를 수행가능하게 한다. 파라미터화 한 policy가 입력 $\\large x$를 분포 파라미터 벡터인 $\\large \\mu_{\\theta}(x)$ 로 사상시킨다고 할때, $\\large \\pi(a \\vert x)$.</p>\n<p>주어진 입력 $\\large x$ 의 KL divergence는 아래 식과 같이 쓸 수 있다.</p>\n<p>$\\LARGE D_{KL} (\\pi_{\\theta_{old} }(\\cdot \\vert x)  \\ \\vert\\vert \\ \\pi_{\\theta}(\\cdot \\vert x)) = kl(\\mu_{\\theta}(x), \\mu_{old})$</p>\n<p>여기서 kl 은 두 mean 파라미터 벡터에 대응하는 분포 사이의 KL divergence이다. kl 을 $\\large \\theta$에 대해 두번 미분하면</p>\n<p>$\\LARGE \\frac{\\partial \\mu_{a}(x)}{\\partial \\theta_i} \\frac{\\partial \\mu_b(x)}{\\partial \\theta_j} kl^{‘’}<em>{ab}(\\mu</em>{\\theta}(x), \\mu_{old}) + \\frac{\\partial^2 \\mu_a(x)}{\\partial \\theta_i \\partial \\theta_j}kl^{‘}<em>a(\\mu</em>{\\theta}(x), \\mu_{old})$</p>\n<p>여기서 두번째 term은 0이 된다. (두번미분하면 사라짐?)</p>\n<p>$\\LARGE J^T M J$</p>\n<h2 id=\"7\"><a href=\"#7\" class=\"headerlink\" title=\"7\"></a>7</h2><h2 id=\"7-Connections-with-Prior-Work-1\"><a href=\"#7-Connections-with-Prior-Work-1\" class=\"headerlink\" title=\"7. Connections with Prior Work - (1)\"></a>7. Connections with Prior Work - (1)</h2><p>NPG는 L의 선형 근사와 $\\large \\bar D_{KL}$ 제약식을 2차근사 하는 아래의 식의 special case.</p>\n<p><strong>equation 17. </strong></p>\n<p>$\\LARGE L_{\\pi}(\\tilde\\pi) = \\eta(\\pi) + \\underset{s}{\\sum}\\rho_{\\pi}(s) \\underset{a}{\\sum}\\tilde\\pi(a\\vert s)A_\\pi(s,a) $ - eq.(3)</p>\n<p>$\\LARGE \\underset{\\theta}{maximize} \\ [\\triangledown_{\\theta}L_{\\theta_{old} }(\\theta) \\ \\rvert_{\\theta=\\theta_{old} } \\cdot (\\theta - \\theta_{old})]$</p>\n<p>$\\LARGE subject \\ to \\ \\frac{1}{2}(\\theta_{old} - \\theta)^{T} A(\\theta_{old})(\\theta_{old}-\\theta) \\leq \\delta$</p>\n<p>$\\LARGE where \\ A(\\theta_{old})_{ij} = $</p>\n<p>$\\huge \\frac{\\partial}{\\partial \\theta_{i} } \\frac{\\partial}{\\partial \\theta_{j} }<br>E_{s \\sim \\rho_{\\pi} }[D_{KL}(\\pi(\\cdot\\rvert s, \\theta_{old})  \\rvert\\rvert \\pi(\\cdot \\rvert s, \\theta))] \\rvert_{\\theta=\\theta_{old} }$</p>\n<h2 id=\"7-Connections-with-Prior-Work-2\"><a href=\"#7-Connections-with-Prior-Work-2\" class=\"headerlink\" title=\"7. Connections with Prior Work - (2)\"></a>7. Connections with Prior Work - (2)</h2><p>업데이트 식</p>\n<p>$\\LARGE \\theta_{new} = \\theta_{old} +  \\color{Red}{\\frac{1}{\\lambda} } A(\\theta_{old})^{-1}\\triangledown_{\\theta}L(\\theta) \\rvert_{\\theta=\\theta_{old} }$</p>\n<p>여기서 step size인 $\\LARGE \\frac{1}{\\lambda}$ 일반적으로 알고리즘의 파라미터로 취급된다.</p>\n<p>이 부분에서 TRPO는 각 업데이트 마다 제약식을 강제한다는 점이 다르다.</p>\n<p>좀 사소한 차이처럼 보이지만, 큰 차원을 다루는 실험에서 알고리즘의 성능을 크게 향상시켰다.</p>\n<p>또한 $\\LARGE \\mathcal{l}^2$ 제약식(혹은 페널티) 를 사용한 표준 Policy Gradient 업데이트 식을 얻었다.</p>\n<p><strong>Equation 18</strong></p>\n<p>$\\LARGE \\underset{\\theta}{maximize}[\\triangledown_{\\theta} L_{\\theta_{old} }(\\theta) \\rvert_{\\theta=\\theta_{old} } \\cdot (\\theta - \\theta_{old})]$</p>\n<p>$\\LARGE subject \\ to \\ \\frac{1}{2} \\vert\\vert \\theta - \\theta_{old} \\vert\\vert^2 \\leq \\delta$</p>\n<p>비제약 문제인 $\\underset{\\pi}{maximize} \\ L_{\\pi_{old} }(\\pi)$을</p>\n<p>$\\LARGE L_{\\pi}(\\tilde\\pi) = \\eta(\\pi) + \\underset{s}{\\sum}\\rho_{\\pi}(s) \\underset{a}{\\sum}\\tilde\\pi(a\\vert s)A_\\pi(s,a) $  - eq.(3)</p>\n<p>을 푸는 것으로 Policy Iteration update를 행한다.</p>\n<h2 id=\"7-Connections-with-Prior-Work-3\"><a href=\"#7-Connections-with-Prior-Work-3\" class=\"headerlink\" title=\"7. Connections with Prior Work - (3)\"></a>7. Connections with Prior Work - (3)</h2><p>$\\LARGE \\underset{\\pi}{maximize} \\ L_{\\pi_{old} }(\\pi)$  식과 유사하게 업데이트를 수행하는 다양한 방법론들이 있다.</p>\n<p><a href=\"https://pdfs.semanticscholar.org/ff47/526838ce85d77a50197a0c5f6ee5095156aa.pdf\" target=\"_blank\" rel=\"noopener\">REPS(Relative Entropy Policy Search)</a> 는</p>\n<p><a href=\"http://rll.berkeley.edu/deeprlcourse/f17docs/lecture_3_rl_intro.pdf\" target=\"_blank\" rel=\"noopener\">state-action marginal</a>인 $p(s,a)$ 를 제약하고 TRPO는 조건부인 $p(a\\vert s)$ 를 제약한다.</p>\n<p>REPS constraint \\vert  TRPO constraint<br>:————–:\\vert :—————:\\vert<br><img src=\"https://i.imgur.com/U5Xgci1.png\" alt=\"\"> \\vert  <img src=\"https://i.imgur.com/12RPFh3.png\" alt=\"\"></p>\n<p>REPS랑은 달리, TRPO는 내부 루프 계산에서 비선형 최적화를 하지않는다.(REPS는 BFGS수행)</p>\n<p><strong># REPS algorithm</strong></p>\n<p><img src=\"https://i.imgur.com/F2m1yMG.png\" alt=\"\"></p>\n<p><br></p>\n<h2 id=\"7-Connections-with-Prior-Work-4\"><a href=\"#7-Connections-with-Prior-Work-4\" class=\"headerlink\" title=\"7. Connections with Prior Work - (4)\"></a>7. Connections with Prior Work - (4)</h2><p><a href=\"https://people.eecs.berkeley.edu/~svlevine/papers/mfcgps.pdf\" target=\"_blank\" rel=\"noopener\">Levine and Abbeel(2014)</a> 또한 KL Divergence 제약식을 사용했다.</p>\n<p>하지만 policy가 추정한 dynamics model이 유효한 영역에서 벗어나지않게 하는데에 목적이 있는 반면,</p>\n<p>TRPO는 system dynamics를 추정하지 않는다.</p>\n<p><a href=\"https://pdfs.semanticscholar.org/f2c5/a91f35004aebc120360679f76488ac2613ee.pdf\" target=\"_blank\" rel=\"noopener\">Pirotta et al. (2013)</a> 또한 <a href=\"https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/KakadeLangford-icml2002.pdf\" target=\"_blank\" rel=\"noopener\">Kakade &amp; Langford</a>의 결과를 일반화했으나 이들은 TRPO와는 다른 알고리즘을 도출했다.</p>\n<p><br></p>\n<h2 id=\"8-Experiments\"><a href=\"#8-Experiments\" class=\"headerlink\" title=\"8. Experiments\"></a>8. Experiments</h2><ul>\n<li><p>3 가지 궁금증을 위한 실험을 설계함</p>\n<ol>\n<li><p>Single-path 와 vine 의 성능면에서 특징을 알고싶음.</p>\n</li>\n<li><p>Fixed penalty coefficient(NPG) 보다 fixed KL divergence를 사용하는 것(TRPO)으로<br>변경한 것이 얼마나 뚜렷한 차이를 보일지, 성능 면에서 어떤 영향을 미치는지 알고싶음</p>\n</li>\n<li><p>TRPO가 큰 스케일의 문제를 해결할 수 있는지, 기존에 연구/적용되어왔던 방법들과<br>성능, 계산시간, 샘플 복잡도 면에서 비교하고 싶음</p>\n</li>\n</ol>\n</li>\n</ul>\n<ul>\n<li><p>1, 2의 궁금증에 대해서 실험 환경에 single path, vine을 비롯한 여러 사전방법들을 함께 실험.</p>\n</li>\n<li><p>3에 대해선, robotic locomotion과 Atari game에 TRPO를 실험함.</p>\n</li>\n</ul>\n<h2 id=\"8-1\"><a href=\"#8-1\" class=\"headerlink\" title=\"8.1\"></a>8.1</h2><h3 id=\"8-1-Simulated-Robotic-Locomotion\"><a href=\"#8-1-Simulated-Robotic-Locomotion\" class=\"headerlink\" title=\"8.1 Simulated Robotic Locomotion\"></a>8.1 Simulated Robotic Locomotion</h3><ul>\n<li>MuJoCo 시뮬레이터를 활용해 로봇 locomotion 실험 진행</li>\n<li>Robot State: positions and velocities, joint torques</li>\n</ul>\n<p><img src=\"https://i.imgur.com/bs5ATS3.png\" alt=\"\"></p>\n<ul>\n<li><p>수영, 점프, 걷기 행동을 학습시키고자 함.</p>\n<ul>\n<li><p>수영: 10 dimensional state space , reward: $\\LARGE r(x,u)=v_x - 10^{-5}\\vert \\vert u\\vert \\vert ^2$, quadratic penalty</p>\n</li>\n<li><p>점프: 12 dimensional state space, episode의 끝은 높이와 각도 threshol로 점프를 판단, non-terminal state에 대해 +1 보상 추가</p>\n</li>\n<li><p>걷기: 18 dimensional state space, 뜀뛰는 듯한 보법(점프에이전트)보다 부드러운 보법을 유도하기위해 페널티를 추가했다함</p>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"8-1-Simulated-Robotic-Locomotion-1\"><a href=\"#8-1-Simulated-Robotic-Locomotion-1\" class=\"headerlink\" title=\"8.1 Simulated Robotic Locomotion\"></a>8.1 Simulated Robotic Locomotion</h3><h4 id=\"Detailed-Experiment-Setup-and-used-parameters-used-network-model\"><a href=\"#Detailed-Experiment-Setup-and-used-parameters-used-network-model\" class=\"headerlink\" title=\"Detailed Experiment Setup and used parameters, used network model\"></a>Detailed Experiment Setup and used parameters, used network model</h4><p>used parameter \\vert  network model<br>:—-:\\vert :—-:<br><img src=\"https://i.imgur.com/zgnsbw6.png\" alt=\"\">\\vert  <img src=\"https://i.imgur.com/FqdWC53.png\" alt=\"\"></p>\n<h3 id=\"8-1-Simulated-Robotic-Locomotion-2\"><a href=\"#8-1-Simulated-Robotic-Locomotion-2\" class=\"headerlink\" title=\"8.1 Simulated Robotic Locomotion\"></a>8.1 Simulated Robotic Locomotion</h3><p>equation (12) : $\\huge maximize L_{\\theta_{old}(\\theta)} \\ , \\ \\ subject \\ to \\bar{D}<em>{KL}^{\\rho</em>{\\theta_{old} }}(\\theta_{old}, \\theta) \\leq \\delta$</p>\n<p>이 실험에서 $\\LARGE \\delta = 0.01$</p>\n<ul>\n<li><p>비교에 사용된 모델들</p>\n<ul>\n<li>TRPO Single-path</li>\n<li>TRPO vine</li>\n<li>CEM(cross-entropy method)</li>\n<li>CMA(covariance matrix adaptation)</li>\n<li>NPG (Lagrange Multiplier penalty coefficien를 사용하는 것이 차이점)</li>\n<li>Empirical FIM</li>\n<li>maximum KL(cartpole)</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"8-1-Simulated-Robotic-Locomotion-3\"><a href=\"#8-1-Simulated-Robotic-Locomotion-3\" class=\"headerlink\" title=\"8.1 Simulated Robotic Locomotion\"></a>8.1 Simulated Robotic Locomotion</h3><p><img src=\"https://i.imgur.com/wSqolvS.png\" alt=\"\"></p>\n<ul>\n<li><p>제안한 single path, vine 모두 task를 잘 학습하는 것을 확인할 수 있음</p>\n</li>\n<li><p>페널티를 고정하는 것보다, KL divergenc를 제약하는 것이 step size를 선택하는 부분에서 Robust 하다.</p>\n</li>\n<li><p>CEM/CMA는 derivative free 알고리즘으로, 샘플 복잡도가 네트워크 파라미터\u000b수 만큼 확장되어 high dimension 문제에서는 속도와 성능이 약한 모습을 보인다.</p>\n</li>\n<li><p>maxKL은 제안방법보다 느린 학습</p>\n</li>\n</ul>\n<h3 id=\"8-1-Simulated-Robotic-Locomotion-4\"><a href=\"#8-1-Simulated-Robotic-Locomotion-4\" class=\"headerlink\" title=\"8.1 Simulated Robotic Locomotion\"></a>8.1 Simulated Robotic Locomotion</h3><p>TRPO video link: <a href=\"https://www.youtube.com/watch?v=jeid0wIrSn4\" target=\"_blank\" rel=\"noopener\">https://www.youtube.com/watch?v=jeid0wIrSn4</a></p>\n<ul>\n<li><p>사전지식이 적은 상태에서 여러 액션을 학습하는 실험으로 TRPO의 성능을 보임</p>\n</li>\n<li><p>지금까지의 균형이나 걸음 같은 개념을 명시적으로 인코딩한 robotic locomotion에서의 사전 연구들과 차이를 보임</p>\n</li>\n</ul>\n<h2 id=\"8-2\"><a href=\"#8-2\" class=\"headerlink\" title=\"8.2\"></a>8.2</h2><h3 id=\"8-2-Playing-Games-from-Images\"><a href=\"#8-2-Playing-Games-from-Images\" class=\"headerlink\" title=\"8.2 Playing Games from Images\"></a>8.2 Playing Games from Images</h3><ul>\n<li>부분관찰가능하며 복잡한 observation인 환경에서 TRPO를 평가하기위해 raw image를 입력으로 하는 아타리게임 환경에서 실험</li>\n</ul>\n<ul>\n<li>Challenging point :<ul>\n<li>High dimensional space</li>\n<li>delayed reward</li>\n<li>non-stationary image (Enduro는 배경 이미지가 바뀌고, 반전처리가 됨)</li>\n<li>complex action sequence (Q*bert는 21개의 다른 발판에서 점프 해야함)</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li>ALE(Arcade Learning Environment) 에서 테스트</li>\n<li>DQN과 전처리 방식은 동일<br>  210 x 160 pixel -&gt; gray-scaling &amp; down-sampling -&gt; 110 x 84  -&gt; crop -&gt; 84 x 84</li>\n</ul>\n<h3 id=\"8-2-Playing-Games-from-Images-1\"><a href=\"#8-2-Playing-Games-from-Images-1\" class=\"headerlink\" title=\"8.2 Playing Games from Images\"></a>8.2 Playing Games from Images</h3><ul>\n<li>2 convolution layer ( 16 channels) , stride 2, FC layer with 20 units</li>\n</ul>\n<p>parameter setup \\vert  network model<br>:————–:\\vert :————-:<br><img src=\"https://i.imgur.com/NJBC69d.png\" alt=\"\"> \\vert  <img src=\"https://i.imgur.com/wTe1OEW.png\" alt=\"\"></p>\n<h3 id=\"8-2-Playing-Games-from-Images-2\"><a href=\"#8-2-Playing-Games-from-Images-2\" class=\"headerlink\" title=\"8.2 Playing Games from Images\"></a>8.2 Playing Games from Images</h3><p><img src=\"https://i.imgur.com/j22VtNQ.png\" alt=\"\"></p>\n<ul>\n<li><p>30시간 동안 500 iteration 의 학습</p>\n</li>\n<li><p>DQN과 MCST를 사용한 모델(UCC-I), 사람이 플레이한 것(Human) 과의 비교</p>\n</li>\n<li><p>이전 방법들에 비해 압도적인 점수를 기록하진 못했으나, 다양한 게임에서 적절한 게임 기록.</p>\n</li>\n<li><p>이전 방법들과 다른 점은, 특정 task에 초점을 맞춰 설계하지않았는데<br><br><br>robotics나 game playing Task에도 적절한 성능을 내는 점에서 <strong>TRPO의 일반화 성능</strong>을 보임</p>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"9-Discussion\"><a href=\"#9-Discussion\" class=\"headerlink\" title=\"9. Discussion\"></a>9. Discussion</h2><ul>\n<li><p>Trust Region Policy Optimization 을 제안</p>\n</li>\n<li><p>KL Divergence 페널티로 $J(\\pi)$를 최적화하는 알고리즘이 monotonically improve 함을 증명</p>\n</li>\n<li><p>Locomotion 도메인에서 여러 행동(수영, 걷기, 점프)을 제어하는 controlle를 학습</p>\n</li>\n<li><p>TRPO가 향후 연구의 도약점이 되었으면 좋겠음</p>\n</li>\n<li><p>Robotics와 게임 실험을 결합해, 시각정보와 센서데이터를 사용하는 로봇제어 정책을 학습시키는 가능성을 봄</p>\n</li>\n<li><p>샘플의 복잡도를 상당히 줄일수있어 실상황 적용가능성을 봄</p>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"END\"><a href=\"#END\" class=\"headerlink\" title=\"END\"></a>END</h1>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Trust-Region-Policy-Optimization\"><a href=\"#Trust-Region-Policy-Optimization\" class=\"headerlink\" title=\"Trust Region Policy Optimization\"></a>Trust Region Policy Optimization</h1><p>Authors: John Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, Pieter Abbeel<br>Proceeding: International Conference on Machine Learning (ICML) 2015</p>\n<p>정리 by 공민서, 김동민</p>\n<h1 id=\"1-들어가며…\"><a href=\"#1-들어가며…\" class=\"headerlink\" title=\"1. 들어가며…\"></a>1. 들어가며…</h1><p>Trust region policy optimization (TRPO)는 상당히 우수한 성능을 보여주는 policy gradient 기법으로 알려져 있습니다. 높은 차원의 action space를 가진 robot locomotion부터 action은 적지만 화면을 그대로 처리하여 플레이하기 때문에 control parameter가 매우 많은 Atari game까지 각 application에 세부적으로 hyperparameter들을 특화시키지 않아도 두루두루 좋은 성능을 나타내기 때문에 일반화성능이 매우 좋은 기법입니다. 이 TRPO에 대해서 알아보겠습니다.</p>\n<p>※TRPO를 매우 재밌게 설명한 Crazymuse AI의 <a href=\"https://www.youtube.com/watch?v=CKaN5PgkSBc&amp;t=90s\" target=\"_blank\" rel=\"noopener\">Youtube video</a>에서 일부 그림을 차용했습니다. 이 비디오를 시청하시는 것을 강력하게 추천합니다!</p>\n<p><br><br></p>\n<h2 id=\"1-1-TRPO-흐름-잡기\"><a href=\"#1-1-TRPO-흐름-잡기\" class=\"headerlink\" title=\"1.1. TRPO 흐름 잡기\"></a>1.1. TRPO 흐름 잡기</h2><p>TRPO 논문은 많은 수식이 등장하여 이 수식들을 따라가다보면 큰 그림을 놓칠 수도 있습니다. 세부적인 내용을 살펴보기 전에 기존 연구에서 출발해서 TRPO로 어떻게 발전해나가는지 간략하게 살펴보겠습니다.</p>\n<h3 id=\"1-1-1-Original-Problem\"><a href=\"#1-1-1-Original-Problem\" class=\"headerlink\" title=\"1.1.1. Original Problem\"></a>1.1.1. Original Problem</h3><p>$$\\max_\\pi \\eta(\\pi)$$</p>\n<p>모든 강화학습이 그렇듯이 expected discounted reward를 최대화하는 policy를 찾는 문제로부터 출발합니다.</p>\n<h3 id=\"1-1-2-Conservative-Policy-Iteration\"><a href=\"#1-1-2-Conservative-Policy-Iteration\" class=\"headerlink\" title=\"1.1.2. Conservative Policy Iteration\"></a>1.1.2. Conservative Policy Iteration</h3><p>$$\\max L_\\pi(\\tilde\\pi) = \\eta(\\pi) + \\sum_s \\rho_\\pi(s)\\sum_a\\tilde\\pi(a\\vert s)A_\\pi(s,a)$$</p>\n<p>$\\eta(\\pi)$를 바로 최대화하는 것은 많은 경우 어렵습니다. $\\eta(\\pi)$의 성능향상을 보장하면서 policy를 update하는 conservative policy iteration 기법이 <a href=\"http://www.cs.cmu.edu/~./jcl/papers/aoarl/Final.pdf\" target=\"_blank\" rel=\"noopener\">Kakade와 Langford</a>에 의하여 제안되었습니다. 이 기법을 이용하면 policy update가 성능을 악화시키지는 않는다는 것이 이론적으로 보장됩니다.</p>\n<h3 id=\"1-1-3-Theorem-1-of-TRPO\"><a href=\"#1-1-3-Theorem-1-of-TRPO\" class=\"headerlink\" title=\"1.1.3. Theorem 1 of TRPO\"></a>1.1.3. Theorem 1 of TRPO</h3><p>$$\\max L_{\\pi_\\mathrm{old} }\\left(\\pi_\\mathrm{new}\\right) - \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2,\\quad\\left(\\alpha=D_\\mathrm{TV}^\\max\\left(\\pi_\\mathrm{old},\\pi_\\mathrm{new}\\right)\\right)$$</p>\n<p>기존의 conservative policy iteration은 과거 policy와 새로운 policy를 섞어서 사용해서 실용적이지 않다는 단점이 있었는데 이것을 보완하여 온전히 새로운 policy만으로 update할 수 있는 기법을 제안합니다. </p>\n<h3 id=\"1-1-4-KL-divergence-version-of-Theorem-1\"><a href=\"#1-1-4-KL-divergence-version-of-Theorem-1\" class=\"headerlink\" title=\"1.1.4. KL divergence version of Theorem 1\"></a>1.1.4. KL divergence version of Theorem 1</h3><p>$$\\max L_{\\pi}\\left(\\tilde\\pi\\right) - C\\cdot D_\\mathrm{KL}^\\max\\left(\\pi,\\tilde\\pi\\right),\\quad\\left(C = \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\right)$$</p>\n<p>distance metric을 KL divergence로 바꿀 수 있습니다.</p>\n<h3 id=\"1-1-5-Using-parameterized-policy\"><a href=\"#1-1-5-Using-parameterized-policy\" class=\"headerlink\" title=\"1.1.5. Using parameterized policy\"></a>1.1.5. Using parameterized policy</h3><p>$$\\max_\\theta L_{\\theta_\\mathrm{old} }(\\theta) - C\\cdot D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right)$$</p>\n<p>최적화문제를 더욱 편리하게 풀 수 있도록 낮은 dimension을 가지는 parameter들로 parameterized된 policy를 사용할 수 있습니다.</p>\n<h3 id=\"1-1-6-Trust-region-constraint\"><a href=\"#1-1-6-Trust-region-constraint\" class=\"headerlink\" title=\"1.1.6. Trust region constraint\"></a>1.1.6. Trust region constraint</h3><p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;L_{\\theta_\\mathrm{old} }(\\theta) \\\\<br>\\mathrm{s.t.\\ }&amp;D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right) \\leq \\delta<br>\\end{align}$$</p>\n<p>5까지는 아직 conservative policy iteration을 약간 변형시킨 것입니다. policy를 update할 때 지나치게 많이 변하는 것을 방지하기 위하여 <a href=\"https://en.wikipedia.org/wiki/Trust_region\" target=\"_blank\" rel=\"noopener\">trust region</a>을 constraint로 설정할 수 있습니다. 이 아이디어로 인해서 TRPO라는 명칭을 가지게 됩니다.</p>\n<h3 id=\"1-1-7-Heuristic-approximation\"><a href=\"#1-1-7-Heuristic-approximation\" class=\"headerlink\" title=\"1.1.7. Heuristic approximation\"></a>1.1.7. Heuristic approximation</h3><p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;L_{\\theta_\\mathrm{old} }(\\theta) \\\\<br>\\mathrm{s.t.\\ }&amp;\\overline{D}_\\mathrm{KL}^{\\rho_\\mathrm{old} }\\left(\\theta_\\mathrm{old}, \\theta\\right) \\leq \\delta<br>\\end{align}$$</p>\n<p>사실 6의 constraint는 모든 state에 대해서 성립해야 하기 때문에 문제를 매우 어렵게 만듭니다. 이것을 좀 더 다루기 쉽게 state distribution에 대한 평균을 취한 것으로 변형합니다.</p>\n<h3 id=\"1-1-8-Monte-Carlo-Simulation\"><a href=\"#1-1-8-Monte-Carlo-Simulation\" class=\"headerlink\" title=\"1.1.8. Monte Carlo Simulation\"></a>1.1.8. Monte Carlo Simulation</h3><p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}Q_{\\theta_\\mathrm{old} }(s,a)\\right] \\\\<br>\\mathrm{s.t.\\ }&amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} }}\\left[D_\\mathrm{KL}\\left(\\pi_{\\theta_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta<br>\\end{align}$$</p>\n<p>Sampling을 통한 계산이 가능하도록 식을 다시 표현할 수 있습니다. </p>\n<h3 id=\"1-1-9-Efficiently-solving-TRPO\"><a href=\"#1-1-9-Efficiently-solving-TRPO\" class=\"headerlink\" title=\"1.1.9. Efficiently solving TRPO\"></a>1.1.9. Efficiently solving TRPO</h3><p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;\\nabla_\\theta \\left. L_{\\theta_\\mathrm{old} }(\\theta)\\right\\vert _{\\theta=\\theta_\\mathrm{old} } \\left(\\theta - \\theta_\\mathrm{old}\\right) \\\\<br>\\mathrm{s.t.\\ }&amp;\\frac{1}{2}\\left(\\theta_\\mathrm{old} - \\theta \\right)^T A\\left(\\theta_\\mathrm{old}\\right)\\left(\\theta_\\mathrm{old} - \\theta \\right) \\leq \\delta \\\\<br>&amp;A_{ij} = \\frac{\\partial}{\\partial \\theta_i}\\frac{\\partial}{\\partial \\theta_j}\\overline{D}_\\mathrm{KL}^{\\rho_\\mathrm{old} }\\left(\\theta_\\mathrm{old}, \\theta\\right)<br>\\end{align}$$</p>\n<p>문제를 효율적으로 풀기 위하여 approximation을 적용할 수 있습니다. objective function은 first order approximation, constraint는 quadratic approximation을 취하면 효율적으로 문제를 풀 수 있는 형태로 바뀌는데 이것을 natural gradient로 풀 수도 있고 conjugate gradient로 풀 수도 있습니다.</p>\n<p>TRPO는 이렇게 다양한 방법으로 문제를 변형한 것입니다! 이제 좀 더 자세히 살펴보겠습니다.</p>\n<p><br><br></p>\n<h1 id=\"2-Preliminaries\"><a href=\"#2-Preliminaries\" class=\"headerlink\" title=\"2. Preliminaries\"></a>2. Preliminaries</h1><p>다음과 같은 파라미터를 가지는 infinite-horizon discounted Markov decision process (MDP)를 고려합니다. </p>\n<ul>\n<li>$\\mathcal{S}$: finite set of states</li>\n<li>$\\mathcal{A}$: finite set of actions</li>\n<li>$P: \\mathcal{S}\\times\\mathcal{A}\\rightarrow\\mathbb{R}$: transition probability distribution</li>\n<li>$r: \\mathcal{S}\\rightarrow\\mathbb{R}$: reward function</li>\n<li>$\\rho_0:\\mathcal{S}\\rightarrow\\mathbb{R}$: distribution of the initial state $s_0$</li>\n<li>$\\gamma\\in(0,1)$: discount factor</li>\n<li>$\\eta(\\pi)=E_{s_0,a_0,\\ldots}\\left[\\sum_{t=0}^\\infty\\gamma^t r\\left(s_t\\right)\\right]$, where $s_0\\sim\\rho_0\\left(s_0\\right), a_t\\sim\\pi\\left(\\left.a_t \\right\\vert s_t\\right), s_{t+1}\\sim P\\left(\\left.s_{t+1}\\right\\vert s_t,a_t\\right)$</li>\n<li>$Q_\\pi\\left(s_t,a_t\\right)=E_{\\color{red}s_{t+1},a_{t+1},\\ldots}\\left[\\sum_{l=0}^\\infty \\gamma^l r\\left(s_{t+l}\\right)\\right]$: action value function</li>\n<li>$V_\\pi\\left(s_t\\right)=E_{\\color{red}a_t, s_{t+1},a_{t+1},\\ldots}\\left[\\sum_{l=0}^\\infty \\gamma^l r\\left(s_{t+l}\\right)\\right]$: value function (action value function과 expectation의 첨자가 다른 부분을 유의하세요.)</li>\n<li>$A_\\pi(s,a) = Q_\\pi(s,a) - V_\\pi(s)$: advantage function</li>\n</ul>\n<h2 id=\"2-1-Useful-identity-Kakade-amp-Langford-2002\"><a href=\"#2-1-Useful-identity-Kakade-amp-Langford-2002\" class=\"headerlink\" title=\"2.1 Useful identity [Kakade &amp; Langford 2002]\"></a>2.1 Useful identity [Kakade &amp; Langford 2002]</h2><p>우리의 목표는 $\\eta(\\pi)$가 최대화되도록 만드는 것입니다. 하지만 $\\pi$의 변화에 따라 $\\eta$가 어떻게 변하는지 알아내는 것도 쉽지 않습니다. $\\pi$는 기존의 policy이고 $\\tilde\\pi$는 새로운 policy를 나타낸다고 할 때 $\\eta$와 policy update 사이의 관계에 대해서 다음과 같은 관계가 있다는 것이 밝혀졌습니다.</p>\n<p><strong>Lemma 1.</strong> $\\eta\\left(\\tilde\\pi\\right) = \\eta(\\pi) + E_{s_{0},a_{0},\\ldots\\sim\\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t A_\\pi\\left(s_t,a_t\\right)\\right]$</p>\n<p><em>Proof.</em> $A_\\pi(s,a)=E_{s’\\sim P\\left(s’\\vert s,a\\right)}\\left[r(s)+\\gamma V_\\pi\\left(s’\\right)-V_\\pi(s)\\right]$로 다시 표현할 수 있습니다. 표기의 편의를 위하여 $\\tau:=\\left(s_0,a_0,s_1,a_1,\\ldots\\right)$를 정의하겠습니다. 다음과 같은 수식 전개가 가능합니다.<br>$$<br>\\begin{align}<br>&amp;E_{\\tau\\vert \\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t A_\\pi\\left(s_t,a_t\\right)\\right] \\\\<br> &amp;= E_{\\tau\\vert \\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t \\left(r(s_t)+\\gamma V_\\pi\\left(s_{t+1}\\right)-V_\\pi(s_t)\\right)\\right] \\\\<br>&amp;= E_{\\tau\\vert \\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t r(s_t)+\\gamma V_\\pi\\left(s_{1}\\right)-V_\\pi(s_0)+\\gamma^2 V_\\pi\\left(s_{2}\\right)-\\gamma V_\\pi(s_1)+\\gamma^3 V_\\pi\\left(s_{3}\\right)-\\gamma^2 V_\\pi(s_2)+\\cdots\\right] \\\\<br>&amp;= E_{\\tau\\vert \\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t r(s_t)+\\cancel{\\gamma V_\\pi\\left(s_{1}\\right)}-V_\\pi(s_0)+\\cancel{\\gamma^2 V_\\pi\\left(s_{2}\\right)}-\\cancel{\\gamma V_\\pi(s_1)}+\\cdots\\right] \\\\<br>&amp;= E_{\\tau\\vert \\tilde\\pi}\\left[-V_\\pi(s_0)+\\sum_{t=0}^\\infty\\gamma^t r(s_t)\\right] \\\\<br>&amp;\\overset{\\underset{\\mathrm{(a)} }{} }{=} -E_{\\color{red}s_0}\\left[V_\\pi(s_0)\\right]+E_{\\tau\\vert \\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t r(s_t)\\right] \\\\<br>&amp;=-\\eta(\\pi) + \\eta\\left(\\tilde\\pi\\right)\\\\<br>&amp;\\quad\\therefore \\eta\\left(\\tilde\\pi\\right) = \\eta(\\pi) + E_{s_{0},a_{0},\\ldots\\sim\\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t A_\\pi\\left(s_t,a_t\\right)\\right]<br>\\end{align}<br>$$<br>위의 전개의 (a) 부분은 $V_\\pi\\left(s_0\\right)=E_{a_0,s_0,a_1,\\ldots}\\left[\\sum_{t=0}^\\infty\\gamma^l r\\left(s_l\\right)\\right]$이므로 $\\tau$ 중 많은 부분들이 이미 expectation이 취해진 값이므로 무시됩니다. 오직 $s_0$만 expectation이 취해지지 않았기 때문에 남아있습니다. $\\square$</p>\n<p><strong>Lemma 1</strong>은 새로운 policy와 기존의 policy 사이의 관계를 규정합니다. 다음을 정의합니다. </p>\n<ul>\n<li>$\\rho_\\pi(s) = P\\left(s_0=s\\right) + \\gamma P\\left(s_1=s\\right)\\ + \\gamma^2 P\\left(s_2=s\\right) + \\cdots $: (unnormalized) discounted visitation frequencies</li>\n</ul>\n<p>이것을 이용해서 <strong>Lemma 1</strong>을 변형시켜 봅시다.</p>\n<p>$$<br>\\begin{align}<br>\\eta\\left(\\tilde\\pi\\right) &amp;= \\eta(\\pi) + \\sum_{t=0}^\\infty\\sum_{s}P\\left(\\left.s_t=s\\right\\vert \\tilde\\pi\\right)\\sum_a\\tilde\\pi(a\\vert s)\\gamma^t A_\\pi(s,a) \\\\<br>&amp;= \\eta(\\pi) + \\sum_{s}{\\color{red}\\sum_{t=0}^\\infty \\gamma^t P\\left(\\left.s_t=s\\right\\vert \\tilde\\pi\\right)}\\sum_a\\tilde\\pi(a\\vert s) A_\\pi(s,a) \\\\<br>&amp;= \\eta(\\pi) + \\sum_{s}{\\color{red}\\rho_\\tilde\\pi(s)}\\sum_a\\tilde\\pi(a\\vert s) A_\\pi(s,a) \\\\<br>\\end{align}<br>$$</p>\n<p>이 수식의 의미가 무엇일까요? 만약 $\\sum_a\\tilde\\pi(a\\vert s) A_\\pi(s,a) \\geq 0$이라면 $\\eta(\\tilde\\pi)$는 항상 $\\eta(\\pi)$보다 큽니다. 즉, policy를 업데이트함으로써 항상 개선됩니다. 즉, 이 수식을 통해서 항상 더 좋은 성능을 내는 policy update가 가져야 할 특징을 알 수 있습니다. 다음과 같은 deterministic policy가 있다고 합시다.</p>\n<p>$$<br>\\tilde\\pi(s) = \\arg\\max_a A_\\pi(s,a)<br>$$</p>\n<p>이 policy는 적어도 하나의 state-action pair에서 0보다 큰 값을 가지는 advantage가 있고 그 때의 확률이 0이 아니라면 항상 성능을 개선시킵니다. 어려운 점은 policy를 바꾸면 $\\rho$도 바뀐다는 점입니다. </p>\n<p>다음 그림을 봅시다. Starting Point에서 여러가지 경로를 거쳐서 Destination으로 갈 수 있습니다. 다른 policy를 이용하는 것은 다른 경로를 이용한 것입니다.</p>\n<p><img src=\"../img/policy_change.png\" alt=\"policy_change\"></p>\n<p>이 때 다른 policy를 이용함에 따라 state visitation frequency도 변하게 됩니다.</p>\n<p><img src=\"../img/state_visitation_change.png\" alt=\"state_visitation_change\"></p>\n<p>이로 인해서 policy를 최적화하는 것은 어려워집니다. 그래서 이러한 변화를 무시하는 다음과 같은 approximation을 취합니다.</p>\n<p>$$<br>\\begin{align}<br>L_\\pi\\left(\\tilde\\pi\\right) &amp;= \\eta(\\pi) + \\sum_{s}{\\color{red}\\rho_\\pi(s)}\\sum_a\\tilde\\pi(a\\vert s) A_\\pi(s,a)<br>\\end{align}<br>$$</p>\n<p>policy가 바뀌었음에도 이전의 state distribution을 게속 이용하는 것입니다. 이것은 policy의 변화가 크지 않다면 어느 정도 허용될 수 있을 것입니다. 그렇지만 얼마나 많은 변화가 허용될까요? 이것을 정하기 위해서 이용하는 것이 trust region입니다.</p>\n<p><br></p>\n<h2 id=\"2-1-Conservative-Policy-Iteration\"><a href=\"#2-1-Conservative-Policy-Iteration\" class=\"headerlink\" title=\"2.1. Conservative Policy Iteration\"></a>2.1. Conservative Policy Iteration</h2><p>policy의 변화를 다루기 용이하게 하기 위해서 policy를 다음과 같이 파라미터를 이용해서 표현합시다.</p>\n<ul>\n<li>$\\pi_\\theta(a\\vert s)$: parameterized policy</li>\n</ul>\n<p>$\\pi_\\theta$는 $\\theta$에 대하여 미분가능한 함수입니다. $L_\\pi\\left(\\tilde\\pi\\right)$은 $\\theta_0$에서 $\\eta(\\pi)$에 대한 first order approximation입니다.</p>\n<p>$$<br>\\begin{align}<br>L_{\\pi_{\\theta_0} }\\left(\\pi_{\\theta_0}\\right) &amp;= \\eta\\left(\\pi_{\\theta_0}\\right) \\\\<br>\\nabla_\\theta \\left.L_{\\pi_{\\theta_0} }\\left(\\pi_{\\theta_0}\\right)\\right\\vert _{\\theta=\\theta_0} &amp;= \\nabla_\\theta\\left.\\eta(\\pi_{\\theta_0})\\right\\vert _\\theta=\\theta_0<br>\\end{align}<br>$$</p>\n<p>이것의 의미는 $\\pi_{\\theta_0}$가 매우 작게 변한다면 $L_{\\pi_{\\theta_0} }$를 개선시키는 것이 $\\eta$를 개선시키는 것이라는 것입니다. 그러나 지금까지의 설명만으로는 $\\pi_{\\theta_0}$를 얼마나 작게 변화시켜야 할지에 대해서는 알 수 없습니다.</p>\n<p>Kakade &amp; Langford가 2002년 발표한 논문에서 이것을 고려하였습니다. 그 논문에서 <em>conservative policy iteration</em>이라는 기법을 제안합니다. 이 논문의 contribution은 다음과 같습니다.</p>\n<ul>\n<li>$\\eta$ 개선의 lower bound를 제공</li>\n<li>기존의 policy를 $\\pi_\\mathrm{old}$라고 하고 $\\pi’$를 $\\pi’=\\arg\\max_{\\pi’}L_{\\pi_\\mathrm{old} }\\left(\\pi’\\right)$과 같이 정의할 때, 새로운 mixture policy $\\pi_\\mathrm{new}$를 다음과 같이 제안<br>$$<br>\\pi_\\mathrm{new}(a\\vert s) = (1-\\alpha)\\pi_\\mathrm{old}(a\\vert s) + \\alpha \\pi’(a\\vert s)<br>$$</li>\n</ul>\n<p>그림으로 표현하면 다음과 같습니다.</p>\n<p><img src=\"../img/mixure_policy.png\" alt=\"mixure_policy\"></p>\n<ul>\n<li>다음과 같은 lower bound를 정의<br>$$<br>\\eta\\left(\\pi_\\mathrm{new}\\right) \\geq L_{\\pi_{\\theta_\\mathrm{old} }}\\left(\\pi_\\mathrm{new}\\right) - \\frac{2\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2,\\\\<br>\\mathrm{where\\ }\\epsilon = \\max_s\\left\\vert E_{a\\sim\\pi’(a\\vert s)}\\left[A_\\pi(s,a)\\right]\\right\\vert<br>$$</li>\n</ul>\n<p>하지만 mixture된 policy라는 것은 실용적이지 않습니다. </p>\n<p><br><br></p>\n<h1 id=\"3-Monotonic-Improvement-Guarantee-for-General-Stochastic-Policies\"><a href=\"#3-Monotonic-Improvement-Guarantee-for-General-Stochastic-Policies\" class=\"headerlink\" title=\"3. Monotonic Improvement Guarantee for General Stochastic Policies\"></a>3. Monotonic Improvement Guarantee for General Stochastic Policies</h1><p>이전 장에서 설명한 lower bound는 오직 mixture policy에 대해서만 성립하고 보다 더 많이 사용되는 stochastic policy에는 적용되지 않습니다. 따라서 이러한 방향으로 개선이 필요합니다. 기존 수식의 두 가지를 바꿈으로써 이것을 달성할 수 있습니다.</p>\n<ul>\n<li>$\\alpha\\rightarrow$ distance measure between $\\pi$ and $\\tilde\\pi$</li>\n<li>constant $\\epsilon\\rightarrow\\max_{s,a}\\left\\vert A_\\pi(s,a)\\right\\vert $</li>\n</ul>\n<p>여기서 distance measure로 total variation divergence를 이용합니다. discrete porbability distribution $p$와 $q$에 대하여 다음과 같이 정의됩니다.<br>$$<br>D_\\mathrm{TV}(p\\parallel q) = \\frac{1}{2}\\sum_i\\left\\vert p_i - q_i\\right\\vert<br>$$</p>\n<p>이것을 이용하여 $D_\\mathrm{TV}^\\max$를 다음과 같이 정의하겠습니다.<br>$$<br>D_\\mathrm{TV}^\\max(\\pi\\parallel \\tilde\\pi) = \\max_s D_\\mathrm{TV}\\left(\\pi(\\cdot\\vert s)\\parallel\\tilde\\pi(\\cdot\\vert s)\\right)<br>$$</p>\n<p>그림으로 표현하면 다음과 같습니다.</p>\n<p><img src=\"../img/tvd.png\" alt=\"tvd\"></p>\n<p>이것을 이용하여 다음과 같은 관계식을 얻을 수 있습니다.</p>\n<p><strong>Theorem 1.</strong> Let $\\alpha=D_\\mathrm{TV}^\\max(\\pi\\parallel \\tilde\\pi)$. Then the following bound holds:<br>$$<br>\\begin{align}<br>\\eta\\left(\\pi_\\mathrm{new}\\right) &amp;\\geq L_{\\pi_\\mathrm{old} }\\left(\\pi_\\mathrm{new}\\right) - \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2, \\\\<br>\\mathrm{where\\ } \\epsilon&amp;= \\max_{s,a}\\left\\vert A_\\pi(s,a)\\right\\vert<br>\\end{align}<br>$$<br><em>Proof.</em> TBD.</p>\n<p>또다른 distance metric으로 아래 그림과 같은 KL divergence가 있습니다.<br>(그런데 왜 하필 KL divergence로 바꿀까요? 논문의 뒤쪽에서 계산효율을 위해서 conjugate gradient method를 이용하는데 이를 위해서 바꾼게 아닐까 싶습니다. Wasserstein distance 같은 다른 방향으로 발전시킬 수도 있을 것 같습니다. Schulmann이 아마 해봤겠죠?^^)</p>\n<p><img src=\"../img/kld.png\" alt=\"kld\"></p>\n<p>total variation divergence와 KL divergence 사이에는 다음과 같은 관계가 있습니다.<br>$$<br>D_\\mathrm{TV}(p\\parallel q)^2 \\leq D_\\mathrm{KL}(p\\parallel q)<br>$$</p>\n<p>다음 수식을 정의합니다.<br>$$<br>D_\\mathrm{KL}^\\max(\\pi\\parallel \\tilde\\pi) = \\max_s D_\\mathrm{KL}\\left(\\pi(\\cdot\\vert s)\\parallel\\tilde\\pi(\\cdot\\vert s)\\right)<br>$$</p>\n<p><strong>Theorem 1</strong>을 이용하여 다음과 같은 수식이 성립함을 알 수 있습니다.<br>$$<br>\\begin{align}<br>\\eta\\left(\\tilde\\pi\\right) &amp;\\geq L_{\\pi}\\left(\\tilde\\pi\\right) - C\\cdot D_\\mathrm{KL}^\\max(\\pi, \\tilde\\pi), \\\\<br>\\mathrm{where\\ } C&amp;= \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}<br>\\end{align}<br>$$</p>\n<p>이러한 policy imporvement bound를 기반으로 다음과 같은 approximate policy iteration 알고리듬을 고안해낼 수 있습니다.</p>\n<p><strong>Algorithm 1</strong> Policy iteration algorithm guaranteeing non-decreasing expected return $\\eta$</p>\n<blockquote>\n<p>Initialize $\\pi_0$<br><strong>for</strong> $i=0,1,2,\\ldots$ until convergence <strong>do</strong><br>$\\quad$Compute all advantage values $A_{\\pi_i}(s,a)$<br>$\\quad$Solve the contstrained optimization problem<br>$\\quad\\pi_{i+1}=\\arg\\max_\\pi\\left[L_{\\pi_i} - CD_\\mathrm{KL}^\\max\\left(\\pi_i,\\pi\\right)\\right]$<br>$\\quad\\quad$where $C = 4\\epsilon\\gamma / (1-\\gamma)^2$<br>$\\quad\\quad$and $L_{\\pi_i}\\left(\\pi\\right) = \\eta(\\pi_i) + \\sum_{s}\\rho_{\\pi_i}(s)\\sum_a\\pi(a\\vert s) A_{\\pi_i}(s,a)$<br><strong>end for</strong></p>\n</blockquote>\n<p><strong>Algorithm 1</strong>은 advantage를 정확하게 계산할 수 있다고 가정하고 있습니다. 이 알고리듬은  monotonical한 성능 증가($\\eta(\\pi_0)\\leq\\eta(\\pi_1)\\leq\\cdots$)를 한다는 것을 다음과 같이 보일 수 있습니다. $M_i(\\pi)=L_{\\pi_i}(\\pi) - CD_\\mathrm{KL}^\\max\\left(\\pi_i,\\pi\\right)$라고 합시다.<br>$$\\begin{align}<br>\\eta_\\left(\\pi_{i+1}\\right) &amp;\\geq M_i\\left(\\pi_{i+1}\\right)\\\\<br>\\eta_\\left(\\pi_{i}\\right) &amp;= M_i\\left(\\pi_{i}\\right) {\\color{blue}\\mathrm{\\ (why\\ same?)} }\\\\<br>\\eta_\\left(\\pi_{i+1}\\right) - \\eta_\\left(\\pi_{i}\\right) &amp;\\geq M_i\\left(\\pi_{i+1}\\right) - M_i\\left(\\pi_{i}\\right)<br>\\end{align}$$</p>\n<p>위 수식과 같이 매 iteration 마다 $M_i$를 최대화함으로써, $\\eta$가 감소하지 않는다는 것을 보장할 수 있습니다. 이와 같은 타입의 알고리듬을 minorization-maximization (MM) algorithm이라고 합니다. EM 알고리듬도 이 타입의 알고리듬 중 하나입니다.</p>\n<p><strong>Algorithm 1</strong>은 아래 그림과 같이 동작합니다.</p>\n<p><img src=\"../img/surrogate.png\" alt=\"surrogate\"></p>\n<p>$M_i$는 $\\pi_i$일 때 equality가 되는 $\\eta$에 대한  surrogate function입니다. TRPO는 이 surrogate function을 최대화하고 KL divergence를 penalty가 아닌 constraint로 두는 알고리듬입니다.</p>\n<p><br><br></p>\n<h1 id=\"4-Optimization-of-Parameterized-Policies\"><a href=\"#4-Optimization-of-Parameterized-Policies\" class=\"headerlink\" title=\"4. Optimization of Parameterized Policies\"></a>4. Optimization of Parameterized Policies</h1><p>표기의 편의를 위해 다음과 같이 notation들을 정의합니다.</p>\n<ul>\n<li>$\\eta(\\theta):=\\eta\\left(\\pi_\\theta\\right)$</li>\n<li>$L_\\theta\\left(\\tilde\\theta\\right):=L_{\\pi_\\theta}\\left(\\pi_{\\tilde\\theta}\\right)$</li>\n<li>$D_\\mathrm{KL}\\left(\\theta\\parallel\\tilde\\theta\\right):=D_\\mathrm{KL}\\left(\\pi_\\theta\\parallel\\pi_{\\tilde\\theta}\\right)$</li>\n<li>$\\theta_\\mathrm{old}$: previous policy parameter</li>\n</ul>\n<h2 id=\"4-1-Trust-Region-Policy-Optimization\"><a href=\"#4-1-Trust-Region-Policy-Optimization\" class=\"headerlink\" title=\"4.1. Trust Region Policy Optimization\"></a>4.1. Trust Region Policy Optimization</h2><p>이전 장의 중요 결과를 위의 notation으로 다시 표기하면 아래와 같습니다.</p>\n<p>$$<br>\\eta\\left(\\theta\\right) \\geq L_{\\theta_\\mathrm{old} }\\left(\\theta\\right) - C\\cdot D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right)<br>$$</p>\n<p>$\\eta$의 성능 향상을 보장하기 위해서 그것의 lower bound를 최대화할 수 있습니다.</p>\n<p>$$<br>\\max_\\theta \\left[L_{\\theta_\\mathrm{old} }\\left(\\theta\\right) - C D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right)\\right]<br>$$</p>\n<p>이 최적화 문제는 step size를 매우 작게 해야 올바른 동작을 합니다. 위에서 살펴봤듯이 first order approximation이기 때문입니다. 좀 더 큰 step size를 가질 수 있도록 이 최적화 문제를 trust region constraint를 도입하여 다음과 같이 바꿉니다.</p>\n<p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;L_{\\theta_\\mathrm{old} }(\\theta) \\\\<br>\\mathrm{s.t.\\ }&amp;D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right) \\leq \\delta<br>\\end{align}$$</p>\n<p>이 최적화문제의 constraint는 모든 state space에 대해서 성립해야 합니다. 또한 maximum값을 매번 찾아야 합니다. state가 많은 경우 constraint의 수가 매우 많아지고 이것은 문제를 풀기 어렵게 만듭니다. constraint의 수를 줄이기 위하여 다음과 같은 avergae KL divergence를 이용하는 heuristic approximation을 취합니다. 이것이 최선의 방법은 아닐 수 있지만 실용적인 방법입니다.</p>\n<p>$$<br>\\overline D_\\mathrm{KL}^{\\rho}\\left(\\theta_1, \\theta_2\\right):=E_{s\\sim\\rho}\\left[D_\\mathrm{KL}\\left(\\pi_{\\theta_1}(\\cdot\\vert s)\\parallel\\pi_{\\theta_2}(\\cdot\\vert s)\\right)\\right]<br>$$</p>\n<p>이것을 기반으로 다음과 같은 최적화 문제를 풀 수 있습니다.</p>\n<p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;L_{\\theta_\\mathrm{old} }(\\theta) \\\\<br>\\mathrm{s.t.\\ }&amp;\\overline{D}_\\mathrm{KL}^{\\rho_\\mathrm{old} }\\left(\\theta_\\mathrm{old}, \\theta\\right) \\leq \\delta<br>\\end{align}$$</p>\n<p>아래 그림처럼 step size에 대해서 고려할 수 있습니다.</p>\n<p><img src=\"../img/heuristic_approx.png\" alt=\"heuristic_approx\"></p>\n<p><br><br></p>\n<h1 id=\"5-Sample-Based-Estimation-of-the-Objective-and-Constraint\"><a href=\"#5-Sample-Based-Estimation-of-the-Objective-and-Constraint\" class=\"headerlink\" title=\"5. Sample-Based Estimation of the Objective and Constraint\"></a>5. Sample-Based Estimation of the Objective and Constraint</h1><p>실용적인 알고리듬을 만들려고 하는 노력은 아직 끝나지 않았습니다. 이제 앞의 알고리듬을 sample-based estimation 즉, Monte Carlo estimation을 할 수 있도록 바꿔보겠습니다. sampling을 편하게 할 수 있도록 아래와 같이 바꿔줍니다.</p>\n<ul>\n<li>$\\sum_s \\rho_{\\theta_\\mathrm{old} }(s)[\\ldots]\\rightarrow\\frac{1}{1-\\gamma}E_{s\\sim\\rho_{\\theta_\\mathrm{old} }}[\\ldots]$</li>\n<li>$A_{\\theta_\\mathrm{old} }\\rightarrow Q_{\\theta_\\mathrm{old} }$</li>\n<li>$\\sum_a\\pi_{\\theta_\\mathrm{old} }(a\\vert s)A_{\\theta_\\mathrm{old} }\\rightarrowE_{a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}A_{\\theta_\\mathrm{old} }\\right]$</li>\n</ul>\n<p>이러한 변화를 그림처럼 도식화할 수 있습니다.</p>\n<p><img src=\"../img/sample-based.png\" alt=\"sample-based\"></p>\n<p>한 가지 짚고 넘어가야 할 점은 action sampling을 할 때 importance sampling을 사용한다는 것입니다.</p>\n<p><img src=\"../img/importance_sampling.png\" alt=\"importance_sampling\"></p>\n<p>바뀐 최적화문제는 아래와 같습니다.</p>\n<p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}Q_{\\theta_\\mathrm{old} }(s,a)\\right] \\\\<br>\\mathrm{s.t.\\ }&amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} }}\\left[D_\\mathrm{KL}\\left(\\pi_{\\theta_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta<br>\\end{align}$$</p>\n<p>이 때 sampling하는 두 가지 방법이 있습니다.</p>\n<p><br></p>\n<h2 id=\"5-1-Single-Path\"><a href=\"#5-1-Single-Path\" class=\"headerlink\" title=\"5.1. Single Path\"></a>5.1. Single Path</h2><p><em>single path</em>는 개별 trajectory들을 이용하는 방법입니다.</p>\n<p><img src=\"../img/single.png\" alt=\"single\"></p>\n<p><br></p>\n<h2 id=\"5-2-Vine\"><a href=\"#5-2-Vine\" class=\"headerlink\" title=\"5.2. Vine\"></a>5.2. Vine</h2><p><em>vine</em>은 한 state에서 rollout을 이용하여 여러 action을 수행하는 방법입니다.</p>\n<p><img src=\"../img/vine1.png\" alt=\"vine1\"></p>\n<p><img src=\"../img/vine2.png\" alt=\"vine2\"></p>\n<p>estimation의 variance를 낮출 수 있지만 계산량이 많고 한 state에서 여러 action을 수행할 수 있어야 하기 때문에 현실적인 문제에 적용하기에는 어려움이 있습니다.</p>\n<h1 id=\"Trust-Region-Policy-Optimization-1\"><a href=\"#Trust-Region-Policy-Optimization-1\" class=\"headerlink\" title=\"Trust Region Policy Optimization\"></a>Trust Region Policy Optimization</h1><h1 id=\"Conjugate-Gradient-Method\"><a href=\"#Conjugate-Gradient-Method\" class=\"headerlink\" title=\"Conjugate Gradient Method\"></a>Conjugate Gradient Method</h1><p><img src=\"https://datascienceschool.net/upfiles/a5ba6251b6f144249cca6eb8cc523682.png\" alt=\"\"></p>\n<p>테일러 급수에 대하여 : <a href=\"https://terms.naver.com/entry.nhn?docId=3572065&amp;cid=58944&amp;categoryId=58970\" target=\"_blank\" rel=\"noopener\">naver</a> , <a href=\"https://ko.wikipedia.org/wiki/%ED%85%8C%EC%9D%BC%EB%9F%AC_%EC%A0%95%EB%A6%AC\" target=\"_blank\" rel=\"noopener\">wikipedia</a></p>\n<p>: 근사 다항식을 구하기위해.</p>\n<p>$\\large f(x) = \\frac{f(a)}{0!} + \\frac{f’(a)}{1!}(x-a)^1 + \\frac{f’’(a)}{2!}(x-a)^2 + … + R_{n+1}(x)$</p>\n<p>$\\large = \\sum_{k=0}^{n} \\frac{f^{(k)}(a)}{k!}(x-a)^k + R_{n+1}(x)$</p>\n<h2 id=\"5-Sample-based-Estimation-of-the-Objective-and-Constraint\"><a href=\"#5-Sample-based-Estimation-of-the-Objective-and-Constraint\" class=\"headerlink\" title=\"5. Sample-based Estimation of the Objective and Constraint\"></a>5. Sample-based Estimation of the Objective and Constraint</h2><p>이전 장에서, 각 업데이트 시에 정책 변화를 제약하면서 기대누적보상 $\\large \\eta$ 의 추정치를 최적화하는</p>\n<p>정책 파라미터의 제약조건이 있는 최적화 문제를 제안했다. - eq.(12)</p>\n<h4 id=\"Equation-12\"><a href=\"#Equation-12\" class=\"headerlink\" title=\"Equation 12\"></a>Equation 12</h4><p>$\\LARGE \\underset{\\theta}{maximize} \\ L_{\\theta_{old} }(\\theta)$</p>\n<p>$\\LARGE subject \\ to \\ \\bar{D}<em>{KL}^{\\rho</em>{\\theta_{old} }} (\\theta_{old}, \\theta) \\leq \\delta$</p>\n<p><br></p>\n<p>이번 5장에선, <strong>어떻게 목적함수와 제약식이 몬테카를로 시뮬레이션을 통해 근사되는지</strong> 설명한다.</p>\n<h2 id=\"5-Sample-based-Estimation-of-the-Objective-and-Constraint-1\"><a href=\"#5-Sample-based-Estimation-of-the-Objective-and-Constraint-1\" class=\"headerlink\" title=\"5. Sample-based Estimation of the Objective and Constraint\"></a>5. Sample-based Estimation of the Objective and Constraint</h2><p>eq. (12)의 $\\large L_{\\theta_{old}(\\theta)}$ 를 확장하면 다음의 식을 얻을 수 있다.</p>\n<p><strong>Equation 13</strong></p>\n<p>$\\LARGE \\underset{\\theta}{maximize} \\ \\sum_s \\rho_{\\theta_{old} }(s) \\sum_a \\pi_{\\theta}(a\\vert s) A_{\\theta_{old} }(s, a)$</p>\n<p>$\\LARGE subject \\ to \\ \\bar{D}<em>{KL}^{\\rho</em>{\\theta_{old} }} (\\theta_{old}, \\theta) \\leq \\delta$</p>\n<p>여기서…</p>\n<p>1) $\\LARGE \\sum_s \\rho_{\\theta_{old} }(s)$  를 $\\LARGE \\frac{1}{1-\\gamma} E_{s \\sim \\rho_{\\theta_{old} }} $ 로 대체.</p>\n<p><strong> 어디서 $\\large \\frac{1}{1-\\gamma}$ 가 나왔을까? Kakade &amp; LangFord 2002 </strong></p>\n<p>-\\vert  -<br>:—————:\\vert :—————:<br><img src=\"https://i.imgur.com/uo0zOqd.png\" alt=\"\"> \\vert  <img src=\"https://i.imgur.com/PeG5gNp.png\" alt=\"\">\\vert </p>\n<p>2) $\\large A_{\\theta_{old} }$ 을 $\\large Q_{\\theta_{old} }$ 로 대체.</p>\n<p>3) 액션의 확률합을 importance sampling으로 대체</p>\n<p>$\\LARGE \\sum_a \\pi_{\\theta}(a\\vert s_n)A_{\\theta_{old} }(s_n, a) = \\color{Red}{E_{a \\sim q} [\\frac{\\pi_{\\theta}(a\\vert s_n)}{q(a\\vert s_n)} } \\ A_{\\theta_{old} }(s_n, a)]$</p>\n<p>대체한 결과는</p>\n<p><strong>Equation 14</strong></p>\n<p>$\\LARGE \\underset{\\theta}{maximize} E_{s \\sim \\rho_{\\theta_{old} } \\ , \\ a \\sim q} [\\frac{\\pi_{\\theta}(a\\vert s_n)}{q(a\\vert s_n)}Q_{\\theta_{old} }(s, a)]$</p>\n<p>$\\LARGE subject \\ to \\ E_{s \\sim \\rho_{\\theta_{old} }}[D_{KL}(\\pi_{\\theta_{old} })(\\cdot \\vert  s) \\ \\vert \\vert  \\ \\pi_{\\theta}(\\cdot \\vert  s)] \\leq \\delta$</p>\n<h2 id=\"5-Sample-based-Estimation-of-the-Objective-and-Constraint-2\"><a href=\"#5-Sample-based-Estimation-of-the-Objective-and-Constraint-2\" class=\"headerlink\" title=\"5. Sample-based Estimation of the Objective and Constraint\"></a>5. Sample-based Estimation of the Objective and Constraint</h2><p>Eq. 14에서 Expectation은 샘플 평균으로 구하고, Q value는 실행했을때의 추정값으로 활용한다.</p>\n<p>아래는 추정을 수행하는 방식이 다른 2가지 에 대해 설명한다.</p>\n<h3 id=\"5-1-Single-Path-1\"><a href=\"#5-1-Single-Path-1\" class=\"headerlink\" title=\"5.1 Single Path\"></a>5.1 Single Path</h3><p>single path 샘플링 방식은 일반적으로 policy gradient estimation에 사용하고</p>\n<p>개별적인 trajectory를 샘플링 한다.</p>\n<p>여기서 $\\large s_0 \\sim \\rho_{\\theta} \\leftarrow$ (discounted visitation distribution)</p>\n<p>상태들의 시퀀스를 모으고, trajectory를 생성하기위해 임의의 타임스텝만큼 policy인 $\\large \\pi_{\\theta_{old} }$ 을 시뮬레이션한다.</p>\n<p>$\\large (s_0 \\ , a_0 \\ , s_1 \\ , a_1 … \\ s_{t-1} \\ , a_{t-1} \\ , s_t  )$</p>\n<p>따라서, $\\LARGE q(a\\vert s) = \\pi_{\\theta_{old} }$ 이다.</p>\n<p>$\\large Q_{\\theta_{old} }(s, a)$ 는 trajectory의 각 state-action 쌍인 $\\large (s_t, a_t)$ 의 $\\large \\sum \\gamma r(s,a)$ 를 계산한다.</p>\n<h3 id=\"5-2-Vine-1\"><a href=\"#5-2-Vine-1\" class=\"headerlink\" title=\"5.2 Vine\"></a>5.2 Vine</h3><p>vine 방식은 roll out set이라는 것을 만들고 roll out set의 각 상태마다 여러 액션을 수행해본다.</p>\n<p>추정 과정에서, 처음엔 $\\large s_0 \\sim \\rho_{\\theta}$ 샘플링하고 $\\large \\pi_{\\theta}$ 로 시뮬레이션을 반복해서 몇개의 trajectory들을 생성한다.</p>\n<p>다음엔 이 trajectory들 에서 N개의 상태를 부분집합으로 고른다. $\\large s_1, s_2, … , s_N$ 이걸 roll out set이라 부른다.</p>\n<p>roll out set 각 상태 $\\large s_n$에서 $\\large a_{n , k} \\sim q(\\cdot \\vert  s_n)$ 에 대해 K개의 액션을 샘플링한다.</p>\n<p>이와 같은 방식은 exploration 측면에 있어 좋은 성능을 보였고, Robotic locomotion 과 Atari game, 즉 Continuous / discrete task 모두 잘 되었다.</p>\n<p>각 상태 $\\large s_n$ 에서 샘플링된 각 액션 $\\large a_{n , k}$에서 roll out을 수행하면서 (짧은 trajectory) $\\large \\overset{\\wedge}{Q}_{\\theta_i}(s_n, a_n, k)$ 을 추정한다.</p>\n<p>각 K개의 roll out에서 노이즈를 발생하기 위해 random number sequence를 사용한 roll out과 Q value의 차이의 분산을 매우 감소시켰다.</p>\n<p>작고 유한한 액션공간에서는 주어진 상태에서 모든 가능한 액션을 rollout을 통해 생성할 수 있었다.</p>\n<p>한 상태 $\\large s_n$ 에서 $\\large L_{\\theta_{old} }$ 에 기여하는 식은 다음과 같다.</p>\n<p><strong>Equation 15</strong></p>\n<p>$\\LARGE L_n(\\theta) = \\sum_{k=1}^K \\pi_{\\theta}(a_k\\vert s_n) \\overset{\\wedge}{Q}(s_n, a_k)$</p>\n<p><br></p>\n<p>상태공간이 크고 continuous 한 상황에서, importance sampling을 활용해 surrogate objective를 추정할 수 있었다.</p>\n<p>한 상태 $\\large s_n$ 에서 $\\large L_{\\theta_{old} }$ 을 얻는 self normalized estimator<a href=\"http://statweb.stanford.edu/~owen/mc/Ch-var-is.pdf\" target=\"_blank\" rel=\"noopener\">(Owen, 2013)</a>는 다음과 같다.</p>\n<p><strong>Equation 16</strong></p>\n<p>$\\LARGE L_n(\\theta) = \\frac{\\sum_{k=1}^{K} \\frac{\\pi_{\\theta}(a_{n,k} \\ \\vert  \\ s_n)}{\\pi_{\\theta_{old} }(a_{n,k} \\ \\vert  \\ s_n)} \\overset{\\wedge}{Q}(s_n, a_{n,k})}{\\sum_{k=1}^{K} \\frac{\\pi_{\\theta}(a_{n,k} \\ \\vert  \\ s_n)}{\\pi_{\\theta_{old} }(a_{n,k} \\ \\vert  \\ s_n)} }$</p>\n<p>여기서 상태 $\\large s_n$ 에서 K개의 액션을 수행한다고 가정한다. $\\large a_{n,1}, a_{n,2} \\ … \\ a_{n,k} $</p>\n<p>이 self normalized estimator는 Q value를 구하기위해 베이스라인을 사용할 필요성을 없애준다. (Q value에 상수를 더함으로서 그라디언트가 바뀌지않음???)</p>\n<p>$\\large s_n \\sim \\rho(\\pi)$ 을 평균을 내줌으로서, $\\large L_{\\theta_{old} }$ estimator와 그라디언트를 얻을 수 있다.</p>\n<h3 id=\"5-2-Vine-2\"><a href=\"#5-2-Vine-2\" class=\"headerlink\" title=\"5.2 Vine\"></a>5.2 Vine</h3><p>vine은 샘플링을 통한 trajectory들이 다양한 포인트(rollout set)에서 여러갈래로 짧은 가지를 치고 있는 것이 마치 덩굴식물 줄기같아서 이름붙였다.</p>\n<p>single-path 보다 vine의 이점은 surrogate objective에서 같은 숫자의 Q-value 샘플들이 주어졌을때, 목적함수의 추정치가 낮은 분산을 갖는 것이다.</p>\n<p>즉, vine 방법은 advantage value의 아주 좋은 추정치를 갖게 해준다.</p>\n<p>vine의 불리한 부분은 반드시 이 advantage 추정 각각을 위해 시뮬레이터가 매우 많은 요청을 수행해야하는 것이다.</p>\n<p>게다가, vine은  rollout set 내부의 각 상태에서 다수의 trajectory들을 생성해야하는데 이것은 시스템이 임의의 상태로 초기화 될수 있는 환경으로 제한해버린다.</p>\n<p>반면, single path는 상태 초기화도 없고, 실세계로 직접 구현 가능하다.</p>\n<p><br></p>\n<h2 id=\"6-Practical-Algorithm\"><a href=\"#6-Practical-Algorithm\" class=\"headerlink\" title=\"6. Practical Algorithm\"></a>6. Practical Algorithm</h2><p>앞서 Single-path, Vine 샘플링을 사용하는 두가지 방식의 policy optimization 알고리즘을 선보였다.</p>\n<p>반복적으로 아래의 과정을 수행한다.</p>\n<p>1) Q-values의 몬테카를로 추정과 state-action 쌍 집합을 single path / vine 과정을 통해 모은다.</p>\n<p>2) 샘플 평균으로, 목적함수와 eq.(14)의 제약식을 추정한다.<br>eq.(14) : <br></p>\n<p>$\\LARGE \\underset{\\theta}{maximize} E_{s \\ \\sim \\ \\rho_{\\theta_{old} } \\ \\text{,} \\ a \\ \\sim \\ q} [\\frac{\\pi_{\\theta}(a\\vert s)}{q(a\\vert s)}Q_{\\theta_{old} }(s, a)]$</p>\n<p>$\\large subject \\ to \\ E_{s \\ \\sim \\ \\rho_{\\theta_{old} }} [D_{KL}(\\pi_{\\theta_{old} }(\\cdot \\vert  s) \\ \\vert \\vert  \\ \\pi_{\\theta}(\\cdot\\vert s))] \\leq \\delta$</p>\n<p>3) policy parameter vector인 $\\theta$를 업데이트 하면서 제약조건이 있는 최적화 문제를 근사적으로 푼다.</p>\n<p>본 논문에서는 1차미분보다는 좀 계산량이 있는 line search와 Conjugate Gradient algorithm을 사용했다.</p>\n<p>3)에 대해서,  그라디언트의 covariance matrix를 사용하지않고 KL Divergence의 헤시안을 해석적으로 계산해서  Fisher Information Matrix를 구성했다.</p>\n<p>다시말해서</p>\n<p>$\\LARGE \\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial}{\\partial \\theta_i} log \\pi_{\\theta}(a_n\\vert s_n) \\frac{\\partial}{\\partial \\theta_j} log \\pi_{\\theta}(a_n\\vert s_n)$</p>\n<p>$\\ \\ \\ \\ \\ \\ \\ \\ \\downarrow$</p>\n<p>$\\LARGE \\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_j} D_{KL}(\\pi_{\\theta_{old} }(\\cdot \\vert  s_n) \\vert \\vert  \\pi_{\\theta}(\\cdot \\vert  s_n))$</p>\n<p><strong>TODO KL divergence 미분과정</strong></p>\n<p><strong>\\TODO # 왜 KL divergence의 헤시안을 계산할때 covariance matrix 얘기가 나왔을까?</strong></p>\n<p><a href=\"https://timvieira.github.io/blog/post/2014/10/06/kl-divergence-as-an-objective-function/\" target=\"_blank\" rel=\"noopener\">https://timvieira.github.io/blog/post/2014/10/06/kl-divergence-as-an-objective-function/</a></p>\n<p>이 analytic estimator는 헤시안이나 trajectories의 모든 그라디언트를 저장하지 않아도 되기 때문에 대규모 환경에서 계산 이점이 있다.</p>\n<p><strong>TODO Appendix C</strong></p>\n<ul>\n<li><p>3장에서 surrogate objective $M(\\pi)$ 과 KL divergence penalty를 활용한 최적화를 증명했으나 <br><br>$C$가 크면 엄청나게 step size가 작아지게 된다. 그래서 $C$를 감소하려하는데, 강건하게 정하기가 어려워서 <br><br>penalty대신, 강한 제약조건으로 KL divergence의 bound $\\delta $ 를 사용했다. -&gt; (Trust Region)</p>\n</li>\n<li><p>$\\large D_{KL}^{max}(\\theta_{old}, \\theta)$ 제약조건은 추정이나 최적화를 하기 어려운 부분이 있어서 <br><br>대신에 $\\large \\bar{D}<em>{KL}(\\theta</em>{old}, \\theta)$ 을 제약했다.</p>\n</li>\n<li><p>본 논문은 advantage function의 추정에러를 무시한다. Kakade&amp;Langford(2002) 에서도 이런 에러를 고려했고 상황도 비슷하지만 <br><br>단순함을 위해 생략했다. (매우 작은 차이라?)</p>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"6-Practical-Algorithm-1\"><a href=\"#6-Practical-Algorithm-1\" class=\"headerlink\" title=\"6. Practical Algorithm\"></a>6. Practical Algorithm</h2><h3 id=\"Appendix-C-Efficiently-Solving-the-Trust-Region-Constrained-Optimization-Problem\"><a href=\"#Appendix-C-Efficiently-Solving-the-Trust-Region-Constrained-Optimization-Problem\" class=\"headerlink\" title=\"Appendix C. Efficiently Solving the Trust-Region Constrained Optimization Problem\"></a>Appendix C. Efficiently Solving the Trust-Region Constrained Optimization Problem</h3><p>풀어야 하는 식</p>\n<p>$\\LARGE maximize \\ L(\\theta) \\  subject \\ to \\ \\bar{D}<em>{KL}(\\theta</em>{old}, \\theta) \\leq \\delta$</p>\n<p>크게 두가지 과정으로 진행</p>\n<p>1) 탐색 방향을 계산, 목적함수의 선형근사 와 제약식의 2차 근사</p>\n<p>2) 계산한 방향으로 line search 수행, 비선형 제약식을 만족시키면서 비선형 목적함수를 개선</p>\n<p>탐색 방향은 $\\large Ax = g$ 수식을 근사적으로 풀면 계산된다. 여기서 $\\large A$ 는 Fisher Information Matrix이고,</p>\n<p>KL divergence 제약식인 $\\large \\bar{D}<em>{KL} (\\theta</em>{old}, \\theta) \\approx \\frac{1}{2}(\\theta - \\theta_{old})^T A(\\theta - \\theta_{old})$ 의 2차 근사이다.</p>\n<p>여기서 $\\large A_{ij} = \\frac{\\partial}{\\partial \\theta_i} \\frac{\\partial}{\\partial \\theta_j} \\bar{D}<em>{KL}(\\theta</em>{old}, \\theta)$</p>\n<p>대규모 문제에서 $A$ 혹은 $A^{-1}$ 을 계산하는 것은 엄청난 계산량이 필요.</p>\n<p>하지만 Conjugate Gradient Algorithm이 전체 $A$행렬(FIM)을 계산하지 않아도 $\\large Ax=g$ 를 근사적으로 해결할 수 있게 해준다.</p>\n<p>탐색 방향 $\\large s \\approx A^{-1}g$ 을 계산하면서, 최대스텝길이(step size) $\\large \\beta$를 계산할 필요가 있다.</p>\n<p>이 계산을 위해서 $\\large \\delta = \\bar{D}_{KL} \\approx \\frac{1}{2}(\\beta s)^T A(\\beta s) = \\frac{1}{2}\\beta^2s^TAs$ 이고</p>\n<p>여기서 $\\large \\beta = \\sqrt{2\\delta / s^T As}$ , 이때 $\\large \\delta$는 KL divergence의 boundary tgerm 이다.</p>\n<p>$\\large s^T As$ 텀은 헤시안 - 벡터 곱으로 계산될 수 있고, CG의 과정에서 계산된다.</p>\n<p>마지막으로, surrogate objective와 KL divergence 제약식을 만족하는 개선을 위해 line search 를 사용할 것이며, surrogate objective와 KL divergence 제약식은 둘 다 $\\large \\theta$ 에 대해 비선형이다.</p>\n<p>목적함수인 $\\large L_{\\theta_{old} }(\\theta) - \\chi [\\bar{D}<em>{KL}(\\theta</em>{old}, \\theta) \\leq \\delta] = 0$ 0이 맞으면 True고 무한으로 발산하면 False</p>\n<p>위에서 계산한 $\\large \\beta$ 의 최대 값부터 시작해서 목적함수가 개선될때까지 점점 줄여갈텐데, line search가 없었다면</p>\n<p>이 알고리즘은 매우 큰 스텝으로 계산될 것이고 성능의 심각한 저하가 발생될 것이다.</p>\n<h3 id=\"C-1-Computing-the-Fisher-Vector-Product\"><a href=\"#C-1-Computing-the-Fisher-Vector-Product\" class=\"headerlink\" title=\"C.1 Computing the Fisher-Vector Product\"></a>C.1 Computing the Fisher-Vector Product</h3><p>임의의 벡터와 평균한 Fisher Information Matrix를 어떻게 계산할지 서술하려한다.</p>\n<p>이 행렬-벡터 곱은 CG를 수행가능하게 한다. 파라미터화 한 policy가 입력 $\\large x$를 분포 파라미터 벡터인 $\\large \\mu_{\\theta}(x)$ 로 사상시킨다고 할때, $\\large \\pi(a \\vert x)$.</p>\n<p>주어진 입력 $\\large x$ 의 KL divergence는 아래 식과 같이 쓸 수 있다.</p>\n<p>$\\LARGE D_{KL} (\\pi_{\\theta_{old} }(\\cdot \\vert x)  \\ \\vert\\vert \\ \\pi_{\\theta}(\\cdot \\vert x)) = kl(\\mu_{\\theta}(x), \\mu_{old})$</p>\n<p>여기서 kl 은 두 mean 파라미터 벡터에 대응하는 분포 사이의 KL divergence이다. kl 을 $\\large \\theta$에 대해 두번 미분하면</p>\n<p>$\\LARGE \\frac{\\partial \\mu_{a}(x)}{\\partial \\theta_i} \\frac{\\partial \\mu_b(x)}{\\partial \\theta_j} kl^{‘’}<em>{ab}(\\mu</em>{\\theta}(x), \\mu_{old}) + \\frac{\\partial^2 \\mu_a(x)}{\\partial \\theta_i \\partial \\theta_j}kl^{‘}<em>a(\\mu</em>{\\theta}(x), \\mu_{old})$</p>\n<p>여기서 두번째 term은 0이 된다. (두번미분하면 사라짐?)</p>\n<p>$\\LARGE J^T M J$</p>\n<h2 id=\"7\"><a href=\"#7\" class=\"headerlink\" title=\"7\"></a>7</h2><h2 id=\"7-Connections-with-Prior-Work-1\"><a href=\"#7-Connections-with-Prior-Work-1\" class=\"headerlink\" title=\"7. Connections with Prior Work - (1)\"></a>7. Connections with Prior Work - (1)</h2><p>NPG는 L의 선형 근사와 $\\large \\bar D_{KL}$ 제약식을 2차근사 하는 아래의 식의 special case.</p>\n<p><strong>equation 17. </strong></p>\n<p>$\\LARGE L_{\\pi}(\\tilde\\pi) = \\eta(\\pi) + \\underset{s}{\\sum}\\rho_{\\pi}(s) \\underset{a}{\\sum}\\tilde\\pi(a\\vert s)A_\\pi(s,a) $ - eq.(3)</p>\n<p>$\\LARGE \\underset{\\theta}{maximize} \\ [\\triangledown_{\\theta}L_{\\theta_{old} }(\\theta) \\ \\rvert_{\\theta=\\theta_{old} } \\cdot (\\theta - \\theta_{old})]$</p>\n<p>$\\LARGE subject \\ to \\ \\frac{1}{2}(\\theta_{old} - \\theta)^{T} A(\\theta_{old})(\\theta_{old}-\\theta) \\leq \\delta$</p>\n<p>$\\LARGE where \\ A(\\theta_{old})_{ij} = $</p>\n<p>$\\huge \\frac{\\partial}{\\partial \\theta_{i} } \\frac{\\partial}{\\partial \\theta_{j} }<br>E_{s \\sim \\rho_{\\pi} }[D_{KL}(\\pi(\\cdot\\rvert s, \\theta_{old})  \\rvert\\rvert \\pi(\\cdot \\rvert s, \\theta))] \\rvert_{\\theta=\\theta_{old} }$</p>\n<h2 id=\"7-Connections-with-Prior-Work-2\"><a href=\"#7-Connections-with-Prior-Work-2\" class=\"headerlink\" title=\"7. Connections with Prior Work - (2)\"></a>7. Connections with Prior Work - (2)</h2><p>업데이트 식</p>\n<p>$\\LARGE \\theta_{new} = \\theta_{old} +  \\color{Red}{\\frac{1}{\\lambda} } A(\\theta_{old})^{-1}\\triangledown_{\\theta}L(\\theta) \\rvert_{\\theta=\\theta_{old} }$</p>\n<p>여기서 step size인 $\\LARGE \\frac{1}{\\lambda}$ 일반적으로 알고리즘의 파라미터로 취급된다.</p>\n<p>이 부분에서 TRPO는 각 업데이트 마다 제약식을 강제한다는 점이 다르다.</p>\n<p>좀 사소한 차이처럼 보이지만, 큰 차원을 다루는 실험에서 알고리즘의 성능을 크게 향상시켰다.</p>\n<p>또한 $\\LARGE \\mathcal{l}^2$ 제약식(혹은 페널티) 를 사용한 표준 Policy Gradient 업데이트 식을 얻었다.</p>\n<p><strong>Equation 18</strong></p>\n<p>$\\LARGE \\underset{\\theta}{maximize}[\\triangledown_{\\theta} L_{\\theta_{old} }(\\theta) \\rvert_{\\theta=\\theta_{old} } \\cdot (\\theta - \\theta_{old})]$</p>\n<p>$\\LARGE subject \\ to \\ \\frac{1}{2} \\vert\\vert \\theta - \\theta_{old} \\vert\\vert^2 \\leq \\delta$</p>\n<p>비제약 문제인 $\\underset{\\pi}{maximize} \\ L_{\\pi_{old} }(\\pi)$을</p>\n<p>$\\LARGE L_{\\pi}(\\tilde\\pi) = \\eta(\\pi) + \\underset{s}{\\sum}\\rho_{\\pi}(s) \\underset{a}{\\sum}\\tilde\\pi(a\\vert s)A_\\pi(s,a) $  - eq.(3)</p>\n<p>을 푸는 것으로 Policy Iteration update를 행한다.</p>\n<h2 id=\"7-Connections-with-Prior-Work-3\"><a href=\"#7-Connections-with-Prior-Work-3\" class=\"headerlink\" title=\"7. Connections with Prior Work - (3)\"></a>7. Connections with Prior Work - (3)</h2><p>$\\LARGE \\underset{\\pi}{maximize} \\ L_{\\pi_{old} }(\\pi)$  식과 유사하게 업데이트를 수행하는 다양한 방법론들이 있다.</p>\n<p><a href=\"https://pdfs.semanticscholar.org/ff47/526838ce85d77a50197a0c5f6ee5095156aa.pdf\" target=\"_blank\" rel=\"noopener\">REPS(Relative Entropy Policy Search)</a> 는</p>\n<p><a href=\"http://rll.berkeley.edu/deeprlcourse/f17docs/lecture_3_rl_intro.pdf\" target=\"_blank\" rel=\"noopener\">state-action marginal</a>인 $p(s,a)$ 를 제약하고 TRPO는 조건부인 $p(a\\vert s)$ 를 제약한다.</p>\n<p>REPS constraint \\vert  TRPO constraint<br>:————–:\\vert :—————:\\vert<br><img src=\"https://i.imgur.com/U5Xgci1.png\" alt=\"\"> \\vert  <img src=\"https://i.imgur.com/12RPFh3.png\" alt=\"\"></p>\n<p>REPS랑은 달리, TRPO는 내부 루프 계산에서 비선형 최적화를 하지않는다.(REPS는 BFGS수행)</p>\n<p><strong># REPS algorithm</strong></p>\n<p><img src=\"https://i.imgur.com/F2m1yMG.png\" alt=\"\"></p>\n<p><br></p>\n<h2 id=\"7-Connections-with-Prior-Work-4\"><a href=\"#7-Connections-with-Prior-Work-4\" class=\"headerlink\" title=\"7. Connections with Prior Work - (4)\"></a>7. Connections with Prior Work - (4)</h2><p><a href=\"https://people.eecs.berkeley.edu/~svlevine/papers/mfcgps.pdf\" target=\"_blank\" rel=\"noopener\">Levine and Abbeel(2014)</a> 또한 KL Divergence 제약식을 사용했다.</p>\n<p>하지만 policy가 추정한 dynamics model이 유효한 영역에서 벗어나지않게 하는데에 목적이 있는 반면,</p>\n<p>TRPO는 system dynamics를 추정하지 않는다.</p>\n<p><a href=\"https://pdfs.semanticscholar.org/f2c5/a91f35004aebc120360679f76488ac2613ee.pdf\" target=\"_blank\" rel=\"noopener\">Pirotta et al. (2013)</a> 또한 <a href=\"https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/KakadeLangford-icml2002.pdf\" target=\"_blank\" rel=\"noopener\">Kakade &amp; Langford</a>의 결과를 일반화했으나 이들은 TRPO와는 다른 알고리즘을 도출했다.</p>\n<p><br></p>\n<h2 id=\"8-Experiments\"><a href=\"#8-Experiments\" class=\"headerlink\" title=\"8. Experiments\"></a>8. Experiments</h2><ul>\n<li><p>3 가지 궁금증을 위한 실험을 설계함</p>\n<ol>\n<li><p>Single-path 와 vine 의 성능면에서 특징을 알고싶음.</p>\n</li>\n<li><p>Fixed penalty coefficient(NPG) 보다 fixed KL divergence를 사용하는 것(TRPO)으로<br>변경한 것이 얼마나 뚜렷한 차이를 보일지, 성능 면에서 어떤 영향을 미치는지 알고싶음</p>\n</li>\n<li><p>TRPO가 큰 스케일의 문제를 해결할 수 있는지, 기존에 연구/적용되어왔던 방법들과<br>성능, 계산시간, 샘플 복잡도 면에서 비교하고 싶음</p>\n</li>\n</ol>\n</li>\n</ul>\n<ul>\n<li><p>1, 2의 궁금증에 대해서 실험 환경에 single path, vine을 비롯한 여러 사전방법들을 함께 실험.</p>\n</li>\n<li><p>3에 대해선, robotic locomotion과 Atari game에 TRPO를 실험함.</p>\n</li>\n</ul>\n<h2 id=\"8-1\"><a href=\"#8-1\" class=\"headerlink\" title=\"8.1\"></a>8.1</h2><h3 id=\"8-1-Simulated-Robotic-Locomotion\"><a href=\"#8-1-Simulated-Robotic-Locomotion\" class=\"headerlink\" title=\"8.1 Simulated Robotic Locomotion\"></a>8.1 Simulated Robotic Locomotion</h3><ul>\n<li>MuJoCo 시뮬레이터를 활용해 로봇 locomotion 실험 진행</li>\n<li>Robot State: positions and velocities, joint torques</li>\n</ul>\n<p><img src=\"https://i.imgur.com/bs5ATS3.png\" alt=\"\"></p>\n<ul>\n<li><p>수영, 점프, 걷기 행동을 학습시키고자 함.</p>\n<ul>\n<li><p>수영: 10 dimensional state space , reward: $\\LARGE r(x,u)=v_x - 10^{-5}\\vert \\vert u\\vert \\vert ^2$, quadratic penalty</p>\n</li>\n<li><p>점프: 12 dimensional state space, episode의 끝은 높이와 각도 threshol로 점프를 판단, non-terminal state에 대해 +1 보상 추가</p>\n</li>\n<li><p>걷기: 18 dimensional state space, 뜀뛰는 듯한 보법(점프에이전트)보다 부드러운 보법을 유도하기위해 페널티를 추가했다함</p>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"8-1-Simulated-Robotic-Locomotion-1\"><a href=\"#8-1-Simulated-Robotic-Locomotion-1\" class=\"headerlink\" title=\"8.1 Simulated Robotic Locomotion\"></a>8.1 Simulated Robotic Locomotion</h3><h4 id=\"Detailed-Experiment-Setup-and-used-parameters-used-network-model\"><a href=\"#Detailed-Experiment-Setup-and-used-parameters-used-network-model\" class=\"headerlink\" title=\"Detailed Experiment Setup and used parameters, used network model\"></a>Detailed Experiment Setup and used parameters, used network model</h4><p>used parameter \\vert  network model<br>:—-:\\vert :—-:<br><img src=\"https://i.imgur.com/zgnsbw6.png\" alt=\"\">\\vert  <img src=\"https://i.imgur.com/FqdWC53.png\" alt=\"\"></p>\n<h3 id=\"8-1-Simulated-Robotic-Locomotion-2\"><a href=\"#8-1-Simulated-Robotic-Locomotion-2\" class=\"headerlink\" title=\"8.1 Simulated Robotic Locomotion\"></a>8.1 Simulated Robotic Locomotion</h3><p>equation (12) : $\\huge maximize L_{\\theta_{old}(\\theta)} \\ , \\ \\ subject \\ to \\bar{D}<em>{KL}^{\\rho</em>{\\theta_{old} }}(\\theta_{old}, \\theta) \\leq \\delta$</p>\n<p>이 실험에서 $\\LARGE \\delta = 0.01$</p>\n<ul>\n<li><p>비교에 사용된 모델들</p>\n<ul>\n<li>TRPO Single-path</li>\n<li>TRPO vine</li>\n<li>CEM(cross-entropy method)</li>\n<li>CMA(covariance matrix adaptation)</li>\n<li>NPG (Lagrange Multiplier penalty coefficien를 사용하는 것이 차이점)</li>\n<li>Empirical FIM</li>\n<li>maximum KL(cartpole)</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"8-1-Simulated-Robotic-Locomotion-3\"><a href=\"#8-1-Simulated-Robotic-Locomotion-3\" class=\"headerlink\" title=\"8.1 Simulated Robotic Locomotion\"></a>8.1 Simulated Robotic Locomotion</h3><p><img src=\"https://i.imgur.com/wSqolvS.png\" alt=\"\"></p>\n<ul>\n<li><p>제안한 single path, vine 모두 task를 잘 학습하는 것을 확인할 수 있음</p>\n</li>\n<li><p>페널티를 고정하는 것보다, KL divergenc를 제약하는 것이 step size를 선택하는 부분에서 Robust 하다.</p>\n</li>\n<li><p>CEM/CMA는 derivative free 알고리즘으로, 샘플 복잡도가 네트워크 파라미터\u000b수 만큼 확장되어 high dimension 문제에서는 속도와 성능이 약한 모습을 보인다.</p>\n</li>\n<li><p>maxKL은 제안방법보다 느린 학습</p>\n</li>\n</ul>\n<h3 id=\"8-1-Simulated-Robotic-Locomotion-4\"><a href=\"#8-1-Simulated-Robotic-Locomotion-4\" class=\"headerlink\" title=\"8.1 Simulated Robotic Locomotion\"></a>8.1 Simulated Robotic Locomotion</h3><p>TRPO video link: <a href=\"https://www.youtube.com/watch?v=jeid0wIrSn4\" target=\"_blank\" rel=\"noopener\">https://www.youtube.com/watch?v=jeid0wIrSn4</a></p>\n<ul>\n<li><p>사전지식이 적은 상태에서 여러 액션을 학습하는 실험으로 TRPO의 성능을 보임</p>\n</li>\n<li><p>지금까지의 균형이나 걸음 같은 개념을 명시적으로 인코딩한 robotic locomotion에서의 사전 연구들과 차이를 보임</p>\n</li>\n</ul>\n<h2 id=\"8-2\"><a href=\"#8-2\" class=\"headerlink\" title=\"8.2\"></a>8.2</h2><h3 id=\"8-2-Playing-Games-from-Images\"><a href=\"#8-2-Playing-Games-from-Images\" class=\"headerlink\" title=\"8.2 Playing Games from Images\"></a>8.2 Playing Games from Images</h3><ul>\n<li>부분관찰가능하며 복잡한 observation인 환경에서 TRPO를 평가하기위해 raw image를 입력으로 하는 아타리게임 환경에서 실험</li>\n</ul>\n<ul>\n<li>Challenging point :<ul>\n<li>High dimensional space</li>\n<li>delayed reward</li>\n<li>non-stationary image (Enduro는 배경 이미지가 바뀌고, 반전처리가 됨)</li>\n<li>complex action sequence (Q*bert는 21개의 다른 발판에서 점프 해야함)</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li>ALE(Arcade Learning Environment) 에서 테스트</li>\n<li>DQN과 전처리 방식은 동일<br>  210 x 160 pixel -&gt; gray-scaling &amp; down-sampling -&gt; 110 x 84  -&gt; crop -&gt; 84 x 84</li>\n</ul>\n<h3 id=\"8-2-Playing-Games-from-Images-1\"><a href=\"#8-2-Playing-Games-from-Images-1\" class=\"headerlink\" title=\"8.2 Playing Games from Images\"></a>8.2 Playing Games from Images</h3><ul>\n<li>2 convolution layer ( 16 channels) , stride 2, FC layer with 20 units</li>\n</ul>\n<p>parameter setup \\vert  network model<br>:————–:\\vert :————-:<br><img src=\"https://i.imgur.com/NJBC69d.png\" alt=\"\"> \\vert  <img src=\"https://i.imgur.com/wTe1OEW.png\" alt=\"\"></p>\n<h3 id=\"8-2-Playing-Games-from-Images-2\"><a href=\"#8-2-Playing-Games-from-Images-2\" class=\"headerlink\" title=\"8.2 Playing Games from Images\"></a>8.2 Playing Games from Images</h3><p><img src=\"https://i.imgur.com/j22VtNQ.png\" alt=\"\"></p>\n<ul>\n<li><p>30시간 동안 500 iteration 의 학습</p>\n</li>\n<li><p>DQN과 MCST를 사용한 모델(UCC-I), 사람이 플레이한 것(Human) 과의 비교</p>\n</li>\n<li><p>이전 방법들에 비해 압도적인 점수를 기록하진 못했으나, 다양한 게임에서 적절한 게임 기록.</p>\n</li>\n<li><p>이전 방법들과 다른 점은, 특정 task에 초점을 맞춰 설계하지않았는데<br><br><br>robotics나 game playing Task에도 적절한 성능을 내는 점에서 <strong>TRPO의 일반화 성능</strong>을 보임</p>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"9-Discussion\"><a href=\"#9-Discussion\" class=\"headerlink\" title=\"9. Discussion\"></a>9. Discussion</h2><ul>\n<li><p>Trust Region Policy Optimization 을 제안</p>\n</li>\n<li><p>KL Divergence 페널티로 $J(\\pi)$를 최적화하는 알고리즘이 monotonically improve 함을 증명</p>\n</li>\n<li><p>Locomotion 도메인에서 여러 행동(수영, 걷기, 점프)을 제어하는 controlle를 학습</p>\n</li>\n<li><p>TRPO가 향후 연구의 도약점이 되었으면 좋겠음</p>\n</li>\n<li><p>Robotics와 게임 실험을 결합해, 시각정보와 센서데이터를 사용하는 로봇제어 정책을 학습시키는 가능성을 봄</p>\n</li>\n<li><p>샘플의 복잡도를 상당히 줄일수있어 실상황 적용가능성을 봄</p>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"END\"><a href=\"#END\" class=\"headerlink\" title=\"END\"></a>END</h1>"}],"PostAsset":[],"PostCategory":[{"post_id":"cjiinnj6n0002j28aigqpmv52","category_id":"cjiinnj6o0003j28aoi038xjq","_id":"cjiinnj6v000cj28a4b9xquku"},{"post_id":"cjiinnj6q0005j28a4kpaxi6x","category_id":"cjiinnj6o0003j28aoi038xjq","_id":"cjiinnj6x000fj28ahxrt77uz"},{"post_id":"cjjgzyg440000xo155e3g8dvy","category_id":"cjiinnj6o0003j28aoi038xjq","_id":"cjjgzyg4d0004xo15qhfekcw3"},{"post_id":"cjjgzyg4a0001xo15qjhkwruc","category_id":"cjiinnj6o0003j28aoi038xjq","_id":"cjjgzyg4e0006xo15arbwx7bl"},{"post_id":"cjjgzyg4h0008xo15m13f942j","category_id":"cjiinnj6o0003j28aoi038xjq","_id":"cjjgzyg4k000bxo155hmgt28u"},{"post_id":"cjjgzyg4q000cxo15a2yfnmeh","category_id":"cjiinnj6o0003j28aoi038xjq","_id":"cjjgzyg4r000fxo15rmjgvhsd"},{"post_id":"cjjl633g90000hz15gd4bxk4d","category_id":"cjiinnj6o0003j28aoi038xjq","_id":"cjjl633gf0003hz150gkwvbrp"}],"PostTag":[{"post_id":"cjiinnj6n0002j28aigqpmv52","tag_id":"cjiinnj6q0004j28ajv2qofj6","_id":"cjiinnj6x000hj28a6oe113d1"},{"post_id":"cjiinnj6n0002j28aigqpmv52","tag_id":"cjiinnj6r0007j28a9ifjaq6c","_id":"cjiinnj6x000ij28ae2s24zk6"},{"post_id":"cjiinnj6q0005j28a4kpaxi6x","tag_id":"cjiinnj6q0004j28ajv2qofj6","_id":"cjiinnj6y000kj28a6c947dl2"},{"post_id":"cjiinnj6q0005j28a4kpaxi6x","tag_id":"cjiinnj6r0007j28a9ifjaq6c","_id":"cjiinnj6y000lj28adzfrq45d"},{"post_id":"cjjgzyg440000xo155e3g8dvy","tag_id":"cjiinnj6q0004j28ajv2qofj6","_id":"cjjgzyg4d0002xo159z9739xo"},{"post_id":"cjjgzyg440000xo155e3g8dvy","tag_id":"cjiinnj6r0007j28a9ifjaq6c","_id":"cjjgzyg4d0003xo15dky9nobj"},{"post_id":"cjjgzyg4a0001xo15qjhkwruc","tag_id":"cjiinnj6q0004j28ajv2qofj6","_id":"cjjgzyg4d0005xo15lberci4o"},{"post_id":"cjjgzyg4a0001xo15qjhkwruc","tag_id":"cjiinnj6r0007j28a9ifjaq6c","_id":"cjjgzyg4f0007xo15gtfyxdmt"},{"post_id":"cjjgzyg4h0008xo15m13f942j","tag_id":"cjiinnj6q0004j28ajv2qofj6","_id":"cjjgzyg4k0009xo15c87l0wgp"},{"post_id":"cjjgzyg4h0008xo15m13f942j","tag_id":"cjiinnj6r0007j28a9ifjaq6c","_id":"cjjgzyg4k000axo15zbyevg11"},{"post_id":"cjjgzyg4q000cxo15a2yfnmeh","tag_id":"cjiinnj6q0004j28ajv2qofj6","_id":"cjjgzyg4r000dxo15tk85qwq2"},{"post_id":"cjjgzyg4q000cxo15a2yfnmeh","tag_id":"cjiinnj6r0007j28a9ifjaq6c","_id":"cjjgzyg4r000exo15ah7nu7a8"},{"post_id":"cjjl633g90000hz15gd4bxk4d","tag_id":"cjiinnj6q0004j28ajv2qofj6","_id":"cjjl633ge0001hz15pyj3kpsd"},{"post_id":"cjjl633g90000hz15gd4bxk4d","tag_id":"cjiinnj6r0007j28a9ifjaq6c","_id":"cjjl633gf0002hz15hki7dj7v"}],"Tag":[{"name":"프로젝트","_id":"cjiinnj6q0004j28ajv2qofj6"},{"name":"피지여행","_id":"cjiinnj6r0007j28a9ifjaq6c"}]}}