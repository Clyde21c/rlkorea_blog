{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/img/Exp_OctopusArm.png","path":"img/Exp_OctopusArm.png","modified":0,"renderable":0},{"_id":"source/img/Exp_ContinuousBandit.png","path":"img/Exp_ContinuousBandit.png","modified":0,"renderable":0},{"_id":"source/img/Ref_Degris2012b_Theorem1.png","path":"img/Ref_Degris2012b_Theorem1.png","modified":0,"renderable":0},{"_id":"source/img/Exp_OctopusArm_Ref.png","path":"img/Exp_OctopusArm_Ref.png","modified":0,"renderable":0},{"_id":"source/img/Exp_ContinuousRL.png","path":"img/Exp_ContinuousRL.png","modified":0,"renderable":0},{"_id":"source/img/Ref_Degris2012b_offpolicygradient.png","path":"img/Ref_Degris2012b_offpolicygradient.png","modified":0,"renderable":0},{"_id":"themes/clean-blog/source/css/article.styl","path":"css/article.styl","modified":0,"renderable":1},{"_id":"themes/clean-blog/source/css/base.styl","path":"css/base.styl","modified":0,"renderable":1},{"_id":"themes/clean-blog/source/css/mixins.styl","path":"css/mixins.styl","modified":0,"renderable":1},{"_id":"themes/clean-blog/source/css/style.styl","path":"css/style.styl","modified":0,"renderable":1},{"_id":"themes/clean-blog/source/css/variables.styl","path":"css/variables.styl","modified":0,"renderable":1},{"_id":"themes/clean-blog/source/img/about-bg.jpg","path":"img/about-bg.jpg","modified":0,"renderable":1},{"_id":"themes/clean-blog/source/img/favicon.ico","path":"img/favicon.ico","modified":0,"renderable":1},{"_id":"themes/clean-blog/source/img/home-bg.jpg","path":"img/home-bg.jpg","modified":0,"renderable":1},{"_id":"themes/clean-blog/source/img/contact-bg.jpg","path":"img/contact-bg.jpg","modified":0,"renderable":1}],"Cache":[{"_id":"source/.DS_Store","hash":"f1afa44a79711985d0665f9b2538a1ed5530da84","modified":1529138775214},{"_id":"themes/clean-blog/.DS_Store","hash":"18b40c0a074fb756ef76fd437c21c2205a08a585","modified":1529045736390},{"_id":"themes/clean-blog/LICENSE","hash":"8726b416df4f067cff579e859f05c4b594b8be09","modified":1529043522273},{"_id":"themes/clean-blog/README.md","hash":"861dd2f959ab75d121226f4f3e2f61f4bc95fddb","modified":1529043522273},{"_id":"themes/clean-blog/_config.yml","hash":"c0ee3fcf1841410d07402e8d6e50e8847f31ae4b","modified":1529047469272},{"_id":"source/_posts/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1529228886819},{"_id":"source/_posts/sutton-pg.md","hash":"17087672254f31f04bfa99d6447dc30a9aa16e8b","modified":1529150062931},{"_id":"source/_posts/피지여행-소개.md","hash":"a3b89bda4864cfb84bdb759847d6531e9a39c491","modified":1529229006186},{"_id":"source/_posts/2018-06-15-npg.md","hash":"ad0e79383db0552d94078ca181c5e7c764b81f57","modified":1529150062936},{"_id":"source/_posts/dpg.md","hash":"9513ae8995eda0086e49dce88cac1669b3adc636","modified":1529158194593},{"_id":"source/img/Exp_OctopusArm.png","hash":"f0ae59d0c2a6600ef961fd0b2ea8903dcfdbd4d9","modified":1529117349098},{"_id":"source/img/Exp_ContinuousBandit.png","hash":"72eddde3b296070075706688038f1267bfd04d31","modified":1529113799292},{"_id":"source/img/Ref_Degris2012b_Theorem1.png","hash":"154f42239538c3dd0508e815014d84685e18f89b","modified":1529106775328},{"_id":"themes/clean-blog/languages/default.yml","hash":"97326c9e6518d9f379778178b3b8f9a58434725d","modified":1529043522274},{"_id":"themes/clean-blog/languages/en.yml","hash":"97326c9e6518d9f379778178b3b8f9a58434725d","modified":1529043522274},{"_id":"themes/clean-blog/languages/de.yml","hash":"424a9c1e6ab69334d7873f6574da02ca960aa572","modified":1529043522273},{"_id":"themes/clean-blog/languages/no.yml","hash":"8ca475a3b4f8efe6603030f0013aae39668230e1","modified":1529043522274},{"_id":"themes/clean-blog/languages/es.yml","hash":"cb4eeca0ed3768a77e0cd216300f2b2549628b1b","modified":1529043522274},{"_id":"themes/clean-blog/languages/fr.yml","hash":"e9e6f7cb362ebb7997f11027498a2748fe3bac95","modified":1529043522274},{"_id":"themes/clean-blog/languages/pt.yml","hash":"1d0c3689eb32fe13f37f8f6f303af7624ebfbaf0","modified":1529043522275},{"_id":"themes/clean-blog/languages/pl.yml","hash":"de7eb5850ae65ba7638e907c805fea90617a988c","modified":1529043522274},{"_id":"themes/clean-blog/languages/ru.yml","hash":"42df7afeb7a35dc46d272b7f4fb880a9d9ebcaa5","modified":1529043522275},{"_id":"themes/clean-blog/languages/zh-TW.yml","hash":"9acac6cc4f8002c3fa53ff69fb8cf66c915bd016","modified":1529043522275},{"_id":"themes/clean-blog/languages/zh-CN.yml","hash":"7bfcb0b8e97d7e5edcfca8ab26d55d9da2573c1c","modified":1529043522275},{"_id":"themes/clean-blog/layout/index.ejs","hash":"425da730d537805046040c5070571d9e739d4b19","modified":1529229786757},{"_id":"themes/clean-blog/layout/archive.ejs","hash":"f2ef73afc3d275333329bb30b9369b82e119da76","modified":1529043522279},{"_id":"themes/clean-blog/layout/layout.ejs","hash":"da2f9018047924ddaf376aee5996c7ddc06cebc1","modified":1529043522279},{"_id":"themes/clean-blog/layout/page.ejs","hash":"591af587e1aae962950de7e79bd25c1f060c69ac","modified":1529043522279},{"_id":"themes/clean-blog/source/.DS_Store","hash":"84f35e390633eadc3c78584a28f5f5f8ce7f43a5","modified":1529045731387},{"_id":"themes/clean-blog/layout/post.ejs","hash":"38382e9bbeb6b8d2eafbd53fff2984111f524c1a","modified":1529043522279},{"_id":"source/img/Exp_OctopusArm_Ref.png","hash":"ebf69a16fc09e447442808f8cc46d770bed0cf10","modified":1529116470075},{"_id":"source/img/Exp_ContinuousRL.png","hash":"85b9ebfb164631804d2fb6be0164e49cd1db746b","modified":1529115804602},{"_id":"source/img/Ref_Degris2012b_offpolicygradient.png","hash":"46f0f17392ff40927579899fd10bdaf6bf562ebd","modified":1529106760142},{"_id":"themes/clean-blog/layout/_partial/article-categories.ejs","hash":"5a0bf5a20f670621d8013c9b9d7976b45c8aa80f","modified":1529043522276},{"_id":"themes/clean-blog/layout/_partial/article-archive.ejs","hash":"3d8d98c6545b8332a6d6ed4f8b00327df03ea945","modified":1529043522276},{"_id":"themes/clean-blog/layout/_partial/after-footer.ejs","hash":"a6ad079ded70024d35264fae798ae73bdbcb0ae6","modified":1529149982621},{"_id":"themes/clean-blog/layout/_partial/article-full.ejs","hash":"6cf24bd7785d57cb7198b3f1ed4fa6a86c84a502","modified":1529043522276},{"_id":"themes/clean-blog/layout/_partial/article-index.ejs","hash":"e433df4e245e2d4c628052c6e59966563542d94d","modified":1529043522276},{"_id":"themes/clean-blog/layout/_partial/article-tags.ejs","hash":"6136434be09056c1466149cecb3cc2e80d107999","modified":1529043522277},{"_id":"themes/clean-blog/layout/_partial/comments.ejs","hash":"3fedb75436439d1d6979b7e4d20d48a593e12be4","modified":1529043522277},{"_id":"themes/clean-blog/layout/_partial/footer.ejs","hash":"a92f5168c006193c3d964fd293ad3c38aae69419","modified":1529043522277},{"_id":"themes/clean-blog/layout/_partial/gallery.ejs","hash":"21e4f28909f4a79ff7d9f10bdfef6a8cb11632bf","modified":1529043522277},{"_id":"themes/clean-blog/layout/_partial/google-analytics.ejs","hash":"4e6e8de9becea5a1636a4dcadcf7a10c06e2426e","modified":1529043522277},{"_id":"themes/clean-blog/layout/_partial/head.ejs","hash":"f8ddbced1627704ab35993e8fc6d6e34cc6f2ba9","modified":1529043522278},{"_id":"themes/clean-blog/layout/_partial/menu_origin.ejs","hash":"cfc30e6b1ef9487cff3ce594d403d1e7c4d9cdf4","modified":1529043522278},{"_id":"themes/clean-blog/layout/_partial/pagination.ejs","hash":"557d6bb069a1d48af49ae912994653f44b32a570","modified":1529043522278},{"_id":"themes/clean-blog/layout/_partial/menu.ejs","hash":"bf186aa55623a77e18c6b1748e6af429ca7e7d67","modified":1529230461987},{"_id":"themes/clean-blog/layout/_partial/tag-category-index.ejs","hash":"10cdc1b7866999c714a666557c150d2c79c1fba9","modified":1529043522278},{"_id":"themes/clean-blog/source/css/article.styl","hash":"f5294d7a3d6127fcb287de3ff0c12aebb1766c7b","modified":1529043522280},{"_id":"themes/clean-blog/source/css/base.styl","hash":"29b54c63060bd2d7f5c501d403d9db5a552ad10c","modified":1529043522280},{"_id":"themes/clean-blog/source/css/mixins.styl","hash":"14264bf86b4e3194a3156447f7b7bce2fd0db5bd","modified":1529043522280},{"_id":"themes/clean-blog/source/css/style.styl","hash":"c40dc495a41007d21c59f342ee42b2d31d7b5ff4","modified":1529043522280},{"_id":"themes/clean-blog/source/css/variables.styl","hash":"cd82df5ca8dfbcfec12d833f01adfac00878e835","modified":1529043522280},{"_id":"themes/clean-blog/source/img/about-bg.jpg","hash":"d39126a6456f2bac0169d1779304725f179c9900","modified":1529043522281},{"_id":"themes/clean-blog/source/img/favicon.ico","hash":"3412e0d657aa5a6cfbbfcf4ef398572c24035565","modified":1529045668405},{"_id":"themes/clean-blog/source/img/home-bg.jpg","hash":"990f6f9dd0ecb5348bfcc47305553d58c0d8f326","modified":1529043522283},{"_id":"themes/clean-blog/source/img/contact-bg.jpg","hash":"6af63305c923899017e727b5ca968a2703bc08cf","modified":1529043522282},{"_id":"public/index.html","hash":"6a303b3b36c0317ea4ecbf0606e76cf5f733a5ed","modified":1529230754823},{"_id":"public/archives/index.html","hash":"a39734e057b9a3996a07e7dac059b0c83a0bc39d","modified":1529230754823},{"_id":"public/archives/2018/index.html","hash":"2a29137fa83898602a0eebff7e4486504d11f4ae","modified":1529230754823},{"_id":"public/archives/2018/06/index.html","hash":"ce1c9b5b68728dc93b9a78fa1f20b2dd8f6337f7","modified":1529230754823},{"_id":"public/categories/프로젝트/index.html","hash":"7a5b7a0101c53e840eb90e4e0be1a5479dc65b3f","modified":1529230754823},{"_id":"public/tags/프로젝트/index.html","hash":"5869e579b1484353e49db32c00ddddb5601302db","modified":1529230754823},{"_id":"public/tags/피지여행/index.html","hash":"b7f7a034af3a90ab373d887f41d6d9be51737d2f","modified":1529230754823},{"_id":"public/2018/06/16/dpg/index.html","hash":"7bc394c8fcb2dca3d19acc45af754700fb5389e7","modified":1529230754824},{"_id":"public/2018/06/15/sutton-pg/index.html","hash":"2a84a7e4ea8168f22bebf2b1150355e14e9efea1","modified":1529230754824},{"_id":"public/2018/06/14/2018-06-15-npg/index.html","hash":"63865f511c586719f341629a55a4354cf9e39371","modified":1529230754824},{"_id":"public/2018/06/17/피지여행-소개/index.html","hash":"4444d94a395bd894dc96f4c1968ca4d457493924","modified":1529230754823}],"Category":[{"name":"프로젝트","_id":"cjiinnj6o0003j28aoi038xjq"}],"Data":[],"Page":[],"Post":[{"title":"피지여행 소개","date":"2018-06-17T09:50:06.000Z","_content":"","source":"_posts/피지여행-소개.md","raw":"---\ntitle: 피지여행 소개\ndate: 2018-06-17 18:50:06\ntags:\n---\n","slug":"피지여행-소개","published":1,"updated":"2018-06-17T09:50:06.186Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjiinnj690000j28ai14rugme","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"A Natural Policy Gradient","date":"2018-06-14T04:18:45.000Z","author":"이웅원","subtitle":"피지여행 4번째 논문","_content":"\n# A Natural Policy Gradient [2001]\n\n<img src=\"https://www.dropbox.com/s/it82tfhfmhg9uwp/Screenshot%202018-06-10%2010.58.52.png?dl=1\">\n\n- 논문 저자: Sham Kakade\n- 논문 링크: [https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf](https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf)\n- 함께 보면 좋을 논문: \n\t- [Policy Gradient Methods for\nReinforcement Learning with Function\nApproximation (2000)](hhttps://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf)\n\t- [Natural Gradient Works Efficiently in Learning(1998)](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.452.7280&rep=rep1&type=pdf)\n- 논문을 보는 이유: TRPO와 NPG는 관련이 많기 때문에 TRPO를 더 잘 이해하기 위해 봄\n\n## 1. Abstract\n---\n\n- natural gradient method를 policy gradient에 적용\n- natural gradient는 steepest descent direction을 가짐\n- gradient descent는 parameter를 한 번에 많이 update 할 수 없는 반면, natural gradient는 가장 좋은 action을 고르도록 학습이 됌 (sutton 논문에서와 같이 compatible value function을 사용할 경우 policy iteration에서 policy improvement 1 step의 과정에서)\n- simple MDP와 tetris MDP에서 테스트함. 성능이 많이 향상\n\n## 2. Personal Interpretation and Thinking\n---\n\n(개인생각) 뉴럴넷을 사용할 경우 gradient가 steepest direction이 아닌 경우가 많다. 뉴럴넷의 parameter space가 우리가 보통 생각하는 직선으로 쭉쭉 뻗어있는 Euclidean space가 아니다. 좀 더 일반적으로는 구의 표면과 같이 휘어져있는 공간 즉, 리만 공간(Riemannian space)로 표현할 수 있다. 이와 같은 공간에서는 natural gradient가 steepest direction이 된다는 연구가 이뤄지고 있었다. 강화학습의 policy gradient은 objective function의 gradient를 따라 policy를 업데이트한다. 이 때, policy는 parameterized 되는데 이 경우에도 gradient 대신에 natural gradient가 좋다는 것을 실험해보는 논문이다. \n\ngradient가 non-covariant 해서 생기는 문제는 간단히 말하자면 다음과 같다. policy가 parameterized된 상황에서는 같은 policy라도 다른 parameter를 가질 수 있다. 이 때, steepest direction은 두 경우에 같은 방향을 가리켜야하는데 non-covariant한 경우 그렇지 못하다. 이것은 결국 느린 학습으로 연결이 된다. \n\n논문에서 2차미분 방법론들과 짧게 비교를 한다. 하지만 2차미분을 이용한 다른 방법들과의 비교가 생각보다 없는 점이 아쉽다.(Hessian을 이용한다거나 conjugate gradient method를 이용한다거나). 실험을 통해 FIM이 hessian에 수렴안하는 거라던지 Hessian 방법론이 local maxima 부근에서 상당히 느리다던지의 결과를 보여줬었으면 좋았을 것 같다. \n\n또한 natural gradient 만으로 업데이트하면 policy의 improvement보장이 안될 수 있다. policy의 improvement를 보장하기 위해 line search도 써야하는데 line search를 어떻게 쓰는지에 대한 자세한 언급이 없다. 즉, 자세한 algorithm 설명이 없다.\n\nnatural policy gradient 논문은 natural gradient + policy gradient를 처음 적용했다는데 의의가 있다. 하지만 이 논문이 문제 삼은 gradient는 non-covariant하다라는 문제를 natural gradient를 통해 해결하지 못했다(Experiment를 통해 covariant gradient가 되지 못했다는 것이 보인다). NPG의 뒤를 잇는 논문이 \"covariant policy search\"와 \"natural actor-critic\"에서 covariant하지 못하다는 것을 해결하기 위해 Fisher Information Matrix를 sample 하나 하나에 대해서 구하는 것이 아니라 trajectory 전체에 대해서 구한다. \n\n또한 논문은 pg의 두 가지 세팅 중에 average-reward setting(infinite horizon)에서만 NPG를 다룬다. \"covariant policy search\" 논문에서는 average-reward setting과 start-state setting 모두에 대해서 npg를 적용한다. \n\nnatural gradient + policy gradient를 처음 제시했다는 것은 좋지만 npg 학습의 과정을 자세하게 설명하지 않았고 다른 2차 미분 방법들과 비교를 많이 하지 않은 점이 아쉬운 논문이다.\n\n\n## 3. Introduction\n---\n\n- direct policy gradient method는 future reward의 gradient를 따라 policy를 update함\n- 하지만 gradient descent는 non-covariant\n- 이 논문에서는 covarient gradient를 제시함 = natural gradient\n- natural gradient와 policy iteration의 연관성을 설명하겠음: natural policy gradient is moving toward choosing a greedy optimal action (이런 연결점은 아마도 step-size를 덜 신경쓰고 싶어서 그런게 아닌가 싶다)\n\n논문의 Introduction 부분에 다음 멘트가 있다. 이 글만 봐서는 이해가 안갔는데 Mackay 논문에 좀 더 자세히 나와있다.\n \n<img src=\"https://www.dropbox.com/s/41xhhr7lgfk24a1/Screenshot%202018-06-10%2011.45.18.png?dl=1\">\n\n[Mackay](http://www.inference.org.uk/mackay/ica.pdf)논문에서는 다음과 같이 언급하고 있다. Back-propagation을 사용할 경우에 learning rate를 dimension에 1/n로 사용하면 수렴한다는 것이 증명됐다. 하지만 너무 느리다. \n\n<img src=\"https://www.dropbox.com/s/us9ezc7vxgrkez6/Screenshot%202018-06-10%2011.47.21.png?dl=1\">\n\n## 4. A Natural Gradient\n---\n### 4.1 환경에 대한 설정\n이 논문에서 제시하는 학습 환경은 다음과 같다.\n\n- MDP: tuple $(S, s_0, A, R, P)$\n- $S$: a finite set of states\n- $s_0$: a start state\n- $A$: a finite set of actions\n- $R$: reward function $R: S \\times A -> [0, R_{max}]$\n- $\\pi(a;s, \\theta)$: stochastic policy parameterized by $\\theta$\n- 모든 정책 $\\pi$는 ergodic: stationary distribution $\\rho^{\\pi}$이 잘 정의되어있음\n- 이 논문에서는 sutton의 pg 논문의 두 세팅(start-state formulation, average-reward formulation) 중에 두 번째인 average-reward formulation을 가정 \n- performance or average reward: $\\eta(\\pi)=\\sum_{s,a}\\rho^{\\pi}(s)\\pi(a;s)R(s,a)$\n- state-action value: $Q^{\\pi}(s,a)=E_{\\pi}[\\sum_{t=0}^{\\infty}R(s_t, a_t)-\\eta(\\pi)\\vert s_0=s, a_0=a]$\n- 정책이 $\\theta$로 parameterize되어있으므로 performance는 $\\eta(\\pi_{\\theta})$인데 $\\eta(\\theta)$로 쓸거임\n\n### 4.2 Natural Gradient\n#### 4.2.1 Policy gradient Theorem\n서튼 pg 논문의 policy gradient theorem에 따라 exact gradient of the average reward는 다음과 같다. 다음 수식이 어떻게 유도되었는지, 어떤 의미인지 모른다면 서튼 pg 논문을 통해 제대로 이해하는 것이 좋다.\n\n$$\\nabla\\eta(\\theta)=\\sum_{s,a}\\rho^{\\pi}(s)\\nabla\\pi(a;s,\\theta)Q^{\\pi}(s,a)$$\n\nsteepest descent direction of $\\eta(\\theta)$는 $\\eta(\\theta + d\\theta)$를 최소화하는 $d\\theta$로 정의된다. 이 때, $\\vert d\\theta \\vert^2$가 일정 크기 이하인 것으로 제약조건을 준다(held to small constant). Euclidian space에서는 $\\eta(\\theta)$가 steepest direction이지만 Riemannian space에서는 natural gradient가 steepest direction이다. \n\n#### 4.2.2 Natural gradient 증명\nRiemannian space에서 거리는 다음과 같이 정의된다. $G(\\theta)$는 특정한 양수로 이루어진 matrix이다.\n\n$$\\vert d\\theta \\vert^2=\\sum_{ij}(\\theta)d\\theta_id\\theta_i=d\\theta^TG(\\theta)d\\theta$$\n\n이 수식은 Natural Gradient Works Efficiently in Learning 논문에서 증명되어있다. 다음은 natural gradient 증명이다. \n\nsteepest direction을 구할 때 $\\theta$의 크기를 제약조건을 준다. 제약조건은 다음과 같다. \n\n$$\\vert d\\theta \\vert^2 = \\epsilon^2$$\n\n그리고 steepest vector인 $d\\theta$는 다음과 같이 정의할 수 있다. \n\n$$d\\theta = \\epsilon a$$\n\n$$\\vert a \\vert^2=a^TG(\\theta)a = 1$$\n\n이 때, $a$가 steepest direction unit vector이 되려면 다음 수식을 최소로 만들어야 한다. (이 수식은 잘 모르겠지만 $\\theta$에서의 1차근사를 가정하는게 아닌가 싶다.\n\n$$\\eta(\\theta + d\\theta) = \\eta(\\theta) + \\epsilon\\nabla\\eta(\\theta)^Ta$$\n\n위 수식이 제약조건 아래 최소가 되는 $a$를 구하기 위해 Lagrangian method를 사용한다. Lagrangian method를 모른다면 [위키피디아](https://en.wikipedia.org/wiki/Lagrange_multiplier)를 참고하는 것을 추천한다. 위 수식이 최소라는 것은 $\\nabla\\eta(\\theta)^Ta$가 최소라는 것이다. \n\n$$\\frac{\\partial}{\\partial a_i}(\\nabla\\eta(\\theta)^Ta - \\lambda a^TG(\\theta)a)=0$$\n\n따라서 $(\\nabla\\eta(\\theta)^Ta - \\lambda a^TG(\\theta)a)=0$는 상수이다. 상수를 미분하면 0이므로 이 식을 $a$로 미분한다. 그러면 다음과 같다. steepest direction을 구한 것이다.\n\n$$\\nabla\\eta(\\theta) = 2 \\lambda G(\\theta)a$$\n\n$$a=\\frac{1}{2\\lambda}G^{-1}\\nabla\\eta(\\theta)$$\n\n이 때, 다음 식을 natural gradient라고 정의한다.\n\n$$\\bar{\\nabla}\\eta(\\theta) = G^{-1}\\nabla\\eta(\\theta)$$\n\nnatural gradient를 이용한 업데이트는 다음과 같다. \n\n$$\\theta_{t+1}=\\theta_t - \\alpha_tG^{-1}\\nabla\\eta(\\theta)$$\n\n여기까지는 natural gradient의 증명이었다. 이 natural gradient를 policy gradient에 적용한 것이 natural policy gradient이다. natural policy gradient는 다음과 같이 정의된다.\n\n$$\\bar{\\nabla}\\eta(\\theta) = F^{-1}\\nabla\\eta(\\theta)$$\n\n$G$ 대신 $F$를 사용했는데 $F$는 Fisher information matix이다. 수식은 다음과 같다.\n\n$$F(\\theta) = E_{\\rho^\\pi(s)}[F_s(\\theta)]$$\n\n$$F_s(\\theta)=E_{\\pi(a;s,\\theta)}[\\frac{\\partial log \\pi(a;s, \\theta)}{\\partial \\theta_i}\\frac{\\partial log \\pi(a;s, \\theta)}{\\partial\\theta_j}]$$\n\n왜 G가 F가 되는지는 아직 잘 모르겠다. 거리라는 개념을 표현하려면 \n\n## 5. The Natural Gradient and Policy Iteration\n---\n### 5.1 Theorem 1\nsutton pg 논문에 따라 $Q^{\\pi}(s,a)$를 approximation한다. approximate하는 함수 $f^{\\pi}(s,a;w)$는 다음과 같다.(compatible value function)\n\n$$f^{\\pi}(s,a;w)=w^T\\psi^{\\pi}(s,a)$$\n\n$$\\psi^{\\pi}(s,a) = \\nabla log\\pi(a;s,\\theta)$$\n\n$w$는 원래 approximate하는 함수 $Q$와 $f$의 차이를 줄이도록 학습한다(mean square error). 수렴한 local minima의 $w$를 $\\bar{w}$라고 하겠다. 에러는 다음과 같은 수식으로 나타낸다. \n\n$$\\epsilon(w,\\pi)\\equiv\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)(f^{\\pi}(s,a;w)-Q^{\\pi}(s,a))^2$$\n\n위 식이 local minima이면 미분값이 0이다. $w$에 대해서 미분하면 다음과 같다. \n\n$$\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)(\\psi^{\\pi}(s,a)^T\\bar{w}-Q^{\\pi}(s,a))=0$$\n\n$$(\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)\\psi^{\\pi}(s,a)^T)\\bar{w}=\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)Q^{\\pi}(s,a))$$\n\n이 때, 위 식의 우변은 $\\psi$의 정의에 의해 policy gradient가 된다. 또한 왼쪽 항에서는 Fisher information matrix가 나온다.\n\n$$F(\\theta)=\\sum_{s,a}\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)\\psi^{\\pi}(s,a)=E_{\\rho^\\pi(s)}[F_s(\\theta)]$$\n\n따라서 다음과 같다.\n\n$$F(\\theta)\\bar{w}=\\nabla\\eta(\\theta)$$\n\n$$\\bar{w}=F(\\theta)^{-1}\\nabla\\eta(\\theta)$$\n\n이 식은 natural gradient 식과 동일하다. 이 식은 policy가 update 될 때, value function approximator의 parameter 방향으로 이동한다는 것을 의미한다. function approximation이 정확하다면 그 parameter의 natural policy gradient와 inner product가 커야한다. \n\n### 5.2 Theorem 2: Greedy Polict Improvement\nnatural policy gradient가 단순히 더 좋은 행동을 고르도록 학습하는게 아니라 가장 좋은 (greedy) 행동을 고르도록 학습한다는 것을 증명하는 파트이다. 이것을 일반적인 형태의 policy에 대해서 증명하기 전에 exponential 형태의 policy에 대해서 증명하는 것이 Theorem 2이다.\n\npolicy를 다음과 같이 정의한다.\n\n$$\\pi(a;s,\\theta) \\propto exp(\\theta^T\\phi_{sa})$$\n\n$\\bar{\\nabla}\\eta(\\theta)$가 0이 아니고 $\\bar{w}$는 approximation error를 최소화한 $w$라고 가정한다. 이 상태에서 natural gradient update를 생각해보자. policy gradient는 gradient ascent임을 기억하자.\n\n$$\\theta_{t+1}=\\theta_t + \\alpha_t\\bar{\\nabla}\\eta(\\theta)$$\n\n이 때 $\\alpha$가 learning rate로 parameter를 얼마나 업데이트하는지를 결정한다. 이 값을 무한대로 늘렸을 때 policy가 어떻게 업데이트되는지 생각해보자. \n\n$$\\pi_{\\infty}(a;s)=lim_{\\alpha\\rightarrow\\infty}\\pi(a;s,\\theta+\\alpha\\bar{\\nabla}\\eta(\\theta))-(1)$$\n\nfunction approximator는 다음과 같다. \n\n$$f^{\\pi}(s,a;w)=w^T\\psi^{\\pi}(s,a)$$\n\nTheorem 1에 의해 위 식은 아래와 같이 쓸 수 있다.\n\n\n$$f^{\\pi}(s,a;w)=\\bar{\\nabla}\\eta(\\theta)^T\\psi^{\\pi}(s,a)$$\n\n$\\theta$의 정의에 의해 $\\psi$는 다음과 같다.\n\n$$\\psi^{\\pi}(s,a)=\\phi_{sa}-E_{\\pi(a';s,\\theta)}[\\phi_{sa'}]$$\n\nfunction approximator는 다음과 같이 다시 쓸 수 있다.\n\n$$f^{\\pi}(s,a;w)=\\bar{\\nabla}\\eta(\\theta)^T(\\phi_{sa}-E_{\\pi(a';s,\\theta)}[\\phi_{sa'}])$$\n\ngreedy policy improvement가 Q function 값 중 가장 큰 값을 가지는 action을 선택하듯이 여기서도 function approximator의 값이 가장 큰 action을 선택하는 상황을 가정해본다. 이 때 function approximator의 argmax는 다음과 같이 쓸 수 있다.\n\n$$argmax_{a'}f^{\\pi}(s,a)=argmax_{a'}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa'}$$\n\n(1) 식을 다시 살펴보자. policy의 정의에 따라 다음과 같이 쓸 수 있다. \n\n$$\\pi(a;s,\\theta + \\alpha\\bar{\\nabla}\\eta(\\theta)) \\propto exp(\\theta^T\\phi_{sa} + \\alpha\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa})$$\n\n$\\bar{\\nabla}\\eta(\\theta) \\neq 0$이고 $\\alpha\\rightarrow\\infty$이면 exp안의 항 중에서 뒤의 항이 dominate하게 된다. 여러 행동 중에 $\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa}$가 가장 큰 행동이 있다면 이 행동의 policy probability가 1이 되고 나머지는 0이 된다. 따라서 다음이 성립한다.\n\n$$\\pi_{\\infty}=0$$ \n\nif and only if \n\n$$a \\notin argmax_{a'}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa'}$$\n\n이 결과로부터 natural policy gradient는 단지 더 좋은 action이 아니라 best action을 고르도록 학습이 된다. 하지만 non-covariant gradient(1차미분) 에서는 그저 더 좋은 action을 고르도록 학습이 된다. 하지만 이 natural policy gradient에 대한 결과는 infinite learning rate 세팅에서만 성립함. 좀 더 일반적인 경우에 대해서 살펴보자.\n\n#### 4.3 Theorem 3 \nTheorem 2에서와는 달리 일반적인 policy를 가정하자(general parameterized policy). Theorem 3는 이 상황에서 natural gradient를 통한 업데이트가 best action를 고르는 방향으로 학습이 된다는 것을 보여준다. \n\nnatural gradien에 따른 policy parameter의 업데이트는 다음과 같다. $\\bar{w}$는 approximation error를 minimize하는 $w$이다.\n\n$$\\delta\\theta = \\theta' - \\theta = \\alpha\\bar{\\nabla}\\eta(\\theta)=\\alpha\\bar{w}$$\n\npolicy에 대해서 1차근사를 하면 다음과 같다. \n\n$$\\pi(a;s,\\theta')=\\pi(a;s,\\theta)+\\frac{\\partial\\pi(a;s,\\theta)^T}{\\partial\\theta}\\delta\\theta + O(\\delta\\theta^2)$$\n\n$$=\\pi(a;s,\\theta)(1+\\psi(s,a)^T\\delta\\theta) + O(\\delta\\theta^2)$$\n\n$$=\\pi(a;s,\\theta)(1+\\alpha\\psi(s,a)^T\\bar{w}) + O(\\delta\\theta^2)$$\n\n$$=\\pi(a;s,\\theta)(1+\\alpha f^{\\pi}(s,a;\\bar{w}) + O(\\delta\\theta^2)$$\n\npolicy 자체가 function approximator의 크기대로 업데이트가 되므로 local하게 best action의 probability는 커지고 다른 probability의 크기는 작아질 것이다. 하지만 만약 greedy improvement가 된다하더라도 그게 performance의 improvement를 보장하는 것은 아니다. 하지만 line search와 함께 사용할 경우 improvement를 보장할 수 있다. \n\n## 6. Metrics and Curvatures\n---\n다음 식에 해당하는 G는 Fisher Information Matrix만 사용할 수 있는 것이 아니다.\n\n$$\\vert d\\theta \\vert^2=\\sum_{ij}(\\theta)d\\theta_id\\theta_i=d\\theta^TG(\\theta)d\\theta$$\n\n이 파트에서는 FIM과 다른 metric 사이의 관계를 다룬다. \n\n- In the different setting of parameter estimation, the Fisher information converges to the ```Hessian```, so it is [asymptotically efficient](https://en.wikipedia.org/wiki/Efficiency_(statistics)#Asymptotic_efficiency)\n- 이 논문의 경우, 아마리 논문의 'blind separation case'와 유사한데 이 때는 꼭 asymtotically efficient하지 않다. 이 말은 즉 2nd order 수렴이 보장되지 않는다는 것이다.\n- [Mackay](http://www.inference.org.uk/mackay/ica.pdf) 논문에서 hessian에서 data independant한 term을 metric으로 가져오는 방법을 제안했다. 그래서 performance를 2번 미분해보면 다음과 같다. 하지만 다음 식에서는 모든 항이 data dependent하다(Q가 있으니까). 첫 번째 항이 그나마 FIM과의 관련성이 있을 수 있지만 Q 값이 curvature에 weight를 주는 방식 때문에 다르다고 할 수 있다.\n\n$$\n\\nabla^2\\eta(\\theta)=\\sum_{sa}\\rho^{\\pi}(s)(\\nabla^2\\pi(a;s)Q^{\\pi}(s,a)+\\nabla\\pi(a;s)\\nabla Q^{\\pi}(s,a)^T+\\nabla Q^{\\pi}(s,a)\\nabla\\pi(a;s)^T)\n$$\n\n\n- hessian은 보통 positive definite가 아닐수도 있다. 따라서 local maxima가 될 때까지 Hessian이 사용하기 별로 안좋다. 그리고 local maxima에서는 Hessian보다는 Conjugate methods가 더 효율적이다. \n\n이 파트에서는 무엇을 말하고 있는지 알기가 어렵다. FIM과 Hessian이 관련이 있다는 것을 알겠다. 하지만 asymtotically efficient와 같은 내용을 모르므로 내용의 이해가 어려웠다.\n\nMackay 논문에서 해당 부분은 다음과 같다. \n\n<img src=\"https://www.dropbox.com/s/x4n6z6pdyi7xtb9/Screenshot%202018-06-10%2012.04.13.png?dl=1\">\n\n## 7. Experiment\n---\n논문에서는 natural gradient를 simple MDP와 tetris MDP에 대해서 테스트했다. practice에서는 Fisher information matrix는 다음과 같은 식으로 업데이트한다.\n\n$$f\\leftarrow f+\\nabla log \\pi(a_t; s_t, \\theta)\\nabla log \\pi(a_t; s_t, \\theta)^T$$\n\nT length trajectory에 대해서 f/T를 통해 F의 estimate를 구한다.\n\n### 7.1 Linear Quadratic regulator\n에이전트를 테스트할 환경은 다음과 같은 dynamics를 가지고 있다. $u(t)$는 control signal로서 에이전트의 행동이라고 생각하면 된다. $\\epsilon$은 noise distribution으로 환경에 가해지는 노이즈이다. 에이전트의 목표는 적절한 $u(t)$를 통해 \nx(t)를 0으로 유지하는 것이다. 제어분야에서의 LQR controller 문제이다.\n\n$$\nx(t+1) = 0.7x(t)+u(t)+\\epsilon(t)\n$$\n\nx(t)를 0으로 유지하기 위해서 $x(t)^2$를 cost로 잡고 이 cost를 최소화하도록 학습한다. 이 시스템을 linear라고 부르는 것은 아래 그림과 같이 선형의 형태를 띄기 때문이다. 이 논문에서 실험할 때는 이 그림에서의 system에 noise를 더해준 것이다. [그림 출처](https://stanford.edu/class/ee363/lectures/dlqr.pdf)\n\n<img src='https://www.dropbox.com/s/vz0q97lcek4oti5/Screenshot%202018-06-08%2014.21.10.png?dl=1'>\n\n이 실험에서 사용한 parameterized policy는 다음과 같다. parameter가 $\\theta_1$과 $\\theta_2$ 밖에 없는 상당히 간단한 policy이다. \n\n$$\n\\pi(u;x,\\theta) \\propto exp(\\theta_1 s_1 x^2 + \\theta_2 s_2 x)\n$$\n\n이 policy를 간단히 numpy와 matplotlib를 이용해서 그려봤다. $\\theta_1$과 $theta_2$를 (0.5, 0.5), (1, 0), (0, 1)로 하고 $s_1$과 $s_2$는 1로 두었다. x는 -1에서 1까지의 범위로 그렸다. x를 0으로 유지하려면 u(t)가 -와 +가 둘 다 가능해야할 것 같은데 위 식으로만 봐서는 action이 하나이고 그 action일 확률을 표시하는 것처럼 나왔다. 아마 -1과 +1이 u(t)가 될 수 있는데 그 중 +1을 선택할 확률이 위와 같이 되는게 아닌가 싶다.\n<center><img src='https://www.dropbox.com/s/v69qyrwn7zurk8c/Screenshot%202018-06-08%2014.57.07.png?dl=1' width='500px'></center>\n\n다음 그림은 1-d LQR을 학습한 그래프이다. cost가 $x^2$이기 때문에 cost가 0으로 갈수록 agent는 0에서 안정적으로 머무른다고 볼 수 있다. 6개의 선 중에서 오른쪽 세 개가 일반적인 gradient 방법을 사용해서 학습한 결과이다. 그리고 왼쪽의 세 개의 선이 natural policy gradient를 통해 학습한 학습 곡선이다. 일반 gradient 방법보다 natural gradient가 훨씬 빠르게 학습한다(time 축이 log scale인 것을 감안하자).\n\n하지만 문제가 있다. npg를 학습한 세 개의 곡선은 $\\theta$를 rescale 한 것이다. $\\theta$앞에 곱해지는 숫자에 따라 학습의 과정이 다르다. 이 것은 coordinate에 따라 steepest gradient가 다르게 측정된다는 것이다. 즉, covariant gradient가 아니라는 뜻이다. 이 논문에서는 natural gradient를 통해 gradient가 covariant하도록 만들고 싶었는데 실패한 것이다. \n\n\n<center><img src=\"https://www.dropbox.com/s/fhn8cgje0rdws0i/Screenshot%202018-06-08%2023.13.37.png?dl=1\" width=\"300px\"></center>\n\nnatural gradient가 covariant하지 않은 이유는 Fisher Information Matrix가 예상했던 바와는 달리 invariant metric이 아니기 때문이다. 또한 FIM이 invariant metric이 아닌 이유는 FIM을 계산할 때 $\\rho_s$가 곱해지기 때문이다(state distribution에 대한 expectation. $\\rho_s$가 곱해지는 것이 invariant에 미치는 영향이 무엇인지는 모르겠다). 하지만 여전히 의의가 있는 것은 기존 gradient 방법들보다 훨씬 빠르게 학습한다는 것이다.\n\n### 7.2 simple 2-state MDP\n이제 다른 예제에서 NPG를 테스트한다. 2개의 state만 가지는 MDP를 고려해보자. [그림출처](http://repository.cmu.edu/cgi/viewcontent.cgi?article=1080&context=robotics). 그림으로보면 다음과 같다. x=0 상태와 x=1 상태 두 개가 존재한다. 에이전트는 각 상태에서 다시 자신의 상태로 되돌아오는 행동을 하거나 다른 상태로 가는 행동을 할 수 있다. 상태 x=0에서 다시 자기 자신으로 돌아오면 1의 보상을 받고 상태 x=1에서 자기 자신으로 돌아오면 2의 보상을 받는다. 따라서 결국 optimal policy는 상태 x=1에서 계속 자기 자신으로 돌아오는 행동을 취하는 것이다. \n\n<img src=\"https://www.dropbox.com/s/g1x9yknzsrip59i/Screenshot%202018-06-08%2023.06.50.png?dl=1\">\n\n문제를 좀 어렵게 만들기 위해 state distribution을 다음과 같이 설정한다. 즉, 대부분의 경우에 상태 x=0에서 에이전트가 시작하는 것이다. \n\n$$\n\\rho(x=0)=0.8,  \\rho(x=1)=0.2\n$$\n\n일반적인 gradient 방법을 사용하면 다음과 같은 policy gradient 식에 따라서 업데이트를 하게 된다. 이 때, $\\rho(s)$가 gradient에 곱해지므로 상태적으로 상태 0에서의 gradient 값이 커지게 된다. 따라서 에이전트는 상태 0에서의 gradient(상태 0에서 스스로에게 돌아오는 행동을 취하도록 정책을 업데이트하는 gradient)를 따라 parameterized policy를 update한다. 따라서 아래 그림의 첫번째 그림에서처럼 Reward가 1에서 오랜 시간동안 머무른다. 즉, 에이전트가 상태 0에서 self-loop를 계속 돌고 있다는 것이다. $\\rho(x=0)$가 $10^{-7}$까지 떨어진다.\n\n$$\\nabla\\eta(\\theta)=\\sum_{s,a}\\rho^{\\pi}(s)\\nabla\\pi(a;s,\\theta)Q^{\\pi}(s,a)$$\n\n<center><img src=\"https://www.dropbox.com/s/xtb77mfazbppnss/Screenshot%202018-06-08%2023.14.24.png?dl=1\" width=\"300px\"></center>\n\n하지만 NPG를 사용할 경우에는 훨씬 빠르게 average reward가 2에 도달한다. gradient 방법이 $1.7X10^(7)$정도의 시간만에 2에 도달한 반면 NPG는 2만에 도달한다. 또한 $\\rho(x=0)$가 $10^{-5}$이하로 떨어지지 않는다.\n\n한 가지 그래프를 더 살펴보자. 다음 그래프는 parameter $\\theta$가 업데이트 되는 과정을 보여준다. 이 그래프에서는 parameter가 2개 있는 것이다. 일반적인 gradient가 아래 그래프에서 실선에 해당한다. 이 실선의 그래프는 보면 처음부터 중반까지 $\\theta_i$만 거의 업데이트하는 것을 볼 수 있다. 그에 비해 NPG는 두 개의 parameter를 균등하게 업데이트하는 것을 볼 수 있다. \n\n<center><img src=\"https://www.dropbox.com/s/g7pazozw2k6rd7x/Screenshot%202018-06-08%2023.23.25.png?dl=1\" width=\"300px\"></center>\n\npolicy가 $\\pi(a;s,\\theta)\\propto exp(\\theta_{sa})$일 때, 다음과 같이 $F_{-1}$이 gradient 앞에 weight로 곱해지는데 이게 $\\rho$와는 달리 각 parameter에 대해 균등하다. 따라서 위 그래프에서와 같이 각 parameter는 비슷한 비율로 업데이트가 되는 것이다.\n\n$$\\bar{\\nabla}\\eta(\\theta) = F^{-1}\\nabla\\eta(\\theta)$$\n\n### 7.3 Tetris\nNPG를 테스트할 tetris 예제는 Neuro Dynamic Programming 책에 소개되어있다. 다음 그림은 tetris 예제를 보여준다. 보통 그림에서와 같이 state의 feature를 정해준다. [그림 출처](http://slideplayer.com/slide/5215520/)\n\n<img src=\"https://www.dropbox.com/s/y1halso9yermy8s/Screenshot%202018-06-08%2023.44.34.png?dl=1\">\n\n이 예제에서도 exponantial family로 policy를 표현한다. $\\pi(a;s,\\theta) \\propto exp(\\theta^T\\phi_{sa})$ 로 표현한다.\n\ntetris는 linear function approximator와 greedy policy iteration을 사용할 경우 performance가 갑자기 떨어지는 현상이 있다. 밑의 그림에서 A의 spike가 있는 그래프가 이 경우이다. 그 밑에 낮게 누워있는 그래프는 일반적인 policy gradient 방법이다. 하지만 Natural policy gradient를 사용할 경우 B 그림에서 오른쪽 그래프와 같이 성능개선이 뚜렷하다. Policy Iteration 처럼 성능이 뚝 떨어지지 않고 안정적으로 유지한다. 또한 그림 C에서 보는 것처럼 오른쪽 그래프인 일반적인 gradient 방법보다 훨씬 빠르게 학습하는 것을 볼 수 있다.\n\n<img src=\"https://www.dropbox.com/s/pr6s2qrqaic0wyj/Screenshot%202018-06-08%2023.40.16.png?dl=1\">\n\n\n## 8. Discussion\n---\n\n- natural gradient method는 policy iteration에서와 같이 greedy action을 선택하도록 학습됌\n- line search와 함께 쓰면 natural gradient method는 더 policy iteration 같아짐\n- greedy policy iteration에서와는 달리 performance improvement가 보장됌\n- 하지만 F(Fisher information matrix)가 asymtotically Hessian으로 수렴하지 않음. asymtotically conjugate gradient method(Hessian의 inverse를 approx.로 구하는 방법)가 더 좋아 보일 수 있음\n- 하지만 Hessian이 항상 informative하지 않고(hessian이 어떤 정보를 주려면 positive definite와 같은 성질을 가져서 해당 함수가 convex인 것을 알 수 있다든지의 경우를 이야기하는데 hessian이 항상 positive definite가 아닐 수 있다는 것이다) tetris에서 봤듯이 natural gradient method가 더 효율적일 수 있음(pushing the policy toward choosing greedy optimal actions)\n- conjugate gradient method가 좀 더 maximum에 빠르게 수렴하지만, performance는 maximum에서 거의 안변하므로 좋다고 말하기 어려움(?). 이 부분에 대해서 추가적인 연구 필요.\n\namet, consectetur adipisicing elit. Vitae ipsum, voluptatem quis officiis inventore dolor totam deserunt, possimus similique eum, accusantium adipisci doloremque omnis excepturi quasi, suscipit repellendus quibusdam? Veritatis.","source":"_posts/2018-06-15-npg.md","raw":"---\ntitle: A Natural Policy Gradient\ndate: 2018-06-14 13:18:45\ntags: [\"프로젝트\", \"피지여행\"]\ncategories: 프로젝트\nauthor: 이웅원\nsubtitle: 피지여행 4번째 논문\n---\n\n# A Natural Policy Gradient [2001]\n\n<img src=\"https://www.dropbox.com/s/it82tfhfmhg9uwp/Screenshot%202018-06-10%2010.58.52.png?dl=1\">\n\n- 논문 저자: Sham Kakade\n- 논문 링크: [https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf](https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf)\n- 함께 보면 좋을 논문: \n\t- [Policy Gradient Methods for\nReinforcement Learning with Function\nApproximation (2000)](hhttps://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf)\n\t- [Natural Gradient Works Efficiently in Learning(1998)](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.452.7280&rep=rep1&type=pdf)\n- 논문을 보는 이유: TRPO와 NPG는 관련이 많기 때문에 TRPO를 더 잘 이해하기 위해 봄\n\n## 1. Abstract\n---\n\n- natural gradient method를 policy gradient에 적용\n- natural gradient는 steepest descent direction을 가짐\n- gradient descent는 parameter를 한 번에 많이 update 할 수 없는 반면, natural gradient는 가장 좋은 action을 고르도록 학습이 됌 (sutton 논문에서와 같이 compatible value function을 사용할 경우 policy iteration에서 policy improvement 1 step의 과정에서)\n- simple MDP와 tetris MDP에서 테스트함. 성능이 많이 향상\n\n## 2. Personal Interpretation and Thinking\n---\n\n(개인생각) 뉴럴넷을 사용할 경우 gradient가 steepest direction이 아닌 경우가 많다. 뉴럴넷의 parameter space가 우리가 보통 생각하는 직선으로 쭉쭉 뻗어있는 Euclidean space가 아니다. 좀 더 일반적으로는 구의 표면과 같이 휘어져있는 공간 즉, 리만 공간(Riemannian space)로 표현할 수 있다. 이와 같은 공간에서는 natural gradient가 steepest direction이 된다는 연구가 이뤄지고 있었다. 강화학습의 policy gradient은 objective function의 gradient를 따라 policy를 업데이트한다. 이 때, policy는 parameterized 되는데 이 경우에도 gradient 대신에 natural gradient가 좋다는 것을 실험해보는 논문이다. \n\ngradient가 non-covariant 해서 생기는 문제는 간단히 말하자면 다음과 같다. policy가 parameterized된 상황에서는 같은 policy라도 다른 parameter를 가질 수 있다. 이 때, steepest direction은 두 경우에 같은 방향을 가리켜야하는데 non-covariant한 경우 그렇지 못하다. 이것은 결국 느린 학습으로 연결이 된다. \n\n논문에서 2차미분 방법론들과 짧게 비교를 한다. 하지만 2차미분을 이용한 다른 방법들과의 비교가 생각보다 없는 점이 아쉽다.(Hessian을 이용한다거나 conjugate gradient method를 이용한다거나). 실험을 통해 FIM이 hessian에 수렴안하는 거라던지 Hessian 방법론이 local maxima 부근에서 상당히 느리다던지의 결과를 보여줬었으면 좋았을 것 같다. \n\n또한 natural gradient 만으로 업데이트하면 policy의 improvement보장이 안될 수 있다. policy의 improvement를 보장하기 위해 line search도 써야하는데 line search를 어떻게 쓰는지에 대한 자세한 언급이 없다. 즉, 자세한 algorithm 설명이 없다.\n\nnatural policy gradient 논문은 natural gradient + policy gradient를 처음 적용했다는데 의의가 있다. 하지만 이 논문이 문제 삼은 gradient는 non-covariant하다라는 문제를 natural gradient를 통해 해결하지 못했다(Experiment를 통해 covariant gradient가 되지 못했다는 것이 보인다). NPG의 뒤를 잇는 논문이 \"covariant policy search\"와 \"natural actor-critic\"에서 covariant하지 못하다는 것을 해결하기 위해 Fisher Information Matrix를 sample 하나 하나에 대해서 구하는 것이 아니라 trajectory 전체에 대해서 구한다. \n\n또한 논문은 pg의 두 가지 세팅 중에 average-reward setting(infinite horizon)에서만 NPG를 다룬다. \"covariant policy search\" 논문에서는 average-reward setting과 start-state setting 모두에 대해서 npg를 적용한다. \n\nnatural gradient + policy gradient를 처음 제시했다는 것은 좋지만 npg 학습의 과정을 자세하게 설명하지 않았고 다른 2차 미분 방법들과 비교를 많이 하지 않은 점이 아쉬운 논문이다.\n\n\n## 3. Introduction\n---\n\n- direct policy gradient method는 future reward의 gradient를 따라 policy를 update함\n- 하지만 gradient descent는 non-covariant\n- 이 논문에서는 covarient gradient를 제시함 = natural gradient\n- natural gradient와 policy iteration의 연관성을 설명하겠음: natural policy gradient is moving toward choosing a greedy optimal action (이런 연결점은 아마도 step-size를 덜 신경쓰고 싶어서 그런게 아닌가 싶다)\n\n논문의 Introduction 부분에 다음 멘트가 있다. 이 글만 봐서는 이해가 안갔는데 Mackay 논문에 좀 더 자세히 나와있다.\n \n<img src=\"https://www.dropbox.com/s/41xhhr7lgfk24a1/Screenshot%202018-06-10%2011.45.18.png?dl=1\">\n\n[Mackay](http://www.inference.org.uk/mackay/ica.pdf)논문에서는 다음과 같이 언급하고 있다. Back-propagation을 사용할 경우에 learning rate를 dimension에 1/n로 사용하면 수렴한다는 것이 증명됐다. 하지만 너무 느리다. \n\n<img src=\"https://www.dropbox.com/s/us9ezc7vxgrkez6/Screenshot%202018-06-10%2011.47.21.png?dl=1\">\n\n## 4. A Natural Gradient\n---\n### 4.1 환경에 대한 설정\n이 논문에서 제시하는 학습 환경은 다음과 같다.\n\n- MDP: tuple $(S, s_0, A, R, P)$\n- $S$: a finite set of states\n- $s_0$: a start state\n- $A$: a finite set of actions\n- $R$: reward function $R: S \\times A -> [0, R_{max}]$\n- $\\pi(a;s, \\theta)$: stochastic policy parameterized by $\\theta$\n- 모든 정책 $\\pi$는 ergodic: stationary distribution $\\rho^{\\pi}$이 잘 정의되어있음\n- 이 논문에서는 sutton의 pg 논문의 두 세팅(start-state formulation, average-reward formulation) 중에 두 번째인 average-reward formulation을 가정 \n- performance or average reward: $\\eta(\\pi)=\\sum_{s,a}\\rho^{\\pi}(s)\\pi(a;s)R(s,a)$\n- state-action value: $Q^{\\pi}(s,a)=E_{\\pi}[\\sum_{t=0}^{\\infty}R(s_t, a_t)-\\eta(\\pi)\\vert s_0=s, a_0=a]$\n- 정책이 $\\theta$로 parameterize되어있으므로 performance는 $\\eta(\\pi_{\\theta})$인데 $\\eta(\\theta)$로 쓸거임\n\n### 4.2 Natural Gradient\n#### 4.2.1 Policy gradient Theorem\n서튼 pg 논문의 policy gradient theorem에 따라 exact gradient of the average reward는 다음과 같다. 다음 수식이 어떻게 유도되었는지, 어떤 의미인지 모른다면 서튼 pg 논문을 통해 제대로 이해하는 것이 좋다.\n\n$$\\nabla\\eta(\\theta)=\\sum_{s,a}\\rho^{\\pi}(s)\\nabla\\pi(a;s,\\theta)Q^{\\pi}(s,a)$$\n\nsteepest descent direction of $\\eta(\\theta)$는 $\\eta(\\theta + d\\theta)$를 최소화하는 $d\\theta$로 정의된다. 이 때, $\\vert d\\theta \\vert^2$가 일정 크기 이하인 것으로 제약조건을 준다(held to small constant). Euclidian space에서는 $\\eta(\\theta)$가 steepest direction이지만 Riemannian space에서는 natural gradient가 steepest direction이다. \n\n#### 4.2.2 Natural gradient 증명\nRiemannian space에서 거리는 다음과 같이 정의된다. $G(\\theta)$는 특정한 양수로 이루어진 matrix이다.\n\n$$\\vert d\\theta \\vert^2=\\sum_{ij}(\\theta)d\\theta_id\\theta_i=d\\theta^TG(\\theta)d\\theta$$\n\n이 수식은 Natural Gradient Works Efficiently in Learning 논문에서 증명되어있다. 다음은 natural gradient 증명이다. \n\nsteepest direction을 구할 때 $\\theta$의 크기를 제약조건을 준다. 제약조건은 다음과 같다. \n\n$$\\vert d\\theta \\vert^2 = \\epsilon^2$$\n\n그리고 steepest vector인 $d\\theta$는 다음과 같이 정의할 수 있다. \n\n$$d\\theta = \\epsilon a$$\n\n$$\\vert a \\vert^2=a^TG(\\theta)a = 1$$\n\n이 때, $a$가 steepest direction unit vector이 되려면 다음 수식을 최소로 만들어야 한다. (이 수식은 잘 모르겠지만 $\\theta$에서의 1차근사를 가정하는게 아닌가 싶다.\n\n$$\\eta(\\theta + d\\theta) = \\eta(\\theta) + \\epsilon\\nabla\\eta(\\theta)^Ta$$\n\n위 수식이 제약조건 아래 최소가 되는 $a$를 구하기 위해 Lagrangian method를 사용한다. Lagrangian method를 모른다면 [위키피디아](https://en.wikipedia.org/wiki/Lagrange_multiplier)를 참고하는 것을 추천한다. 위 수식이 최소라는 것은 $\\nabla\\eta(\\theta)^Ta$가 최소라는 것이다. \n\n$$\\frac{\\partial}{\\partial a_i}(\\nabla\\eta(\\theta)^Ta - \\lambda a^TG(\\theta)a)=0$$\n\n따라서 $(\\nabla\\eta(\\theta)^Ta - \\lambda a^TG(\\theta)a)=0$는 상수이다. 상수를 미분하면 0이므로 이 식을 $a$로 미분한다. 그러면 다음과 같다. steepest direction을 구한 것이다.\n\n$$\\nabla\\eta(\\theta) = 2 \\lambda G(\\theta)a$$\n\n$$a=\\frac{1}{2\\lambda}G^{-1}\\nabla\\eta(\\theta)$$\n\n이 때, 다음 식을 natural gradient라고 정의한다.\n\n$$\\bar{\\nabla}\\eta(\\theta) = G^{-1}\\nabla\\eta(\\theta)$$\n\nnatural gradient를 이용한 업데이트는 다음과 같다. \n\n$$\\theta_{t+1}=\\theta_t - \\alpha_tG^{-1}\\nabla\\eta(\\theta)$$\n\n여기까지는 natural gradient의 증명이었다. 이 natural gradient를 policy gradient에 적용한 것이 natural policy gradient이다. natural policy gradient는 다음과 같이 정의된다.\n\n$$\\bar{\\nabla}\\eta(\\theta) = F^{-1}\\nabla\\eta(\\theta)$$\n\n$G$ 대신 $F$를 사용했는데 $F$는 Fisher information matix이다. 수식은 다음과 같다.\n\n$$F(\\theta) = E_{\\rho^\\pi(s)}[F_s(\\theta)]$$\n\n$$F_s(\\theta)=E_{\\pi(a;s,\\theta)}[\\frac{\\partial log \\pi(a;s, \\theta)}{\\partial \\theta_i}\\frac{\\partial log \\pi(a;s, \\theta)}{\\partial\\theta_j}]$$\n\n왜 G가 F가 되는지는 아직 잘 모르겠다. 거리라는 개념을 표현하려면 \n\n## 5. The Natural Gradient and Policy Iteration\n---\n### 5.1 Theorem 1\nsutton pg 논문에 따라 $Q^{\\pi}(s,a)$를 approximation한다. approximate하는 함수 $f^{\\pi}(s,a;w)$는 다음과 같다.(compatible value function)\n\n$$f^{\\pi}(s,a;w)=w^T\\psi^{\\pi}(s,a)$$\n\n$$\\psi^{\\pi}(s,a) = \\nabla log\\pi(a;s,\\theta)$$\n\n$w$는 원래 approximate하는 함수 $Q$와 $f$의 차이를 줄이도록 학습한다(mean square error). 수렴한 local minima의 $w$를 $\\bar{w}$라고 하겠다. 에러는 다음과 같은 수식으로 나타낸다. \n\n$$\\epsilon(w,\\pi)\\equiv\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)(f^{\\pi}(s,a;w)-Q^{\\pi}(s,a))^2$$\n\n위 식이 local minima이면 미분값이 0이다. $w$에 대해서 미분하면 다음과 같다. \n\n$$\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)(\\psi^{\\pi}(s,a)^T\\bar{w}-Q^{\\pi}(s,a))=0$$\n\n$$(\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)\\psi^{\\pi}(s,a)^T)\\bar{w}=\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)Q^{\\pi}(s,a))$$\n\n이 때, 위 식의 우변은 $\\psi$의 정의에 의해 policy gradient가 된다. 또한 왼쪽 항에서는 Fisher information matrix가 나온다.\n\n$$F(\\theta)=\\sum_{s,a}\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)\\psi^{\\pi}(s,a)=E_{\\rho^\\pi(s)}[F_s(\\theta)]$$\n\n따라서 다음과 같다.\n\n$$F(\\theta)\\bar{w}=\\nabla\\eta(\\theta)$$\n\n$$\\bar{w}=F(\\theta)^{-1}\\nabla\\eta(\\theta)$$\n\n이 식은 natural gradient 식과 동일하다. 이 식은 policy가 update 될 때, value function approximator의 parameter 방향으로 이동한다는 것을 의미한다. function approximation이 정확하다면 그 parameter의 natural policy gradient와 inner product가 커야한다. \n\n### 5.2 Theorem 2: Greedy Polict Improvement\nnatural policy gradient가 단순히 더 좋은 행동을 고르도록 학습하는게 아니라 가장 좋은 (greedy) 행동을 고르도록 학습한다는 것을 증명하는 파트이다. 이것을 일반적인 형태의 policy에 대해서 증명하기 전에 exponential 형태의 policy에 대해서 증명하는 것이 Theorem 2이다.\n\npolicy를 다음과 같이 정의한다.\n\n$$\\pi(a;s,\\theta) \\propto exp(\\theta^T\\phi_{sa})$$\n\n$\\bar{\\nabla}\\eta(\\theta)$가 0이 아니고 $\\bar{w}$는 approximation error를 최소화한 $w$라고 가정한다. 이 상태에서 natural gradient update를 생각해보자. policy gradient는 gradient ascent임을 기억하자.\n\n$$\\theta_{t+1}=\\theta_t + \\alpha_t\\bar{\\nabla}\\eta(\\theta)$$\n\n이 때 $\\alpha$가 learning rate로 parameter를 얼마나 업데이트하는지를 결정한다. 이 값을 무한대로 늘렸을 때 policy가 어떻게 업데이트되는지 생각해보자. \n\n$$\\pi_{\\infty}(a;s)=lim_{\\alpha\\rightarrow\\infty}\\pi(a;s,\\theta+\\alpha\\bar{\\nabla}\\eta(\\theta))-(1)$$\n\nfunction approximator는 다음과 같다. \n\n$$f^{\\pi}(s,a;w)=w^T\\psi^{\\pi}(s,a)$$\n\nTheorem 1에 의해 위 식은 아래와 같이 쓸 수 있다.\n\n\n$$f^{\\pi}(s,a;w)=\\bar{\\nabla}\\eta(\\theta)^T\\psi^{\\pi}(s,a)$$\n\n$\\theta$의 정의에 의해 $\\psi$는 다음과 같다.\n\n$$\\psi^{\\pi}(s,a)=\\phi_{sa}-E_{\\pi(a';s,\\theta)}[\\phi_{sa'}]$$\n\nfunction approximator는 다음과 같이 다시 쓸 수 있다.\n\n$$f^{\\pi}(s,a;w)=\\bar{\\nabla}\\eta(\\theta)^T(\\phi_{sa}-E_{\\pi(a';s,\\theta)}[\\phi_{sa'}])$$\n\ngreedy policy improvement가 Q function 값 중 가장 큰 값을 가지는 action을 선택하듯이 여기서도 function approximator의 값이 가장 큰 action을 선택하는 상황을 가정해본다. 이 때 function approximator의 argmax는 다음과 같이 쓸 수 있다.\n\n$$argmax_{a'}f^{\\pi}(s,a)=argmax_{a'}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa'}$$\n\n(1) 식을 다시 살펴보자. policy의 정의에 따라 다음과 같이 쓸 수 있다. \n\n$$\\pi(a;s,\\theta + \\alpha\\bar{\\nabla}\\eta(\\theta)) \\propto exp(\\theta^T\\phi_{sa} + \\alpha\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa})$$\n\n$\\bar{\\nabla}\\eta(\\theta) \\neq 0$이고 $\\alpha\\rightarrow\\infty$이면 exp안의 항 중에서 뒤의 항이 dominate하게 된다. 여러 행동 중에 $\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa}$가 가장 큰 행동이 있다면 이 행동의 policy probability가 1이 되고 나머지는 0이 된다. 따라서 다음이 성립한다.\n\n$$\\pi_{\\infty}=0$$ \n\nif and only if \n\n$$a \\notin argmax_{a'}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa'}$$\n\n이 결과로부터 natural policy gradient는 단지 더 좋은 action이 아니라 best action을 고르도록 학습이 된다. 하지만 non-covariant gradient(1차미분) 에서는 그저 더 좋은 action을 고르도록 학습이 된다. 하지만 이 natural policy gradient에 대한 결과는 infinite learning rate 세팅에서만 성립함. 좀 더 일반적인 경우에 대해서 살펴보자.\n\n#### 4.3 Theorem 3 \nTheorem 2에서와는 달리 일반적인 policy를 가정하자(general parameterized policy). Theorem 3는 이 상황에서 natural gradient를 통한 업데이트가 best action를 고르는 방향으로 학습이 된다는 것을 보여준다. \n\nnatural gradien에 따른 policy parameter의 업데이트는 다음과 같다. $\\bar{w}$는 approximation error를 minimize하는 $w$이다.\n\n$$\\delta\\theta = \\theta' - \\theta = \\alpha\\bar{\\nabla}\\eta(\\theta)=\\alpha\\bar{w}$$\n\npolicy에 대해서 1차근사를 하면 다음과 같다. \n\n$$\\pi(a;s,\\theta')=\\pi(a;s,\\theta)+\\frac{\\partial\\pi(a;s,\\theta)^T}{\\partial\\theta}\\delta\\theta + O(\\delta\\theta^2)$$\n\n$$=\\pi(a;s,\\theta)(1+\\psi(s,a)^T\\delta\\theta) + O(\\delta\\theta^2)$$\n\n$$=\\pi(a;s,\\theta)(1+\\alpha\\psi(s,a)^T\\bar{w}) + O(\\delta\\theta^2)$$\n\n$$=\\pi(a;s,\\theta)(1+\\alpha f^{\\pi}(s,a;\\bar{w}) + O(\\delta\\theta^2)$$\n\npolicy 자체가 function approximator의 크기대로 업데이트가 되므로 local하게 best action의 probability는 커지고 다른 probability의 크기는 작아질 것이다. 하지만 만약 greedy improvement가 된다하더라도 그게 performance의 improvement를 보장하는 것은 아니다. 하지만 line search와 함께 사용할 경우 improvement를 보장할 수 있다. \n\n## 6. Metrics and Curvatures\n---\n다음 식에 해당하는 G는 Fisher Information Matrix만 사용할 수 있는 것이 아니다.\n\n$$\\vert d\\theta \\vert^2=\\sum_{ij}(\\theta)d\\theta_id\\theta_i=d\\theta^TG(\\theta)d\\theta$$\n\n이 파트에서는 FIM과 다른 metric 사이의 관계를 다룬다. \n\n- In the different setting of parameter estimation, the Fisher information converges to the ```Hessian```, so it is [asymptotically efficient](https://en.wikipedia.org/wiki/Efficiency_(statistics)#Asymptotic_efficiency)\n- 이 논문의 경우, 아마리 논문의 'blind separation case'와 유사한데 이 때는 꼭 asymtotically efficient하지 않다. 이 말은 즉 2nd order 수렴이 보장되지 않는다는 것이다.\n- [Mackay](http://www.inference.org.uk/mackay/ica.pdf) 논문에서 hessian에서 data independant한 term을 metric으로 가져오는 방법을 제안했다. 그래서 performance를 2번 미분해보면 다음과 같다. 하지만 다음 식에서는 모든 항이 data dependent하다(Q가 있으니까). 첫 번째 항이 그나마 FIM과의 관련성이 있을 수 있지만 Q 값이 curvature에 weight를 주는 방식 때문에 다르다고 할 수 있다.\n\n$$\n\\nabla^2\\eta(\\theta)=\\sum_{sa}\\rho^{\\pi}(s)(\\nabla^2\\pi(a;s)Q^{\\pi}(s,a)+\\nabla\\pi(a;s)\\nabla Q^{\\pi}(s,a)^T+\\nabla Q^{\\pi}(s,a)\\nabla\\pi(a;s)^T)\n$$\n\n\n- hessian은 보통 positive definite가 아닐수도 있다. 따라서 local maxima가 될 때까지 Hessian이 사용하기 별로 안좋다. 그리고 local maxima에서는 Hessian보다는 Conjugate methods가 더 효율적이다. \n\n이 파트에서는 무엇을 말하고 있는지 알기가 어렵다. FIM과 Hessian이 관련이 있다는 것을 알겠다. 하지만 asymtotically efficient와 같은 내용을 모르므로 내용의 이해가 어려웠다.\n\nMackay 논문에서 해당 부분은 다음과 같다. \n\n<img src=\"https://www.dropbox.com/s/x4n6z6pdyi7xtb9/Screenshot%202018-06-10%2012.04.13.png?dl=1\">\n\n## 7. Experiment\n---\n논문에서는 natural gradient를 simple MDP와 tetris MDP에 대해서 테스트했다. practice에서는 Fisher information matrix는 다음과 같은 식으로 업데이트한다.\n\n$$f\\leftarrow f+\\nabla log \\pi(a_t; s_t, \\theta)\\nabla log \\pi(a_t; s_t, \\theta)^T$$\n\nT length trajectory에 대해서 f/T를 통해 F의 estimate를 구한다.\n\n### 7.1 Linear Quadratic regulator\n에이전트를 테스트할 환경은 다음과 같은 dynamics를 가지고 있다. $u(t)$는 control signal로서 에이전트의 행동이라고 생각하면 된다. $\\epsilon$은 noise distribution으로 환경에 가해지는 노이즈이다. 에이전트의 목표는 적절한 $u(t)$를 통해 \nx(t)를 0으로 유지하는 것이다. 제어분야에서의 LQR controller 문제이다.\n\n$$\nx(t+1) = 0.7x(t)+u(t)+\\epsilon(t)\n$$\n\nx(t)를 0으로 유지하기 위해서 $x(t)^2$를 cost로 잡고 이 cost를 최소화하도록 학습한다. 이 시스템을 linear라고 부르는 것은 아래 그림과 같이 선형의 형태를 띄기 때문이다. 이 논문에서 실험할 때는 이 그림에서의 system에 noise를 더해준 것이다. [그림 출처](https://stanford.edu/class/ee363/lectures/dlqr.pdf)\n\n<img src='https://www.dropbox.com/s/vz0q97lcek4oti5/Screenshot%202018-06-08%2014.21.10.png?dl=1'>\n\n이 실험에서 사용한 parameterized policy는 다음과 같다. parameter가 $\\theta_1$과 $\\theta_2$ 밖에 없는 상당히 간단한 policy이다. \n\n$$\n\\pi(u;x,\\theta) \\propto exp(\\theta_1 s_1 x^2 + \\theta_2 s_2 x)\n$$\n\n이 policy를 간단히 numpy와 matplotlib를 이용해서 그려봤다. $\\theta_1$과 $theta_2$를 (0.5, 0.5), (1, 0), (0, 1)로 하고 $s_1$과 $s_2$는 1로 두었다. x는 -1에서 1까지의 범위로 그렸다. x를 0으로 유지하려면 u(t)가 -와 +가 둘 다 가능해야할 것 같은데 위 식으로만 봐서는 action이 하나이고 그 action일 확률을 표시하는 것처럼 나왔다. 아마 -1과 +1이 u(t)가 될 수 있는데 그 중 +1을 선택할 확률이 위와 같이 되는게 아닌가 싶다.\n<center><img src='https://www.dropbox.com/s/v69qyrwn7zurk8c/Screenshot%202018-06-08%2014.57.07.png?dl=1' width='500px'></center>\n\n다음 그림은 1-d LQR을 학습한 그래프이다. cost가 $x^2$이기 때문에 cost가 0으로 갈수록 agent는 0에서 안정적으로 머무른다고 볼 수 있다. 6개의 선 중에서 오른쪽 세 개가 일반적인 gradient 방법을 사용해서 학습한 결과이다. 그리고 왼쪽의 세 개의 선이 natural policy gradient를 통해 학습한 학습 곡선이다. 일반 gradient 방법보다 natural gradient가 훨씬 빠르게 학습한다(time 축이 log scale인 것을 감안하자).\n\n하지만 문제가 있다. npg를 학습한 세 개의 곡선은 $\\theta$를 rescale 한 것이다. $\\theta$앞에 곱해지는 숫자에 따라 학습의 과정이 다르다. 이 것은 coordinate에 따라 steepest gradient가 다르게 측정된다는 것이다. 즉, covariant gradient가 아니라는 뜻이다. 이 논문에서는 natural gradient를 통해 gradient가 covariant하도록 만들고 싶었는데 실패한 것이다. \n\n\n<center><img src=\"https://www.dropbox.com/s/fhn8cgje0rdws0i/Screenshot%202018-06-08%2023.13.37.png?dl=1\" width=\"300px\"></center>\n\nnatural gradient가 covariant하지 않은 이유는 Fisher Information Matrix가 예상했던 바와는 달리 invariant metric이 아니기 때문이다. 또한 FIM이 invariant metric이 아닌 이유는 FIM을 계산할 때 $\\rho_s$가 곱해지기 때문이다(state distribution에 대한 expectation. $\\rho_s$가 곱해지는 것이 invariant에 미치는 영향이 무엇인지는 모르겠다). 하지만 여전히 의의가 있는 것은 기존 gradient 방법들보다 훨씬 빠르게 학습한다는 것이다.\n\n### 7.2 simple 2-state MDP\n이제 다른 예제에서 NPG를 테스트한다. 2개의 state만 가지는 MDP를 고려해보자. [그림출처](http://repository.cmu.edu/cgi/viewcontent.cgi?article=1080&context=robotics). 그림으로보면 다음과 같다. x=0 상태와 x=1 상태 두 개가 존재한다. 에이전트는 각 상태에서 다시 자신의 상태로 되돌아오는 행동을 하거나 다른 상태로 가는 행동을 할 수 있다. 상태 x=0에서 다시 자기 자신으로 돌아오면 1의 보상을 받고 상태 x=1에서 자기 자신으로 돌아오면 2의 보상을 받는다. 따라서 결국 optimal policy는 상태 x=1에서 계속 자기 자신으로 돌아오는 행동을 취하는 것이다. \n\n<img src=\"https://www.dropbox.com/s/g1x9yknzsrip59i/Screenshot%202018-06-08%2023.06.50.png?dl=1\">\n\n문제를 좀 어렵게 만들기 위해 state distribution을 다음과 같이 설정한다. 즉, 대부분의 경우에 상태 x=0에서 에이전트가 시작하는 것이다. \n\n$$\n\\rho(x=0)=0.8,  \\rho(x=1)=0.2\n$$\n\n일반적인 gradient 방법을 사용하면 다음과 같은 policy gradient 식에 따라서 업데이트를 하게 된다. 이 때, $\\rho(s)$가 gradient에 곱해지므로 상태적으로 상태 0에서의 gradient 값이 커지게 된다. 따라서 에이전트는 상태 0에서의 gradient(상태 0에서 스스로에게 돌아오는 행동을 취하도록 정책을 업데이트하는 gradient)를 따라 parameterized policy를 update한다. 따라서 아래 그림의 첫번째 그림에서처럼 Reward가 1에서 오랜 시간동안 머무른다. 즉, 에이전트가 상태 0에서 self-loop를 계속 돌고 있다는 것이다. $\\rho(x=0)$가 $10^{-7}$까지 떨어진다.\n\n$$\\nabla\\eta(\\theta)=\\sum_{s,a}\\rho^{\\pi}(s)\\nabla\\pi(a;s,\\theta)Q^{\\pi}(s,a)$$\n\n<center><img src=\"https://www.dropbox.com/s/xtb77mfazbppnss/Screenshot%202018-06-08%2023.14.24.png?dl=1\" width=\"300px\"></center>\n\n하지만 NPG를 사용할 경우에는 훨씬 빠르게 average reward가 2에 도달한다. gradient 방법이 $1.7X10^(7)$정도의 시간만에 2에 도달한 반면 NPG는 2만에 도달한다. 또한 $\\rho(x=0)$가 $10^{-5}$이하로 떨어지지 않는다.\n\n한 가지 그래프를 더 살펴보자. 다음 그래프는 parameter $\\theta$가 업데이트 되는 과정을 보여준다. 이 그래프에서는 parameter가 2개 있는 것이다. 일반적인 gradient가 아래 그래프에서 실선에 해당한다. 이 실선의 그래프는 보면 처음부터 중반까지 $\\theta_i$만 거의 업데이트하는 것을 볼 수 있다. 그에 비해 NPG는 두 개의 parameter를 균등하게 업데이트하는 것을 볼 수 있다. \n\n<center><img src=\"https://www.dropbox.com/s/g7pazozw2k6rd7x/Screenshot%202018-06-08%2023.23.25.png?dl=1\" width=\"300px\"></center>\n\npolicy가 $\\pi(a;s,\\theta)\\propto exp(\\theta_{sa})$일 때, 다음과 같이 $F_{-1}$이 gradient 앞에 weight로 곱해지는데 이게 $\\rho$와는 달리 각 parameter에 대해 균등하다. 따라서 위 그래프에서와 같이 각 parameter는 비슷한 비율로 업데이트가 되는 것이다.\n\n$$\\bar{\\nabla}\\eta(\\theta) = F^{-1}\\nabla\\eta(\\theta)$$\n\n### 7.3 Tetris\nNPG를 테스트할 tetris 예제는 Neuro Dynamic Programming 책에 소개되어있다. 다음 그림은 tetris 예제를 보여준다. 보통 그림에서와 같이 state의 feature를 정해준다. [그림 출처](http://slideplayer.com/slide/5215520/)\n\n<img src=\"https://www.dropbox.com/s/y1halso9yermy8s/Screenshot%202018-06-08%2023.44.34.png?dl=1\">\n\n이 예제에서도 exponantial family로 policy를 표현한다. $\\pi(a;s,\\theta) \\propto exp(\\theta^T\\phi_{sa})$ 로 표현한다.\n\ntetris는 linear function approximator와 greedy policy iteration을 사용할 경우 performance가 갑자기 떨어지는 현상이 있다. 밑의 그림에서 A의 spike가 있는 그래프가 이 경우이다. 그 밑에 낮게 누워있는 그래프는 일반적인 policy gradient 방법이다. 하지만 Natural policy gradient를 사용할 경우 B 그림에서 오른쪽 그래프와 같이 성능개선이 뚜렷하다. Policy Iteration 처럼 성능이 뚝 떨어지지 않고 안정적으로 유지한다. 또한 그림 C에서 보는 것처럼 오른쪽 그래프인 일반적인 gradient 방법보다 훨씬 빠르게 학습하는 것을 볼 수 있다.\n\n<img src=\"https://www.dropbox.com/s/pr6s2qrqaic0wyj/Screenshot%202018-06-08%2023.40.16.png?dl=1\">\n\n\n## 8. Discussion\n---\n\n- natural gradient method는 policy iteration에서와 같이 greedy action을 선택하도록 학습됌\n- line search와 함께 쓰면 natural gradient method는 더 policy iteration 같아짐\n- greedy policy iteration에서와는 달리 performance improvement가 보장됌\n- 하지만 F(Fisher information matrix)가 asymtotically Hessian으로 수렴하지 않음. asymtotically conjugate gradient method(Hessian의 inverse를 approx.로 구하는 방법)가 더 좋아 보일 수 있음\n- 하지만 Hessian이 항상 informative하지 않고(hessian이 어떤 정보를 주려면 positive definite와 같은 성질을 가져서 해당 함수가 convex인 것을 알 수 있다든지의 경우를 이야기하는데 hessian이 항상 positive definite가 아닐 수 있다는 것이다) tetris에서 봤듯이 natural gradient method가 더 효율적일 수 있음(pushing the policy toward choosing greedy optimal actions)\n- conjugate gradient method가 좀 더 maximum에 빠르게 수렴하지만, performance는 maximum에서 거의 안변하므로 좋다고 말하기 어려움(?). 이 부분에 대해서 추가적인 연구 필요.\n\namet, consectetur adipisicing elit. Vitae ipsum, voluptatem quis officiis inventore dolor totam deserunt, possimus similique eum, accusantium adipisci doloremque omnis excepturi quasi, suscipit repellendus quibusdam? Veritatis.","slug":"2018-06-15-npg","published":1,"updated":"2018-06-16T11:54:22.936Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjiinnj6m0001j28ap6bd88y6","content":"<h1 id=\"A-Natural-Policy-Gradient-2001\"><a href=\"#A-Natural-Policy-Gradient-2001\" class=\"headerlink\" title=\"A Natural Policy Gradient [2001]\"></a>A Natural Policy Gradient [2001]</h1><p><img src=\"https://www.dropbox.com/s/it82tfhfmhg9uwp/Screenshot%202018-06-10%2010.58.52.png?dl=1\"></p>\n<ul>\n<li>논문 저자: Sham Kakade</li>\n<li>논문 링크: <a href=\"https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf\" target=\"_blank\" rel=\"noopener\">https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf</a></li>\n<li>함께 보면 좋을 논문: <ul>\n<li><a href=\"hhttps://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf\" target=\"_blank\" rel=\"noopener\">Policy Gradient Methods for<br>Reinforcement Learning with Function<br>Approximation (2000)</a></li>\n<li><a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.452.7280&amp;rep=rep1&amp;type=pdf\" target=\"_blank\" rel=\"noopener\">Natural Gradient Works Efficiently in Learning(1998)</a></li>\n</ul>\n</li>\n<li>논문을 보는 이유: TRPO와 NPG는 관련이 많기 때문에 TRPO를 더 잘 이해하기 위해 봄</li>\n</ul>\n<h2 id=\"1-Abstract\"><a href=\"#1-Abstract\" class=\"headerlink\" title=\"1. Abstract\"></a>1. Abstract</h2><hr>\n<ul>\n<li>natural gradient method를 policy gradient에 적용</li>\n<li>natural gradient는 steepest descent direction을 가짐</li>\n<li>gradient descent는 parameter를 한 번에 많이 update 할 수 없는 반면, natural gradient는 가장 좋은 action을 고르도록 학습이 됌 (sutton 논문에서와 같이 compatible value function을 사용할 경우 policy iteration에서 policy improvement 1 step의 과정에서)</li>\n<li>simple MDP와 tetris MDP에서 테스트함. 성능이 많이 향상</li>\n</ul>\n<h2 id=\"2-Personal-Interpretation-and-Thinking\"><a href=\"#2-Personal-Interpretation-and-Thinking\" class=\"headerlink\" title=\"2. Personal Interpretation and Thinking\"></a>2. Personal Interpretation and Thinking</h2><hr>\n<p>(개인생각) 뉴럴넷을 사용할 경우 gradient가 steepest direction이 아닌 경우가 많다. 뉴럴넷의 parameter space가 우리가 보통 생각하는 직선으로 쭉쭉 뻗어있는 Euclidean space가 아니다. 좀 더 일반적으로는 구의 표면과 같이 휘어져있는 공간 즉, 리만 공간(Riemannian space)로 표현할 수 있다. 이와 같은 공간에서는 natural gradient가 steepest direction이 된다는 연구가 이뤄지고 있었다. 강화학습의 policy gradient은 objective function의 gradient를 따라 policy를 업데이트한다. 이 때, policy는 parameterized 되는데 이 경우에도 gradient 대신에 natural gradient가 좋다는 것을 실험해보는 논문이다. </p>\n<p>gradient가 non-covariant 해서 생기는 문제는 간단히 말하자면 다음과 같다. policy가 parameterized된 상황에서는 같은 policy라도 다른 parameter를 가질 수 있다. 이 때, steepest direction은 두 경우에 같은 방향을 가리켜야하는데 non-covariant한 경우 그렇지 못하다. 이것은 결국 느린 학습으로 연결이 된다. </p>\n<p>논문에서 2차미분 방법론들과 짧게 비교를 한다. 하지만 2차미분을 이용한 다른 방법들과의 비교가 생각보다 없는 점이 아쉽다.(Hessian을 이용한다거나 conjugate gradient method를 이용한다거나). 실험을 통해 FIM이 hessian에 수렴안하는 거라던지 Hessian 방법론이 local maxima 부근에서 상당히 느리다던지의 결과를 보여줬었으면 좋았을 것 같다. </p>\n<p>또한 natural gradient 만으로 업데이트하면 policy의 improvement보장이 안될 수 있다. policy의 improvement를 보장하기 위해 line search도 써야하는데 line search를 어떻게 쓰는지에 대한 자세한 언급이 없다. 즉, 자세한 algorithm 설명이 없다.</p>\n<p>natural policy gradient 논문은 natural gradient + policy gradient를 처음 적용했다는데 의의가 있다. 하지만 이 논문이 문제 삼은 gradient는 non-covariant하다라는 문제를 natural gradient를 통해 해결하지 못했다(Experiment를 통해 covariant gradient가 되지 못했다는 것이 보인다). NPG의 뒤를 잇는 논문이 “covariant policy search”와 “natural actor-critic”에서 covariant하지 못하다는 것을 해결하기 위해 Fisher Information Matrix를 sample 하나 하나에 대해서 구하는 것이 아니라 trajectory 전체에 대해서 구한다. </p>\n<p>또한 논문은 pg의 두 가지 세팅 중에 average-reward setting(infinite horizon)에서만 NPG를 다룬다. “covariant policy search” 논문에서는 average-reward setting과 start-state setting 모두에 대해서 npg를 적용한다. </p>\n<p>natural gradient + policy gradient를 처음 제시했다는 것은 좋지만 npg 학습의 과정을 자세하게 설명하지 않았고 다른 2차 미분 방법들과 비교를 많이 하지 않은 점이 아쉬운 논문이다.</p>\n<h2 id=\"3-Introduction\"><a href=\"#3-Introduction\" class=\"headerlink\" title=\"3. Introduction\"></a>3. Introduction</h2><hr>\n<ul>\n<li>direct policy gradient method는 future reward의 gradient를 따라 policy를 update함</li>\n<li>하지만 gradient descent는 non-covariant</li>\n<li>이 논문에서는 covarient gradient를 제시함 = natural gradient</li>\n<li>natural gradient와 policy iteration의 연관성을 설명하겠음: natural policy gradient is moving toward choosing a greedy optimal action (이런 연결점은 아마도 step-size를 덜 신경쓰고 싶어서 그런게 아닌가 싶다)</li>\n</ul>\n<p>논문의 Introduction 부분에 다음 멘트가 있다. 이 글만 봐서는 이해가 안갔는데 Mackay 논문에 좀 더 자세히 나와있다.</p>\n<p><img src=\"https://www.dropbox.com/s/41xhhr7lgfk24a1/Screenshot%202018-06-10%2011.45.18.png?dl=1\"></p>\n<p><a href=\"http://www.inference.org.uk/mackay/ica.pdf\" target=\"_blank\" rel=\"noopener\">Mackay</a>논문에서는 다음과 같이 언급하고 있다. Back-propagation을 사용할 경우에 learning rate를 dimension에 1/n로 사용하면 수렴한다는 것이 증명됐다. 하지만 너무 느리다. </p>\n<p><img src=\"https://www.dropbox.com/s/us9ezc7vxgrkez6/Screenshot%202018-06-10%2011.47.21.png?dl=1\"></p>\n<h2 id=\"4-A-Natural-Gradient\"><a href=\"#4-A-Natural-Gradient\" class=\"headerlink\" title=\"4. A Natural Gradient\"></a>4. A Natural Gradient</h2><hr>\n<h3 id=\"4-1-환경에-대한-설정\"><a href=\"#4-1-환경에-대한-설정\" class=\"headerlink\" title=\"4.1 환경에 대한 설정\"></a>4.1 환경에 대한 설정</h3><p>이 논문에서 제시하는 학습 환경은 다음과 같다.</p>\n<ul>\n<li>MDP: tuple $(S, s_0, A, R, P)$</li>\n<li>$S$: a finite set of states</li>\n<li>$s_0$: a start state</li>\n<li>$A$: a finite set of actions</li>\n<li>$R$: reward function $R: S \\times A -&gt; [0, R_{max}]$</li>\n<li>$\\pi(a;s, \\theta)$: stochastic policy parameterized by $\\theta$</li>\n<li>모든 정책 $\\pi$는 ergodic: stationary distribution $\\rho^{\\pi}$이 잘 정의되어있음</li>\n<li>이 논문에서는 sutton의 pg 논문의 두 세팅(start-state formulation, average-reward formulation) 중에 두 번째인 average-reward formulation을 가정 </li>\n<li>performance or average reward: $\\eta(\\pi)=\\sum_{s,a}\\rho^{\\pi}(s)\\pi(a;s)R(s,a)$</li>\n<li>state-action value: $Q^{\\pi}(s,a)=E_{\\pi}[\\sum_{t=0}^{\\infty}R(s_t, a_t)-\\eta(\\pi)\\vert s_0=s, a_0=a]$</li>\n<li>정책이 $\\theta$로 parameterize되어있으므로 performance는 $\\eta(\\pi_{\\theta})$인데 $\\eta(\\theta)$로 쓸거임</li>\n</ul>\n<h3 id=\"4-2-Natural-Gradient\"><a href=\"#4-2-Natural-Gradient\" class=\"headerlink\" title=\"4.2 Natural Gradient\"></a>4.2 Natural Gradient</h3><h4 id=\"4-2-1-Policy-gradient-Theorem\"><a href=\"#4-2-1-Policy-gradient-Theorem\" class=\"headerlink\" title=\"4.2.1 Policy gradient Theorem\"></a>4.2.1 Policy gradient Theorem</h4><p>서튼 pg 논문의 policy gradient theorem에 따라 exact gradient of the average reward는 다음과 같다. 다음 수식이 어떻게 유도되었는지, 어떤 의미인지 모른다면 서튼 pg 논문을 통해 제대로 이해하는 것이 좋다.</p>\n<p>$$\\nabla\\eta(\\theta)=\\sum_{s,a}\\rho^{\\pi}(s)\\nabla\\pi(a;s,\\theta)Q^{\\pi}(s,a)$$</p>\n<p>steepest descent direction of $\\eta(\\theta)$는 $\\eta(\\theta + d\\theta)$를 최소화하는 $d\\theta$로 정의된다. 이 때, $\\vert d\\theta \\vert^2$가 일정 크기 이하인 것으로 제약조건을 준다(held to small constant). Euclidian space에서는 $\\eta(\\theta)$가 steepest direction이지만 Riemannian space에서는 natural gradient가 steepest direction이다. </p>\n<h4 id=\"4-2-2-Natural-gradient-증명\"><a href=\"#4-2-2-Natural-gradient-증명\" class=\"headerlink\" title=\"4.2.2 Natural gradient 증명\"></a>4.2.2 Natural gradient 증명</h4><p>Riemannian space에서 거리는 다음과 같이 정의된다. $G(\\theta)$는 특정한 양수로 이루어진 matrix이다.</p>\n<p>$$\\vert d\\theta \\vert^2=\\sum_{ij}(\\theta)d\\theta_id\\theta_i=d\\theta^TG(\\theta)d\\theta$$</p>\n<p>이 수식은 Natural Gradient Works Efficiently in Learning 논문에서 증명되어있다. 다음은 natural gradient 증명이다. </p>\n<p>steepest direction을 구할 때 $\\theta$의 크기를 제약조건을 준다. 제약조건은 다음과 같다. </p>\n<p>$$\\vert d\\theta \\vert^2 = \\epsilon^2$$</p>\n<p>그리고 steepest vector인 $d\\theta$는 다음과 같이 정의할 수 있다. </p>\n<p>$$d\\theta = \\epsilon a$$</p>\n<p>$$\\vert a \\vert^2=a^TG(\\theta)a = 1$$</p>\n<p>이 때, $a$가 steepest direction unit vector이 되려면 다음 수식을 최소로 만들어야 한다. (이 수식은 잘 모르겠지만 $\\theta$에서의 1차근사를 가정하는게 아닌가 싶다.</p>\n<p>$$\\eta(\\theta + d\\theta) = \\eta(\\theta) + \\epsilon\\nabla\\eta(\\theta)^Ta$$</p>\n<p>위 수식이 제약조건 아래 최소가 되는 $a$를 구하기 위해 Lagrangian method를 사용한다. Lagrangian method를 모른다면 <a href=\"https://en.wikipedia.org/wiki/Lagrange_multiplier\" target=\"_blank\" rel=\"noopener\">위키피디아</a>를 참고하는 것을 추천한다. 위 수식이 최소라는 것은 $\\nabla\\eta(\\theta)^Ta$가 최소라는 것이다. </p>\n<p>$$\\frac{\\partial}{\\partial a_i}(\\nabla\\eta(\\theta)^Ta - \\lambda a^TG(\\theta)a)=0$$</p>\n<p>따라서 $(\\nabla\\eta(\\theta)^Ta - \\lambda a^TG(\\theta)a)=0$는 상수이다. 상수를 미분하면 0이므로 이 식을 $a$로 미분한다. 그러면 다음과 같다. steepest direction을 구한 것이다.</p>\n<p>$$\\nabla\\eta(\\theta) = 2 \\lambda G(\\theta)a$$</p>\n<p>$$a=\\frac{1}{2\\lambda}G^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>이 때, 다음 식을 natural gradient라고 정의한다.</p>\n<p>$$\\bar{\\nabla}\\eta(\\theta) = G^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>natural gradient를 이용한 업데이트는 다음과 같다. </p>\n<p>$$\\theta_{t+1}=\\theta_t - \\alpha_tG^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>여기까지는 natural gradient의 증명이었다. 이 natural gradient를 policy gradient에 적용한 것이 natural policy gradient이다. natural policy gradient는 다음과 같이 정의된다.</p>\n<p>$$\\bar{\\nabla}\\eta(\\theta) = F^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>$G$ 대신 $F$를 사용했는데 $F$는 Fisher information matix이다. 수식은 다음과 같다.</p>\n<p>$$F(\\theta) = E_{\\rho^\\pi(s)}[F_s(\\theta)]$$</p>\n<p>$$F_s(\\theta)=E_{\\pi(a;s,\\theta)}[\\frac{\\partial log \\pi(a;s, \\theta)}{\\partial \\theta_i}\\frac{\\partial log \\pi(a;s, \\theta)}{\\partial\\theta_j}]$$</p>\n<p>왜 G가 F가 되는지는 아직 잘 모르겠다. 거리라는 개념을 표현하려면 </p>\n<h2 id=\"5-The-Natural-Gradient-and-Policy-Iteration\"><a href=\"#5-The-Natural-Gradient-and-Policy-Iteration\" class=\"headerlink\" title=\"5. The Natural Gradient and Policy Iteration\"></a>5. The Natural Gradient and Policy Iteration</h2><hr>\n<h3 id=\"5-1-Theorem-1\"><a href=\"#5-1-Theorem-1\" class=\"headerlink\" title=\"5.1 Theorem 1\"></a>5.1 Theorem 1</h3><p>sutton pg 논문에 따라 $Q^{\\pi}(s,a)$를 approximation한다. approximate하는 함수 $f^{\\pi}(s,a;w)$는 다음과 같다.(compatible value function)</p>\n<p>$$f^{\\pi}(s,a;w)=w^T\\psi^{\\pi}(s,a)$$</p>\n<p>$$\\psi^{\\pi}(s,a) = \\nabla log\\pi(a;s,\\theta)$$</p>\n<p>$w$는 원래 approximate하는 함수 $Q$와 $f$의 차이를 줄이도록 학습한다(mean square error). 수렴한 local minima의 $w$를 $\\bar{w}$라고 하겠다. 에러는 다음과 같은 수식으로 나타낸다. </p>\n<p>$$\\epsilon(w,\\pi)\\equiv\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)(f^{\\pi}(s,a;w)-Q^{\\pi}(s,a))^2$$</p>\n<p>위 식이 local minima이면 미분값이 0이다. $w$에 대해서 미분하면 다음과 같다. </p>\n<p>$$\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)(\\psi^{\\pi}(s,a)^T\\bar{w}-Q^{\\pi}(s,a))=0$$</p>\n<p>$$(\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)\\psi^{\\pi}(s,a)^T)\\bar{w}=\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)Q^{\\pi}(s,a))$$</p>\n<p>이 때, 위 식의 우변은 $\\psi$의 정의에 의해 policy gradient가 된다. 또한 왼쪽 항에서는 Fisher information matrix가 나온다.</p>\n<p>$$F(\\theta)=\\sum_{s,a}\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)\\psi^{\\pi}(s,a)=E_{\\rho^\\pi(s)}[F_s(\\theta)]$$</p>\n<p>따라서 다음과 같다.</p>\n<p>$$F(\\theta)\\bar{w}=\\nabla\\eta(\\theta)$$</p>\n<p>$$\\bar{w}=F(\\theta)^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>이 식은 natural gradient 식과 동일하다. 이 식은 policy가 update 될 때, value function approximator의 parameter 방향으로 이동한다는 것을 의미한다. function approximation이 정확하다면 그 parameter의 natural policy gradient와 inner product가 커야한다. </p>\n<h3 id=\"5-2-Theorem-2-Greedy-Polict-Improvement\"><a href=\"#5-2-Theorem-2-Greedy-Polict-Improvement\" class=\"headerlink\" title=\"5.2 Theorem 2: Greedy Polict Improvement\"></a>5.2 Theorem 2: Greedy Polict Improvement</h3><p>natural policy gradient가 단순히 더 좋은 행동을 고르도록 학습하는게 아니라 가장 좋은 (greedy) 행동을 고르도록 학습한다는 것을 증명하는 파트이다. 이것을 일반적인 형태의 policy에 대해서 증명하기 전에 exponential 형태의 policy에 대해서 증명하는 것이 Theorem 2이다.</p>\n<p>policy를 다음과 같이 정의한다.</p>\n<p>$$\\pi(a;s,\\theta) \\propto exp(\\theta^T\\phi_{sa})$$</p>\n<p>$\\bar{\\nabla}\\eta(\\theta)$가 0이 아니고 $\\bar{w}$는 approximation error를 최소화한 $w$라고 가정한다. 이 상태에서 natural gradient update를 생각해보자. policy gradient는 gradient ascent임을 기억하자.</p>\n<p>$$\\theta_{t+1}=\\theta_t + \\alpha_t\\bar{\\nabla}\\eta(\\theta)$$</p>\n<p>이 때 $\\alpha$가 learning rate로 parameter를 얼마나 업데이트하는지를 결정한다. 이 값을 무한대로 늘렸을 때 policy가 어떻게 업데이트되는지 생각해보자. </p>\n<p>$$\\pi_{\\infty}(a;s)=lim_{\\alpha\\rightarrow\\infty}\\pi(a;s,\\theta+\\alpha\\bar{\\nabla}\\eta(\\theta))-(1)$$</p>\n<p>function approximator는 다음과 같다. </p>\n<p>$$f^{\\pi}(s,a;w)=w^T\\psi^{\\pi}(s,a)$$</p>\n<p>Theorem 1에 의해 위 식은 아래와 같이 쓸 수 있다.</p>\n<p>$$f^{\\pi}(s,a;w)=\\bar{\\nabla}\\eta(\\theta)^T\\psi^{\\pi}(s,a)$$</p>\n<p>$\\theta$의 정의에 의해 $\\psi$는 다음과 같다.</p>\n<p>$$\\psi^{\\pi}(s,a)=\\phi_{sa}-E_{\\pi(a’;s,\\theta)}[\\phi_{sa’}]$$</p>\n<p>function approximator는 다음과 같이 다시 쓸 수 있다.</p>\n<p>$$f^{\\pi}(s,a;w)=\\bar{\\nabla}\\eta(\\theta)^T(\\phi_{sa}-E_{\\pi(a’;s,\\theta)}[\\phi_{sa’}])$$</p>\n<p>greedy policy improvement가 Q function 값 중 가장 큰 값을 가지는 action을 선택하듯이 여기서도 function approximator의 값이 가장 큰 action을 선택하는 상황을 가정해본다. 이 때 function approximator의 argmax는 다음과 같이 쓸 수 있다.</p>\n<p>$$argmax_{a’}f^{\\pi}(s,a)=argmax_{a’}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa’}$$</p>\n<p>(1) 식을 다시 살펴보자. policy의 정의에 따라 다음과 같이 쓸 수 있다. </p>\n<p>$$\\pi(a;s,\\theta + \\alpha\\bar{\\nabla}\\eta(\\theta)) \\propto exp(\\theta^T\\phi_{sa} + \\alpha\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa})$$</p>\n<p>$\\bar{\\nabla}\\eta(\\theta) \\neq 0$이고 $\\alpha\\rightarrow\\infty$이면 exp안의 항 중에서 뒤의 항이 dominate하게 된다. 여러 행동 중에 $\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa}$가 가장 큰 행동이 있다면 이 행동의 policy probability가 1이 되고 나머지는 0이 된다. 따라서 다음이 성립한다.</p>\n<p>$$\\pi_{\\infty}=0$$ </p>\n<p>if and only if </p>\n<p>$$a \\notin argmax_{a’}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa’}$$</p>\n<p>이 결과로부터 natural policy gradient는 단지 더 좋은 action이 아니라 best action을 고르도록 학습이 된다. 하지만 non-covariant gradient(1차미분) 에서는 그저 더 좋은 action을 고르도록 학습이 된다. 하지만 이 natural policy gradient에 대한 결과는 infinite learning rate 세팅에서만 성립함. 좀 더 일반적인 경우에 대해서 살펴보자.</p>\n<h4 id=\"4-3-Theorem-3\"><a href=\"#4-3-Theorem-3\" class=\"headerlink\" title=\"4.3 Theorem 3\"></a>4.3 Theorem 3</h4><p>Theorem 2에서와는 달리 일반적인 policy를 가정하자(general parameterized policy). Theorem 3는 이 상황에서 natural gradient를 통한 업데이트가 best action를 고르는 방향으로 학습이 된다는 것을 보여준다. </p>\n<p>natural gradien에 따른 policy parameter의 업데이트는 다음과 같다. $\\bar{w}$는 approximation error를 minimize하는 $w$이다.</p>\n<p>$$\\delta\\theta = \\theta’ - \\theta = \\alpha\\bar{\\nabla}\\eta(\\theta)=\\alpha\\bar{w}$$</p>\n<p>policy에 대해서 1차근사를 하면 다음과 같다. </p>\n<p>$$\\pi(a;s,\\theta’)=\\pi(a;s,\\theta)+\\frac{\\partial\\pi(a;s,\\theta)^T}{\\partial\\theta}\\delta\\theta + O(\\delta\\theta^2)$$</p>\n<p>$$=\\pi(a;s,\\theta)(1+\\psi(s,a)^T\\delta\\theta) + O(\\delta\\theta^2)$$</p>\n<p>$$=\\pi(a;s,\\theta)(1+\\alpha\\psi(s,a)^T\\bar{w}) + O(\\delta\\theta^2)$$</p>\n<p>$$=\\pi(a;s,\\theta)(1+\\alpha f^{\\pi}(s,a;\\bar{w}) + O(\\delta\\theta^2)$$</p>\n<p>policy 자체가 function approximator의 크기대로 업데이트가 되므로 local하게 best action의 probability는 커지고 다른 probability의 크기는 작아질 것이다. 하지만 만약 greedy improvement가 된다하더라도 그게 performance의 improvement를 보장하는 것은 아니다. 하지만 line search와 함께 사용할 경우 improvement를 보장할 수 있다. </p>\n<h2 id=\"6-Metrics-and-Curvatures\"><a href=\"#6-Metrics-and-Curvatures\" class=\"headerlink\" title=\"6. Metrics and Curvatures\"></a>6. Metrics and Curvatures</h2><hr>\n<p>다음 식에 해당하는 G는 Fisher Information Matrix만 사용할 수 있는 것이 아니다.</p>\n<p>$$\\vert d\\theta \\vert^2=\\sum_{ij}(\\theta)d\\theta_id\\theta_i=d\\theta^TG(\\theta)d\\theta$$</p>\n<p>이 파트에서는 FIM과 다른 metric 사이의 관계를 다룬다. </p>\n<ul>\n<li>In the different setting of parameter estimation, the Fisher information converges to the <code>Hessian</code>, so it is <a href=\"https://en.wikipedia.org/wiki/Efficiency_(statistics\" target=\"_blank\" rel=\"noopener\">asymptotically efficient</a>#Asymptotic_efficiency)</li>\n<li>이 논문의 경우, 아마리 논문의 ‘blind separation case’와 유사한데 이 때는 꼭 asymtotically efficient하지 않다. 이 말은 즉 2nd order 수렴이 보장되지 않는다는 것이다.</li>\n<li><a href=\"http://www.inference.org.uk/mackay/ica.pdf\" target=\"_blank\" rel=\"noopener\">Mackay</a> 논문에서 hessian에서 data independant한 term을 metric으로 가져오는 방법을 제안했다. 그래서 performance를 2번 미분해보면 다음과 같다. 하지만 다음 식에서는 모든 항이 data dependent하다(Q가 있으니까). 첫 번째 항이 그나마 FIM과의 관련성이 있을 수 있지만 Q 값이 curvature에 weight를 주는 방식 때문에 다르다고 할 수 있다.</li>\n</ul>\n<p>$$<br>\\nabla^2\\eta(\\theta)=\\sum_{sa}\\rho^{\\pi}(s)(\\nabla^2\\pi(a;s)Q^{\\pi}(s,a)+\\nabla\\pi(a;s)\\nabla Q^{\\pi}(s,a)^T+\\nabla Q^{\\pi}(s,a)\\nabla\\pi(a;s)^T)<br>$$</p>\n<ul>\n<li>hessian은 보통 positive definite가 아닐수도 있다. 따라서 local maxima가 될 때까지 Hessian이 사용하기 별로 안좋다. 그리고 local maxima에서는 Hessian보다는 Conjugate methods가 더 효율적이다. </li>\n</ul>\n<p>이 파트에서는 무엇을 말하고 있는지 알기가 어렵다. FIM과 Hessian이 관련이 있다는 것을 알겠다. 하지만 asymtotically efficient와 같은 내용을 모르므로 내용의 이해가 어려웠다.</p>\n<p>Mackay 논문에서 해당 부분은 다음과 같다. </p>\n<p><img src=\"https://www.dropbox.com/s/x4n6z6pdyi7xtb9/Screenshot%202018-06-10%2012.04.13.png?dl=1\"></p>\n<h2 id=\"7-Experiment\"><a href=\"#7-Experiment\" class=\"headerlink\" title=\"7. Experiment\"></a>7. Experiment</h2><hr>\n<p>논문에서는 natural gradient를 simple MDP와 tetris MDP에 대해서 테스트했다. practice에서는 Fisher information matrix는 다음과 같은 식으로 업데이트한다.</p>\n<p>$$f\\leftarrow f+\\nabla log \\pi(a_t; s_t, \\theta)\\nabla log \\pi(a_t; s_t, \\theta)^T$$</p>\n<p>T length trajectory에 대해서 f/T를 통해 F의 estimate를 구한다.</p>\n<h3 id=\"7-1-Linear-Quadratic-regulator\"><a href=\"#7-1-Linear-Quadratic-regulator\" class=\"headerlink\" title=\"7.1 Linear Quadratic regulator\"></a>7.1 Linear Quadratic regulator</h3><p>에이전트를 테스트할 환경은 다음과 같은 dynamics를 가지고 있다. $u(t)$는 control signal로서 에이전트의 행동이라고 생각하면 된다. $\\epsilon$은 noise distribution으로 환경에 가해지는 노이즈이다. 에이전트의 목표는 적절한 $u(t)$를 통해<br>x(t)를 0으로 유지하는 것이다. 제어분야에서의 LQR controller 문제이다.</p>\n<p>$$<br>x(t+1) = 0.7x(t)+u(t)+\\epsilon(t)<br>$$</p>\n<p>x(t)를 0으로 유지하기 위해서 $x(t)^2$를 cost로 잡고 이 cost를 최소화하도록 학습한다. 이 시스템을 linear라고 부르는 것은 아래 그림과 같이 선형의 형태를 띄기 때문이다. 이 논문에서 실험할 때는 이 그림에서의 system에 noise를 더해준 것이다. <a href=\"https://stanford.edu/class/ee363/lectures/dlqr.pdf\" target=\"_blank\" rel=\"noopener\">그림 출처</a></p>\n<p><img src=\"https://www.dropbox.com/s/vz0q97lcek4oti5/Screenshot%202018-06-08%2014.21.10.png?dl=1\"></p>\n<p>이 실험에서 사용한 parameterized policy는 다음과 같다. parameter가 $\\theta_1$과 $\\theta_2$ 밖에 없는 상당히 간단한 policy이다. </p>\n<p>$$<br>\\pi(u;x,\\theta) \\propto exp(\\theta_1 s_1 x^2 + \\theta_2 s_2 x)<br>$$</p>\n<p>이 policy를 간단히 numpy와 matplotlib를 이용해서 그려봤다. $\\theta_1$과 $theta_2$를 (0.5, 0.5), (1, 0), (0, 1)로 하고 $s_1$과 $s_2$는 1로 두었다. x는 -1에서 1까지의 범위로 그렸다. x를 0으로 유지하려면 u(t)가 -와 +가 둘 다 가능해야할 것 같은데 위 식으로만 봐서는 action이 하나이고 그 action일 확률을 표시하는 것처럼 나왔다. 아마 -1과 +1이 u(t)가 될 수 있는데 그 중 +1을 선택할 확률이 위와 같이 되는게 아닌가 싶다.</p>\n<center><img src=\"https://www.dropbox.com/s/v69qyrwn7zurk8c/Screenshot%202018-06-08%2014.57.07.png?dl=1\" width=\"500px\"></center>\n\n<p>다음 그림은 1-d LQR을 학습한 그래프이다. cost가 $x^2$이기 때문에 cost가 0으로 갈수록 agent는 0에서 안정적으로 머무른다고 볼 수 있다. 6개의 선 중에서 오른쪽 세 개가 일반적인 gradient 방법을 사용해서 학습한 결과이다. 그리고 왼쪽의 세 개의 선이 natural policy gradient를 통해 학습한 학습 곡선이다. 일반 gradient 방법보다 natural gradient가 훨씬 빠르게 학습한다(time 축이 log scale인 것을 감안하자).</p>\n<p>하지만 문제가 있다. npg를 학습한 세 개의 곡선은 $\\theta$를 rescale 한 것이다. $\\theta$앞에 곱해지는 숫자에 따라 학습의 과정이 다르다. 이 것은 coordinate에 따라 steepest gradient가 다르게 측정된다는 것이다. 즉, covariant gradient가 아니라는 뜻이다. 이 논문에서는 natural gradient를 통해 gradient가 covariant하도록 만들고 싶었는데 실패한 것이다. </p>\n<center><img src=\"https://www.dropbox.com/s/fhn8cgje0rdws0i/Screenshot%202018-06-08%2023.13.37.png?dl=1\" width=\"300px\"></center>\n\n<p>natural gradient가 covariant하지 않은 이유는 Fisher Information Matrix가 예상했던 바와는 달리 invariant metric이 아니기 때문이다. 또한 FIM이 invariant metric이 아닌 이유는 FIM을 계산할 때 $\\rho_s$가 곱해지기 때문이다(state distribution에 대한 expectation. $\\rho_s$가 곱해지는 것이 invariant에 미치는 영향이 무엇인지는 모르겠다). 하지만 여전히 의의가 있는 것은 기존 gradient 방법들보다 훨씬 빠르게 학습한다는 것이다.</p>\n<h3 id=\"7-2-simple-2-state-MDP\"><a href=\"#7-2-simple-2-state-MDP\" class=\"headerlink\" title=\"7.2 simple 2-state MDP\"></a>7.2 simple 2-state MDP</h3><p>이제 다른 예제에서 NPG를 테스트한다. 2개의 state만 가지는 MDP를 고려해보자. <a href=\"http://repository.cmu.edu/cgi/viewcontent.cgi?article=1080&amp;context=robotics\" target=\"_blank\" rel=\"noopener\">그림출처</a>. 그림으로보면 다음과 같다. x=0 상태와 x=1 상태 두 개가 존재한다. 에이전트는 각 상태에서 다시 자신의 상태로 되돌아오는 행동을 하거나 다른 상태로 가는 행동을 할 수 있다. 상태 x=0에서 다시 자기 자신으로 돌아오면 1의 보상을 받고 상태 x=1에서 자기 자신으로 돌아오면 2의 보상을 받는다. 따라서 결국 optimal policy는 상태 x=1에서 계속 자기 자신으로 돌아오는 행동을 취하는 것이다. </p>\n<p><img src=\"https://www.dropbox.com/s/g1x9yknzsrip59i/Screenshot%202018-06-08%2023.06.50.png?dl=1\"></p>\n<p>문제를 좀 어렵게 만들기 위해 state distribution을 다음과 같이 설정한다. 즉, 대부분의 경우에 상태 x=0에서 에이전트가 시작하는 것이다. </p>\n<p>$$<br>\\rho(x=0)=0.8,  \\rho(x=1)=0.2<br>$$</p>\n<p>일반적인 gradient 방법을 사용하면 다음과 같은 policy gradient 식에 따라서 업데이트를 하게 된다. 이 때, $\\rho(s)$가 gradient에 곱해지므로 상태적으로 상태 0에서의 gradient 값이 커지게 된다. 따라서 에이전트는 상태 0에서의 gradient(상태 0에서 스스로에게 돌아오는 행동을 취하도록 정책을 업데이트하는 gradient)를 따라 parameterized policy를 update한다. 따라서 아래 그림의 첫번째 그림에서처럼 Reward가 1에서 오랜 시간동안 머무른다. 즉, 에이전트가 상태 0에서 self-loop를 계속 돌고 있다는 것이다. $\\rho(x=0)$가 $10^{-7}$까지 떨어진다.</p>\n<p>$$\\nabla\\eta(\\theta)=\\sum_{s,a}\\rho^{\\pi}(s)\\nabla\\pi(a;s,\\theta)Q^{\\pi}(s,a)$$</p>\n<center><img src=\"https://www.dropbox.com/s/xtb77mfazbppnss/Screenshot%202018-06-08%2023.14.24.png?dl=1\" width=\"300px\"></center>\n\n<p>하지만 NPG를 사용할 경우에는 훨씬 빠르게 average reward가 2에 도달한다. gradient 방법이 $1.7X10^(7)$정도의 시간만에 2에 도달한 반면 NPG는 2만에 도달한다. 또한 $\\rho(x=0)$가 $10^{-5}$이하로 떨어지지 않는다.</p>\n<p>한 가지 그래프를 더 살펴보자. 다음 그래프는 parameter $\\theta$가 업데이트 되는 과정을 보여준다. 이 그래프에서는 parameter가 2개 있는 것이다. 일반적인 gradient가 아래 그래프에서 실선에 해당한다. 이 실선의 그래프는 보면 처음부터 중반까지 $\\theta_i$만 거의 업데이트하는 것을 볼 수 있다. 그에 비해 NPG는 두 개의 parameter를 균등하게 업데이트하는 것을 볼 수 있다. </p>\n<center><img src=\"https://www.dropbox.com/s/g7pazozw2k6rd7x/Screenshot%202018-06-08%2023.23.25.png?dl=1\" width=\"300px\"></center>\n\n<p>policy가 $\\pi(a;s,\\theta)\\propto exp(\\theta_{sa})$일 때, 다음과 같이 $F_{-1}$이 gradient 앞에 weight로 곱해지는데 이게 $\\rho$와는 달리 각 parameter에 대해 균등하다. 따라서 위 그래프에서와 같이 각 parameter는 비슷한 비율로 업데이트가 되는 것이다.</p>\n<p>$$\\bar{\\nabla}\\eta(\\theta) = F^{-1}\\nabla\\eta(\\theta)$$</p>\n<h3 id=\"7-3-Tetris\"><a href=\"#7-3-Tetris\" class=\"headerlink\" title=\"7.3 Tetris\"></a>7.3 Tetris</h3><p>NPG를 테스트할 tetris 예제는 Neuro Dynamic Programming 책에 소개되어있다. 다음 그림은 tetris 예제를 보여준다. 보통 그림에서와 같이 state의 feature를 정해준다. <a href=\"http://slideplayer.com/slide/5215520/\" target=\"_blank\" rel=\"noopener\">그림 출처</a></p>\n<p><img src=\"https://www.dropbox.com/s/y1halso9yermy8s/Screenshot%202018-06-08%2023.44.34.png?dl=1\"></p>\n<p>이 예제에서도 exponantial family로 policy를 표현한다. $\\pi(a;s,\\theta) \\propto exp(\\theta^T\\phi_{sa})$ 로 표현한다.</p>\n<p>tetris는 linear function approximator와 greedy policy iteration을 사용할 경우 performance가 갑자기 떨어지는 현상이 있다. 밑의 그림에서 A의 spike가 있는 그래프가 이 경우이다. 그 밑에 낮게 누워있는 그래프는 일반적인 policy gradient 방법이다. 하지만 Natural policy gradient를 사용할 경우 B 그림에서 오른쪽 그래프와 같이 성능개선이 뚜렷하다. Policy Iteration 처럼 성능이 뚝 떨어지지 않고 안정적으로 유지한다. 또한 그림 C에서 보는 것처럼 오른쪽 그래프인 일반적인 gradient 방법보다 훨씬 빠르게 학습하는 것을 볼 수 있다.</p>\n<p><img src=\"https://www.dropbox.com/s/pr6s2qrqaic0wyj/Screenshot%202018-06-08%2023.40.16.png?dl=1\"></p>\n<h2 id=\"8-Discussion\"><a href=\"#8-Discussion\" class=\"headerlink\" title=\"8. Discussion\"></a>8. Discussion</h2><hr>\n<ul>\n<li>natural gradient method는 policy iteration에서와 같이 greedy action을 선택하도록 학습됌</li>\n<li>line search와 함께 쓰면 natural gradient method는 더 policy iteration 같아짐</li>\n<li>greedy policy iteration에서와는 달리 performance improvement가 보장됌</li>\n<li>하지만 F(Fisher information matrix)가 asymtotically Hessian으로 수렴하지 않음. asymtotically conjugate gradient method(Hessian의 inverse를 approx.로 구하는 방법)가 더 좋아 보일 수 있음</li>\n<li>하지만 Hessian이 항상 informative하지 않고(hessian이 어떤 정보를 주려면 positive definite와 같은 성질을 가져서 해당 함수가 convex인 것을 알 수 있다든지의 경우를 이야기하는데 hessian이 항상 positive definite가 아닐 수 있다는 것이다) tetris에서 봤듯이 natural gradient method가 더 효율적일 수 있음(pushing the policy toward choosing greedy optimal actions)</li>\n<li>conjugate gradient method가 좀 더 maximum에 빠르게 수렴하지만, performance는 maximum에서 거의 안변하므로 좋다고 말하기 어려움(?). 이 부분에 대해서 추가적인 연구 필요.</li>\n</ul>\n<p>amet, consectetur adipisicing elit. Vitae ipsum, voluptatem quis officiis inventore dolor totam deserunt, possimus similique eum, accusantium adipisci doloremque omnis excepturi quasi, suscipit repellendus quibusdam? Veritatis.</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"A-Natural-Policy-Gradient-2001\"><a href=\"#A-Natural-Policy-Gradient-2001\" class=\"headerlink\" title=\"A Natural Policy Gradient [2001]\"></a>A Natural Policy Gradient [2001]</h1><p><img src=\"https://www.dropbox.com/s/it82tfhfmhg9uwp/Screenshot%202018-06-10%2010.58.52.png?dl=1\"></p>\n<ul>\n<li>논문 저자: Sham Kakade</li>\n<li>논문 링크: <a href=\"https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf\" target=\"_blank\" rel=\"noopener\">https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf</a></li>\n<li>함께 보면 좋을 논문: <ul>\n<li><a href=\"hhttps://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf\" target=\"_blank\" rel=\"noopener\">Policy Gradient Methods for<br>Reinforcement Learning with Function<br>Approximation (2000)</a></li>\n<li><a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.452.7280&amp;rep=rep1&amp;type=pdf\" target=\"_blank\" rel=\"noopener\">Natural Gradient Works Efficiently in Learning(1998)</a></li>\n</ul>\n</li>\n<li>논문을 보는 이유: TRPO와 NPG는 관련이 많기 때문에 TRPO를 더 잘 이해하기 위해 봄</li>\n</ul>\n<h2 id=\"1-Abstract\"><a href=\"#1-Abstract\" class=\"headerlink\" title=\"1. Abstract\"></a>1. Abstract</h2><hr>\n<ul>\n<li>natural gradient method를 policy gradient에 적용</li>\n<li>natural gradient는 steepest descent direction을 가짐</li>\n<li>gradient descent는 parameter를 한 번에 많이 update 할 수 없는 반면, natural gradient는 가장 좋은 action을 고르도록 학습이 됌 (sutton 논문에서와 같이 compatible value function을 사용할 경우 policy iteration에서 policy improvement 1 step의 과정에서)</li>\n<li>simple MDP와 tetris MDP에서 테스트함. 성능이 많이 향상</li>\n</ul>\n<h2 id=\"2-Personal-Interpretation-and-Thinking\"><a href=\"#2-Personal-Interpretation-and-Thinking\" class=\"headerlink\" title=\"2. Personal Interpretation and Thinking\"></a>2. Personal Interpretation and Thinking</h2><hr>\n<p>(개인생각) 뉴럴넷을 사용할 경우 gradient가 steepest direction이 아닌 경우가 많다. 뉴럴넷의 parameter space가 우리가 보통 생각하는 직선으로 쭉쭉 뻗어있는 Euclidean space가 아니다. 좀 더 일반적으로는 구의 표면과 같이 휘어져있는 공간 즉, 리만 공간(Riemannian space)로 표현할 수 있다. 이와 같은 공간에서는 natural gradient가 steepest direction이 된다는 연구가 이뤄지고 있었다. 강화학습의 policy gradient은 objective function의 gradient를 따라 policy를 업데이트한다. 이 때, policy는 parameterized 되는데 이 경우에도 gradient 대신에 natural gradient가 좋다는 것을 실험해보는 논문이다. </p>\n<p>gradient가 non-covariant 해서 생기는 문제는 간단히 말하자면 다음과 같다. policy가 parameterized된 상황에서는 같은 policy라도 다른 parameter를 가질 수 있다. 이 때, steepest direction은 두 경우에 같은 방향을 가리켜야하는데 non-covariant한 경우 그렇지 못하다. 이것은 결국 느린 학습으로 연결이 된다. </p>\n<p>논문에서 2차미분 방법론들과 짧게 비교를 한다. 하지만 2차미분을 이용한 다른 방법들과의 비교가 생각보다 없는 점이 아쉽다.(Hessian을 이용한다거나 conjugate gradient method를 이용한다거나). 실험을 통해 FIM이 hessian에 수렴안하는 거라던지 Hessian 방법론이 local maxima 부근에서 상당히 느리다던지의 결과를 보여줬었으면 좋았을 것 같다. </p>\n<p>또한 natural gradient 만으로 업데이트하면 policy의 improvement보장이 안될 수 있다. policy의 improvement를 보장하기 위해 line search도 써야하는데 line search를 어떻게 쓰는지에 대한 자세한 언급이 없다. 즉, 자세한 algorithm 설명이 없다.</p>\n<p>natural policy gradient 논문은 natural gradient + policy gradient를 처음 적용했다는데 의의가 있다. 하지만 이 논문이 문제 삼은 gradient는 non-covariant하다라는 문제를 natural gradient를 통해 해결하지 못했다(Experiment를 통해 covariant gradient가 되지 못했다는 것이 보인다). NPG의 뒤를 잇는 논문이 “covariant policy search”와 “natural actor-critic”에서 covariant하지 못하다는 것을 해결하기 위해 Fisher Information Matrix를 sample 하나 하나에 대해서 구하는 것이 아니라 trajectory 전체에 대해서 구한다. </p>\n<p>또한 논문은 pg의 두 가지 세팅 중에 average-reward setting(infinite horizon)에서만 NPG를 다룬다. “covariant policy search” 논문에서는 average-reward setting과 start-state setting 모두에 대해서 npg를 적용한다. </p>\n<p>natural gradient + policy gradient를 처음 제시했다는 것은 좋지만 npg 학습의 과정을 자세하게 설명하지 않았고 다른 2차 미분 방법들과 비교를 많이 하지 않은 점이 아쉬운 논문이다.</p>\n<h2 id=\"3-Introduction\"><a href=\"#3-Introduction\" class=\"headerlink\" title=\"3. Introduction\"></a>3. Introduction</h2><hr>\n<ul>\n<li>direct policy gradient method는 future reward의 gradient를 따라 policy를 update함</li>\n<li>하지만 gradient descent는 non-covariant</li>\n<li>이 논문에서는 covarient gradient를 제시함 = natural gradient</li>\n<li>natural gradient와 policy iteration의 연관성을 설명하겠음: natural policy gradient is moving toward choosing a greedy optimal action (이런 연결점은 아마도 step-size를 덜 신경쓰고 싶어서 그런게 아닌가 싶다)</li>\n</ul>\n<p>논문의 Introduction 부분에 다음 멘트가 있다. 이 글만 봐서는 이해가 안갔는데 Mackay 논문에 좀 더 자세히 나와있다.</p>\n<p><img src=\"https://www.dropbox.com/s/41xhhr7lgfk24a1/Screenshot%202018-06-10%2011.45.18.png?dl=1\"></p>\n<p><a href=\"http://www.inference.org.uk/mackay/ica.pdf\" target=\"_blank\" rel=\"noopener\">Mackay</a>논문에서는 다음과 같이 언급하고 있다. Back-propagation을 사용할 경우에 learning rate를 dimension에 1/n로 사용하면 수렴한다는 것이 증명됐다. 하지만 너무 느리다. </p>\n<p><img src=\"https://www.dropbox.com/s/us9ezc7vxgrkez6/Screenshot%202018-06-10%2011.47.21.png?dl=1\"></p>\n<h2 id=\"4-A-Natural-Gradient\"><a href=\"#4-A-Natural-Gradient\" class=\"headerlink\" title=\"4. A Natural Gradient\"></a>4. A Natural Gradient</h2><hr>\n<h3 id=\"4-1-환경에-대한-설정\"><a href=\"#4-1-환경에-대한-설정\" class=\"headerlink\" title=\"4.1 환경에 대한 설정\"></a>4.1 환경에 대한 설정</h3><p>이 논문에서 제시하는 학습 환경은 다음과 같다.</p>\n<ul>\n<li>MDP: tuple $(S, s_0, A, R, P)$</li>\n<li>$S$: a finite set of states</li>\n<li>$s_0$: a start state</li>\n<li>$A$: a finite set of actions</li>\n<li>$R$: reward function $R: S \\times A -&gt; [0, R_{max}]$</li>\n<li>$\\pi(a;s, \\theta)$: stochastic policy parameterized by $\\theta$</li>\n<li>모든 정책 $\\pi$는 ergodic: stationary distribution $\\rho^{\\pi}$이 잘 정의되어있음</li>\n<li>이 논문에서는 sutton의 pg 논문의 두 세팅(start-state formulation, average-reward formulation) 중에 두 번째인 average-reward formulation을 가정 </li>\n<li>performance or average reward: $\\eta(\\pi)=\\sum_{s,a}\\rho^{\\pi}(s)\\pi(a;s)R(s,a)$</li>\n<li>state-action value: $Q^{\\pi}(s,a)=E_{\\pi}[\\sum_{t=0}^{\\infty}R(s_t, a_t)-\\eta(\\pi)\\vert s_0=s, a_0=a]$</li>\n<li>정책이 $\\theta$로 parameterize되어있으므로 performance는 $\\eta(\\pi_{\\theta})$인데 $\\eta(\\theta)$로 쓸거임</li>\n</ul>\n<h3 id=\"4-2-Natural-Gradient\"><a href=\"#4-2-Natural-Gradient\" class=\"headerlink\" title=\"4.2 Natural Gradient\"></a>4.2 Natural Gradient</h3><h4 id=\"4-2-1-Policy-gradient-Theorem\"><a href=\"#4-2-1-Policy-gradient-Theorem\" class=\"headerlink\" title=\"4.2.1 Policy gradient Theorem\"></a>4.2.1 Policy gradient Theorem</h4><p>서튼 pg 논문의 policy gradient theorem에 따라 exact gradient of the average reward는 다음과 같다. 다음 수식이 어떻게 유도되었는지, 어떤 의미인지 모른다면 서튼 pg 논문을 통해 제대로 이해하는 것이 좋다.</p>\n<p>$$\\nabla\\eta(\\theta)=\\sum_{s,a}\\rho^{\\pi}(s)\\nabla\\pi(a;s,\\theta)Q^{\\pi}(s,a)$$</p>\n<p>steepest descent direction of $\\eta(\\theta)$는 $\\eta(\\theta + d\\theta)$를 최소화하는 $d\\theta$로 정의된다. 이 때, $\\vert d\\theta \\vert^2$가 일정 크기 이하인 것으로 제약조건을 준다(held to small constant). Euclidian space에서는 $\\eta(\\theta)$가 steepest direction이지만 Riemannian space에서는 natural gradient가 steepest direction이다. </p>\n<h4 id=\"4-2-2-Natural-gradient-증명\"><a href=\"#4-2-2-Natural-gradient-증명\" class=\"headerlink\" title=\"4.2.2 Natural gradient 증명\"></a>4.2.2 Natural gradient 증명</h4><p>Riemannian space에서 거리는 다음과 같이 정의된다. $G(\\theta)$는 특정한 양수로 이루어진 matrix이다.</p>\n<p>$$\\vert d\\theta \\vert^2=\\sum_{ij}(\\theta)d\\theta_id\\theta_i=d\\theta^TG(\\theta)d\\theta$$</p>\n<p>이 수식은 Natural Gradient Works Efficiently in Learning 논문에서 증명되어있다. 다음은 natural gradient 증명이다. </p>\n<p>steepest direction을 구할 때 $\\theta$의 크기를 제약조건을 준다. 제약조건은 다음과 같다. </p>\n<p>$$\\vert d\\theta \\vert^2 = \\epsilon^2$$</p>\n<p>그리고 steepest vector인 $d\\theta$는 다음과 같이 정의할 수 있다. </p>\n<p>$$d\\theta = \\epsilon a$$</p>\n<p>$$\\vert a \\vert^2=a^TG(\\theta)a = 1$$</p>\n<p>이 때, $a$가 steepest direction unit vector이 되려면 다음 수식을 최소로 만들어야 한다. (이 수식은 잘 모르겠지만 $\\theta$에서의 1차근사를 가정하는게 아닌가 싶다.</p>\n<p>$$\\eta(\\theta + d\\theta) = \\eta(\\theta) + \\epsilon\\nabla\\eta(\\theta)^Ta$$</p>\n<p>위 수식이 제약조건 아래 최소가 되는 $a$를 구하기 위해 Lagrangian method를 사용한다. Lagrangian method를 모른다면 <a href=\"https://en.wikipedia.org/wiki/Lagrange_multiplier\" target=\"_blank\" rel=\"noopener\">위키피디아</a>를 참고하는 것을 추천한다. 위 수식이 최소라는 것은 $\\nabla\\eta(\\theta)^Ta$가 최소라는 것이다. </p>\n<p>$$\\frac{\\partial}{\\partial a_i}(\\nabla\\eta(\\theta)^Ta - \\lambda a^TG(\\theta)a)=0$$</p>\n<p>따라서 $(\\nabla\\eta(\\theta)^Ta - \\lambda a^TG(\\theta)a)=0$는 상수이다. 상수를 미분하면 0이므로 이 식을 $a$로 미분한다. 그러면 다음과 같다. steepest direction을 구한 것이다.</p>\n<p>$$\\nabla\\eta(\\theta) = 2 \\lambda G(\\theta)a$$</p>\n<p>$$a=\\frac{1}{2\\lambda}G^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>이 때, 다음 식을 natural gradient라고 정의한다.</p>\n<p>$$\\bar{\\nabla}\\eta(\\theta) = G^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>natural gradient를 이용한 업데이트는 다음과 같다. </p>\n<p>$$\\theta_{t+1}=\\theta_t - \\alpha_tG^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>여기까지는 natural gradient의 증명이었다. 이 natural gradient를 policy gradient에 적용한 것이 natural policy gradient이다. natural policy gradient는 다음과 같이 정의된다.</p>\n<p>$$\\bar{\\nabla}\\eta(\\theta) = F^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>$G$ 대신 $F$를 사용했는데 $F$는 Fisher information matix이다. 수식은 다음과 같다.</p>\n<p>$$F(\\theta) = E_{\\rho^\\pi(s)}[F_s(\\theta)]$$</p>\n<p>$$F_s(\\theta)=E_{\\pi(a;s,\\theta)}[\\frac{\\partial log \\pi(a;s, \\theta)}{\\partial \\theta_i}\\frac{\\partial log \\pi(a;s, \\theta)}{\\partial\\theta_j}]$$</p>\n<p>왜 G가 F가 되는지는 아직 잘 모르겠다. 거리라는 개념을 표현하려면 </p>\n<h2 id=\"5-The-Natural-Gradient-and-Policy-Iteration\"><a href=\"#5-The-Natural-Gradient-and-Policy-Iteration\" class=\"headerlink\" title=\"5. The Natural Gradient and Policy Iteration\"></a>5. The Natural Gradient and Policy Iteration</h2><hr>\n<h3 id=\"5-1-Theorem-1\"><a href=\"#5-1-Theorem-1\" class=\"headerlink\" title=\"5.1 Theorem 1\"></a>5.1 Theorem 1</h3><p>sutton pg 논문에 따라 $Q^{\\pi}(s,a)$를 approximation한다. approximate하는 함수 $f^{\\pi}(s,a;w)$는 다음과 같다.(compatible value function)</p>\n<p>$$f^{\\pi}(s,a;w)=w^T\\psi^{\\pi}(s,a)$$</p>\n<p>$$\\psi^{\\pi}(s,a) = \\nabla log\\pi(a;s,\\theta)$$</p>\n<p>$w$는 원래 approximate하는 함수 $Q$와 $f$의 차이를 줄이도록 학습한다(mean square error). 수렴한 local minima의 $w$를 $\\bar{w}$라고 하겠다. 에러는 다음과 같은 수식으로 나타낸다. </p>\n<p>$$\\epsilon(w,\\pi)\\equiv\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)(f^{\\pi}(s,a;w)-Q^{\\pi}(s,a))^2$$</p>\n<p>위 식이 local minima이면 미분값이 0이다. $w$에 대해서 미분하면 다음과 같다. </p>\n<p>$$\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)(\\psi^{\\pi}(s,a)^T\\bar{w}-Q^{\\pi}(s,a))=0$$</p>\n<p>$$(\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)\\psi^{\\pi}(s,a)^T)\\bar{w}=\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)Q^{\\pi}(s,a))$$</p>\n<p>이 때, 위 식의 우변은 $\\psi$의 정의에 의해 policy gradient가 된다. 또한 왼쪽 항에서는 Fisher information matrix가 나온다.</p>\n<p>$$F(\\theta)=\\sum_{s,a}\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)\\psi^{\\pi}(s,a)=E_{\\rho^\\pi(s)}[F_s(\\theta)]$$</p>\n<p>따라서 다음과 같다.</p>\n<p>$$F(\\theta)\\bar{w}=\\nabla\\eta(\\theta)$$</p>\n<p>$$\\bar{w}=F(\\theta)^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>이 식은 natural gradient 식과 동일하다. 이 식은 policy가 update 될 때, value function approximator의 parameter 방향으로 이동한다는 것을 의미한다. function approximation이 정확하다면 그 parameter의 natural policy gradient와 inner product가 커야한다. </p>\n<h3 id=\"5-2-Theorem-2-Greedy-Polict-Improvement\"><a href=\"#5-2-Theorem-2-Greedy-Polict-Improvement\" class=\"headerlink\" title=\"5.2 Theorem 2: Greedy Polict Improvement\"></a>5.2 Theorem 2: Greedy Polict Improvement</h3><p>natural policy gradient가 단순히 더 좋은 행동을 고르도록 학습하는게 아니라 가장 좋은 (greedy) 행동을 고르도록 학습한다는 것을 증명하는 파트이다. 이것을 일반적인 형태의 policy에 대해서 증명하기 전에 exponential 형태의 policy에 대해서 증명하는 것이 Theorem 2이다.</p>\n<p>policy를 다음과 같이 정의한다.</p>\n<p>$$\\pi(a;s,\\theta) \\propto exp(\\theta^T\\phi_{sa})$$</p>\n<p>$\\bar{\\nabla}\\eta(\\theta)$가 0이 아니고 $\\bar{w}$는 approximation error를 최소화한 $w$라고 가정한다. 이 상태에서 natural gradient update를 생각해보자. policy gradient는 gradient ascent임을 기억하자.</p>\n<p>$$\\theta_{t+1}=\\theta_t + \\alpha_t\\bar{\\nabla}\\eta(\\theta)$$</p>\n<p>이 때 $\\alpha$가 learning rate로 parameter를 얼마나 업데이트하는지를 결정한다. 이 값을 무한대로 늘렸을 때 policy가 어떻게 업데이트되는지 생각해보자. </p>\n<p>$$\\pi_{\\infty}(a;s)=lim_{\\alpha\\rightarrow\\infty}\\pi(a;s,\\theta+\\alpha\\bar{\\nabla}\\eta(\\theta))-(1)$$</p>\n<p>function approximator는 다음과 같다. </p>\n<p>$$f^{\\pi}(s,a;w)=w^T\\psi^{\\pi}(s,a)$$</p>\n<p>Theorem 1에 의해 위 식은 아래와 같이 쓸 수 있다.</p>\n<p>$$f^{\\pi}(s,a;w)=\\bar{\\nabla}\\eta(\\theta)^T\\psi^{\\pi}(s,a)$$</p>\n<p>$\\theta$의 정의에 의해 $\\psi$는 다음과 같다.</p>\n<p>$$\\psi^{\\pi}(s,a)=\\phi_{sa}-E_{\\pi(a’;s,\\theta)}[\\phi_{sa’}]$$</p>\n<p>function approximator는 다음과 같이 다시 쓸 수 있다.</p>\n<p>$$f^{\\pi}(s,a;w)=\\bar{\\nabla}\\eta(\\theta)^T(\\phi_{sa}-E_{\\pi(a’;s,\\theta)}[\\phi_{sa’}])$$</p>\n<p>greedy policy improvement가 Q function 값 중 가장 큰 값을 가지는 action을 선택하듯이 여기서도 function approximator의 값이 가장 큰 action을 선택하는 상황을 가정해본다. 이 때 function approximator의 argmax는 다음과 같이 쓸 수 있다.</p>\n<p>$$argmax_{a’}f^{\\pi}(s,a)=argmax_{a’}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa’}$$</p>\n<p>(1) 식을 다시 살펴보자. policy의 정의에 따라 다음과 같이 쓸 수 있다. </p>\n<p>$$\\pi(a;s,\\theta + \\alpha\\bar{\\nabla}\\eta(\\theta)) \\propto exp(\\theta^T\\phi_{sa} + \\alpha\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa})$$</p>\n<p>$\\bar{\\nabla}\\eta(\\theta) \\neq 0$이고 $\\alpha\\rightarrow\\infty$이면 exp안의 항 중에서 뒤의 항이 dominate하게 된다. 여러 행동 중에 $\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa}$가 가장 큰 행동이 있다면 이 행동의 policy probability가 1이 되고 나머지는 0이 된다. 따라서 다음이 성립한다.</p>\n<p>$$\\pi_{\\infty}=0$$ </p>\n<p>if and only if </p>\n<p>$$a \\notin argmax_{a’}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa’}$$</p>\n<p>이 결과로부터 natural policy gradient는 단지 더 좋은 action이 아니라 best action을 고르도록 학습이 된다. 하지만 non-covariant gradient(1차미분) 에서는 그저 더 좋은 action을 고르도록 학습이 된다. 하지만 이 natural policy gradient에 대한 결과는 infinite learning rate 세팅에서만 성립함. 좀 더 일반적인 경우에 대해서 살펴보자.</p>\n<h4 id=\"4-3-Theorem-3\"><a href=\"#4-3-Theorem-3\" class=\"headerlink\" title=\"4.3 Theorem 3\"></a>4.3 Theorem 3</h4><p>Theorem 2에서와는 달리 일반적인 policy를 가정하자(general parameterized policy). Theorem 3는 이 상황에서 natural gradient를 통한 업데이트가 best action를 고르는 방향으로 학습이 된다는 것을 보여준다. </p>\n<p>natural gradien에 따른 policy parameter의 업데이트는 다음과 같다. $\\bar{w}$는 approximation error를 minimize하는 $w$이다.</p>\n<p>$$\\delta\\theta = \\theta’ - \\theta = \\alpha\\bar{\\nabla}\\eta(\\theta)=\\alpha\\bar{w}$$</p>\n<p>policy에 대해서 1차근사를 하면 다음과 같다. </p>\n<p>$$\\pi(a;s,\\theta’)=\\pi(a;s,\\theta)+\\frac{\\partial\\pi(a;s,\\theta)^T}{\\partial\\theta}\\delta\\theta + O(\\delta\\theta^2)$$</p>\n<p>$$=\\pi(a;s,\\theta)(1+\\psi(s,a)^T\\delta\\theta) + O(\\delta\\theta^2)$$</p>\n<p>$$=\\pi(a;s,\\theta)(1+\\alpha\\psi(s,a)^T\\bar{w}) + O(\\delta\\theta^2)$$</p>\n<p>$$=\\pi(a;s,\\theta)(1+\\alpha f^{\\pi}(s,a;\\bar{w}) + O(\\delta\\theta^2)$$</p>\n<p>policy 자체가 function approximator의 크기대로 업데이트가 되므로 local하게 best action의 probability는 커지고 다른 probability의 크기는 작아질 것이다. 하지만 만약 greedy improvement가 된다하더라도 그게 performance의 improvement를 보장하는 것은 아니다. 하지만 line search와 함께 사용할 경우 improvement를 보장할 수 있다. </p>\n<h2 id=\"6-Metrics-and-Curvatures\"><a href=\"#6-Metrics-and-Curvatures\" class=\"headerlink\" title=\"6. Metrics and Curvatures\"></a>6. Metrics and Curvatures</h2><hr>\n<p>다음 식에 해당하는 G는 Fisher Information Matrix만 사용할 수 있는 것이 아니다.</p>\n<p>$$\\vert d\\theta \\vert^2=\\sum_{ij}(\\theta)d\\theta_id\\theta_i=d\\theta^TG(\\theta)d\\theta$$</p>\n<p>이 파트에서는 FIM과 다른 metric 사이의 관계를 다룬다. </p>\n<ul>\n<li>In the different setting of parameter estimation, the Fisher information converges to the <code>Hessian</code>, so it is <a href=\"https://en.wikipedia.org/wiki/Efficiency_(statistics\" target=\"_blank\" rel=\"noopener\">asymptotically efficient</a>#Asymptotic_efficiency)</li>\n<li>이 논문의 경우, 아마리 논문의 ‘blind separation case’와 유사한데 이 때는 꼭 asymtotically efficient하지 않다. 이 말은 즉 2nd order 수렴이 보장되지 않는다는 것이다.</li>\n<li><a href=\"http://www.inference.org.uk/mackay/ica.pdf\" target=\"_blank\" rel=\"noopener\">Mackay</a> 논문에서 hessian에서 data independant한 term을 metric으로 가져오는 방법을 제안했다. 그래서 performance를 2번 미분해보면 다음과 같다. 하지만 다음 식에서는 모든 항이 data dependent하다(Q가 있으니까). 첫 번째 항이 그나마 FIM과의 관련성이 있을 수 있지만 Q 값이 curvature에 weight를 주는 방식 때문에 다르다고 할 수 있다.</li>\n</ul>\n<p>$$<br>\\nabla^2\\eta(\\theta)=\\sum_{sa}\\rho^{\\pi}(s)(\\nabla^2\\pi(a;s)Q^{\\pi}(s,a)+\\nabla\\pi(a;s)\\nabla Q^{\\pi}(s,a)^T+\\nabla Q^{\\pi}(s,a)\\nabla\\pi(a;s)^T)<br>$$</p>\n<ul>\n<li>hessian은 보통 positive definite가 아닐수도 있다. 따라서 local maxima가 될 때까지 Hessian이 사용하기 별로 안좋다. 그리고 local maxima에서는 Hessian보다는 Conjugate methods가 더 효율적이다. </li>\n</ul>\n<p>이 파트에서는 무엇을 말하고 있는지 알기가 어렵다. FIM과 Hessian이 관련이 있다는 것을 알겠다. 하지만 asymtotically efficient와 같은 내용을 모르므로 내용의 이해가 어려웠다.</p>\n<p>Mackay 논문에서 해당 부분은 다음과 같다. </p>\n<p><img src=\"https://www.dropbox.com/s/x4n6z6pdyi7xtb9/Screenshot%202018-06-10%2012.04.13.png?dl=1\"></p>\n<h2 id=\"7-Experiment\"><a href=\"#7-Experiment\" class=\"headerlink\" title=\"7. Experiment\"></a>7. Experiment</h2><hr>\n<p>논문에서는 natural gradient를 simple MDP와 tetris MDP에 대해서 테스트했다. practice에서는 Fisher information matrix는 다음과 같은 식으로 업데이트한다.</p>\n<p>$$f\\leftarrow f+\\nabla log \\pi(a_t; s_t, \\theta)\\nabla log \\pi(a_t; s_t, \\theta)^T$$</p>\n<p>T length trajectory에 대해서 f/T를 통해 F의 estimate를 구한다.</p>\n<h3 id=\"7-1-Linear-Quadratic-regulator\"><a href=\"#7-1-Linear-Quadratic-regulator\" class=\"headerlink\" title=\"7.1 Linear Quadratic regulator\"></a>7.1 Linear Quadratic regulator</h3><p>에이전트를 테스트할 환경은 다음과 같은 dynamics를 가지고 있다. $u(t)$는 control signal로서 에이전트의 행동이라고 생각하면 된다. $\\epsilon$은 noise distribution으로 환경에 가해지는 노이즈이다. 에이전트의 목표는 적절한 $u(t)$를 통해<br>x(t)를 0으로 유지하는 것이다. 제어분야에서의 LQR controller 문제이다.</p>\n<p>$$<br>x(t+1) = 0.7x(t)+u(t)+\\epsilon(t)<br>$$</p>\n<p>x(t)를 0으로 유지하기 위해서 $x(t)^2$를 cost로 잡고 이 cost를 최소화하도록 학습한다. 이 시스템을 linear라고 부르는 것은 아래 그림과 같이 선형의 형태를 띄기 때문이다. 이 논문에서 실험할 때는 이 그림에서의 system에 noise를 더해준 것이다. <a href=\"https://stanford.edu/class/ee363/lectures/dlqr.pdf\" target=\"_blank\" rel=\"noopener\">그림 출처</a></p>\n<p><img src=\"https://www.dropbox.com/s/vz0q97lcek4oti5/Screenshot%202018-06-08%2014.21.10.png?dl=1\"></p>\n<p>이 실험에서 사용한 parameterized policy는 다음과 같다. parameter가 $\\theta_1$과 $\\theta_2$ 밖에 없는 상당히 간단한 policy이다. </p>\n<p>$$<br>\\pi(u;x,\\theta) \\propto exp(\\theta_1 s_1 x^2 + \\theta_2 s_2 x)<br>$$</p>\n<p>이 policy를 간단히 numpy와 matplotlib를 이용해서 그려봤다. $\\theta_1$과 $theta_2$를 (0.5, 0.5), (1, 0), (0, 1)로 하고 $s_1$과 $s_2$는 1로 두었다. x는 -1에서 1까지의 범위로 그렸다. x를 0으로 유지하려면 u(t)가 -와 +가 둘 다 가능해야할 것 같은데 위 식으로만 봐서는 action이 하나이고 그 action일 확률을 표시하는 것처럼 나왔다. 아마 -1과 +1이 u(t)가 될 수 있는데 그 중 +1을 선택할 확률이 위와 같이 되는게 아닌가 싶다.</p>\n<center><img src=\"https://www.dropbox.com/s/v69qyrwn7zurk8c/Screenshot%202018-06-08%2014.57.07.png?dl=1\" width=\"500px\"></center>\n\n<p>다음 그림은 1-d LQR을 학습한 그래프이다. cost가 $x^2$이기 때문에 cost가 0으로 갈수록 agent는 0에서 안정적으로 머무른다고 볼 수 있다. 6개의 선 중에서 오른쪽 세 개가 일반적인 gradient 방법을 사용해서 학습한 결과이다. 그리고 왼쪽의 세 개의 선이 natural policy gradient를 통해 학습한 학습 곡선이다. 일반 gradient 방법보다 natural gradient가 훨씬 빠르게 학습한다(time 축이 log scale인 것을 감안하자).</p>\n<p>하지만 문제가 있다. npg를 학습한 세 개의 곡선은 $\\theta$를 rescale 한 것이다. $\\theta$앞에 곱해지는 숫자에 따라 학습의 과정이 다르다. 이 것은 coordinate에 따라 steepest gradient가 다르게 측정된다는 것이다. 즉, covariant gradient가 아니라는 뜻이다. 이 논문에서는 natural gradient를 통해 gradient가 covariant하도록 만들고 싶었는데 실패한 것이다. </p>\n<center><img src=\"https://www.dropbox.com/s/fhn8cgje0rdws0i/Screenshot%202018-06-08%2023.13.37.png?dl=1\" width=\"300px\"></center>\n\n<p>natural gradient가 covariant하지 않은 이유는 Fisher Information Matrix가 예상했던 바와는 달리 invariant metric이 아니기 때문이다. 또한 FIM이 invariant metric이 아닌 이유는 FIM을 계산할 때 $\\rho_s$가 곱해지기 때문이다(state distribution에 대한 expectation. $\\rho_s$가 곱해지는 것이 invariant에 미치는 영향이 무엇인지는 모르겠다). 하지만 여전히 의의가 있는 것은 기존 gradient 방법들보다 훨씬 빠르게 학습한다는 것이다.</p>\n<h3 id=\"7-2-simple-2-state-MDP\"><a href=\"#7-2-simple-2-state-MDP\" class=\"headerlink\" title=\"7.2 simple 2-state MDP\"></a>7.2 simple 2-state MDP</h3><p>이제 다른 예제에서 NPG를 테스트한다. 2개의 state만 가지는 MDP를 고려해보자. <a href=\"http://repository.cmu.edu/cgi/viewcontent.cgi?article=1080&amp;context=robotics\" target=\"_blank\" rel=\"noopener\">그림출처</a>. 그림으로보면 다음과 같다. x=0 상태와 x=1 상태 두 개가 존재한다. 에이전트는 각 상태에서 다시 자신의 상태로 되돌아오는 행동을 하거나 다른 상태로 가는 행동을 할 수 있다. 상태 x=0에서 다시 자기 자신으로 돌아오면 1의 보상을 받고 상태 x=1에서 자기 자신으로 돌아오면 2의 보상을 받는다. 따라서 결국 optimal policy는 상태 x=1에서 계속 자기 자신으로 돌아오는 행동을 취하는 것이다. </p>\n<p><img src=\"https://www.dropbox.com/s/g1x9yknzsrip59i/Screenshot%202018-06-08%2023.06.50.png?dl=1\"></p>\n<p>문제를 좀 어렵게 만들기 위해 state distribution을 다음과 같이 설정한다. 즉, 대부분의 경우에 상태 x=0에서 에이전트가 시작하는 것이다. </p>\n<p>$$<br>\\rho(x=0)=0.8,  \\rho(x=1)=0.2<br>$$</p>\n<p>일반적인 gradient 방법을 사용하면 다음과 같은 policy gradient 식에 따라서 업데이트를 하게 된다. 이 때, $\\rho(s)$가 gradient에 곱해지므로 상태적으로 상태 0에서의 gradient 값이 커지게 된다. 따라서 에이전트는 상태 0에서의 gradient(상태 0에서 스스로에게 돌아오는 행동을 취하도록 정책을 업데이트하는 gradient)를 따라 parameterized policy를 update한다. 따라서 아래 그림의 첫번째 그림에서처럼 Reward가 1에서 오랜 시간동안 머무른다. 즉, 에이전트가 상태 0에서 self-loop를 계속 돌고 있다는 것이다. $\\rho(x=0)$가 $10^{-7}$까지 떨어진다.</p>\n<p>$$\\nabla\\eta(\\theta)=\\sum_{s,a}\\rho^{\\pi}(s)\\nabla\\pi(a;s,\\theta)Q^{\\pi}(s,a)$$</p>\n<center><img src=\"https://www.dropbox.com/s/xtb77mfazbppnss/Screenshot%202018-06-08%2023.14.24.png?dl=1\" width=\"300px\"></center>\n\n<p>하지만 NPG를 사용할 경우에는 훨씬 빠르게 average reward가 2에 도달한다. gradient 방법이 $1.7X10^(7)$정도의 시간만에 2에 도달한 반면 NPG는 2만에 도달한다. 또한 $\\rho(x=0)$가 $10^{-5}$이하로 떨어지지 않는다.</p>\n<p>한 가지 그래프를 더 살펴보자. 다음 그래프는 parameter $\\theta$가 업데이트 되는 과정을 보여준다. 이 그래프에서는 parameter가 2개 있는 것이다. 일반적인 gradient가 아래 그래프에서 실선에 해당한다. 이 실선의 그래프는 보면 처음부터 중반까지 $\\theta_i$만 거의 업데이트하는 것을 볼 수 있다. 그에 비해 NPG는 두 개의 parameter를 균등하게 업데이트하는 것을 볼 수 있다. </p>\n<center><img src=\"https://www.dropbox.com/s/g7pazozw2k6rd7x/Screenshot%202018-06-08%2023.23.25.png?dl=1\" width=\"300px\"></center>\n\n<p>policy가 $\\pi(a;s,\\theta)\\propto exp(\\theta_{sa})$일 때, 다음과 같이 $F_{-1}$이 gradient 앞에 weight로 곱해지는데 이게 $\\rho$와는 달리 각 parameter에 대해 균등하다. 따라서 위 그래프에서와 같이 각 parameter는 비슷한 비율로 업데이트가 되는 것이다.</p>\n<p>$$\\bar{\\nabla}\\eta(\\theta) = F^{-1}\\nabla\\eta(\\theta)$$</p>\n<h3 id=\"7-3-Tetris\"><a href=\"#7-3-Tetris\" class=\"headerlink\" title=\"7.3 Tetris\"></a>7.3 Tetris</h3><p>NPG를 테스트할 tetris 예제는 Neuro Dynamic Programming 책에 소개되어있다. 다음 그림은 tetris 예제를 보여준다. 보통 그림에서와 같이 state의 feature를 정해준다. <a href=\"http://slideplayer.com/slide/5215520/\" target=\"_blank\" rel=\"noopener\">그림 출처</a></p>\n<p><img src=\"https://www.dropbox.com/s/y1halso9yermy8s/Screenshot%202018-06-08%2023.44.34.png?dl=1\"></p>\n<p>이 예제에서도 exponantial family로 policy를 표현한다. $\\pi(a;s,\\theta) \\propto exp(\\theta^T\\phi_{sa})$ 로 표현한다.</p>\n<p>tetris는 linear function approximator와 greedy policy iteration을 사용할 경우 performance가 갑자기 떨어지는 현상이 있다. 밑의 그림에서 A의 spike가 있는 그래프가 이 경우이다. 그 밑에 낮게 누워있는 그래프는 일반적인 policy gradient 방법이다. 하지만 Natural policy gradient를 사용할 경우 B 그림에서 오른쪽 그래프와 같이 성능개선이 뚜렷하다. Policy Iteration 처럼 성능이 뚝 떨어지지 않고 안정적으로 유지한다. 또한 그림 C에서 보는 것처럼 오른쪽 그래프인 일반적인 gradient 방법보다 훨씬 빠르게 학습하는 것을 볼 수 있다.</p>\n<p><img src=\"https://www.dropbox.com/s/pr6s2qrqaic0wyj/Screenshot%202018-06-08%2023.40.16.png?dl=1\"></p>\n<h2 id=\"8-Discussion\"><a href=\"#8-Discussion\" class=\"headerlink\" title=\"8. Discussion\"></a>8. Discussion</h2><hr>\n<ul>\n<li>natural gradient method는 policy iteration에서와 같이 greedy action을 선택하도록 학습됌</li>\n<li>line search와 함께 쓰면 natural gradient method는 더 policy iteration 같아짐</li>\n<li>greedy policy iteration에서와는 달리 performance improvement가 보장됌</li>\n<li>하지만 F(Fisher information matrix)가 asymtotically Hessian으로 수렴하지 않음. asymtotically conjugate gradient method(Hessian의 inverse를 approx.로 구하는 방법)가 더 좋아 보일 수 있음</li>\n<li>하지만 Hessian이 항상 informative하지 않고(hessian이 어떤 정보를 주려면 positive definite와 같은 성질을 가져서 해당 함수가 convex인 것을 알 수 있다든지의 경우를 이야기하는데 hessian이 항상 positive definite가 아닐 수 있다는 것이다) tetris에서 봤듯이 natural gradient method가 더 효율적일 수 있음(pushing the policy toward choosing greedy optimal actions)</li>\n<li>conjugate gradient method가 좀 더 maximum에 빠르게 수렴하지만, performance는 maximum에서 거의 안변하므로 좋다고 말하기 어려움(?). 이 부분에 대해서 추가적인 연구 필요.</li>\n</ul>\n<p>amet, consectetur adipisicing elit. Vitae ipsum, voluptatem quis officiis inventore dolor totam deserunt, possimus similique eum, accusantium adipisci doloremque omnis excepturi quasi, suscipit repellendus quibusdam? Veritatis.</p>\n"},{"title":"Policy Gradient Methods for Reinforcement Learning with Function Approximation","date":"2018-06-15T09:29:32.000Z","author":"이웅원","subtitle":"피지여행 1번째 논문","_content":"\n---\n# 1. Intro to Policy Gradient\n\n이 논문은 policy gradient (PG) 기법의 효시와도 같으며 향후 많은 파생연구를 낳은 중요한 논문입니다. 이 논문을 이해하기 위해 필요한 배경지식을 먼저 설명하고 논문을 차근차근 살펴보도록 하겠습니다.\n<br>\n\n## 1.1 Value Function Approach\n전통적으로 강화학습 기법은 value function을 기반으로 동작하였습니다. 특정 state에서 vaue function 또는 value function을 근사하는 함수(function approximation)를 최대화하는 action을 찾는 greedy action-selection policy가  대표적입니다.\n\n논문에서는 이러한 방법은 deterministic한 policy를 찾는 쪽으로 나아가게 되지만 종종 최적의 policy는 stochastic한 성질을 가지기 때문에 이 방법으로는 최적의 policy를 찾을 수 없다고 언급하고 있습니다. (이 논문이 나오고 나서 한참 후 개발된 deterministic policy gradient 기법과는 반대되는 서술일 수 있습니다.) 아마도 이러한 부분은 $\\epsilon$-greedy action-selection policy로 개선될 수 있을 것입니다. 또 하나의 문제는 value function의 작은 변화로 인해서 action이 크게 변할 수도 있다는 것입니다. 이것은 알고리듬의 수렴성에 문제를 야기할 수 있습니다. 이러한 문제점을 해결하기 위하여 policy search라는 새로운 기법이 고안됩니다.\n<br>\n\n## 1.2 Policy Search\npolicy search는 최적의 policy $\\pi^*$를 직접 찾기 때문에 value function 모델링이 필요가 없습니다. (뒤에서보겠지만 사실 꼭 그렇지만도 않습니다.) policy는 좀 더 다루기 쉽도록 parameter $\\theta$를 이용하여 $\\pi_{\\theta}$로 표현할 수 있습니다. 최적의 policy를 찾기 위하여 expected return $E\\left[R|\\theta\\right]$이 최대화되도록 parameter를 조정합니다. 이를 간단한 수식으로 정리하면 다음과 같습니다.\n\n$$\n\\pi^* = \\arg \\max\\limits_\\pi  E\\left[ {\\left. R \\right|\\pi } \\right] \\to {\\text{original problem} } \n\\\\\\\\  {\\Downarrow} \\\\\\\\\n{\\text{policy parameterization by } } \\pi_{\\theta}: \\Theta  \\to \\Pi \\\\\\\\ {\\Downarrow} $$\n\n$$\\pi^* = \\arg \\max\\limits_{\\theta}  E\\left[ {\\left. R \\right|\\theta } \\right] \\to {\\text{policy search problem} }\n$$\n\n\npolicy search 방법은 크게 두 가지로 분류할 수 있습니다. 첫 번째는 gradient-based optimization으로 policy gradient method가 이에 속합니다. 두 번째는 gradient-free optimization으로 진화(evolutionary) 연산을 이용하는 것이 이에 속합니다.\n<br>\n\n## 1.3 How to Obtain the Expected Return\nexpected return을 최대화하는 방향으로 policy를 update한다고 하였습니다. 그렇다면 expected return은 어떻게 구할 수 있을까요? 여기에도 크게 두 가지 방법이 있습니다. 첫 번째는 deterministic approximation으로 Markov decision process의 dynamics를 모델링한 후 수식적으로 구하는 것입니다. 두 번째 방법은 monte carlo estimation으로 dynamics에 대한 모델을 하지 않고 많은 sample들을 얻은 후 empirical하게 expected return을 계산하는 방법입니다. 두 번째 방법이 좀 더 현실적이지만 gradient를 구하는 것은 더 어렵습니다. 결국 gradient를 esimate하는방법을 고안해야 합니다. 가장 유명한 gradient estimation 방법이 1992년 R. J. Williams에 의해서 제안된 REINFORCE 기법입니다. REINFORCE는 Monte Carlo estimate 또는 likelihood-ratio estimate라고 부르는 방법을 이용해서 gradient와 expectation의 위치를 서로 바꿔줍니다.\n\n### Monte Carlo Gradient Estimation\n다음과 같은 parameter $\\theta$를 가지는 random variable $X$가 있습니다: $X:\\Omega\\mapsto\\mathcal{X}$. 그리고 이 $x$에 대한 함수 $f$가 있습니다: $f:\\mathcal{X}\\mapsto\\mathbb{R}$. expected return처럼 $E[f(x)]$를 최대화하고자 합니다. 이를 위해서는 $\\nabla_{\\theta}E[f(x)]$를 구해야 합니다. 이 때 다음과 같은 log derivate trick을 이용하여 다음과 같이 수식을 변형시킬 수 있습니다.\n\n$$\n\\begin{align}\n\\nabla_{\\theta}E_{p(x;\\theta)}\\left[ {f(x)} \\right] \n&= \\nabla_\\theta\\int{f\\left( x \\right)p\\left( {x;\\theta} \\right) dx}\n\\\\\\\\\n&= \\int { {\\nabla_\\theta }p\\left( {x;\\theta } \\right)f\\left( x \\right)dx} \n\\\\\\\\\n&= \\int {\\frac{ {p\\left( {x;\\theta } \\right)} }{ {p\\left( {x;\\theta } \\right)} }{\\nabla_\\theta }p\\left( {x;\\theta } \\right)f\\left( x \\right)dx} \n\\\\\\\\\n&= \\int {p\\left( {x;\\theta } \\right){\\nabla_\\theta }\\log p\\left( {x;\\theta } \\right)f\\left( x \\right)dx} \n\\\\\\\\\n&= E_{p(x;\\theta)}[f(x)\\nabla_\\theta \\log p({x;\\theta})]\n\\end{align}\n$$\n\n이제 expectation은 많은 샘플들을 모아서 평균을 취하는 Monte Carlo 기법으로 근사화할 수 있습니다.\n\n$${\\nabla_{\\theta} }{E_{p\\left( {x;\\theta } \\right)} }\\left[ {f\\left( x \\right)} \\right] = \\frac{1}{N}\\sum_{n=1}^N f\\left(X_n\\right)\\nabla_\\theta\\log p\\left(X_n;\\theta\\right)$$\n\n이 방법을 사용하기 위해서 필요한 조건은 $\\log p\\left(X_n;\\theta\\right)$가 미분가능해야 한다는 것 뿐입니다. 그러나 이 방법은 얻은 샘플들에 의존하기 때문에 경우에 따라서 큰 variance를 가질 수 있습니다.\n\n<br><br>\n\n---\n# 2. Policy Gradient Methods\n이제 본격적으로 policy gradient 기법에 대해서 알아보겠습니다.\n\n## 2.1 Policy Gradent Approach\npolicy gradient는 stochastic policy를 자체적인 파리미터를 가진 function approximator를 이용해서 근사화시킵니다. 몇 가지 notation들을 정의하겠습니다.\n\n* $\\theta$: policy parameter들의 vector  \n* $\\rho$: 해당 policy들의 성능을 나타내는 척도 (예. average reward per step)  \n* $\\alpha$: positive-definite한 step size \n\npolicy parameter는 gradient에 비례하여 업데이트됩니다.\n\n$$\\Delta\\theta \\approx \\alpha\\frac{\\partial\\rho}{\\partial\\theta}$$\n\nlocal optimal policy로 수렴합니다. 논문의 가장 중요한 contribution은 특정 조건을 만족하는 function approximator를 이용하여 경험(experience, sample)을 축적하고 이것들을 이용하여 위의 gradient를 unbiased estimate할 수 있다는 것을 증명한 것입니다.\n<br>\n\n## 2.2 Summary\n\n논문에서 설명한 policy gradient 기법을 요약하면 다음과 같습니다.\n\n* Original maximization problem: $\\max\\rho_{\\theta}=E\\left[R\\right]$\n* gradient ascent method를 이용한 parameter update: $\\theta_{t+1} = \\theta_{t} + \\alpha\\left.\\frac{\\partial E[R]}{\\partial\\theta}\\right|_{\\theta_t}$\n    - 그러나 우리는 gradient를 모름. 추정하기도 어려움. 왜냐하면 expectation이 안에 있기 때문임.\n    - 그렇다면 이것을 추정 가능한 형태로 바꿀 수 있을까? 즉, expectation이 밖에 있는 형태로!\n    - Expectation이 밖에 있으면 왜 추정이 유리할까? 바로 Sample mean을 취하면 되기 때문임!\n* Approximate gradient\n    - 방법1: 논문의 Theorem 1을 이용하면 gradient와 expectation의 위치를 바꿀 수 있음!\n    $$\\frac{\\partial E[R]}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)$$\n    - 방법2: Log derivative trick을 이용하면 gradient와 expectation의 위치를 바꿀 수 있음! (Theorem 2)\n    $$\\theta_{t+1} = \\theta_{t} + \\alpha E[R\\frac{\\partial}{\\partial\\theta}\\log p_{\\theta}|\\theta_t]$$\n    - 그리고 특정 trajectory를 따라가면서 return값을 구하고 이것을 여러 번 수행하여 sample mean을 취하면 gradient를 추정하는 것이 가능함.\n    - 이러한 기법을 제시한 기존의 연구가 REINFORCE임.\n    - 여러 Trajectory를 이용하므로 variance가 높을 수 밖에 없음.\n    - Advantage를 활용하거나 하는 방식으로 이후 여러 연구가 진행되었음.\n<br>\n\n## 2.3 System Model\n시스템모델은 다음과 같습니다.\n\n* Markov decision process (MDP) at each time $t\\in{0,1,2,...}$\n* $S_t\\in\\mathcal{S}$: state\n* $A_t\\in\\mathcal{A}$: action\n* $r_t\\in E$: reward\n* $\\theta\\in\\mathbb{R}^l$: vector of policy parameters, for $l\\ll\\left|\\mathcal{S}\\right|$\n* $\\mathcal{P}_{s s'}^a = \\Pr[S_{t+1}=s'|S_t=s,A_t=a]$: state transition probabilities\n* $\\mathcal{R}_{s s'}^a = E[R_{t+1}|S_t=s,A_t=a]$: expected reward\n* $\\pi(s,a,\\theta)=\\Pr[A_t=a|S_t=s,\\theta]$: policy, shortened as $\\pi(s,a)$ or $\\pi(a|s)$\n\n다음으로는 reward를 표현하는 두 가지 방법에 대해서 알아보겠습니다.\n<br>\n\n## 2.4 Average Reward Formulation\nAverage rewward formulation은 시간의 흐름에 따른 reward를 표현한다기보다는 모든 시간의 reward를 평균을 내서 표현하는 방법입니다.\n\n* Long‐term expected reward per step\n$$\\rho(\\pi)=\\lim_{n\\to\\infty}\\frac{1}{n}E[r_1+r_2+\\cdots+r_n|\\pi] = \\sum_s d^\\pi(s)\\sum_a\\pi(s,a)\\mathcal{R}_s^a$$\n\n    - $d^\\pi(s): \\lim_{t\\to\\infty}\\Pr[S_t=s|s_0,\\pi]$: stationary distribution of states under $\\pi$\n    - $d^\\pi(s)$가 존재한다고 가정합니다. 그리고 모든 policy에 대해서 $d^\\pi(s)$는 $s_0$와 independent합니다.\n    - 위의 두 표현식은 왜 같을까요? Time average와 Ensemble average가 같다는 뜻으로 ergodic한 system에서 성립합니다.\n\n* Value of a state-action pair given a policy\n$$Q^{\\pi(s,a)} = \\sum_{t=1}^\\infty E[r_t - \\rho(\\pi)|S_0=s, A_0=a, \\pi]$$\n* State-valu function\n$$V^\\pi(s) = \\sum_{a} \\pi(s,a)Q^\\pi(s,a)$$\n<br>\n\n## 2.5 Start-State Formulation\nstart-state formulation은 시간의 흐름에 따라 감소하는 reward들을 표현합니다.\n\n* Long‐term expected reward per step with a designated start state $S_0$\n$$\\rho(\\pi) = E\\left[\\left.\\sum_{t=1}^\\infty \\gamma^{t-1}r_t\\right|S_0,\\pi\\right]$$\n    - $\\gamma\\in\\left[0,1\\right]$: discount rate\n\n* Value of a state‐action pair given a policy\n$$Q^\\pi(s,a) = E\\left[\\left.\\sum_{k=1}^\\infty \\gamma^{k-1} r_{t+k}\\right|S_t=s, A_t=a, \\pi\\right]$$\n\n* Alternative form of $Q^\\pi(s,a)$\n$$\nQ^\\pi(s,a) =R_s^a + \\gamma\\sum_{s'}P_{s s'}^a V^{\\pi}(s')\n$$\n<br>\n\n## 2.6 Policy Gradient Theorem\n논문의 중요한 결과인 Theorem 1은 다음과 같습니다.\n\n*For any MDP, in either the average‐reward or start‐state formulations,*\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)$$\n\n놀라운 부분은 expected return의 gradient를 취할 때 $\\frac{\\partial d^\\pi(s,a)}{\\partial\\theta}$를 구하지 않아도 된다는 점입니다. 즉, policy의 변화가 state distribution에 영향을 주지 않는다는 것입니다. 이것은 다시 말하면 아래 수식과 같이 우리는 $\\frac{\\partial\\rho}{\\partial\\theta}$에 대한 unbiased estimator를 구할 수 있다는 뜻입니다.\n\n$$E_s\\left[\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)\\right] = \\frac{\\partial\\rho}{\\partial\\theta}$$\n\n이로인해서 우리는 gradient를 sampling을 통해서 추정할 수 있습니다. 그렇지만 이것은 sample이 아주 많을 때만 성립합니다!\n\n또 한 가지 문제는 우리는 $Q^\\pi(s,a)$의 정확한 값을 알 수 없다는 것 입니다. 이것을 estimate하기 위해서 현재의 return값 $R_t$를 이용할 수 있습니다.\n<br>\n\n## 2.7 Proof of Policy Gradient Theorem\n증명을 살펴보겠습니다.\n\n* average-reward formulation을 이용할 때의 증명은 아래와 같습니다.\n\n$$\n\\begin{align}\n\\frac{\\partial V^\\pi(s)}{\\partial\\theta} &\\overset{\\underset{\\mathrm{def} }{} }{=}\\frac{\\partial}{\\partial\\theta}\\sum_a \\pi(s,a)Q^\\pi(s,a)\n\\\\\\\\\n&=\\sum_a \\frac{\\partial}{\\partial\\theta}\\left[\\pi(s,a)Q^\\pi(s,a)\\right]\\\\\\\\\n&=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\frac{\\partial Q^\\pi(s,a)}{\\partial\\theta}\\right]\\\\\\\\\n&=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\frac{\\partial}{\\partial\\theta}\\left[R_s^a -\\rho(\\pi) + \\sum_{s'}P_{s s'}^a V^{\\pi}(s')\\right]\\right]\\\\\\\\\n&=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\left[-\\frac{\\partial\\rho}{\\partial\\theta} + \\sum_{s'}\\mathcal{P}_{s s'}^a \\frac{\\partial V^{\\pi}(s')}{\\partial\\theta}\\right]\\right]\\\\\\\\\n\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\sum_{s'}\\mathcal{P}_{s s'}^a \\frac{\\partial V^{\\pi}(s')}{\\partial\\theta}\\right] - \\frac{\\partial V^\\pi(s)}{\\partial\\theta}\n\\end{align}\n$$\n\n양변에 stationary distribution에 대한 평균을 취합니다.\n$$\n\\begin{align}\n\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+ \\sum_s d^\\pi(s)\\sum_a\\pi(s,a)\\sum_{s'}\\mathcal{P}_{s s'}^a \\frac{\\partial V^{\\pi}(s')}{\\partial\\theta} \n\\\\\\\\\n&- \\sum_s d^\\pi(s)\\frac{\\partial V^\\pi(s)}{\\partial\\theta}\n\\end{align}\n$$\n\n모든 state $s$에서 모든 state $s'$으로 이동하는 것은 state $s'$에 존재할 확률이므로 다음과 같이 표현할 수 있습니다.\n\n$$\n\\begin{align}\n\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+ \\sum_s d^\\pi(s') \\frac{\\partial V^{\\pi}(s')}{\\partial\\theta} - \\sum_s d^\\pi(s)\\frac{\\partial V^\\pi(s)}{\\partial\\theta}\n\\end{align}\n$$\n\n뒤의 두 항은 같은 식이므로 이것은 다시 다음과 같이 표현됩니다.\n$$\n\\begin{align}\n\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)\n\\end{align}\n$$\n\n$\\frac{\\partial\\rho}{\\partial\\theta}$는 $s$에 대한 함수가 아니므로 $\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}=\\frac{\\partial\\rho}{\\partial\\theta}$입니다.\n이를 이용해서 위의 식을 다시 표현하면 아래와 같고 이를 통해 증명이 완성됩니다.\n\n$$\n\\begin{align}\n\\therefore\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)\n\\end{align}\n$$\n\n### From Sutton Book: Proof of Policy Gradient Theorem\n- start-state formulation (episodic case)은 서튼책에서도 나오는데 기존의 논문과 다르게 좀 더 진행되는 부분이 있다.\n\nstart-state formulation에 대한 증명 (episodic case)\n\n$$\\nabla{v_\\pi}(s)=\\nabla\\big[\\sum_a\\pi(a|s)q_\\pi(s,a)\\big]$$ \n\n- 다음으로 $(f\\cdot g)'=f'\\cdot g+f\\cdot g'$ (product rule of calculus)\n에 의해 다음과 같이 나타낼 수 있다.\n$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\nabla q_\\pi(s,a)\\big]$$  \n\n- 이어서 $p(s',r|s,a)\\cong Pr[S_t=s', R_t=r|S_{t-1}=s,A_{t-1}=a ]$ (서튼책 3.2 공식) 에 의해 다음과 같이 나타낼 수 있다.\n$$=\\sum_a\\big[\\nabla\\pi(a|s)q_{\\pi(s,a)}+\\pi(a|s)\\nabla\\big[\\sum_{s', r}p(s',r|s,a)(r+v_{\\pi}(s'))\\big]\\big]$$  \n- 계속해서 $p(s'|s,a)\\cong Pr[S_t=s', R_t=r|S_{t-1}=s,A_{t-1}=a ]=\\sum_{r\\in R}p(s',r|s,a)$ (서튼책 3.4 공식) 에 의해 다음과 같이 나타낼 수 있다.\n$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\sum_{s'}p(s'|s,a)\\nabla v_\\pi(s') \\big]$$ \n\n- 여기서 $\\nabla v_\\pi(s')$ 부분을 unrolling을 하면 다음과 같이 나타낼 수 있다.\n$$\n\\begin{align}\n=&\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\sum_{s'}p(s'|s,a)\\sum_{a'}\\big[\\nabla\\pi(a'|s')q_\\pi(s',a')\n\\\\\\\\\n&+\\pi(a'|s')\\sum_{s''}p(s''|s',a')\\nabla v_\\pi(s''))\\big] \\big]\n\\end{align}\n$$ \n\n- unrolling을 반복하다보면 다음과 같이 나타낼 수 있다.\n$$=\\sum_{x\\in S}\\sum_{k=0}^{\\infty}Pr(s\\rightarrow x,k,\\pi)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$\n\n- 여기에서 $Pr(s\\rightarrow x,k,\\pi)$ 는 policy $\\pi$ 에 대해 state s에서 state x까지 k step만큼 움직일 때의 변환 확률이다. 따라서 위의 수식을 아래처럼 다시 나타낼 수 있다.\n$$\n\\begin{align}\n\\nabla J(\\theta)= &\\nabla v_\\pi (s_0)\n\\\\\\\\\n=&\\sum_s(\\sum_{k=0}^{\\infty}Pr(s_0\\rightarrow s,k,\\pi))\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)\n\\\\\\\\\n=&\\sum_s \\eta(s) \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)\n\\end{align}$$ \n\n- 서튼책에서 $\\eta(s)$는 다음과 같이 표현할 수 있다.\n\n    $\\eta(s)$ = 평균적으로 하나의 episode마다 state에서 머무른 time step의 수 (page 199)\n\n    논문에서 $\\eta(s)$는 다음과 같다. \n\n    $d^\\pi(s)=\\sum_{t=0}^{\\infty}\\gamma^t Pr(s_t=s|s_0,\\pi)$ ($\\gamma=1$ is allowed only in episodic tasks) = discounted weighting of states)\n$$\\sum_s \\eta(s) \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$\n- 위의 수식을 아래의 수식처럼 바꿀 수 있다.\n$$\\sum_{s'} \\eta(s') \\cdot\\sum_s \\frac{\\eta(s)}{\\sum_{s'}\\eta(s')} \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$\n- 그러면 이것을 새로운 기호로 다시 나타낼 수 있다.\n$$\\sum_{s'} \\eta(s') \\cdot\\sum_s \\mu(s)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$ \n\n- 여기서 $\\mu(s)$ 는 state distribution이라고 하며, weight를 state distribution으로 바꿔주는 과정이라고 생각할 수 있다. 쉽게 말해 어떠한 state에 agent가 머무르는 확률이다.\n\n- 위의 수식은 아래의 수식과 비례한다. 따라서 최종 형태는 다음과 같다.\n$$\\propto \\sum_s \\mu(s)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$\n<br><br>\n\n---\n# 3. Policy Gradient with Approximation\n- 이번 section은 앞서 다뤘던 Theorem 1 중에 $Q^\\pi$에 대해서 중점적으로 다룬다.\n- 어떠한 $Q^\\pi$가 학습된 function approximator로 근사된다고 하자.\n- $f_w$ ($f_w$는 $S × A \\rightarrow \\Re$)를 parameter $w$를 가지는 어떠한 $Q^\\pi$ 에 대한 근사치라고 하자.\n    - 학습된 function approximator에 대해서 생각하기 때문에, 학습된 $f_w$는 error를 최소화하는 방향으로 parameter $w$를 업데이트된 것을 의미한다.\n- 그러면 하나의 rule에 의해서 $\\pi$를 따르고 $w$를 업데이트하는 $f_w$에 대해서 다음과 같이 생각할 수 있다.\n$$\\Delta w_t \\propto \\frac{\\partial}{\\partial w}\\big[ {\\hat Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]^2 \\propto \\big[{\\hat Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s_t,a_t)}{\\partial w}$$ \n(여기서 ${\\hat Q}^\\pi(s_t,a_t)$ 는 $Q^\\pi(s_t,a_t)$의 unbiased estimator $R_t$이다.)\n- 그러면 위와 같은 수식이 local optimum에 수렴을 했을 때, 다음과 같이 나타낼 수 있다.\n$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$\n\n- 위의 수식에 대한 추가 설명\n    - local optimum으로 수렴을 하기 때문에 당연히 위의 수식은 0이 된다.\n    - 또한 stochastic policy이기 때문에 local optimum으로 수렴하려면 모든 state, action에 대해 expectation을 해야한다. 따라서 $\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)$이 붙게 된다.\n<br>\n\n## 3.1 Theorem 2: Policy Gradient with Function Approximation\n- 만약 $f_w$가 아래의 등식을 만족한다고 하자.\n$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$\n\n또한 $f_w$가 policy parameterization과 양립할 수 있다고 하자. policy parameterization에 대한 여러가지 의미가 있는데 그 중에서 이 논문에서는 다음을 의미한다.\n$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}$$\n- 위의 수식에 대한 추가 설명\n    - 이 수식은 Monte-Calro Estimation을 이용하기 위한 조건이다.\n    - Compatibility Condition이라고 부른다.\n    - 우변(= $\\nabla\\log\\pi(s,a)$)으로 유도함으로써 아래의 수식과 같이 sampling을 이용한 추정이 가능해진다.\n\n- 따라서 위의 두 수식을 이용하여 아래와 같이 나타낼 수 있다.\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}f_w(s,a)$$\n<br>\n\n## 3.2 Proof of Theorem 2\n$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$\n\n$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}$$\n\n- 1와 2를 합치면 다음과 같이 나타낼 수 있다.\n$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}=0$$\n\n- 분자와 분모에 있는 $\\pi(s,a)$를 지우면 아래와 같다. \n$$\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]=0$$\n\n- 이어서 위의 수식이 0이기 때문에 policy gradient theorem(앞서 다뤘던 Theorem 1)에서 위의 수식을 뺄 수 있다.\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)-\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ Q^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]$$\n\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ Q^\\pi(s_t,a_t)-Q^\\pi(s_t,a_t)+f_w(s_t,a_t) \\big]$$\n\n$$\\therefore\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}f_w(s_t,a_t)$$\n<br><br>\n\n---\n# 4. Application to Deriving Algorithms and Advantages\n\n## 4.1 Application to Deriving Algorithms\n- feature의 linear combination에서 Gibbs distribution (softmax)를 하나의 policy로 생각해보자.\n$$\\pi(s,a)=\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_b e^{\\theta^{T}\\phi_{sb} }}$$\n\n($\\phi_{sa}$: state-action pair s,a을 나타내는 l-dimensional feature vector)\n\n- compatible condition을 추가하면 다음과 같다.\n$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\frac{1}{\\pi(s,a)}=\\phi_{sa}-\\sum_b \\pi(s,b)\\phi_{sb}$$ \n    - Proof of application of compatible condition 참고\n\n- 이어서 $f_w$을 적분한 natural parameterization은 다음과 같다.\n$$f_w(s,a)=w^T\\big[ \\phi_{sa}-\\sum_b \\pi(s,b)\\phi_{sb}\\big]$$\n- 위의 수식에 대한 추가 설명\n    - $f_w$는 policy로서 같은 feature들에 대해 linear하다.\n    - $f_w$는 각각의 state에 대해서 평균이 0이다. ($\\sum_a\\pi(s,a)f_w(s,a)=0$)\n    - advantage function $A^\\pi(s,a)=Q^\\pi(s,a)-V^\\pi(s)$의 하나의 근사치로서 $f_w$를 생각하는 것 좋다.\n<br>\n\n## 4.2 Proof of application of compatible condition\n증명을 하기 전에 다음을 가정한다.\n1. $(\\frac{f(x)}{g(x)})'=f'(x)\\cdot \\frac{1}{g(x)}-f(x)\\cdot \\frac{g'(x)}{g(x)^{2} }=\\frac{f'(x)\\cdot g(x)-f(x)\\cdot g'(x)}{g(x)^{2} }$\n2. $f(\\theta)=e^{\\theta^{T}\\phi_{sa} }$, $g(\\theta)=\\sum_be^{\\theta^{T}\\phi_{sb} }$\n\n$$\n\\begin{align}\n\\frac{\\partial f_w(s,a)}{\\partial w}&=\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\frac{1}{\\pi(s,a)}\n\\\\\\\\\n&=\\big(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }}\\big)'\\frac{1}{\\pi(s,a)}\n\\\\\\\\\n&=\\big[ \\phi_{sa}(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }})-(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }})\\frac{\\sum_b\\phi_{sb}e^{ {\\theta^T}\\phi_{sb} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }}\\big]\\frac{1}{\\pi(s,a)}\\\\\\\\\n&=\\big[ \\phi_{sa}\\pi(s,a)-\\pi(s,a)\\sum_b\\phi_{sb}\\pi(s,b)\\big]\\frac{1}{\\pi(s,a)}\n\\\\\\\\\n&=\\pi(s,a)\\big[ \\phi_{sa}-\\sum_b\\pi(s,b)\\phi_{sb}\\big]\\frac{1}{\\pi(s,a)}\n\\\\\\\\\n&\\therefore\\phi_{sa}-\\sum_b\\pi(s,b)\\phi_{sb}\n\\end{align}\n$$\n\n<br>\n\n## 4.3 Application to Advantages\n- Policy Gradient with Function Approximation Theorem (Theorem 2)는 advantage function으로 확장될 수 있다.\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_sd^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ f_w(s,a)+v(s) \\big]$$\n- 위의 수식에 대한 추가설명\n    - $v$ ($v$는 $S\\rightarrow\\Re$) 는 arbitrary function이다.\n    - 이 수식은 $\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}=0$이기 때문에 가능해진다.\n        - 즉, 이 수식은 $\\pi(s,a)$의 gradient에만 dependent하기 때문에 advantage 역학을 하는 함수들을 넣어도 아무런 상관이 없다.\n    - $v$의 선택은 Theorem들에 영향을 미치지 못하지만, 실질적으로 gradient estimator의 variance에 영향을 미친다.\n    - 이러한 문제는 전체적으로 이전의 연구에 reinforcement baseline의 사용에 있어서 유사하다.\n    - (comment) 위의 수식에서 $f_w(s,a)+v(s)$와 Application to Deriving Algorithms의 $f_w(s,a)$와는 다른 것이다. Application to Deriving Algorithms의 $f_w(s,a)$은 softmax에 의해 스스로 advantage function의 역할을 할 수 있다. 하지만 보통의 경우에는 그러지 못할 수도 있기 때문에 위의 수식처럼 $f_w(s,a)+v(s)$을 추가하여 zero mean만들어서 variance를 줄일 수 있다.\n<br><br>\n\n---\n# 5. Convergence of Policy Iteration with Function Approximation\n\n## 5.1 Theorem 3: Convergence of Policy Iteration with Function Approximation\n\npolicy iteration with function approximation의 형태는 locally optimal policy에 수렴한다. $\\pi$와 $f_w$를 \n\n1. compatibility condition을 만족하는 policy와 value function에 대해\n2. 어떠한 $\\max_{\\theta, s, a, i, j}\\big| \\frac{\\partial^2\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}\\big|<B<\\infty$ 을\n만족하는 어떠한 미분가능한 function approximator라고 하자. \n\n(comment) $\\pi(s,a)$라는 function은 이계미분값이 존재하고, 임의의 상수인 (Bound) B에 유계하기 때문에 function의 그래프는 smooth하다고 볼 수 있다. (오른쪽 그림 중 빨간색 그래프 참고)\n\n- ${\\alpha_k}$는 $\\lim_{k\\rightarrow\\infty}\\alpha_k=0$ 이며 $\\sum_k\\alpha_k=\\infty$를 만족하는 step-size sequence라고 하자.\n- 그 때, bounded reward를 가진 MDP에 대해\n    1. 어떠한 $\\theta_0, \\pi_k=\\pi(\\cdot,\\cdot,\\theta_k)$ \n    2. $\\sum_sd^{\\pi_{k} }(s)\\sum_a\\pi_k(s,a)\\big[ Q^{\\pi_{k} }(s,a)-f_w(s,a)\\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$로 인하여 $w_k=w$, $\\theta_{k+1}=\\theta_k+\\alpha_k\\sum_sd^{\\pi_{k} }(s)\\sum_a\\frac{\\partial\\pi_k(s,a)}{\\partial\\theta}f_{w_{k} }(s,a)$\n\n    으로 정의된 sequence $\\rho(\\pi_k)$는 $\\lim_{k\\rightarrow\\infty}\\frac{\\partial\\rho(\\pi_k)}{\\partial\\theta}=0$이기 때문에 수렴한다.\n        \n    - sequence $\\rho(\\pi_k)_{k=0}^\\infty$에 대한 추가 설명\n        - $\\theta_{k+1}=\\theta_k+\\alpha_k\\sum_sd^{\\pi_{k} }(s)\\sum_a\\frac{\\partial\\pi_k(s,a)}{\\partial\\theta}f_{w_{k} }(s,a)$ 에 따라 $\\theta$가 1, 2, ..., $\\infty$로 갈텐데, 거기에 따른 objective function or performance의 sequence이다.\n        - (comment) 굳이 sequence라는 표현이 없어도 될 것 같다. 어짜피 k가 $\\infty$로 가면 $\\rho(\\pi_k)$가 수렴한다는 의미이기 때문에 불필요해보인다.\n\n## 5.2 Proof of Theorem 3\n- Theorem 2는 $\\theta_k$ update가 gradient의 error를 최소화한다는 것을 증명했다.\n- $\\frac{\\partial^{2}\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}$와 MDP의 reward에서의 bound는, $\\frac{\\partial^{2}\\rho}{\\partial\\theta_i\\partial\\theta_j}$ 또한 bound된다는 것을 증명한다.\n- step-size 필요조건 때문에 이러한 bound된 것들은 Proposition 3.5 from page 94 of Bertsekas and Tsitsiklis(1996)에 적용하기 위해 필요한 조건이다.\n- Proposition 3.5 from page 94 of Bertsekas and Tsitsiklis(1996)은 local optimum으로 수렴한다는 것을 증명했다. \n","source":"_posts/sutton-pg.md","raw":"---\ntitle: Policy Gradient Methods for Reinforcement Learning with Function Approximation\ndate: 2018-06-15 18:29:32\ntags: [\"프로젝트\", \"피지여행\"]\ncategories: 프로젝트\nauthor: 이웅원\nsubtitle: 피지여행 1번째 논문\n---\n\n---\n# 1. Intro to Policy Gradient\n\n이 논문은 policy gradient (PG) 기법의 효시와도 같으며 향후 많은 파생연구를 낳은 중요한 논문입니다. 이 논문을 이해하기 위해 필요한 배경지식을 먼저 설명하고 논문을 차근차근 살펴보도록 하겠습니다.\n<br>\n\n## 1.1 Value Function Approach\n전통적으로 강화학습 기법은 value function을 기반으로 동작하였습니다. 특정 state에서 vaue function 또는 value function을 근사하는 함수(function approximation)를 최대화하는 action을 찾는 greedy action-selection policy가  대표적입니다.\n\n논문에서는 이러한 방법은 deterministic한 policy를 찾는 쪽으로 나아가게 되지만 종종 최적의 policy는 stochastic한 성질을 가지기 때문에 이 방법으로는 최적의 policy를 찾을 수 없다고 언급하고 있습니다. (이 논문이 나오고 나서 한참 후 개발된 deterministic policy gradient 기법과는 반대되는 서술일 수 있습니다.) 아마도 이러한 부분은 $\\epsilon$-greedy action-selection policy로 개선될 수 있을 것입니다. 또 하나의 문제는 value function의 작은 변화로 인해서 action이 크게 변할 수도 있다는 것입니다. 이것은 알고리듬의 수렴성에 문제를 야기할 수 있습니다. 이러한 문제점을 해결하기 위하여 policy search라는 새로운 기법이 고안됩니다.\n<br>\n\n## 1.2 Policy Search\npolicy search는 최적의 policy $\\pi^*$를 직접 찾기 때문에 value function 모델링이 필요가 없습니다. (뒤에서보겠지만 사실 꼭 그렇지만도 않습니다.) policy는 좀 더 다루기 쉽도록 parameter $\\theta$를 이용하여 $\\pi_{\\theta}$로 표현할 수 있습니다. 최적의 policy를 찾기 위하여 expected return $E\\left[R|\\theta\\right]$이 최대화되도록 parameter를 조정합니다. 이를 간단한 수식으로 정리하면 다음과 같습니다.\n\n$$\n\\pi^* = \\arg \\max\\limits_\\pi  E\\left[ {\\left. R \\right|\\pi } \\right] \\to {\\text{original problem} } \n\\\\\\\\  {\\Downarrow} \\\\\\\\\n{\\text{policy parameterization by } } \\pi_{\\theta}: \\Theta  \\to \\Pi \\\\\\\\ {\\Downarrow} $$\n\n$$\\pi^* = \\arg \\max\\limits_{\\theta}  E\\left[ {\\left. R \\right|\\theta } \\right] \\to {\\text{policy search problem} }\n$$\n\n\npolicy search 방법은 크게 두 가지로 분류할 수 있습니다. 첫 번째는 gradient-based optimization으로 policy gradient method가 이에 속합니다. 두 번째는 gradient-free optimization으로 진화(evolutionary) 연산을 이용하는 것이 이에 속합니다.\n<br>\n\n## 1.3 How to Obtain the Expected Return\nexpected return을 최대화하는 방향으로 policy를 update한다고 하였습니다. 그렇다면 expected return은 어떻게 구할 수 있을까요? 여기에도 크게 두 가지 방법이 있습니다. 첫 번째는 deterministic approximation으로 Markov decision process의 dynamics를 모델링한 후 수식적으로 구하는 것입니다. 두 번째 방법은 monte carlo estimation으로 dynamics에 대한 모델을 하지 않고 많은 sample들을 얻은 후 empirical하게 expected return을 계산하는 방법입니다. 두 번째 방법이 좀 더 현실적이지만 gradient를 구하는 것은 더 어렵습니다. 결국 gradient를 esimate하는방법을 고안해야 합니다. 가장 유명한 gradient estimation 방법이 1992년 R. J. Williams에 의해서 제안된 REINFORCE 기법입니다. REINFORCE는 Monte Carlo estimate 또는 likelihood-ratio estimate라고 부르는 방법을 이용해서 gradient와 expectation의 위치를 서로 바꿔줍니다.\n\n### Monte Carlo Gradient Estimation\n다음과 같은 parameter $\\theta$를 가지는 random variable $X$가 있습니다: $X:\\Omega\\mapsto\\mathcal{X}$. 그리고 이 $x$에 대한 함수 $f$가 있습니다: $f:\\mathcal{X}\\mapsto\\mathbb{R}$. expected return처럼 $E[f(x)]$를 최대화하고자 합니다. 이를 위해서는 $\\nabla_{\\theta}E[f(x)]$를 구해야 합니다. 이 때 다음과 같은 log derivate trick을 이용하여 다음과 같이 수식을 변형시킬 수 있습니다.\n\n$$\n\\begin{align}\n\\nabla_{\\theta}E_{p(x;\\theta)}\\left[ {f(x)} \\right] \n&= \\nabla_\\theta\\int{f\\left( x \\right)p\\left( {x;\\theta} \\right) dx}\n\\\\\\\\\n&= \\int { {\\nabla_\\theta }p\\left( {x;\\theta } \\right)f\\left( x \\right)dx} \n\\\\\\\\\n&= \\int {\\frac{ {p\\left( {x;\\theta } \\right)} }{ {p\\left( {x;\\theta } \\right)} }{\\nabla_\\theta }p\\left( {x;\\theta } \\right)f\\left( x \\right)dx} \n\\\\\\\\\n&= \\int {p\\left( {x;\\theta } \\right){\\nabla_\\theta }\\log p\\left( {x;\\theta } \\right)f\\left( x \\right)dx} \n\\\\\\\\\n&= E_{p(x;\\theta)}[f(x)\\nabla_\\theta \\log p({x;\\theta})]\n\\end{align}\n$$\n\n이제 expectation은 많은 샘플들을 모아서 평균을 취하는 Monte Carlo 기법으로 근사화할 수 있습니다.\n\n$${\\nabla_{\\theta} }{E_{p\\left( {x;\\theta } \\right)} }\\left[ {f\\left( x \\right)} \\right] = \\frac{1}{N}\\sum_{n=1}^N f\\left(X_n\\right)\\nabla_\\theta\\log p\\left(X_n;\\theta\\right)$$\n\n이 방법을 사용하기 위해서 필요한 조건은 $\\log p\\left(X_n;\\theta\\right)$가 미분가능해야 한다는 것 뿐입니다. 그러나 이 방법은 얻은 샘플들에 의존하기 때문에 경우에 따라서 큰 variance를 가질 수 있습니다.\n\n<br><br>\n\n---\n# 2. Policy Gradient Methods\n이제 본격적으로 policy gradient 기법에 대해서 알아보겠습니다.\n\n## 2.1 Policy Gradent Approach\npolicy gradient는 stochastic policy를 자체적인 파리미터를 가진 function approximator를 이용해서 근사화시킵니다. 몇 가지 notation들을 정의하겠습니다.\n\n* $\\theta$: policy parameter들의 vector  \n* $\\rho$: 해당 policy들의 성능을 나타내는 척도 (예. average reward per step)  \n* $\\alpha$: positive-definite한 step size \n\npolicy parameter는 gradient에 비례하여 업데이트됩니다.\n\n$$\\Delta\\theta \\approx \\alpha\\frac{\\partial\\rho}{\\partial\\theta}$$\n\nlocal optimal policy로 수렴합니다. 논문의 가장 중요한 contribution은 특정 조건을 만족하는 function approximator를 이용하여 경험(experience, sample)을 축적하고 이것들을 이용하여 위의 gradient를 unbiased estimate할 수 있다는 것을 증명한 것입니다.\n<br>\n\n## 2.2 Summary\n\n논문에서 설명한 policy gradient 기법을 요약하면 다음과 같습니다.\n\n* Original maximization problem: $\\max\\rho_{\\theta}=E\\left[R\\right]$\n* gradient ascent method를 이용한 parameter update: $\\theta_{t+1} = \\theta_{t} + \\alpha\\left.\\frac{\\partial E[R]}{\\partial\\theta}\\right|_{\\theta_t}$\n    - 그러나 우리는 gradient를 모름. 추정하기도 어려움. 왜냐하면 expectation이 안에 있기 때문임.\n    - 그렇다면 이것을 추정 가능한 형태로 바꿀 수 있을까? 즉, expectation이 밖에 있는 형태로!\n    - Expectation이 밖에 있으면 왜 추정이 유리할까? 바로 Sample mean을 취하면 되기 때문임!\n* Approximate gradient\n    - 방법1: 논문의 Theorem 1을 이용하면 gradient와 expectation의 위치를 바꿀 수 있음!\n    $$\\frac{\\partial E[R]}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)$$\n    - 방법2: Log derivative trick을 이용하면 gradient와 expectation의 위치를 바꿀 수 있음! (Theorem 2)\n    $$\\theta_{t+1} = \\theta_{t} + \\alpha E[R\\frac{\\partial}{\\partial\\theta}\\log p_{\\theta}|\\theta_t]$$\n    - 그리고 특정 trajectory를 따라가면서 return값을 구하고 이것을 여러 번 수행하여 sample mean을 취하면 gradient를 추정하는 것이 가능함.\n    - 이러한 기법을 제시한 기존의 연구가 REINFORCE임.\n    - 여러 Trajectory를 이용하므로 variance가 높을 수 밖에 없음.\n    - Advantage를 활용하거나 하는 방식으로 이후 여러 연구가 진행되었음.\n<br>\n\n## 2.3 System Model\n시스템모델은 다음과 같습니다.\n\n* Markov decision process (MDP) at each time $t\\in{0,1,2,...}$\n* $S_t\\in\\mathcal{S}$: state\n* $A_t\\in\\mathcal{A}$: action\n* $r_t\\in E$: reward\n* $\\theta\\in\\mathbb{R}^l$: vector of policy parameters, for $l\\ll\\left|\\mathcal{S}\\right|$\n* $\\mathcal{P}_{s s'}^a = \\Pr[S_{t+1}=s'|S_t=s,A_t=a]$: state transition probabilities\n* $\\mathcal{R}_{s s'}^a = E[R_{t+1}|S_t=s,A_t=a]$: expected reward\n* $\\pi(s,a,\\theta)=\\Pr[A_t=a|S_t=s,\\theta]$: policy, shortened as $\\pi(s,a)$ or $\\pi(a|s)$\n\n다음으로는 reward를 표현하는 두 가지 방법에 대해서 알아보겠습니다.\n<br>\n\n## 2.4 Average Reward Formulation\nAverage rewward formulation은 시간의 흐름에 따른 reward를 표현한다기보다는 모든 시간의 reward를 평균을 내서 표현하는 방법입니다.\n\n* Long‐term expected reward per step\n$$\\rho(\\pi)=\\lim_{n\\to\\infty}\\frac{1}{n}E[r_1+r_2+\\cdots+r_n|\\pi] = \\sum_s d^\\pi(s)\\sum_a\\pi(s,a)\\mathcal{R}_s^a$$\n\n    - $d^\\pi(s): \\lim_{t\\to\\infty}\\Pr[S_t=s|s_0,\\pi]$: stationary distribution of states under $\\pi$\n    - $d^\\pi(s)$가 존재한다고 가정합니다. 그리고 모든 policy에 대해서 $d^\\pi(s)$는 $s_0$와 independent합니다.\n    - 위의 두 표현식은 왜 같을까요? Time average와 Ensemble average가 같다는 뜻으로 ergodic한 system에서 성립합니다.\n\n* Value of a state-action pair given a policy\n$$Q^{\\pi(s,a)} = \\sum_{t=1}^\\infty E[r_t - \\rho(\\pi)|S_0=s, A_0=a, \\pi]$$\n* State-valu function\n$$V^\\pi(s) = \\sum_{a} \\pi(s,a)Q^\\pi(s,a)$$\n<br>\n\n## 2.5 Start-State Formulation\nstart-state formulation은 시간의 흐름에 따라 감소하는 reward들을 표현합니다.\n\n* Long‐term expected reward per step with a designated start state $S_0$\n$$\\rho(\\pi) = E\\left[\\left.\\sum_{t=1}^\\infty \\gamma^{t-1}r_t\\right|S_0,\\pi\\right]$$\n    - $\\gamma\\in\\left[0,1\\right]$: discount rate\n\n* Value of a state‐action pair given a policy\n$$Q^\\pi(s,a) = E\\left[\\left.\\sum_{k=1}^\\infty \\gamma^{k-1} r_{t+k}\\right|S_t=s, A_t=a, \\pi\\right]$$\n\n* Alternative form of $Q^\\pi(s,a)$\n$$\nQ^\\pi(s,a) =R_s^a + \\gamma\\sum_{s'}P_{s s'}^a V^{\\pi}(s')\n$$\n<br>\n\n## 2.6 Policy Gradient Theorem\n논문의 중요한 결과인 Theorem 1은 다음과 같습니다.\n\n*For any MDP, in either the average‐reward or start‐state formulations,*\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)$$\n\n놀라운 부분은 expected return의 gradient를 취할 때 $\\frac{\\partial d^\\pi(s,a)}{\\partial\\theta}$를 구하지 않아도 된다는 점입니다. 즉, policy의 변화가 state distribution에 영향을 주지 않는다는 것입니다. 이것은 다시 말하면 아래 수식과 같이 우리는 $\\frac{\\partial\\rho}{\\partial\\theta}$에 대한 unbiased estimator를 구할 수 있다는 뜻입니다.\n\n$$E_s\\left[\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)\\right] = \\frac{\\partial\\rho}{\\partial\\theta}$$\n\n이로인해서 우리는 gradient를 sampling을 통해서 추정할 수 있습니다. 그렇지만 이것은 sample이 아주 많을 때만 성립합니다!\n\n또 한 가지 문제는 우리는 $Q^\\pi(s,a)$의 정확한 값을 알 수 없다는 것 입니다. 이것을 estimate하기 위해서 현재의 return값 $R_t$를 이용할 수 있습니다.\n<br>\n\n## 2.7 Proof of Policy Gradient Theorem\n증명을 살펴보겠습니다.\n\n* average-reward formulation을 이용할 때의 증명은 아래와 같습니다.\n\n$$\n\\begin{align}\n\\frac{\\partial V^\\pi(s)}{\\partial\\theta} &\\overset{\\underset{\\mathrm{def} }{} }{=}\\frac{\\partial}{\\partial\\theta}\\sum_a \\pi(s,a)Q^\\pi(s,a)\n\\\\\\\\\n&=\\sum_a \\frac{\\partial}{\\partial\\theta}\\left[\\pi(s,a)Q^\\pi(s,a)\\right]\\\\\\\\\n&=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\frac{\\partial Q^\\pi(s,a)}{\\partial\\theta}\\right]\\\\\\\\\n&=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\frac{\\partial}{\\partial\\theta}\\left[R_s^a -\\rho(\\pi) + \\sum_{s'}P_{s s'}^a V^{\\pi}(s')\\right]\\right]\\\\\\\\\n&=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\left[-\\frac{\\partial\\rho}{\\partial\\theta} + \\sum_{s'}\\mathcal{P}_{s s'}^a \\frac{\\partial V^{\\pi}(s')}{\\partial\\theta}\\right]\\right]\\\\\\\\\n\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\sum_{s'}\\mathcal{P}_{s s'}^a \\frac{\\partial V^{\\pi}(s')}{\\partial\\theta}\\right] - \\frac{\\partial V^\\pi(s)}{\\partial\\theta}\n\\end{align}\n$$\n\n양변에 stationary distribution에 대한 평균을 취합니다.\n$$\n\\begin{align}\n\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+ \\sum_s d^\\pi(s)\\sum_a\\pi(s,a)\\sum_{s'}\\mathcal{P}_{s s'}^a \\frac{\\partial V^{\\pi}(s')}{\\partial\\theta} \n\\\\\\\\\n&- \\sum_s d^\\pi(s)\\frac{\\partial V^\\pi(s)}{\\partial\\theta}\n\\end{align}\n$$\n\n모든 state $s$에서 모든 state $s'$으로 이동하는 것은 state $s'$에 존재할 확률이므로 다음과 같이 표현할 수 있습니다.\n\n$$\n\\begin{align}\n\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+ \\sum_s d^\\pi(s') \\frac{\\partial V^{\\pi}(s')}{\\partial\\theta} - \\sum_s d^\\pi(s)\\frac{\\partial V^\\pi(s)}{\\partial\\theta}\n\\end{align}\n$$\n\n뒤의 두 항은 같은 식이므로 이것은 다시 다음과 같이 표현됩니다.\n$$\n\\begin{align}\n\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)\n\\end{align}\n$$\n\n$\\frac{\\partial\\rho}{\\partial\\theta}$는 $s$에 대한 함수가 아니므로 $\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}=\\frac{\\partial\\rho}{\\partial\\theta}$입니다.\n이를 이용해서 위의 식을 다시 표현하면 아래와 같고 이를 통해 증명이 완성됩니다.\n\n$$\n\\begin{align}\n\\therefore\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)\n\\end{align}\n$$\n\n### From Sutton Book: Proof of Policy Gradient Theorem\n- start-state formulation (episodic case)은 서튼책에서도 나오는데 기존의 논문과 다르게 좀 더 진행되는 부분이 있다.\n\nstart-state formulation에 대한 증명 (episodic case)\n\n$$\\nabla{v_\\pi}(s)=\\nabla\\big[\\sum_a\\pi(a|s)q_\\pi(s,a)\\big]$$ \n\n- 다음으로 $(f\\cdot g)'=f'\\cdot g+f\\cdot g'$ (product rule of calculus)\n에 의해 다음과 같이 나타낼 수 있다.\n$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\nabla q_\\pi(s,a)\\big]$$  \n\n- 이어서 $p(s',r|s,a)\\cong Pr[S_t=s', R_t=r|S_{t-1}=s,A_{t-1}=a ]$ (서튼책 3.2 공식) 에 의해 다음과 같이 나타낼 수 있다.\n$$=\\sum_a\\big[\\nabla\\pi(a|s)q_{\\pi(s,a)}+\\pi(a|s)\\nabla\\big[\\sum_{s', r}p(s',r|s,a)(r+v_{\\pi}(s'))\\big]\\big]$$  \n- 계속해서 $p(s'|s,a)\\cong Pr[S_t=s', R_t=r|S_{t-1}=s,A_{t-1}=a ]=\\sum_{r\\in R}p(s',r|s,a)$ (서튼책 3.4 공식) 에 의해 다음과 같이 나타낼 수 있다.\n$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\sum_{s'}p(s'|s,a)\\nabla v_\\pi(s') \\big]$$ \n\n- 여기서 $\\nabla v_\\pi(s')$ 부분을 unrolling을 하면 다음과 같이 나타낼 수 있다.\n$$\n\\begin{align}\n=&\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\sum_{s'}p(s'|s,a)\\sum_{a'}\\big[\\nabla\\pi(a'|s')q_\\pi(s',a')\n\\\\\\\\\n&+\\pi(a'|s')\\sum_{s''}p(s''|s',a')\\nabla v_\\pi(s''))\\big] \\big]\n\\end{align}\n$$ \n\n- unrolling을 반복하다보면 다음과 같이 나타낼 수 있다.\n$$=\\sum_{x\\in S}\\sum_{k=0}^{\\infty}Pr(s\\rightarrow x,k,\\pi)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$\n\n- 여기에서 $Pr(s\\rightarrow x,k,\\pi)$ 는 policy $\\pi$ 에 대해 state s에서 state x까지 k step만큼 움직일 때의 변환 확률이다. 따라서 위의 수식을 아래처럼 다시 나타낼 수 있다.\n$$\n\\begin{align}\n\\nabla J(\\theta)= &\\nabla v_\\pi (s_0)\n\\\\\\\\\n=&\\sum_s(\\sum_{k=0}^{\\infty}Pr(s_0\\rightarrow s,k,\\pi))\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)\n\\\\\\\\\n=&\\sum_s \\eta(s) \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)\n\\end{align}$$ \n\n- 서튼책에서 $\\eta(s)$는 다음과 같이 표현할 수 있다.\n\n    $\\eta(s)$ = 평균적으로 하나의 episode마다 state에서 머무른 time step의 수 (page 199)\n\n    논문에서 $\\eta(s)$는 다음과 같다. \n\n    $d^\\pi(s)=\\sum_{t=0}^{\\infty}\\gamma^t Pr(s_t=s|s_0,\\pi)$ ($\\gamma=1$ is allowed only in episodic tasks) = discounted weighting of states)\n$$\\sum_s \\eta(s) \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$\n- 위의 수식을 아래의 수식처럼 바꿀 수 있다.\n$$\\sum_{s'} \\eta(s') \\cdot\\sum_s \\frac{\\eta(s)}{\\sum_{s'}\\eta(s')} \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$\n- 그러면 이것을 새로운 기호로 다시 나타낼 수 있다.\n$$\\sum_{s'} \\eta(s') \\cdot\\sum_s \\mu(s)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$ \n\n- 여기서 $\\mu(s)$ 는 state distribution이라고 하며, weight를 state distribution으로 바꿔주는 과정이라고 생각할 수 있다. 쉽게 말해 어떠한 state에 agent가 머무르는 확률이다.\n\n- 위의 수식은 아래의 수식과 비례한다. 따라서 최종 형태는 다음과 같다.\n$$\\propto \\sum_s \\mu(s)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$\n<br><br>\n\n---\n# 3. Policy Gradient with Approximation\n- 이번 section은 앞서 다뤘던 Theorem 1 중에 $Q^\\pi$에 대해서 중점적으로 다룬다.\n- 어떠한 $Q^\\pi$가 학습된 function approximator로 근사된다고 하자.\n- $f_w$ ($f_w$는 $S × A \\rightarrow \\Re$)를 parameter $w$를 가지는 어떠한 $Q^\\pi$ 에 대한 근사치라고 하자.\n    - 학습된 function approximator에 대해서 생각하기 때문에, 학습된 $f_w$는 error를 최소화하는 방향으로 parameter $w$를 업데이트된 것을 의미한다.\n- 그러면 하나의 rule에 의해서 $\\pi$를 따르고 $w$를 업데이트하는 $f_w$에 대해서 다음과 같이 생각할 수 있다.\n$$\\Delta w_t \\propto \\frac{\\partial}{\\partial w}\\big[ {\\hat Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]^2 \\propto \\big[{\\hat Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s_t,a_t)}{\\partial w}$$ \n(여기서 ${\\hat Q}^\\pi(s_t,a_t)$ 는 $Q^\\pi(s_t,a_t)$의 unbiased estimator $R_t$이다.)\n- 그러면 위와 같은 수식이 local optimum에 수렴을 했을 때, 다음과 같이 나타낼 수 있다.\n$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$\n\n- 위의 수식에 대한 추가 설명\n    - local optimum으로 수렴을 하기 때문에 당연히 위의 수식은 0이 된다.\n    - 또한 stochastic policy이기 때문에 local optimum으로 수렴하려면 모든 state, action에 대해 expectation을 해야한다. 따라서 $\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)$이 붙게 된다.\n<br>\n\n## 3.1 Theorem 2: Policy Gradient with Function Approximation\n- 만약 $f_w$가 아래의 등식을 만족한다고 하자.\n$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$\n\n또한 $f_w$가 policy parameterization과 양립할 수 있다고 하자. policy parameterization에 대한 여러가지 의미가 있는데 그 중에서 이 논문에서는 다음을 의미한다.\n$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}$$\n- 위의 수식에 대한 추가 설명\n    - 이 수식은 Monte-Calro Estimation을 이용하기 위한 조건이다.\n    - Compatibility Condition이라고 부른다.\n    - 우변(= $\\nabla\\log\\pi(s,a)$)으로 유도함으로써 아래의 수식과 같이 sampling을 이용한 추정이 가능해진다.\n\n- 따라서 위의 두 수식을 이용하여 아래와 같이 나타낼 수 있다.\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}f_w(s,a)$$\n<br>\n\n## 3.2 Proof of Theorem 2\n$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$\n\n$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}$$\n\n- 1와 2를 합치면 다음과 같이 나타낼 수 있다.\n$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}=0$$\n\n- 분자와 분모에 있는 $\\pi(s,a)$를 지우면 아래와 같다. \n$$\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]=0$$\n\n- 이어서 위의 수식이 0이기 때문에 policy gradient theorem(앞서 다뤘던 Theorem 1)에서 위의 수식을 뺄 수 있다.\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)-\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ Q^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]$$\n\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ Q^\\pi(s_t,a_t)-Q^\\pi(s_t,a_t)+f_w(s_t,a_t) \\big]$$\n\n$$\\therefore\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}f_w(s_t,a_t)$$\n<br><br>\n\n---\n# 4. Application to Deriving Algorithms and Advantages\n\n## 4.1 Application to Deriving Algorithms\n- feature의 linear combination에서 Gibbs distribution (softmax)를 하나의 policy로 생각해보자.\n$$\\pi(s,a)=\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_b e^{\\theta^{T}\\phi_{sb} }}$$\n\n($\\phi_{sa}$: state-action pair s,a을 나타내는 l-dimensional feature vector)\n\n- compatible condition을 추가하면 다음과 같다.\n$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\frac{1}{\\pi(s,a)}=\\phi_{sa}-\\sum_b \\pi(s,b)\\phi_{sb}$$ \n    - Proof of application of compatible condition 참고\n\n- 이어서 $f_w$을 적분한 natural parameterization은 다음과 같다.\n$$f_w(s,a)=w^T\\big[ \\phi_{sa}-\\sum_b \\pi(s,b)\\phi_{sb}\\big]$$\n- 위의 수식에 대한 추가 설명\n    - $f_w$는 policy로서 같은 feature들에 대해 linear하다.\n    - $f_w$는 각각의 state에 대해서 평균이 0이다. ($\\sum_a\\pi(s,a)f_w(s,a)=0$)\n    - advantage function $A^\\pi(s,a)=Q^\\pi(s,a)-V^\\pi(s)$의 하나의 근사치로서 $f_w$를 생각하는 것 좋다.\n<br>\n\n## 4.2 Proof of application of compatible condition\n증명을 하기 전에 다음을 가정한다.\n1. $(\\frac{f(x)}{g(x)})'=f'(x)\\cdot \\frac{1}{g(x)}-f(x)\\cdot \\frac{g'(x)}{g(x)^{2} }=\\frac{f'(x)\\cdot g(x)-f(x)\\cdot g'(x)}{g(x)^{2} }$\n2. $f(\\theta)=e^{\\theta^{T}\\phi_{sa} }$, $g(\\theta)=\\sum_be^{\\theta^{T}\\phi_{sb} }$\n\n$$\n\\begin{align}\n\\frac{\\partial f_w(s,a)}{\\partial w}&=\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\frac{1}{\\pi(s,a)}\n\\\\\\\\\n&=\\big(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }}\\big)'\\frac{1}{\\pi(s,a)}\n\\\\\\\\\n&=\\big[ \\phi_{sa}(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }})-(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }})\\frac{\\sum_b\\phi_{sb}e^{ {\\theta^T}\\phi_{sb} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }}\\big]\\frac{1}{\\pi(s,a)}\\\\\\\\\n&=\\big[ \\phi_{sa}\\pi(s,a)-\\pi(s,a)\\sum_b\\phi_{sb}\\pi(s,b)\\big]\\frac{1}{\\pi(s,a)}\n\\\\\\\\\n&=\\pi(s,a)\\big[ \\phi_{sa}-\\sum_b\\pi(s,b)\\phi_{sb}\\big]\\frac{1}{\\pi(s,a)}\n\\\\\\\\\n&\\therefore\\phi_{sa}-\\sum_b\\pi(s,b)\\phi_{sb}\n\\end{align}\n$$\n\n<br>\n\n## 4.3 Application to Advantages\n- Policy Gradient with Function Approximation Theorem (Theorem 2)는 advantage function으로 확장될 수 있다.\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_sd^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ f_w(s,a)+v(s) \\big]$$\n- 위의 수식에 대한 추가설명\n    - $v$ ($v$는 $S\\rightarrow\\Re$) 는 arbitrary function이다.\n    - 이 수식은 $\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}=0$이기 때문에 가능해진다.\n        - 즉, 이 수식은 $\\pi(s,a)$의 gradient에만 dependent하기 때문에 advantage 역학을 하는 함수들을 넣어도 아무런 상관이 없다.\n    - $v$의 선택은 Theorem들에 영향을 미치지 못하지만, 실질적으로 gradient estimator의 variance에 영향을 미친다.\n    - 이러한 문제는 전체적으로 이전의 연구에 reinforcement baseline의 사용에 있어서 유사하다.\n    - (comment) 위의 수식에서 $f_w(s,a)+v(s)$와 Application to Deriving Algorithms의 $f_w(s,a)$와는 다른 것이다. Application to Deriving Algorithms의 $f_w(s,a)$은 softmax에 의해 스스로 advantage function의 역할을 할 수 있다. 하지만 보통의 경우에는 그러지 못할 수도 있기 때문에 위의 수식처럼 $f_w(s,a)+v(s)$을 추가하여 zero mean만들어서 variance를 줄일 수 있다.\n<br><br>\n\n---\n# 5. Convergence of Policy Iteration with Function Approximation\n\n## 5.1 Theorem 3: Convergence of Policy Iteration with Function Approximation\n\npolicy iteration with function approximation의 형태는 locally optimal policy에 수렴한다. $\\pi$와 $f_w$를 \n\n1. compatibility condition을 만족하는 policy와 value function에 대해\n2. 어떠한 $\\max_{\\theta, s, a, i, j}\\big| \\frac{\\partial^2\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}\\big|<B<\\infty$ 을\n만족하는 어떠한 미분가능한 function approximator라고 하자. \n\n(comment) $\\pi(s,a)$라는 function은 이계미분값이 존재하고, 임의의 상수인 (Bound) B에 유계하기 때문에 function의 그래프는 smooth하다고 볼 수 있다. (오른쪽 그림 중 빨간색 그래프 참고)\n\n- ${\\alpha_k}$는 $\\lim_{k\\rightarrow\\infty}\\alpha_k=0$ 이며 $\\sum_k\\alpha_k=\\infty$를 만족하는 step-size sequence라고 하자.\n- 그 때, bounded reward를 가진 MDP에 대해\n    1. 어떠한 $\\theta_0, \\pi_k=\\pi(\\cdot,\\cdot,\\theta_k)$ \n    2. $\\sum_sd^{\\pi_{k} }(s)\\sum_a\\pi_k(s,a)\\big[ Q^{\\pi_{k} }(s,a)-f_w(s,a)\\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$로 인하여 $w_k=w$, $\\theta_{k+1}=\\theta_k+\\alpha_k\\sum_sd^{\\pi_{k} }(s)\\sum_a\\frac{\\partial\\pi_k(s,a)}{\\partial\\theta}f_{w_{k} }(s,a)$\n\n    으로 정의된 sequence $\\rho(\\pi_k)$는 $\\lim_{k\\rightarrow\\infty}\\frac{\\partial\\rho(\\pi_k)}{\\partial\\theta}=0$이기 때문에 수렴한다.\n        \n    - sequence $\\rho(\\pi_k)_{k=0}^\\infty$에 대한 추가 설명\n        - $\\theta_{k+1}=\\theta_k+\\alpha_k\\sum_sd^{\\pi_{k} }(s)\\sum_a\\frac{\\partial\\pi_k(s,a)}{\\partial\\theta}f_{w_{k} }(s,a)$ 에 따라 $\\theta$가 1, 2, ..., $\\infty$로 갈텐데, 거기에 따른 objective function or performance의 sequence이다.\n        - (comment) 굳이 sequence라는 표현이 없어도 될 것 같다. 어짜피 k가 $\\infty$로 가면 $\\rho(\\pi_k)$가 수렴한다는 의미이기 때문에 불필요해보인다.\n\n## 5.2 Proof of Theorem 3\n- Theorem 2는 $\\theta_k$ update가 gradient의 error를 최소화한다는 것을 증명했다.\n- $\\frac{\\partial^{2}\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}$와 MDP의 reward에서의 bound는, $\\frac{\\partial^{2}\\rho}{\\partial\\theta_i\\partial\\theta_j}$ 또한 bound된다는 것을 증명한다.\n- step-size 필요조건 때문에 이러한 bound된 것들은 Proposition 3.5 from page 94 of Bertsekas and Tsitsiklis(1996)에 적용하기 위해 필요한 조건이다.\n- Proposition 3.5 from page 94 of Bertsekas and Tsitsiklis(1996)은 local optimum으로 수렴한다는 것을 증명했다. \n","slug":"sutton-pg","published":1,"updated":"2018-06-16T11:54:22.931Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjiinnj6n0002j28aigqpmv52","content":"<hr>\n<h1 id=\"1-Intro-to-Policy-Gradient\"><a href=\"#1-Intro-to-Policy-Gradient\" class=\"headerlink\" title=\"1. Intro to Policy Gradient\"></a>1. Intro to Policy Gradient</h1><p>이 논문은 policy gradient (PG) 기법의 효시와도 같으며 향후 많은 파생연구를 낳은 중요한 논문입니다. 이 논문을 이해하기 위해 필요한 배경지식을 먼저 설명하고 논문을 차근차근 살펴보도록 하겠습니다.<br><br></p>\n<h2 id=\"1-1-Value-Function-Approach\"><a href=\"#1-1-Value-Function-Approach\" class=\"headerlink\" title=\"1.1 Value Function Approach\"></a>1.1 Value Function Approach</h2><p>전통적으로 강화학습 기법은 value function을 기반으로 동작하였습니다. 특정 state에서 vaue function 또는 value function을 근사하는 함수(function approximation)를 최대화하는 action을 찾는 greedy action-selection policy가  대표적입니다.</p>\n<p>논문에서는 이러한 방법은 deterministic한 policy를 찾는 쪽으로 나아가게 되지만 종종 최적의 policy는 stochastic한 성질을 가지기 때문에 이 방법으로는 최적의 policy를 찾을 수 없다고 언급하고 있습니다. (이 논문이 나오고 나서 한참 후 개발된 deterministic policy gradient 기법과는 반대되는 서술일 수 있습니다.) 아마도 이러한 부분은 $\\epsilon$-greedy action-selection policy로 개선될 수 있을 것입니다. 또 하나의 문제는 value function의 작은 변화로 인해서 action이 크게 변할 수도 있다는 것입니다. 이것은 알고리듬의 수렴성에 문제를 야기할 수 있습니다. 이러한 문제점을 해결하기 위하여 policy search라는 새로운 기법이 고안됩니다.<br><br></p>\n<h2 id=\"1-2-Policy-Search\"><a href=\"#1-2-Policy-Search\" class=\"headerlink\" title=\"1.2 Policy Search\"></a>1.2 Policy Search</h2><p>policy search는 최적의 policy $\\pi^*$를 직접 찾기 때문에 value function 모델링이 필요가 없습니다. (뒤에서보겠지만 사실 꼭 그렇지만도 않습니다.) policy는 좀 더 다루기 쉽도록 parameter $\\theta$를 이용하여 $\\pi_{\\theta}$로 표현할 수 있습니다. 최적의 policy를 찾기 위하여 expected return $E\\left[R|\\theta\\right]$이 최대화되도록 parameter를 조정합니다. 이를 간단한 수식으로 정리하면 다음과 같습니다.</p>\n<p>$$<br>\\pi^* = \\arg \\max\\limits_\\pi  E\\left[ {\\left. R \\right|\\pi } \\right] \\to {\\text{original problem} }<br>\\\\  {\\Downarrow} \\\\<br>{\\text{policy parameterization by } } \\pi_{\\theta}: \\Theta  \\to \\Pi \\\\ {\\Downarrow} $$</p>\n<p>$$\\pi^* = \\arg \\max\\limits_{\\theta}  E\\left[ {\\left. R \\right|\\theta } \\right] \\to {\\text{policy search problem} }<br>$$</p>\n<p>policy search 방법은 크게 두 가지로 분류할 수 있습니다. 첫 번째는 gradient-based optimization으로 policy gradient method가 이에 속합니다. 두 번째는 gradient-free optimization으로 진화(evolutionary) 연산을 이용하는 것이 이에 속합니다.<br><br></p>\n<h2 id=\"1-3-How-to-Obtain-the-Expected-Return\"><a href=\"#1-3-How-to-Obtain-the-Expected-Return\" class=\"headerlink\" title=\"1.3 How to Obtain the Expected Return\"></a>1.3 How to Obtain the Expected Return</h2><p>expected return을 최대화하는 방향으로 policy를 update한다고 하였습니다. 그렇다면 expected return은 어떻게 구할 수 있을까요? 여기에도 크게 두 가지 방법이 있습니다. 첫 번째는 deterministic approximation으로 Markov decision process의 dynamics를 모델링한 후 수식적으로 구하는 것입니다. 두 번째 방법은 monte carlo estimation으로 dynamics에 대한 모델을 하지 않고 많은 sample들을 얻은 후 empirical하게 expected return을 계산하는 방법입니다. 두 번째 방법이 좀 더 현실적이지만 gradient를 구하는 것은 더 어렵습니다. 결국 gradient를 esimate하는방법을 고안해야 합니다. 가장 유명한 gradient estimation 방법이 1992년 R. J. Williams에 의해서 제안된 REINFORCE 기법입니다. REINFORCE는 Monte Carlo estimate 또는 likelihood-ratio estimate라고 부르는 방법을 이용해서 gradient와 expectation의 위치를 서로 바꿔줍니다.</p>\n<h3 id=\"Monte-Carlo-Gradient-Estimation\"><a href=\"#Monte-Carlo-Gradient-Estimation\" class=\"headerlink\" title=\"Monte Carlo Gradient Estimation\"></a>Monte Carlo Gradient Estimation</h3><p>다음과 같은 parameter $\\theta$를 가지는 random variable $X$가 있습니다: $X:\\Omega\\mapsto\\mathcal{X}$. 그리고 이 $x$에 대한 함수 $f$가 있습니다: $f:\\mathcal{X}\\mapsto\\mathbb{R}$. expected return처럼 $E[f(x)]$를 최대화하고자 합니다. 이를 위해서는 $\\nabla_{\\theta}E[f(x)]$를 구해야 합니다. 이 때 다음과 같은 log derivate trick을 이용하여 다음과 같이 수식을 변형시킬 수 있습니다.</p>\n<p>$$<br>\\begin{align}<br>\\nabla_{\\theta}E_{p(x;\\theta)}\\left[ {f(x)} \\right]<br>&amp;= \\nabla_\\theta\\int{f\\left( x \\right)p\\left( {x;\\theta} \\right) dx}<br>\\\\<br>&amp;= \\int { {\\nabla_\\theta }p\\left( {x;\\theta } \\right)f\\left( x \\right)dx}<br>\\\\<br>&amp;= \\int {\\frac{ {p\\left( {x;\\theta } \\right)} }{ {p\\left( {x;\\theta } \\right)} }{\\nabla_\\theta }p\\left( {x;\\theta } \\right)f\\left( x \\right)dx}<br>\\\\<br>&amp;= \\int {p\\left( {x;\\theta } \\right){\\nabla_\\theta }\\log p\\left( {x;\\theta } \\right)f\\left( x \\right)dx}<br>\\\\<br>&amp;= E_{p(x;\\theta)}[f(x)\\nabla_\\theta \\log p({x;\\theta})]<br>\\end{align}<br>$$</p>\n<p>이제 expectation은 많은 샘플들을 모아서 평균을 취하는 Monte Carlo 기법으로 근사화할 수 있습니다.</p>\n<p>$${\\nabla_{\\theta} }{E_{p\\left( {x;\\theta } \\right)} }\\left[ {f\\left( x \\right)} \\right] = \\frac{1}{N}\\sum_{n=1}^N f\\left(X_n\\right)\\nabla_\\theta\\log p\\left(X_n;\\theta\\right)$$</p>\n<p>이 방법을 사용하기 위해서 필요한 조건은 $\\log p\\left(X_n;\\theta\\right)$가 미분가능해야 한다는 것 뿐입니다. 그러나 이 방법은 얻은 샘플들에 의존하기 때문에 경우에 따라서 큰 variance를 가질 수 있습니다.</p>\n<p><br><br></p>\n<hr>\n<h1 id=\"2-Policy-Gradient-Methods\"><a href=\"#2-Policy-Gradient-Methods\" class=\"headerlink\" title=\"2. Policy Gradient Methods\"></a>2. Policy Gradient Methods</h1><p>이제 본격적으로 policy gradient 기법에 대해서 알아보겠습니다.</p>\n<h2 id=\"2-1-Policy-Gradent-Approach\"><a href=\"#2-1-Policy-Gradent-Approach\" class=\"headerlink\" title=\"2.1 Policy Gradent Approach\"></a>2.1 Policy Gradent Approach</h2><p>policy gradient는 stochastic policy를 자체적인 파리미터를 가진 function approximator를 이용해서 근사화시킵니다. 몇 가지 notation들을 정의하겠습니다.</p>\n<ul>\n<li>$\\theta$: policy parameter들의 vector  </li>\n<li>$\\rho$: 해당 policy들의 성능을 나타내는 척도 (예. average reward per step)  </li>\n<li>$\\alpha$: positive-definite한 step size </li>\n</ul>\n<p>policy parameter는 gradient에 비례하여 업데이트됩니다.</p>\n<p>$$\\Delta\\theta \\approx \\alpha\\frac{\\partial\\rho}{\\partial\\theta}$$</p>\n<p>local optimal policy로 수렴합니다. 논문의 가장 중요한 contribution은 특정 조건을 만족하는 function approximator를 이용하여 경험(experience, sample)을 축적하고 이것들을 이용하여 위의 gradient를 unbiased estimate할 수 있다는 것을 증명한 것입니다.<br><br></p>\n<h2 id=\"2-2-Summary\"><a href=\"#2-2-Summary\" class=\"headerlink\" title=\"2.2 Summary\"></a>2.2 Summary</h2><p>논문에서 설명한 policy gradient 기법을 요약하면 다음과 같습니다.</p>\n<ul>\n<li>Original maximization problem: $\\max\\rho_{\\theta}=E\\left[R\\right]$</li>\n<li>gradient ascent method를 이용한 parameter update: $\\theta_{t+1} = \\theta_{t} + \\alpha\\left.\\frac{\\partial E[R]}{\\partial\\theta}\\right|_{\\theta_t}$<ul>\n<li>그러나 우리는 gradient를 모름. 추정하기도 어려움. 왜냐하면 expectation이 안에 있기 때문임.</li>\n<li>그렇다면 이것을 추정 가능한 형태로 바꿀 수 있을까? 즉, expectation이 밖에 있는 형태로!</li>\n<li>Expectation이 밖에 있으면 왜 추정이 유리할까? 바로 Sample mean을 취하면 되기 때문임!</li>\n</ul>\n</li>\n<li>Approximate gradient<ul>\n<li>방법1: 논문의 Theorem 1을 이용하면 gradient와 expectation의 위치를 바꿀 수 있음!<br>$$\\frac{\\partial E[R]}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)$$</li>\n<li>방법2: Log derivative trick을 이용하면 gradient와 expectation의 위치를 바꿀 수 있음! (Theorem 2)<br>$$\\theta_{t+1} = \\theta_{t} + \\alpha E[R\\frac{\\partial}{\\partial\\theta}\\log p_{\\theta}|\\theta_t]$$</li>\n<li>그리고 특정 trajectory를 따라가면서 return값을 구하고 이것을 여러 번 수행하여 sample mean을 취하면 gradient를 추정하는 것이 가능함.</li>\n<li>이러한 기법을 제시한 기존의 연구가 REINFORCE임.</li>\n<li>여러 Trajectory를 이용하므로 variance가 높을 수 밖에 없음.</li>\n<li>Advantage를 활용하거나 하는 방식으로 이후 여러 연구가 진행되었음.<br><br></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"2-3-System-Model\"><a href=\"#2-3-System-Model\" class=\"headerlink\" title=\"2.3 System Model\"></a>2.3 System Model</h2><p>시스템모델은 다음과 같습니다.</p>\n<ul>\n<li>Markov decision process (MDP) at each time $t\\in{0,1,2,…}$</li>\n<li>$S_t\\in\\mathcal{S}$: state</li>\n<li>$A_t\\in\\mathcal{A}$: action</li>\n<li>$r_t\\in E$: reward</li>\n<li>$\\theta\\in\\mathbb{R}^l$: vector of policy parameters, for $l\\ll\\left|\\mathcal{S}\\right|$</li>\n<li>$\\mathcal{P}<em>{s s’}^a = \\Pr[S</em>{t+1}=s’|S_t=s,A_t=a]$: state transition probabilities</li>\n<li>$\\mathcal{R}<em>{s s’}^a = E[R</em>{t+1}|S_t=s,A_t=a]$: expected reward</li>\n<li>$\\pi(s,a,\\theta)=\\Pr[A_t=a|S_t=s,\\theta]$: policy, shortened as $\\pi(s,a)$ or $\\pi(a|s)$</li>\n</ul>\n<p>다음으로는 reward를 표현하는 두 가지 방법에 대해서 알아보겠습니다.<br><br></p>\n<h2 id=\"2-4-Average-Reward-Formulation\"><a href=\"#2-4-Average-Reward-Formulation\" class=\"headerlink\" title=\"2.4 Average Reward Formulation\"></a>2.4 Average Reward Formulation</h2><p>Average rewward formulation은 시간의 흐름에 따른 reward를 표현한다기보다는 모든 시간의 reward를 평균을 내서 표현하는 방법입니다.</p>\n<ul>\n<li><p>Long‐term expected reward per step<br>$$\\rho(\\pi)=\\lim_{n\\to\\infty}\\frac{1}{n}E[r_1+r_2+\\cdots+r_n|\\pi] = \\sum_s d^\\pi(s)\\sum_a\\pi(s,a)\\mathcal{R}_s^a$$</p>\n<ul>\n<li>$d^\\pi(s): \\lim_{t\\to\\infty}\\Pr[S_t=s|s_0,\\pi]$: stationary distribution of states under $\\pi$</li>\n<li>$d^\\pi(s)$가 존재한다고 가정합니다. 그리고 모든 policy에 대해서 $d^\\pi(s)$는 $s_0$와 independent합니다.</li>\n<li>위의 두 표현식은 왜 같을까요? Time average와 Ensemble average가 같다는 뜻으로 ergodic한 system에서 성립합니다.</li>\n</ul>\n</li>\n<li><p>Value of a state-action pair given a policy<br>$$Q^{\\pi(s,a)} = \\sum_{t=1}^\\infty E[r_t - \\rho(\\pi)|S_0=s, A_0=a, \\pi]$$</p>\n</li>\n<li>State-valu function<br>$$V^\\pi(s) = \\sum_{a} \\pi(s,a)Q^\\pi(s,a)$$<br><br></li>\n</ul>\n<h2 id=\"2-5-Start-State-Formulation\"><a href=\"#2-5-Start-State-Formulation\" class=\"headerlink\" title=\"2.5 Start-State Formulation\"></a>2.5 Start-State Formulation</h2><p>start-state formulation은 시간의 흐름에 따라 감소하는 reward들을 표현합니다.</p>\n<ul>\n<li><p>Long‐term expected reward per step with a designated start state $S_0$<br>$$\\rho(\\pi) = E\\left[\\left.\\sum_{t=1}^\\infty \\gamma^{t-1}r_t\\right|S_0,\\pi\\right]$$</p>\n<ul>\n<li>$\\gamma\\in\\left[0,1\\right]$: discount rate</li>\n</ul>\n</li>\n<li><p>Value of a state‐action pair given a policy<br>$$Q^\\pi(s,a) = E\\left[\\left.\\sum_{k=1}^\\infty \\gamma^{k-1} r_{t+k}\\right|S_t=s, A_t=a, \\pi\\right]$$</p>\n</li>\n<li><p>Alternative form of $Q^\\pi(s,a)$<br>$$<br>Q^\\pi(s,a) =R_s^a + \\gamma\\sum_{s’}P_{s s’}^a V^{\\pi}(s’)<br>$$<br><br></p>\n</li>\n</ul>\n<h2 id=\"2-6-Policy-Gradient-Theorem\"><a href=\"#2-6-Policy-Gradient-Theorem\" class=\"headerlink\" title=\"2.6 Policy Gradient Theorem\"></a>2.6 Policy Gradient Theorem</h2><p>논문의 중요한 결과인 Theorem 1은 다음과 같습니다.</p>\n<p><em>For any MDP, in either the average‐reward or start‐state formulations,</em><br>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)$$</p>\n<p>놀라운 부분은 expected return의 gradient를 취할 때 $\\frac{\\partial d^\\pi(s,a)}{\\partial\\theta}$를 구하지 않아도 된다는 점입니다. 즉, policy의 변화가 state distribution에 영향을 주지 않는다는 것입니다. 이것은 다시 말하면 아래 수식과 같이 우리는 $\\frac{\\partial\\rho}{\\partial\\theta}$에 대한 unbiased estimator를 구할 수 있다는 뜻입니다.</p>\n<p>$$E_s\\left[\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)\\right] = \\frac{\\partial\\rho}{\\partial\\theta}$$</p>\n<p>이로인해서 우리는 gradient를 sampling을 통해서 추정할 수 있습니다. 그렇지만 이것은 sample이 아주 많을 때만 성립합니다!</p>\n<p>또 한 가지 문제는 우리는 $Q^\\pi(s,a)$의 정확한 값을 알 수 없다는 것 입니다. 이것을 estimate하기 위해서 현재의 return값 $R_t$를 이용할 수 있습니다.<br><br></p>\n<h2 id=\"2-7-Proof-of-Policy-Gradient-Theorem\"><a href=\"#2-7-Proof-of-Policy-Gradient-Theorem\" class=\"headerlink\" title=\"2.7 Proof of Policy Gradient Theorem\"></a>2.7 Proof of Policy Gradient Theorem</h2><p>증명을 살펴보겠습니다.</p>\n<ul>\n<li>average-reward formulation을 이용할 때의 증명은 아래와 같습니다.</li>\n</ul>\n<p>$$<br>\\begin{align}<br>\\frac{\\partial V^\\pi(s)}{\\partial\\theta} &amp;\\overset{\\underset{\\mathrm{def} }{} }{=}\\frac{\\partial}{\\partial\\theta}\\sum_a \\pi(s,a)Q^\\pi(s,a)<br>\\\\<br>&amp;=\\sum_a \\frac{\\partial}{\\partial\\theta}\\left[\\pi(s,a)Q^\\pi(s,a)\\right]\\\\<br>&amp;=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\frac{\\partial Q^\\pi(s,a)}{\\partial\\theta}\\right]\\\\<br>&amp;=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\frac{\\partial}{\\partial\\theta}\\left[R_s^a -\\rho(\\pi) + \\sum_{s’}P_{s s’}^a V^{\\pi}(s’)\\right]\\right]\\\\<br>&amp;=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\left[-\\frac{\\partial\\rho}{\\partial\\theta} + \\sum_{s’}\\mathcal{P}_{s s’}^a \\frac{\\partial V^{\\pi}(s’)}{\\partial\\theta}\\right]\\right]\\\\<br>\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\sum_{s’}\\mathcal{P}_{s s’}^a \\frac{\\partial V^{\\pi}(s’)}{\\partial\\theta}\\right] - \\frac{\\partial V^\\pi(s)}{\\partial\\theta}<br>\\end{align}<br>$$</p>\n<p>양변에 stationary distribution에 대한 평균을 취합니다.<br>$$<br>\\begin{align}<br>\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+ \\sum_s d^\\pi(s)\\sum_a\\pi(s,a)\\sum_{s’}\\mathcal{P}_{s s’}^a \\frac{\\partial V^{\\pi}(s’)}{\\partial\\theta}<br>\\\\<br>&amp;- \\sum_s d^\\pi(s)\\frac{\\partial V^\\pi(s)}{\\partial\\theta}<br>\\end{align}<br>$$</p>\n<p>모든 state $s$에서 모든 state $s’$으로 이동하는 것은 state $s’$에 존재할 확률이므로 다음과 같이 표현할 수 있습니다.</p>\n<p>$$<br>\\begin{align}<br>\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+ \\sum_s d^\\pi(s’) \\frac{\\partial V^{\\pi}(s’)}{\\partial\\theta} - \\sum_s d^\\pi(s)\\frac{\\partial V^\\pi(s)}{\\partial\\theta}<br>\\end{align}<br>$$</p>\n<p>뒤의 두 항은 같은 식이므로 이것은 다시 다음과 같이 표현됩니다.<br>$$<br>\\begin{align}<br>\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)<br>\\end{align}<br>$$</p>\n<p>$\\frac{\\partial\\rho}{\\partial\\theta}$는 $s$에 대한 함수가 아니므로 $\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}=\\frac{\\partial\\rho}{\\partial\\theta}$입니다.<br>이를 이용해서 위의 식을 다시 표현하면 아래와 같고 이를 통해 증명이 완성됩니다.</p>\n<p>$$<br>\\begin{align}<br>\\therefore\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)<br>\\end{align}<br>$$</p>\n<h3 id=\"From-Sutton-Book-Proof-of-Policy-Gradient-Theorem\"><a href=\"#From-Sutton-Book-Proof-of-Policy-Gradient-Theorem\" class=\"headerlink\" title=\"From Sutton Book: Proof of Policy Gradient Theorem\"></a>From Sutton Book: Proof of Policy Gradient Theorem</h3><ul>\n<li>start-state formulation (episodic case)은 서튼책에서도 나오는데 기존의 논문과 다르게 좀 더 진행되는 부분이 있다.</li>\n</ul>\n<p>start-state formulation에 대한 증명 (episodic case)</p>\n<p>$$\\nabla{v_\\pi}(s)=\\nabla\\big[\\sum_a\\pi(a|s)q_\\pi(s,a)\\big]$$ </p>\n<ul>\n<li><p>다음으로 $(f\\cdot g)’=f’\\cdot g+f\\cdot g’$ (product rule of calculus)<br>에 의해 다음과 같이 나타낼 수 있다.<br>$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\nabla q_\\pi(s,a)\\big]$$  </p>\n</li>\n<li><p>이어서 $p(s’,r|s,a)\\cong Pr[S_t=s’, R_t=r|S_{t-1}=s,A_{t-1}=a ]$ (서튼책 3.2 공식) 에 의해 다음과 같이 나타낼 수 있다.<br>$$=\\sum_a\\big[\\nabla\\pi(a|s)q_{\\pi(s,a)}+\\pi(a|s)\\nabla\\big[\\sum_{s’, r}p(s’,r|s,a)(r+v_{\\pi}(s’))\\big]\\big]$$  </p>\n</li>\n<li><p>계속해서 $p(s’|s,a)\\cong Pr[S_t=s’, R_t=r|S_{t-1}=s,A_{t-1}=a ]=\\sum_{r\\in R}p(s’,r|s,a)$ (서튼책 3.4 공식) 에 의해 다음과 같이 나타낼 수 있다.<br>$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\sum_{s’}p(s’|s,a)\\nabla v_\\pi(s’) \\big]$$ </p>\n</li>\n<li><p>여기서 $\\nabla v_\\pi(s’)$ 부분을 unrolling을 하면 다음과 같이 나타낼 수 있다.<br>$$<br>\\begin{align}<br>=&amp;\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\sum_{s’}p(s’|s,a)\\sum_{a’}\\big[\\nabla\\pi(a’|s’)q_\\pi(s’,a’)<br>\\\\<br>&amp;+\\pi(a’|s’)\\sum_{s’’}p(s’’|s’,a’)\\nabla v_\\pi(s’’))\\big] \\big]<br>\\end{align}<br>$$ </p>\n</li>\n<li><p>unrolling을 반복하다보면 다음과 같이 나타낼 수 있다.<br>$$=\\sum_{x\\in S}\\sum_{k=0}^{\\infty}Pr(s\\rightarrow x,k,\\pi)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$</p>\n</li>\n<li><p>여기에서 $Pr(s\\rightarrow x,k,\\pi)$ 는 policy $\\pi$ 에 대해 state s에서 state x까지 k step만큼 움직일 때의 변환 확률이다. 따라서 위의 수식을 아래처럼 다시 나타낼 수 있다.<br>$$<br>\\begin{align}<br>\\nabla J(\\theta)= &amp;\\nabla v_\\pi (s_0)<br>\\\\<br>=&amp;\\sum_s(\\sum_{k=0}^{\\infty}Pr(s_0\\rightarrow s,k,\\pi))\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)<br>\\\\<br>=&amp;\\sum_s \\eta(s) \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)<br>\\end{align}$$ </p>\n</li>\n<li><p>서튼책에서 $\\eta(s)$는 다음과 같이 표현할 수 있다.</p>\n<p>  $\\eta(s)$ = 평균적으로 하나의 episode마다 state에서 머무른 time step의 수 (page 199)</p>\n<p>  논문에서 $\\eta(s)$는 다음과 같다. </p>\n<p>  $d^\\pi(s)=\\sum_{t=0}^{\\infty}\\gamma^t Pr(s_t=s|s_0,\\pi)$ ($\\gamma=1$ is allowed only in episodic tasks) = discounted weighting of states)<br>$$\\sum_s \\eta(s) \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$</p>\n</li>\n<li>위의 수식을 아래의 수식처럼 바꿀 수 있다.<br>$$\\sum_{s’} \\eta(s’) \\cdot\\sum_s \\frac{\\eta(s)}{\\sum_{s’}\\eta(s’)} \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$</li>\n<li><p>그러면 이것을 새로운 기호로 다시 나타낼 수 있다.<br>$$\\sum_{s’} \\eta(s’) \\cdot\\sum_s \\mu(s)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$ </p>\n</li>\n<li><p>여기서 $\\mu(s)$ 는 state distribution이라고 하며, weight를 state distribution으로 바꿔주는 과정이라고 생각할 수 있다. 쉽게 말해 어떠한 state에 agent가 머무르는 확률이다.</p>\n</li>\n<li><p>위의 수식은 아래의 수식과 비례한다. 따라서 최종 형태는 다음과 같다.<br>$$\\propto \\sum_s \\mu(s)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$<br><br><br></p>\n</li>\n</ul>\n<hr>\n<h1 id=\"3-Policy-Gradient-with-Approximation\"><a href=\"#3-Policy-Gradient-with-Approximation\" class=\"headerlink\" title=\"3. Policy Gradient with Approximation\"></a>3. Policy Gradient with Approximation</h1><ul>\n<li>이번 section은 앞서 다뤘던 Theorem 1 중에 $Q^\\pi$에 대해서 중점적으로 다룬다.</li>\n<li>어떠한 $Q^\\pi$가 학습된 function approximator로 근사된다고 하자.</li>\n<li>$f_w$ ($f_w$는 $S × A \\rightarrow \\Re$)를 parameter $w$를 가지는 어떠한 $Q^\\pi$ 에 대한 근사치라고 하자.<ul>\n<li>학습된 function approximator에 대해서 생각하기 때문에, 학습된 $f_w$는 error를 최소화하는 방향으로 parameter $w$를 업데이트된 것을 의미한다.</li>\n</ul>\n</li>\n<li>그러면 하나의 rule에 의해서 $\\pi$를 따르고 $w$를 업데이트하는 $f_w$에 대해서 다음과 같이 생각할 수 있다.<br>$$\\Delta w_t \\propto \\frac{\\partial}{\\partial w}\\big[ {\\hat Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]^2 \\propto \\big[{\\hat Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s_t,a_t)}{\\partial w}$$<br>(여기서 ${\\hat Q}^\\pi(s_t,a_t)$ 는 $Q^\\pi(s_t,a_t)$의 unbiased estimator $R_t$이다.)</li>\n<li><p>그러면 위와 같은 수식이 local optimum에 수렴을 했을 때, 다음과 같이 나타낼 수 있다.<br>$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$</p>\n</li>\n<li><p>위의 수식에 대한 추가 설명</p>\n<ul>\n<li>local optimum으로 수렴을 하기 때문에 당연히 위의 수식은 0이 된다.</li>\n<li>또한 stochastic policy이기 때문에 local optimum으로 수렴하려면 모든 state, action에 대해 expectation을 해야한다. 따라서 $\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)$이 붙게 된다.<br><br></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"3-1-Theorem-2-Policy-Gradient-with-Function-Approximation\"><a href=\"#3-1-Theorem-2-Policy-Gradient-with-Function-Approximation\" class=\"headerlink\" title=\"3.1 Theorem 2: Policy Gradient with Function Approximation\"></a>3.1 Theorem 2: Policy Gradient with Function Approximation</h2><ul>\n<li>만약 $f_w$가 아래의 등식을 만족한다고 하자.<br>$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$</li>\n</ul>\n<p>또한 $f_w$가 policy parameterization과 양립할 수 있다고 하자. policy parameterization에 대한 여러가지 의미가 있는데 그 중에서 이 논문에서는 다음을 의미한다.<br>$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}$$</p>\n<ul>\n<li><p>위의 수식에 대한 추가 설명</p>\n<ul>\n<li>이 수식은 Monte-Calro Estimation을 이용하기 위한 조건이다.</li>\n<li>Compatibility Condition이라고 부른다.</li>\n<li>우변(= $\\nabla\\log\\pi(s,a)$)으로 유도함으로써 아래의 수식과 같이 sampling을 이용한 추정이 가능해진다.</li>\n</ul>\n</li>\n<li><p>따라서 위의 두 수식을 이용하여 아래와 같이 나타낼 수 있다.<br>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}f_w(s,a)$$<br><br></p>\n</li>\n</ul>\n<h2 id=\"3-2-Proof-of-Theorem-2\"><a href=\"#3-2-Proof-of-Theorem-2\" class=\"headerlink\" title=\"3.2 Proof of Theorem 2\"></a>3.2 Proof of Theorem 2</h2><p>$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$</p>\n<p>$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}$$</p>\n<ul>\n<li><p>1와 2를 합치면 다음과 같이 나타낼 수 있다.<br>$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}=0$$</p>\n</li>\n<li><p>분자와 분모에 있는 $\\pi(s,a)$를 지우면 아래와 같다.<br>$$\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]=0$$</p>\n</li>\n<li><p>이어서 위의 수식이 0이기 때문에 policy gradient theorem(앞서 다뤘던 Theorem 1)에서 위의 수식을 뺄 수 있다.<br>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)-\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ Q^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]$$</p>\n</li>\n</ul>\n<p>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ Q^\\pi(s_t,a_t)-Q^\\pi(s_t,a_t)+f_w(s_t,a_t) \\big]$$</p>\n<p>$$\\therefore\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}f_w(s_t,a_t)$$<br><br><br></p>\n<hr>\n<h1 id=\"4-Application-to-Deriving-Algorithms-and-Advantages\"><a href=\"#4-Application-to-Deriving-Algorithms-and-Advantages\" class=\"headerlink\" title=\"4. Application to Deriving Algorithms and Advantages\"></a>4. Application to Deriving Algorithms and Advantages</h1><h2 id=\"4-1-Application-to-Deriving-Algorithms\"><a href=\"#4-1-Application-to-Deriving-Algorithms\" class=\"headerlink\" title=\"4.1 Application to Deriving Algorithms\"></a>4.1 Application to Deriving Algorithms</h2><ul>\n<li>feature의 linear combination에서 Gibbs distribution (softmax)를 하나의 policy로 생각해보자.<br>$$\\pi(s,a)=\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_b e^{\\theta^{T}\\phi_{sb} }}$$</li>\n</ul>\n<p>($\\phi_{sa}$: state-action pair s,a을 나타내는 l-dimensional feature vector)</p>\n<ul>\n<li><p>compatible condition을 추가하면 다음과 같다.<br>$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\frac{1}{\\pi(s,a)}=\\phi_{sa}-\\sum_b \\pi(s,b)\\phi_{sb}$$ </p>\n<ul>\n<li>Proof of application of compatible condition 참고</li>\n</ul>\n</li>\n<li><p>이어서 $f_w$을 적분한 natural parameterization은 다음과 같다.<br>$$f_w(s,a)=w^T\\big[ \\phi_{sa}-\\sum_b \\pi(s,b)\\phi_{sb}\\big]$$</p>\n</li>\n<li>위의 수식에 대한 추가 설명<ul>\n<li>$f_w$는 policy로서 같은 feature들에 대해 linear하다.</li>\n<li>$f_w$는 각각의 state에 대해서 평균이 0이다. ($\\sum_a\\pi(s,a)f_w(s,a)=0$)</li>\n<li>advantage function $A^\\pi(s,a)=Q^\\pi(s,a)-V^\\pi(s)$의 하나의 근사치로서 $f_w$를 생각하는 것 좋다.<br><br></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"4-2-Proof-of-application-of-compatible-condition\"><a href=\"#4-2-Proof-of-application-of-compatible-condition\" class=\"headerlink\" title=\"4.2 Proof of application of compatible condition\"></a>4.2 Proof of application of compatible condition</h2><p>증명을 하기 전에 다음을 가정한다.</p>\n<ol>\n<li>$(\\frac{f(x)}{g(x)})’=f’(x)\\cdot \\frac{1}{g(x)}-f(x)\\cdot \\frac{g’(x)}{g(x)^{2} }=\\frac{f’(x)\\cdot g(x)-f(x)\\cdot g’(x)}{g(x)^{2} }$</li>\n<li>$f(\\theta)=e^{\\theta^{T}\\phi_{sa} }$, $g(\\theta)=\\sum_be^{\\theta^{T}\\phi_{sb} }$</li>\n</ol>\n<p>$$<br>\\begin{align}<br>\\frac{\\partial f_w(s,a)}{\\partial w}&amp;=\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\frac{1}{\\pi(s,a)}<br>\\\\<br>&amp;=\\big(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }}\\big)’\\frac{1}{\\pi(s,a)}<br>\\\\<br>&amp;=\\big[ \\phi_{sa}(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }})-(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }})\\frac{\\sum_b\\phi_{sb}e^{ {\\theta^T}\\phi_{sb} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }}\\big]\\frac{1}{\\pi(s,a)}\\\\<br>&amp;=\\big[ \\phi_{sa}\\pi(s,a)-\\pi(s,a)\\sum_b\\phi_{sb}\\pi(s,b)\\big]\\frac{1}{\\pi(s,a)}<br>\\\\<br>&amp;=\\pi(s,a)\\big[ \\phi_{sa}-\\sum_b\\pi(s,b)\\phi_{sb}\\big]\\frac{1}{\\pi(s,a)}<br>\\\\<br>&amp;\\therefore\\phi_{sa}-\\sum_b\\pi(s,b)\\phi_{sb}<br>\\end{align}<br>$$</p>\n<p><br></p>\n<h2 id=\"4-3-Application-to-Advantages\"><a href=\"#4-3-Application-to-Advantages\" class=\"headerlink\" title=\"4.3 Application to Advantages\"></a>4.3 Application to Advantages</h2><ul>\n<li>Policy Gradient with Function Approximation Theorem (Theorem 2)는 advantage function으로 확장될 수 있다.<br>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_sd^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ f_w(s,a)+v(s) \\big]$$</li>\n<li>위의 수식에 대한 추가설명<ul>\n<li>$v$ ($v$는 $S\\rightarrow\\Re$) 는 arbitrary function이다.</li>\n<li>이 수식은 $\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}=0$이기 때문에 가능해진다.<ul>\n<li>즉, 이 수식은 $\\pi(s,a)$의 gradient에만 dependent하기 때문에 advantage 역학을 하는 함수들을 넣어도 아무런 상관이 없다.</li>\n</ul>\n</li>\n<li>$v$의 선택은 Theorem들에 영향을 미치지 못하지만, 실질적으로 gradient estimator의 variance에 영향을 미친다.</li>\n<li>이러한 문제는 전체적으로 이전의 연구에 reinforcement baseline의 사용에 있어서 유사하다.</li>\n<li>(comment) 위의 수식에서 $f_w(s,a)+v(s)$와 Application to Deriving Algorithms의 $f_w(s,a)$와는 다른 것이다. Application to Deriving Algorithms의 $f_w(s,a)$은 softmax에 의해 스스로 advantage function의 역할을 할 수 있다. 하지만 보통의 경우에는 그러지 못할 수도 있기 때문에 위의 수식처럼 $f_w(s,a)+v(s)$을 추가하여 zero mean만들어서 variance를 줄일 수 있다.<br><br><br></li>\n</ul>\n</li>\n</ul>\n<hr>\n<h1 id=\"5-Convergence-of-Policy-Iteration-with-Function-Approximation\"><a href=\"#5-Convergence-of-Policy-Iteration-with-Function-Approximation\" class=\"headerlink\" title=\"5. Convergence of Policy Iteration with Function Approximation\"></a>5. Convergence of Policy Iteration with Function Approximation</h1><h2 id=\"5-1-Theorem-3-Convergence-of-Policy-Iteration-with-Function-Approximation\"><a href=\"#5-1-Theorem-3-Convergence-of-Policy-Iteration-with-Function-Approximation\" class=\"headerlink\" title=\"5.1 Theorem 3: Convergence of Policy Iteration with Function Approximation\"></a>5.1 Theorem 3: Convergence of Policy Iteration with Function Approximation</h2><p>policy iteration with function approximation의 형태는 locally optimal policy에 수렴한다. $\\pi$와 $f_w$를 </p>\n<ol>\n<li>compatibility condition을 만족하는 policy와 value function에 대해</li>\n<li>어떠한 $\\max_{\\theta, s, a, i, j}\\big| \\frac{\\partial^2\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}\\big|&lt;B&lt;\\infty$ 을<br>만족하는 어떠한 미분가능한 function approximator라고 하자. </li>\n</ol>\n<p>(comment) $\\pi(s,a)$라는 function은 이계미분값이 존재하고, 임의의 상수인 (Bound) B에 유계하기 때문에 function의 그래프는 smooth하다고 볼 수 있다. (오른쪽 그림 중 빨간색 그래프 참고)</p>\n<ul>\n<li>${\\alpha_k}$는 $\\lim_{k\\rightarrow\\infty}\\alpha_k=0$ 이며 $\\sum_k\\alpha_k=\\infty$를 만족하는 step-size sequence라고 하자.</li>\n<li><p>그 때, bounded reward를 가진 MDP에 대해</p>\n<ol>\n<li>어떠한 $\\theta_0, \\pi_k=\\pi(\\cdot,\\cdot,\\theta_k)$ </li>\n<li><p>$\\sum_sd^{\\pi_{k} }(s)\\sum_a\\pi_k(s,a)\\big[ Q^{\\pi_{k} }(s,a)-f_w(s,a)\\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$로 인하여 $w_k=w$, $\\theta_{k+1}=\\theta_k+\\alpha_k\\sum_sd^{\\pi_{k} }(s)\\sum_a\\frac{\\partial\\pi_k(s,a)}{\\partial\\theta}f_{w_{k} }(s,a)$</p>\n<p>으로 정의된 sequence $\\rho(\\pi_k)$는 $\\lim_{k\\rightarrow\\infty}\\frac{\\partial\\rho(\\pi_k)}{\\partial\\theta}=0$이기 때문에 수렴한다.</p>\n</li>\n</ol>\n<ul>\n<li>sequence $\\rho(\\pi_k)_{k=0}^\\infty$에 대한 추가 설명<ul>\n<li>$\\theta_{k+1}=\\theta_k+\\alpha_k\\sum_sd^{\\pi_{k} }(s)\\sum_a\\frac{\\partial\\pi_k(s,a)}{\\partial\\theta}f_{w_{k} }(s,a)$ 에 따라 $\\theta$가 1, 2, …, $\\infty$로 갈텐데, 거기에 따른 objective function or performance의 sequence이다.</li>\n<li>(comment) 굳이 sequence라는 표현이 없어도 될 것 같다. 어짜피 k가 $\\infty$로 가면 $\\rho(\\pi_k)$가 수렴한다는 의미이기 때문에 불필요해보인다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"5-2-Proof-of-Theorem-3\"><a href=\"#5-2-Proof-of-Theorem-3\" class=\"headerlink\" title=\"5.2 Proof of Theorem 3\"></a>5.2 Proof of Theorem 3</h2><ul>\n<li>Theorem 2는 $\\theta_k$ update가 gradient의 error를 최소화한다는 것을 증명했다.</li>\n<li>$\\frac{\\partial^{2}\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}$와 MDP의 reward에서의 bound는, $\\frac{\\partial^{2}\\rho}{\\partial\\theta_i\\partial\\theta_j}$ 또한 bound된다는 것을 증명한다.</li>\n<li>step-size 필요조건 때문에 이러한 bound된 것들은 Proposition 3.5 from page 94 of Bertsekas and Tsitsiklis(1996)에 적용하기 위해 필요한 조건이다.</li>\n<li>Proposition 3.5 from page 94 of Bertsekas and Tsitsiklis(1996)은 local optimum으로 수렴한다는 것을 증명했다. </li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<hr>\n<h1 id=\"1-Intro-to-Policy-Gradient\"><a href=\"#1-Intro-to-Policy-Gradient\" class=\"headerlink\" title=\"1. Intro to Policy Gradient\"></a>1. Intro to Policy Gradient</h1><p>이 논문은 policy gradient (PG) 기법의 효시와도 같으며 향후 많은 파생연구를 낳은 중요한 논문입니다. 이 논문을 이해하기 위해 필요한 배경지식을 먼저 설명하고 논문을 차근차근 살펴보도록 하겠습니다.<br><br></p>\n<h2 id=\"1-1-Value-Function-Approach\"><a href=\"#1-1-Value-Function-Approach\" class=\"headerlink\" title=\"1.1 Value Function Approach\"></a>1.1 Value Function Approach</h2><p>전통적으로 강화학습 기법은 value function을 기반으로 동작하였습니다. 특정 state에서 vaue function 또는 value function을 근사하는 함수(function approximation)를 최대화하는 action을 찾는 greedy action-selection policy가  대표적입니다.</p>\n<p>논문에서는 이러한 방법은 deterministic한 policy를 찾는 쪽으로 나아가게 되지만 종종 최적의 policy는 stochastic한 성질을 가지기 때문에 이 방법으로는 최적의 policy를 찾을 수 없다고 언급하고 있습니다. (이 논문이 나오고 나서 한참 후 개발된 deterministic policy gradient 기법과는 반대되는 서술일 수 있습니다.) 아마도 이러한 부분은 $\\epsilon$-greedy action-selection policy로 개선될 수 있을 것입니다. 또 하나의 문제는 value function의 작은 변화로 인해서 action이 크게 변할 수도 있다는 것입니다. 이것은 알고리듬의 수렴성에 문제를 야기할 수 있습니다. 이러한 문제점을 해결하기 위하여 policy search라는 새로운 기법이 고안됩니다.<br><br></p>\n<h2 id=\"1-2-Policy-Search\"><a href=\"#1-2-Policy-Search\" class=\"headerlink\" title=\"1.2 Policy Search\"></a>1.2 Policy Search</h2><p>policy search는 최적의 policy $\\pi^*$를 직접 찾기 때문에 value function 모델링이 필요가 없습니다. (뒤에서보겠지만 사실 꼭 그렇지만도 않습니다.) policy는 좀 더 다루기 쉽도록 parameter $\\theta$를 이용하여 $\\pi_{\\theta}$로 표현할 수 있습니다. 최적의 policy를 찾기 위하여 expected return $E\\left[R|\\theta\\right]$이 최대화되도록 parameter를 조정합니다. 이를 간단한 수식으로 정리하면 다음과 같습니다.</p>\n<p>$$<br>\\pi^* = \\arg \\max\\limits_\\pi  E\\left[ {\\left. R \\right|\\pi } \\right] \\to {\\text{original problem} }<br>\\\\  {\\Downarrow} \\\\<br>{\\text{policy parameterization by } } \\pi_{\\theta}: \\Theta  \\to \\Pi \\\\ {\\Downarrow} $$</p>\n<p>$$\\pi^* = \\arg \\max\\limits_{\\theta}  E\\left[ {\\left. R \\right|\\theta } \\right] \\to {\\text{policy search problem} }<br>$$</p>\n<p>policy search 방법은 크게 두 가지로 분류할 수 있습니다. 첫 번째는 gradient-based optimization으로 policy gradient method가 이에 속합니다. 두 번째는 gradient-free optimization으로 진화(evolutionary) 연산을 이용하는 것이 이에 속합니다.<br><br></p>\n<h2 id=\"1-3-How-to-Obtain-the-Expected-Return\"><a href=\"#1-3-How-to-Obtain-the-Expected-Return\" class=\"headerlink\" title=\"1.3 How to Obtain the Expected Return\"></a>1.3 How to Obtain the Expected Return</h2><p>expected return을 최대화하는 방향으로 policy를 update한다고 하였습니다. 그렇다면 expected return은 어떻게 구할 수 있을까요? 여기에도 크게 두 가지 방법이 있습니다. 첫 번째는 deterministic approximation으로 Markov decision process의 dynamics를 모델링한 후 수식적으로 구하는 것입니다. 두 번째 방법은 monte carlo estimation으로 dynamics에 대한 모델을 하지 않고 많은 sample들을 얻은 후 empirical하게 expected return을 계산하는 방법입니다. 두 번째 방법이 좀 더 현실적이지만 gradient를 구하는 것은 더 어렵습니다. 결국 gradient를 esimate하는방법을 고안해야 합니다. 가장 유명한 gradient estimation 방법이 1992년 R. J. Williams에 의해서 제안된 REINFORCE 기법입니다. REINFORCE는 Monte Carlo estimate 또는 likelihood-ratio estimate라고 부르는 방법을 이용해서 gradient와 expectation의 위치를 서로 바꿔줍니다.</p>\n<h3 id=\"Monte-Carlo-Gradient-Estimation\"><a href=\"#Monte-Carlo-Gradient-Estimation\" class=\"headerlink\" title=\"Monte Carlo Gradient Estimation\"></a>Monte Carlo Gradient Estimation</h3><p>다음과 같은 parameter $\\theta$를 가지는 random variable $X$가 있습니다: $X:\\Omega\\mapsto\\mathcal{X}$. 그리고 이 $x$에 대한 함수 $f$가 있습니다: $f:\\mathcal{X}\\mapsto\\mathbb{R}$. expected return처럼 $E[f(x)]$를 최대화하고자 합니다. 이를 위해서는 $\\nabla_{\\theta}E[f(x)]$를 구해야 합니다. 이 때 다음과 같은 log derivate trick을 이용하여 다음과 같이 수식을 변형시킬 수 있습니다.</p>\n<p>$$<br>\\begin{align}<br>\\nabla_{\\theta}E_{p(x;\\theta)}\\left[ {f(x)} \\right]<br>&amp;= \\nabla_\\theta\\int{f\\left( x \\right)p\\left( {x;\\theta} \\right) dx}<br>\\\\<br>&amp;= \\int { {\\nabla_\\theta }p\\left( {x;\\theta } \\right)f\\left( x \\right)dx}<br>\\\\<br>&amp;= \\int {\\frac{ {p\\left( {x;\\theta } \\right)} }{ {p\\left( {x;\\theta } \\right)} }{\\nabla_\\theta }p\\left( {x;\\theta } \\right)f\\left( x \\right)dx}<br>\\\\<br>&amp;= \\int {p\\left( {x;\\theta } \\right){\\nabla_\\theta }\\log p\\left( {x;\\theta } \\right)f\\left( x \\right)dx}<br>\\\\<br>&amp;= E_{p(x;\\theta)}[f(x)\\nabla_\\theta \\log p({x;\\theta})]<br>\\end{align}<br>$$</p>\n<p>이제 expectation은 많은 샘플들을 모아서 평균을 취하는 Monte Carlo 기법으로 근사화할 수 있습니다.</p>\n<p>$${\\nabla_{\\theta} }{E_{p\\left( {x;\\theta } \\right)} }\\left[ {f\\left( x \\right)} \\right] = \\frac{1}{N}\\sum_{n=1}^N f\\left(X_n\\right)\\nabla_\\theta\\log p\\left(X_n;\\theta\\right)$$</p>\n<p>이 방법을 사용하기 위해서 필요한 조건은 $\\log p\\left(X_n;\\theta\\right)$가 미분가능해야 한다는 것 뿐입니다. 그러나 이 방법은 얻은 샘플들에 의존하기 때문에 경우에 따라서 큰 variance를 가질 수 있습니다.</p>\n<p><br><br></p>\n<hr>\n<h1 id=\"2-Policy-Gradient-Methods\"><a href=\"#2-Policy-Gradient-Methods\" class=\"headerlink\" title=\"2. Policy Gradient Methods\"></a>2. Policy Gradient Methods</h1><p>이제 본격적으로 policy gradient 기법에 대해서 알아보겠습니다.</p>\n<h2 id=\"2-1-Policy-Gradent-Approach\"><a href=\"#2-1-Policy-Gradent-Approach\" class=\"headerlink\" title=\"2.1 Policy Gradent Approach\"></a>2.1 Policy Gradent Approach</h2><p>policy gradient는 stochastic policy를 자체적인 파리미터를 가진 function approximator를 이용해서 근사화시킵니다. 몇 가지 notation들을 정의하겠습니다.</p>\n<ul>\n<li>$\\theta$: policy parameter들의 vector  </li>\n<li>$\\rho$: 해당 policy들의 성능을 나타내는 척도 (예. average reward per step)  </li>\n<li>$\\alpha$: positive-definite한 step size </li>\n</ul>\n<p>policy parameter는 gradient에 비례하여 업데이트됩니다.</p>\n<p>$$\\Delta\\theta \\approx \\alpha\\frac{\\partial\\rho}{\\partial\\theta}$$</p>\n<p>local optimal policy로 수렴합니다. 논문의 가장 중요한 contribution은 특정 조건을 만족하는 function approximator를 이용하여 경험(experience, sample)을 축적하고 이것들을 이용하여 위의 gradient를 unbiased estimate할 수 있다는 것을 증명한 것입니다.<br><br></p>\n<h2 id=\"2-2-Summary\"><a href=\"#2-2-Summary\" class=\"headerlink\" title=\"2.2 Summary\"></a>2.2 Summary</h2><p>논문에서 설명한 policy gradient 기법을 요약하면 다음과 같습니다.</p>\n<ul>\n<li>Original maximization problem: $\\max\\rho_{\\theta}=E\\left[R\\right]$</li>\n<li>gradient ascent method를 이용한 parameter update: $\\theta_{t+1} = \\theta_{t} + \\alpha\\left.\\frac{\\partial E[R]}{\\partial\\theta}\\right|_{\\theta_t}$<ul>\n<li>그러나 우리는 gradient를 모름. 추정하기도 어려움. 왜냐하면 expectation이 안에 있기 때문임.</li>\n<li>그렇다면 이것을 추정 가능한 형태로 바꿀 수 있을까? 즉, expectation이 밖에 있는 형태로!</li>\n<li>Expectation이 밖에 있으면 왜 추정이 유리할까? 바로 Sample mean을 취하면 되기 때문임!</li>\n</ul>\n</li>\n<li>Approximate gradient<ul>\n<li>방법1: 논문의 Theorem 1을 이용하면 gradient와 expectation의 위치를 바꿀 수 있음!<br>$$\\frac{\\partial E[R]}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)$$</li>\n<li>방법2: Log derivative trick을 이용하면 gradient와 expectation의 위치를 바꿀 수 있음! (Theorem 2)<br>$$\\theta_{t+1} = \\theta_{t} + \\alpha E[R\\frac{\\partial}{\\partial\\theta}\\log p_{\\theta}|\\theta_t]$$</li>\n<li>그리고 특정 trajectory를 따라가면서 return값을 구하고 이것을 여러 번 수행하여 sample mean을 취하면 gradient를 추정하는 것이 가능함.</li>\n<li>이러한 기법을 제시한 기존의 연구가 REINFORCE임.</li>\n<li>여러 Trajectory를 이용하므로 variance가 높을 수 밖에 없음.</li>\n<li>Advantage를 활용하거나 하는 방식으로 이후 여러 연구가 진행되었음.<br><br></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"2-3-System-Model\"><a href=\"#2-3-System-Model\" class=\"headerlink\" title=\"2.3 System Model\"></a>2.3 System Model</h2><p>시스템모델은 다음과 같습니다.</p>\n<ul>\n<li>Markov decision process (MDP) at each time $t\\in{0,1,2,…}$</li>\n<li>$S_t\\in\\mathcal{S}$: state</li>\n<li>$A_t\\in\\mathcal{A}$: action</li>\n<li>$r_t\\in E$: reward</li>\n<li>$\\theta\\in\\mathbb{R}^l$: vector of policy parameters, for $l\\ll\\left|\\mathcal{S}\\right|$</li>\n<li>$\\mathcal{P}<em>{s s’}^a = \\Pr[S</em>{t+1}=s’|S_t=s,A_t=a]$: state transition probabilities</li>\n<li>$\\mathcal{R}<em>{s s’}^a = E[R</em>{t+1}|S_t=s,A_t=a]$: expected reward</li>\n<li>$\\pi(s,a,\\theta)=\\Pr[A_t=a|S_t=s,\\theta]$: policy, shortened as $\\pi(s,a)$ or $\\pi(a|s)$</li>\n</ul>\n<p>다음으로는 reward를 표현하는 두 가지 방법에 대해서 알아보겠습니다.<br><br></p>\n<h2 id=\"2-4-Average-Reward-Formulation\"><a href=\"#2-4-Average-Reward-Formulation\" class=\"headerlink\" title=\"2.4 Average Reward Formulation\"></a>2.4 Average Reward Formulation</h2><p>Average rewward formulation은 시간의 흐름에 따른 reward를 표현한다기보다는 모든 시간의 reward를 평균을 내서 표현하는 방법입니다.</p>\n<ul>\n<li><p>Long‐term expected reward per step<br>$$\\rho(\\pi)=\\lim_{n\\to\\infty}\\frac{1}{n}E[r_1+r_2+\\cdots+r_n|\\pi] = \\sum_s d^\\pi(s)\\sum_a\\pi(s,a)\\mathcal{R}_s^a$$</p>\n<ul>\n<li>$d^\\pi(s): \\lim_{t\\to\\infty}\\Pr[S_t=s|s_0,\\pi]$: stationary distribution of states under $\\pi$</li>\n<li>$d^\\pi(s)$가 존재한다고 가정합니다. 그리고 모든 policy에 대해서 $d^\\pi(s)$는 $s_0$와 independent합니다.</li>\n<li>위의 두 표현식은 왜 같을까요? Time average와 Ensemble average가 같다는 뜻으로 ergodic한 system에서 성립합니다.</li>\n</ul>\n</li>\n<li><p>Value of a state-action pair given a policy<br>$$Q^{\\pi(s,a)} = \\sum_{t=1}^\\infty E[r_t - \\rho(\\pi)|S_0=s, A_0=a, \\pi]$$</p>\n</li>\n<li>State-valu function<br>$$V^\\pi(s) = \\sum_{a} \\pi(s,a)Q^\\pi(s,a)$$<br><br></li>\n</ul>\n<h2 id=\"2-5-Start-State-Formulation\"><a href=\"#2-5-Start-State-Formulation\" class=\"headerlink\" title=\"2.5 Start-State Formulation\"></a>2.5 Start-State Formulation</h2><p>start-state formulation은 시간의 흐름에 따라 감소하는 reward들을 표현합니다.</p>\n<ul>\n<li><p>Long‐term expected reward per step with a designated start state $S_0$<br>$$\\rho(\\pi) = E\\left[\\left.\\sum_{t=1}^\\infty \\gamma^{t-1}r_t\\right|S_0,\\pi\\right]$$</p>\n<ul>\n<li>$\\gamma\\in\\left[0,1\\right]$: discount rate</li>\n</ul>\n</li>\n<li><p>Value of a state‐action pair given a policy<br>$$Q^\\pi(s,a) = E\\left[\\left.\\sum_{k=1}^\\infty \\gamma^{k-1} r_{t+k}\\right|S_t=s, A_t=a, \\pi\\right]$$</p>\n</li>\n<li><p>Alternative form of $Q^\\pi(s,a)$<br>$$<br>Q^\\pi(s,a) =R_s^a + \\gamma\\sum_{s’}P_{s s’}^a V^{\\pi}(s’)<br>$$<br><br></p>\n</li>\n</ul>\n<h2 id=\"2-6-Policy-Gradient-Theorem\"><a href=\"#2-6-Policy-Gradient-Theorem\" class=\"headerlink\" title=\"2.6 Policy Gradient Theorem\"></a>2.6 Policy Gradient Theorem</h2><p>논문의 중요한 결과인 Theorem 1은 다음과 같습니다.</p>\n<p><em>For any MDP, in either the average‐reward or start‐state formulations,</em><br>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)$$</p>\n<p>놀라운 부분은 expected return의 gradient를 취할 때 $\\frac{\\partial d^\\pi(s,a)}{\\partial\\theta}$를 구하지 않아도 된다는 점입니다. 즉, policy의 변화가 state distribution에 영향을 주지 않는다는 것입니다. 이것은 다시 말하면 아래 수식과 같이 우리는 $\\frac{\\partial\\rho}{\\partial\\theta}$에 대한 unbiased estimator를 구할 수 있다는 뜻입니다.</p>\n<p>$$E_s\\left[\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)\\right] = \\frac{\\partial\\rho}{\\partial\\theta}$$</p>\n<p>이로인해서 우리는 gradient를 sampling을 통해서 추정할 수 있습니다. 그렇지만 이것은 sample이 아주 많을 때만 성립합니다!</p>\n<p>또 한 가지 문제는 우리는 $Q^\\pi(s,a)$의 정확한 값을 알 수 없다는 것 입니다. 이것을 estimate하기 위해서 현재의 return값 $R_t$를 이용할 수 있습니다.<br><br></p>\n<h2 id=\"2-7-Proof-of-Policy-Gradient-Theorem\"><a href=\"#2-7-Proof-of-Policy-Gradient-Theorem\" class=\"headerlink\" title=\"2.7 Proof of Policy Gradient Theorem\"></a>2.7 Proof of Policy Gradient Theorem</h2><p>증명을 살펴보겠습니다.</p>\n<ul>\n<li>average-reward formulation을 이용할 때의 증명은 아래와 같습니다.</li>\n</ul>\n<p>$$<br>\\begin{align}<br>\\frac{\\partial V^\\pi(s)}{\\partial\\theta} &amp;\\overset{\\underset{\\mathrm{def} }{} }{=}\\frac{\\partial}{\\partial\\theta}\\sum_a \\pi(s,a)Q^\\pi(s,a)<br>\\\\<br>&amp;=\\sum_a \\frac{\\partial}{\\partial\\theta}\\left[\\pi(s,a)Q^\\pi(s,a)\\right]\\\\<br>&amp;=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\frac{\\partial Q^\\pi(s,a)}{\\partial\\theta}\\right]\\\\<br>&amp;=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\frac{\\partial}{\\partial\\theta}\\left[R_s^a -\\rho(\\pi) + \\sum_{s’}P_{s s’}^a V^{\\pi}(s’)\\right]\\right]\\\\<br>&amp;=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\left[-\\frac{\\partial\\rho}{\\partial\\theta} + \\sum_{s’}\\mathcal{P}_{s s’}^a \\frac{\\partial V^{\\pi}(s’)}{\\partial\\theta}\\right]\\right]\\\\<br>\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\sum_{s’}\\mathcal{P}_{s s’}^a \\frac{\\partial V^{\\pi}(s’)}{\\partial\\theta}\\right] - \\frac{\\partial V^\\pi(s)}{\\partial\\theta}<br>\\end{align}<br>$$</p>\n<p>양변에 stationary distribution에 대한 평균을 취합니다.<br>$$<br>\\begin{align}<br>\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+ \\sum_s d^\\pi(s)\\sum_a\\pi(s,a)\\sum_{s’}\\mathcal{P}_{s s’}^a \\frac{\\partial V^{\\pi}(s’)}{\\partial\\theta}<br>\\\\<br>&amp;- \\sum_s d^\\pi(s)\\frac{\\partial V^\\pi(s)}{\\partial\\theta}<br>\\end{align}<br>$$</p>\n<p>모든 state $s$에서 모든 state $s’$으로 이동하는 것은 state $s’$에 존재할 확률이므로 다음과 같이 표현할 수 있습니다.</p>\n<p>$$<br>\\begin{align}<br>\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+ \\sum_s d^\\pi(s’) \\frac{\\partial V^{\\pi}(s’)}{\\partial\\theta} - \\sum_s d^\\pi(s)\\frac{\\partial V^\\pi(s)}{\\partial\\theta}<br>\\end{align}<br>$$</p>\n<p>뒤의 두 항은 같은 식이므로 이것은 다시 다음과 같이 표현됩니다.<br>$$<br>\\begin{align}<br>\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)<br>\\end{align}<br>$$</p>\n<p>$\\frac{\\partial\\rho}{\\partial\\theta}$는 $s$에 대한 함수가 아니므로 $\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}=\\frac{\\partial\\rho}{\\partial\\theta}$입니다.<br>이를 이용해서 위의 식을 다시 표현하면 아래와 같고 이를 통해 증명이 완성됩니다.</p>\n<p>$$<br>\\begin{align}<br>\\therefore\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)<br>\\end{align}<br>$$</p>\n<h3 id=\"From-Sutton-Book-Proof-of-Policy-Gradient-Theorem\"><a href=\"#From-Sutton-Book-Proof-of-Policy-Gradient-Theorem\" class=\"headerlink\" title=\"From Sutton Book: Proof of Policy Gradient Theorem\"></a>From Sutton Book: Proof of Policy Gradient Theorem</h3><ul>\n<li>start-state formulation (episodic case)은 서튼책에서도 나오는데 기존의 논문과 다르게 좀 더 진행되는 부분이 있다.</li>\n</ul>\n<p>start-state formulation에 대한 증명 (episodic case)</p>\n<p>$$\\nabla{v_\\pi}(s)=\\nabla\\big[\\sum_a\\pi(a|s)q_\\pi(s,a)\\big]$$ </p>\n<ul>\n<li><p>다음으로 $(f\\cdot g)’=f’\\cdot g+f\\cdot g’$ (product rule of calculus)<br>에 의해 다음과 같이 나타낼 수 있다.<br>$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\nabla q_\\pi(s,a)\\big]$$  </p>\n</li>\n<li><p>이어서 $p(s’,r|s,a)\\cong Pr[S_t=s’, R_t=r|S_{t-1}=s,A_{t-1}=a ]$ (서튼책 3.2 공식) 에 의해 다음과 같이 나타낼 수 있다.<br>$$=\\sum_a\\big[\\nabla\\pi(a|s)q_{\\pi(s,a)}+\\pi(a|s)\\nabla\\big[\\sum_{s’, r}p(s’,r|s,a)(r+v_{\\pi}(s’))\\big]\\big]$$  </p>\n</li>\n<li><p>계속해서 $p(s’|s,a)\\cong Pr[S_t=s’, R_t=r|S_{t-1}=s,A_{t-1}=a ]=\\sum_{r\\in R}p(s’,r|s,a)$ (서튼책 3.4 공식) 에 의해 다음과 같이 나타낼 수 있다.<br>$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\sum_{s’}p(s’|s,a)\\nabla v_\\pi(s’) \\big]$$ </p>\n</li>\n<li><p>여기서 $\\nabla v_\\pi(s’)$ 부분을 unrolling을 하면 다음과 같이 나타낼 수 있다.<br>$$<br>\\begin{align}<br>=&amp;\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\sum_{s’}p(s’|s,a)\\sum_{a’}\\big[\\nabla\\pi(a’|s’)q_\\pi(s’,a’)<br>\\\\<br>&amp;+\\pi(a’|s’)\\sum_{s’’}p(s’’|s’,a’)\\nabla v_\\pi(s’’))\\big] \\big]<br>\\end{align}<br>$$ </p>\n</li>\n<li><p>unrolling을 반복하다보면 다음과 같이 나타낼 수 있다.<br>$$=\\sum_{x\\in S}\\sum_{k=0}^{\\infty}Pr(s\\rightarrow x,k,\\pi)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$</p>\n</li>\n<li><p>여기에서 $Pr(s\\rightarrow x,k,\\pi)$ 는 policy $\\pi$ 에 대해 state s에서 state x까지 k step만큼 움직일 때의 변환 확률이다. 따라서 위의 수식을 아래처럼 다시 나타낼 수 있다.<br>$$<br>\\begin{align}<br>\\nabla J(\\theta)= &amp;\\nabla v_\\pi (s_0)<br>\\\\<br>=&amp;\\sum_s(\\sum_{k=0}^{\\infty}Pr(s_0\\rightarrow s,k,\\pi))\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)<br>\\\\<br>=&amp;\\sum_s \\eta(s) \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)<br>\\end{align}$$ </p>\n</li>\n<li><p>서튼책에서 $\\eta(s)$는 다음과 같이 표현할 수 있다.</p>\n<p>  $\\eta(s)$ = 평균적으로 하나의 episode마다 state에서 머무른 time step의 수 (page 199)</p>\n<p>  논문에서 $\\eta(s)$는 다음과 같다. </p>\n<p>  $d^\\pi(s)=\\sum_{t=0}^{\\infty}\\gamma^t Pr(s_t=s|s_0,\\pi)$ ($\\gamma=1$ is allowed only in episodic tasks) = discounted weighting of states)<br>$$\\sum_s \\eta(s) \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$</p>\n</li>\n<li>위의 수식을 아래의 수식처럼 바꿀 수 있다.<br>$$\\sum_{s’} \\eta(s’) \\cdot\\sum_s \\frac{\\eta(s)}{\\sum_{s’}\\eta(s’)} \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$</li>\n<li><p>그러면 이것을 새로운 기호로 다시 나타낼 수 있다.<br>$$\\sum_{s’} \\eta(s’) \\cdot\\sum_s \\mu(s)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$ </p>\n</li>\n<li><p>여기서 $\\mu(s)$ 는 state distribution이라고 하며, weight를 state distribution으로 바꿔주는 과정이라고 생각할 수 있다. 쉽게 말해 어떠한 state에 agent가 머무르는 확률이다.</p>\n</li>\n<li><p>위의 수식은 아래의 수식과 비례한다. 따라서 최종 형태는 다음과 같다.<br>$$\\propto \\sum_s \\mu(s)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$<br><br><br></p>\n</li>\n</ul>\n<hr>\n<h1 id=\"3-Policy-Gradient-with-Approximation\"><a href=\"#3-Policy-Gradient-with-Approximation\" class=\"headerlink\" title=\"3. Policy Gradient with Approximation\"></a>3. Policy Gradient with Approximation</h1><ul>\n<li>이번 section은 앞서 다뤘던 Theorem 1 중에 $Q^\\pi$에 대해서 중점적으로 다룬다.</li>\n<li>어떠한 $Q^\\pi$가 학습된 function approximator로 근사된다고 하자.</li>\n<li>$f_w$ ($f_w$는 $S × A \\rightarrow \\Re$)를 parameter $w$를 가지는 어떠한 $Q^\\pi$ 에 대한 근사치라고 하자.<ul>\n<li>학습된 function approximator에 대해서 생각하기 때문에, 학습된 $f_w$는 error를 최소화하는 방향으로 parameter $w$를 업데이트된 것을 의미한다.</li>\n</ul>\n</li>\n<li>그러면 하나의 rule에 의해서 $\\pi$를 따르고 $w$를 업데이트하는 $f_w$에 대해서 다음과 같이 생각할 수 있다.<br>$$\\Delta w_t \\propto \\frac{\\partial}{\\partial w}\\big[ {\\hat Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]^2 \\propto \\big[{\\hat Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s_t,a_t)}{\\partial w}$$<br>(여기서 ${\\hat Q}^\\pi(s_t,a_t)$ 는 $Q^\\pi(s_t,a_t)$의 unbiased estimator $R_t$이다.)</li>\n<li><p>그러면 위와 같은 수식이 local optimum에 수렴을 했을 때, 다음과 같이 나타낼 수 있다.<br>$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$</p>\n</li>\n<li><p>위의 수식에 대한 추가 설명</p>\n<ul>\n<li>local optimum으로 수렴을 하기 때문에 당연히 위의 수식은 0이 된다.</li>\n<li>또한 stochastic policy이기 때문에 local optimum으로 수렴하려면 모든 state, action에 대해 expectation을 해야한다. 따라서 $\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)$이 붙게 된다.<br><br></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"3-1-Theorem-2-Policy-Gradient-with-Function-Approximation\"><a href=\"#3-1-Theorem-2-Policy-Gradient-with-Function-Approximation\" class=\"headerlink\" title=\"3.1 Theorem 2: Policy Gradient with Function Approximation\"></a>3.1 Theorem 2: Policy Gradient with Function Approximation</h2><ul>\n<li>만약 $f_w$가 아래의 등식을 만족한다고 하자.<br>$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$</li>\n</ul>\n<p>또한 $f_w$가 policy parameterization과 양립할 수 있다고 하자. policy parameterization에 대한 여러가지 의미가 있는데 그 중에서 이 논문에서는 다음을 의미한다.<br>$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}$$</p>\n<ul>\n<li><p>위의 수식에 대한 추가 설명</p>\n<ul>\n<li>이 수식은 Monte-Calro Estimation을 이용하기 위한 조건이다.</li>\n<li>Compatibility Condition이라고 부른다.</li>\n<li>우변(= $\\nabla\\log\\pi(s,a)$)으로 유도함으로써 아래의 수식과 같이 sampling을 이용한 추정이 가능해진다.</li>\n</ul>\n</li>\n<li><p>따라서 위의 두 수식을 이용하여 아래와 같이 나타낼 수 있다.<br>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}f_w(s,a)$$<br><br></p>\n</li>\n</ul>\n<h2 id=\"3-2-Proof-of-Theorem-2\"><a href=\"#3-2-Proof-of-Theorem-2\" class=\"headerlink\" title=\"3.2 Proof of Theorem 2\"></a>3.2 Proof of Theorem 2</h2><p>$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$</p>\n<p>$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}$$</p>\n<ul>\n<li><p>1와 2를 합치면 다음과 같이 나타낼 수 있다.<br>$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}=0$$</p>\n</li>\n<li><p>분자와 분모에 있는 $\\pi(s,a)$를 지우면 아래와 같다.<br>$$\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]=0$$</p>\n</li>\n<li><p>이어서 위의 수식이 0이기 때문에 policy gradient theorem(앞서 다뤘던 Theorem 1)에서 위의 수식을 뺄 수 있다.<br>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)-\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ Q^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]$$</p>\n</li>\n</ul>\n<p>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ Q^\\pi(s_t,a_t)-Q^\\pi(s_t,a_t)+f_w(s_t,a_t) \\big]$$</p>\n<p>$$\\therefore\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}f_w(s_t,a_t)$$<br><br><br></p>\n<hr>\n<h1 id=\"4-Application-to-Deriving-Algorithms-and-Advantages\"><a href=\"#4-Application-to-Deriving-Algorithms-and-Advantages\" class=\"headerlink\" title=\"4. Application to Deriving Algorithms and Advantages\"></a>4. Application to Deriving Algorithms and Advantages</h1><h2 id=\"4-1-Application-to-Deriving-Algorithms\"><a href=\"#4-1-Application-to-Deriving-Algorithms\" class=\"headerlink\" title=\"4.1 Application to Deriving Algorithms\"></a>4.1 Application to Deriving Algorithms</h2><ul>\n<li>feature의 linear combination에서 Gibbs distribution (softmax)를 하나의 policy로 생각해보자.<br>$$\\pi(s,a)=\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_b e^{\\theta^{T}\\phi_{sb} }}$$</li>\n</ul>\n<p>($\\phi_{sa}$: state-action pair s,a을 나타내는 l-dimensional feature vector)</p>\n<ul>\n<li><p>compatible condition을 추가하면 다음과 같다.<br>$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\frac{1}{\\pi(s,a)}=\\phi_{sa}-\\sum_b \\pi(s,b)\\phi_{sb}$$ </p>\n<ul>\n<li>Proof of application of compatible condition 참고</li>\n</ul>\n</li>\n<li><p>이어서 $f_w$을 적분한 natural parameterization은 다음과 같다.<br>$$f_w(s,a)=w^T\\big[ \\phi_{sa}-\\sum_b \\pi(s,b)\\phi_{sb}\\big]$$</p>\n</li>\n<li>위의 수식에 대한 추가 설명<ul>\n<li>$f_w$는 policy로서 같은 feature들에 대해 linear하다.</li>\n<li>$f_w$는 각각의 state에 대해서 평균이 0이다. ($\\sum_a\\pi(s,a)f_w(s,a)=0$)</li>\n<li>advantage function $A^\\pi(s,a)=Q^\\pi(s,a)-V^\\pi(s)$의 하나의 근사치로서 $f_w$를 생각하는 것 좋다.<br><br></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"4-2-Proof-of-application-of-compatible-condition\"><a href=\"#4-2-Proof-of-application-of-compatible-condition\" class=\"headerlink\" title=\"4.2 Proof of application of compatible condition\"></a>4.2 Proof of application of compatible condition</h2><p>증명을 하기 전에 다음을 가정한다.</p>\n<ol>\n<li>$(\\frac{f(x)}{g(x)})’=f’(x)\\cdot \\frac{1}{g(x)}-f(x)\\cdot \\frac{g’(x)}{g(x)^{2} }=\\frac{f’(x)\\cdot g(x)-f(x)\\cdot g’(x)}{g(x)^{2} }$</li>\n<li>$f(\\theta)=e^{\\theta^{T}\\phi_{sa} }$, $g(\\theta)=\\sum_be^{\\theta^{T}\\phi_{sb} }$</li>\n</ol>\n<p>$$<br>\\begin{align}<br>\\frac{\\partial f_w(s,a)}{\\partial w}&amp;=\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\frac{1}{\\pi(s,a)}<br>\\\\<br>&amp;=\\big(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }}\\big)’\\frac{1}{\\pi(s,a)}<br>\\\\<br>&amp;=\\big[ \\phi_{sa}(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }})-(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }})\\frac{\\sum_b\\phi_{sb}e^{ {\\theta^T}\\phi_{sb} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }}\\big]\\frac{1}{\\pi(s,a)}\\\\<br>&amp;=\\big[ \\phi_{sa}\\pi(s,a)-\\pi(s,a)\\sum_b\\phi_{sb}\\pi(s,b)\\big]\\frac{1}{\\pi(s,a)}<br>\\\\<br>&amp;=\\pi(s,a)\\big[ \\phi_{sa}-\\sum_b\\pi(s,b)\\phi_{sb}\\big]\\frac{1}{\\pi(s,a)}<br>\\\\<br>&amp;\\therefore\\phi_{sa}-\\sum_b\\pi(s,b)\\phi_{sb}<br>\\end{align}<br>$$</p>\n<p><br></p>\n<h2 id=\"4-3-Application-to-Advantages\"><a href=\"#4-3-Application-to-Advantages\" class=\"headerlink\" title=\"4.3 Application to Advantages\"></a>4.3 Application to Advantages</h2><ul>\n<li>Policy Gradient with Function Approximation Theorem (Theorem 2)는 advantage function으로 확장될 수 있다.<br>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_sd^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ f_w(s,a)+v(s) \\big]$$</li>\n<li>위의 수식에 대한 추가설명<ul>\n<li>$v$ ($v$는 $S\\rightarrow\\Re$) 는 arbitrary function이다.</li>\n<li>이 수식은 $\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}=0$이기 때문에 가능해진다.<ul>\n<li>즉, 이 수식은 $\\pi(s,a)$의 gradient에만 dependent하기 때문에 advantage 역학을 하는 함수들을 넣어도 아무런 상관이 없다.</li>\n</ul>\n</li>\n<li>$v$의 선택은 Theorem들에 영향을 미치지 못하지만, 실질적으로 gradient estimator의 variance에 영향을 미친다.</li>\n<li>이러한 문제는 전체적으로 이전의 연구에 reinforcement baseline의 사용에 있어서 유사하다.</li>\n<li>(comment) 위의 수식에서 $f_w(s,a)+v(s)$와 Application to Deriving Algorithms의 $f_w(s,a)$와는 다른 것이다. Application to Deriving Algorithms의 $f_w(s,a)$은 softmax에 의해 스스로 advantage function의 역할을 할 수 있다. 하지만 보통의 경우에는 그러지 못할 수도 있기 때문에 위의 수식처럼 $f_w(s,a)+v(s)$을 추가하여 zero mean만들어서 variance를 줄일 수 있다.<br><br><br></li>\n</ul>\n</li>\n</ul>\n<hr>\n<h1 id=\"5-Convergence-of-Policy-Iteration-with-Function-Approximation\"><a href=\"#5-Convergence-of-Policy-Iteration-with-Function-Approximation\" class=\"headerlink\" title=\"5. Convergence of Policy Iteration with Function Approximation\"></a>5. Convergence of Policy Iteration with Function Approximation</h1><h2 id=\"5-1-Theorem-3-Convergence-of-Policy-Iteration-with-Function-Approximation\"><a href=\"#5-1-Theorem-3-Convergence-of-Policy-Iteration-with-Function-Approximation\" class=\"headerlink\" title=\"5.1 Theorem 3: Convergence of Policy Iteration with Function Approximation\"></a>5.1 Theorem 3: Convergence of Policy Iteration with Function Approximation</h2><p>policy iteration with function approximation의 형태는 locally optimal policy에 수렴한다. $\\pi$와 $f_w$를 </p>\n<ol>\n<li>compatibility condition을 만족하는 policy와 value function에 대해</li>\n<li>어떠한 $\\max_{\\theta, s, a, i, j}\\big| \\frac{\\partial^2\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}\\big|&lt;B&lt;\\infty$ 을<br>만족하는 어떠한 미분가능한 function approximator라고 하자. </li>\n</ol>\n<p>(comment) $\\pi(s,a)$라는 function은 이계미분값이 존재하고, 임의의 상수인 (Bound) B에 유계하기 때문에 function의 그래프는 smooth하다고 볼 수 있다. (오른쪽 그림 중 빨간색 그래프 참고)</p>\n<ul>\n<li>${\\alpha_k}$는 $\\lim_{k\\rightarrow\\infty}\\alpha_k=0$ 이며 $\\sum_k\\alpha_k=\\infty$를 만족하는 step-size sequence라고 하자.</li>\n<li><p>그 때, bounded reward를 가진 MDP에 대해</p>\n<ol>\n<li>어떠한 $\\theta_0, \\pi_k=\\pi(\\cdot,\\cdot,\\theta_k)$ </li>\n<li><p>$\\sum_sd^{\\pi_{k} }(s)\\sum_a\\pi_k(s,a)\\big[ Q^{\\pi_{k} }(s,a)-f_w(s,a)\\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$로 인하여 $w_k=w$, $\\theta_{k+1}=\\theta_k+\\alpha_k\\sum_sd^{\\pi_{k} }(s)\\sum_a\\frac{\\partial\\pi_k(s,a)}{\\partial\\theta}f_{w_{k} }(s,a)$</p>\n<p>으로 정의된 sequence $\\rho(\\pi_k)$는 $\\lim_{k\\rightarrow\\infty}\\frac{\\partial\\rho(\\pi_k)}{\\partial\\theta}=0$이기 때문에 수렴한다.</p>\n</li>\n</ol>\n<ul>\n<li>sequence $\\rho(\\pi_k)_{k=0}^\\infty$에 대한 추가 설명<ul>\n<li>$\\theta_{k+1}=\\theta_k+\\alpha_k\\sum_sd^{\\pi_{k} }(s)\\sum_a\\frac{\\partial\\pi_k(s,a)}{\\partial\\theta}f_{w_{k} }(s,a)$ 에 따라 $\\theta$가 1, 2, …, $\\infty$로 갈텐데, 거기에 따른 objective function or performance의 sequence이다.</li>\n<li>(comment) 굳이 sequence라는 표현이 없어도 될 것 같다. 어짜피 k가 $\\infty$로 가면 $\\rho(\\pi_k)$가 수렴한다는 의미이기 때문에 불필요해보인다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"5-2-Proof-of-Theorem-3\"><a href=\"#5-2-Proof-of-Theorem-3\" class=\"headerlink\" title=\"5.2 Proof of Theorem 3\"></a>5.2 Proof of Theorem 3</h2><ul>\n<li>Theorem 2는 $\\theta_k$ update가 gradient의 error를 최소화한다는 것을 증명했다.</li>\n<li>$\\frac{\\partial^{2}\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}$와 MDP의 reward에서의 bound는, $\\frac{\\partial^{2}\\rho}{\\partial\\theta_i\\partial\\theta_j}$ 또한 bound된다는 것을 증명한다.</li>\n<li>step-size 필요조건 때문에 이러한 bound된 것들은 Proposition 3.5 from page 94 of Bertsekas and Tsitsiklis(1996)에 적용하기 위해 필요한 조건이다.</li>\n<li>Proposition 3.5 from page 94 of Bertsekas and Tsitsiklis(1996)은 local optimum으로 수렴한다는 것을 증명했다. </li>\n</ul>\n"},{"title":"Deterministic Policy Gradient Algorithms","date":"2018-06-16T08:21:48.000Z","author":"이웅원","subtitle":"피지여행 2번째 논문","_content":"\nAuthors: David Silver<sup>1)</sup>, Guy Lever<sup>2)</sup>, Nicloas Heess<sup>1)</sup>, Thomas Degris<sup>1)</sup>, Daan Wierstra<sup>1)</sup>, Martin Riedmiller<sup>1)</sup>\nAffiliation: 1) Google Deepmind, 2) University College London (UCL)\nProceeding: International Conference on Machine Learning (ICML) 2014\n\n* [Deterministic Policy Gradient Algorithms](#deterministic-policy-gradient-algorithms)\n\t* [Summary](#summary)\n\t* [Background](#background)\n\t\t* [Performance objective function](#performance-objective-function)\n\t\t* [SPG Theorem](#spg-theorem)\n\t\t* [Stochastic Actor-Critic Algorithms](#stochastic-actor-critic-algorithms)\n\t\t* [Off-policy Actor-Critic](#off-policy-actor-critic)\n\t* [Gradient of Deterministic Policies](#gradient-of-deterministic-policies)\n\t\t* [Regulariy Conditions](#regulariy-conditions)\n\t\t* [Deterministic Policy Gradient Theorem](#deterministic-policy-gradient-theorem)\n\t\t* [DPG 형태에 대한 informal intuition](#dpg-형태에-대한-informal-intuition)\n\t\t* [DPG 는 SPG 의 limiting case 임](#dpg-는-spg-의-limiting-case-임)\n\t* [Deterministic Actor-Critic Algorithms](#deterministic-actor-critic-algorithms)\n\t* [Experiments](#experiments)\n\t\t* [Continuous Bandit](#continuous-bandit)\n\t\t* [Continuous Reinforcement Learning](#continuous-reinforcement-learning)\n\t\t* [Octopus Arm](#octopus-arm)\n\n<!-- /code_chunk_output -->\n\n<br>\n\n---\n## 1. Summary\n- Deterministic Policy Gradient (DPG) Theorem 제안함 [[Theorem 1](#deterministic-policy-gradient-theorem)]\n    1) DPG는 존재하며,\n    2) DPG는 Expected gradient of the action-value function의 형태를 띈다.\n- Policy variance가 0에 수렴할 경우, DPG는 Stochastic Policy Gradient(SPG)와 동일해짐을 보임 [[Theorem 2](#dpg-는-spg-의-limiting-case-임)]\n    - Theorem 2 로 인해 기존 Policy Gradient (PG) 와 관련된 기법들을 DPG 에 적용할 수 있게 됨\n        - 예. Sutton PG, natural gradients, actor-critic, episodic/batch methods\n- 적절한 exploration 을 위해 model-free, off-policy Actor-Critic algorithm 을 제안함\n    - Action-value function approximator 사용으로 인해 policy gradient가 bias되는 것을 방지하기 위해 compatibility conditions을 제공 [[Theorem 3](##deterministic-actor-critic-algorithms)]\n- DPG 는 SPG 보다 성능이 좋음\n    - 특히 high dimensional action spaces 을 가지는 tasks에서의 성능 향상이 큼\n        - SPG의 policy gradient는 state와 action spaces 모두에 대해서, DPG의 policy gradient는  state spaces에 대해서만 평균을 취함\n        - 결과적으로, action spaces의 dimension이 커질수록 data efficiency가 높은 DPG의 학습이 더 잘 이뤄지게 됨\n        - 무한정 학습을 시키게 되면, SPG도 최적으로 수렴할 것으로 예상되기에 위 성능 비교는 일정 iteration 내로 한정함\n    - 기존 기법들에 비해 computation 양이 많지 않음\n        - Computation 은 action dimensionality 와 policy parameters 수에 비례함\n<br>\n\n---\n## 2. Background\n### 2.1 Performance objective function\n\n$$\n\\begin{align}\nJ(\\pi_{\\theta}) &= \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\pi_{\\theta}(s,a)r(s,a)dads = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[r(s,a)]\n\\end{align}\n$$\n\n### 2.2 SPG Theorem\n- State distribution $ \\rho^{\\pi}(s) $ 은 policy parameters에 영향을 받지만, policy gradient 를 계산할 때는 state distribution 의 gradient 를 고려할 필요가 없다.\n- $$\\begin{eqnarray}\\nabla_{\\theta}J(\\pi_{\\theta}) &=& \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\\\ &=& E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]\n\\end{eqnarray}$$\n\n### 2.3 Stochastic Actor-Critic Algorithms\n- Actor 와 Critic 이 번갈아가면서 동작하며 stochastic policy 를 최적화하는 기법\n- Actor: $ Q^{\\pi}(s,a) $ 를 근사한 $ Q^w(s,a) $ 를 이용해 stochastic policy gradient 를 ascent 하는 방향으로 policy parameter $ \\theta $ 를 업데이트함으로써 stochastic policy 를 발전시킴\n    - $ \\nabla_{\\theta}J(\\pi_{\\theta}) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}log\\pi_{\\theta}(a|s)Q^{w}(s,a)] $\n- Critic: SARSA 나 Q-learning 같은 Temporal-difference (TD) learning 을 이용해 action-value function의 parameter, $ w $ 를 업데이트함으로써 $ Q^w(s,a) $ 가 $ Q^{\\pi}(s,a) $ 과 유사해지도록 함\n- 실제 값인 $ Q^{\\pi}(s,a) $ 대신 이를 근사한 $ Q^w(s,a) $ 를 사용하게 되면, 일반적으로 bias 가 발생하게 된다. 하지만, compatible condition 에 부합하는 $ Q^w(s,a) $ 를 사용하게 되면, bias 가 발생하지 않는다.\n\n### 2.4 Off-policy Actor-Critic\n- Distinct behavior policy $ \\beta(a|s) ( \\neq \\pi_{\\theta}(a|s) ) $ 로부터 샘플링된 trajectories 를 이용한 Actor-Critic\n- Performance objective function\n    - $\\begin{eqnarray}\n        J_{\\beta}(\\pi_{\\theta}) \n        &=& \\int_{S}\\rho^{\\beta}(s)V^{\\pi}(s)ds \\nonumber \\\\\\\\\n        &=& \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads\n        \\end{eqnarray} $\n- off-policy policy gradient\n    - $ \\begin{eqnarray}\n        \\nabla_{\\theta}J_{\\beta}(\\pi_{\\theta}) &\\approx& \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\end{eqnarray} $\n      $=E_{s \\sim \\rho^{\\beta}, a \\sim \\beta}[\\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)}\\nabla_{\\theta}log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]$\n    - off-policy policy gradient 식에서의 물결 표시는 [Degris, 2012b] 논문에 근거함\n        - [Degris, 2012b] \"Linear off-policy actor-critic,\" ICML 2012\n        - Exact off-policy policy gradient 와 이를 approximate 한 policy gradient 는 아래와 같음. (빨간색 상자에 있는 항목을 삭제함으로써 근사함)\n            - <img src=\"https://www.dropbox.com/s/xzpv3okc139c1fs/Screenshot%202018-06-16%2017.48.51.png?dl=1\" width=500px>\n        - [Degris, 2012b] Theorem 1 에 의해 policy parameter 가 approximated policy gradient ( $\\nabla_{u}𝑄^{\\pi,\\gamma}(𝑠,𝑎)$ term 제거)에 따라 업데이트되어도 policy 는 improve 가 됨이 보장되기에 exact off-policy policy gradient 대신 approximated off-policy policy gradient 를 사용해도 괜찮음.\n            - <img src=\"https://www.dropbox.com/s/mk13931r4scjngo/Screenshot%202018-06-16%2017.49.24.png?dl=1\" width=500px>\n    - off-policy policy gradient 식에서 $ \\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)} $ 는 importance sampling ratio 임\n        - off-policy actor-critic 에서는 $ \\beta $ 에 의해 샘플링된 trajectory 를 이용해서 stochastic policy $ \\pi $ 를 예측하는 것이기 때문에 imnportance sampling 이 필요함\n<br>\n\n---\n## 3. Gradient of Deterministic Policies\n### 3.1 Regulariy Conditions\n- 어떠한 이론이 성립하기 위한 전제 조건\n- Regularity conditions A.1\n    - $ p(s'|s,a), \\nabla_{a}p(s'|s,a), \\mu_{\\theta}(s), \\nabla_{\\theta}\\mu_{\\theta}(s), r(s,a), \\nabla_{a}r(s,a), p_{1}(s) $ are continuous in all parameters and variables $ s, a, s' $ and $ x $.\n- regularity conditions A.2\n    - There exists a $ b $ and $ L $ such that $ \\sup_{s}p_{1}(s) < b $, $ \\sup_{a,s,s'}p(s'|s,a) < b $, $ \\sup_{a,s}r(s,a) < b $, $ \\sup_{a,s,s'}\\|\\nabla_{a}p(s'|s,a)\\| < L $, and $ \\sup_{a,s}\\|\\nabla_{a}r(s,a)\\| < L $.\n\n### 3.2 Deterministic Policy Gradient Theorem\n- Deterministic policy\n    - $ \\mu_{\\theta} : S \\to A $ with parameter vector $ \\theta \\in \\rm I \\! R^n $\n- Probability distribution\n    - $ p(s \\to s', t, \\mu) $\n- Discounted state distribution\n    - $ \\rho^{\\mu}(s) $\n- Performance objective\n\n$$\nJ(\\mu_{\\theta}) = E[r^{\\gamma}_{1} | \\mu] \n$$\n\n$$\n= \\int_{S}\\rho^{\\mu}(s)r(s,\\mu_{\\theta}(s))ds \n= E_{s \\sim \\rho^{\\mu}}[r(s,\\mu_{\\theta}(s))]\n$$\n\n- DPG Theorem\n    - MDP 가 A.1 만족한다면, 아래 식(9)이 성립함\n    $\\nabla_{\\theta}J(\\mu_{\\theta}) = \\int_{S}\\rho^{\\mu}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}ds \\nonumber$\n    $= E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]   \\nonumber $ (9)\n    \n\t- DPG는 State space 에 대해서만 평균을 취하면 되기에, State와 Action space 모두에 대해 평균을 취해야 하는 [SPG](#spg-theorem)에 비해 data efficiency가 좋다. 즉, 더 적은 양의 데이터로도 학습이 잘 이뤄지게 된다.\n\n    \n### 3.3 DPG 형태에 대한 informal intuition\n- Generalized policy iteration\n    - 정책 평가와 정책 발전을 한 번 씩 번갈아 가면서 실행하는 정책 iteration\n        - 위와 같이 해도 정책 평가에서 예측한 가치함수가 최적 가치함수에 수렴함\n- 정책 평가\n    - action-value function $ Q^{\\pi}(s,a) $ or $ Q^{\\mu}(s,a) $ 을 estimate 하는 것\n- 정책 발전\n    - 위 estimated action-value function 에 따라 정책을 update 하는 것\n    - 주로 action-value function 에 대한 greedy maximisation 사용함\n        - $ \\mu^{k+1}(s) = \\arg\\max\\limits_{a}Q^{\\mu^{k}}(s,a) $\n        - greedy 정책 발전은 매 단계마다 global maximization을 해야하는데, 이는 continuous action spaces 에서 계산량이 급격히 늘어나게 됨.\n    - 그렇기에 policy gradient 방법이 나옴\n        - policy 를 $ \\theta $ 에 대해서 parameterize 함\n        - 매 단계마다 global maximisation 수행하는 대신, 방문하는 state $ s $ 마다 policy parameter 를 action-value function $ Q $ 의 $ \\theta $ 에 대한 gradient $ \\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s)) $ 방향으로 proportional 하게 update 함\n        - 하지만 각 state 는 다른 방향을 제시할 수 있기에, state distribution $ \\rho^{\\mu}(s) $ 에 대한 기대값을 취해 policy parameter 를 update 할 수도 있음\n            - $ \\theta^{k+1} = \\theta^{k} + \\alpha \\rm I\\!E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s))] $\n        - 이는 chain-rule 에 따라 아래와 같이 분리될 수 있음\n            - $ \\theta^{k+1} = \\theta^{k} + \\alpha \\rm I\\!E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu^{k}}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $ (7)\n            - chain rule: $ \\frac{\\partial Q}{\\partial \\theta} = \\frac{\\partial a}{\\partial \\theta} \\frac{\\partial Q}{\\partial a} $\n        - 하지만 state distribution $ \\rho^{\\mu} $ 은 정책에 dependent 함\n            - 정책이 바꾸게 되면, 바뀐 정책에 따라 방문하게 되는 state 가 변하게 되기 때문에 state distribution이 변하게 됨\n        - 그렇기에 정책 update 시 state distribution에 대한 gradient 를 고려하지 않는데 정책 발전이 이뤄진다는 것은 직관적으로 와닿지 않을 수 있음\n        - deterministic policy gradient theorem 은 state distribution에 대한 gradient 계산없이 위 식(7) 대로만 update 해도 performance objective 의 gradient 를 정확하게 따름을 의미한다.\n\n\n### 3.4 DPG 는 SPG 의 limiting case 임\n- stochastic policy parameterization\n    - $ \\pi_{\\mu_{\\theta},\\sigma} $ by a deterministic policy $ \\mu_{\\theta} : S \\to A $ and a variance parameter $ \\sigma $\n    - $ \\sigma = 0 $ 이면, $ \\pi_{\\mu_{\\theta},\\sigma} \\equiv \\mu_{\\theta} $\n- Theorem 2. Policy 의 variance 가 0 에 수렴하면, 즉, $ \\sigma \\to 0 $, stochastic policy gradient 와 deterministic policy gradient 는 동일해짐\n    - 조건 : stochastic policy $ \\pi_{\\mu_{\\theta},\\sigma} = \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $\n        - $ \\sigma $ 는 variance\n        - $ \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $ 는 conditions B.1 만족\n        - MDP 는 conditions A.1 및 A.2 만족\n    - 결과 :\n        - $ \\lim\\limits_{\\sigma\\downarrow0}\\nabla_{\\theta}J(\\pi_{\\mu_{\\theta},\\sigma}) = \\nabla_{\\theta}J(\\mu_{\\theta})  $\n            - 좌변은 standard stochastic gradient 이며, 우변은 deterministic gradient.\n    - 의미 :\n        - deterministic policy gradient 는 stochastic policy gradient 의 특수 case 임\n        - 기존 유명한 policy gradients 기법들에 deterministic policy gradients 를 적용할 수 있음\n            - 기존 기법들 예: compatible function approximation (Sutton, 1999), natural gradients (Kakade, 2001), actor-critic (Bhatnagar, 2007) or episodic/batch methods (Peters, 2005)\n<br>\n \n---\n## 4. Deterministic Actor-Critic Algorithms\n1. 살사 critic 을 이용한 on-policy actor-critic\n    - 단점\n        - deterministic policy 에 의해 행동하면 exploration 이 잘 되지 않기에, sub-optimal 에 빠지기 쉬움\n    - 목적\n        - 교훈/정보제공\n        - 환경에서 충분한 noise 를 제공하여 exploration을 시킬 수 있다면, deterministic policy 를 사용한다고 하여도 좋은 학습 결과를 얻을 수도 있음$\n            - 예. 바람이 agent의 행동에 영향(noise)을 줌\n    - Remind: 살사(SARSA) update rule\n        - $ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma Q(s_{t+1},a_{t+1}) - Q(s_{t},a_{t})) $\n    - Algorithm\n        - Critic 은 MSE 를 $ \\bf minimize $ 하는 방향, 즉, action-value function 을 stochastic gradient $ \\bf descent $ 방법으로 update 함\n            - $ MSE = [Q^{\\mu}(s,a) - Q^{w}(s,a)]^2 $\n                - critic 은 실제 $ Q^{\\mu}(s,a) $ 대신 미분 가능한 $ Q^{w}(s,a) $ 로 대체하여 action-value function 을 estimate 하며, 이 둘 간 Mean Square Error 를 최소화하는 것이 목표\n            - $ \\nabla_{w}MSE \\approx -2 * [r + \\gamma Q^{w}(s',a') - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $\n                - $ \\nabla_{w}MSE = -2 * [Q^{\\mu}(s,a) - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $\n                - $ Q^{\\mu}(s,a) $ 를 $ r + \\gamma Q^{w}(s',a') $ 로 대체\n                    - $ Q^{\\mu}(s,a) = r + \\gamma Q^{\\mu}(s',a') $\n            - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $\n                - $w_{t+1} = w_{t} - \\alpha_{w}\\nabla_{w}MSE  \\nonumber$\n                $ \\approx w_{t} - \\alpha_{w} * (-2 * [r + \\gamma Q^{w}(s',a') - Q^{w}(s,a)] \\nabla_{w}Q^{w}(s,a)$\n                - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},a_{t+1}) - Q^{w}(s_{t},a_{t}) $\n        - Actor 는 식(9)에 따라 보상이 $ \\bf maximize $ 되는 방향, 즉, deterministic policy 를 stochastic gradient $ \\bf ascent $ 방법으로 update함\n            - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $\n2. Q-learning 을 이용한 off-policy actor-critic\n    - stochastic behavior policy $ \\beta(a|s) $ 에 의해 생성된 trajectories 로부터 deterministic target policy $ \\mu_{\\theta}(s) $ 를 학습하는 off-policy actor-critic\n    - performance objective\n        - $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)V^{\\mu}(s)ds \\nonumber \\\\\\\\$\n          $= \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds \\nonumber \\\\\\\\$\n          $= E_{s \\sim \\rho^{\\beta}}[Q^{\\mu}(s,\\mu_{\\theta}(s))]$\n    - off-policy deterministic policy gradient\n        - $ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $\n            - 논문에는 아래와 같이 나와있는데, 물결 표시 부분은 오류로 판단됨.\n            - $ \\begin{eqnarray}\n                \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) &\\approx& \\int_{S}\\rho^{\\beta}(s)\\nabla_{\\theta}\\mu_{\\theta}(a|s)Q^{\\mu}(s,a)ds \\nonumber \\\\\n                &=& E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]\n                \\end{eqnarray} $\n            - 근거: Action이 deterministic 하기에 stochastic 경우와는 다르게 performance objective 에서 action 에 대해 평균을 구할 필요가 없음. 그렇기에, 곱의 미분이 있을 필요가 없고, [Degris, 2012b] 에서처럼 곱의 미분을 통해 생기는 action-value function 에 대한 gradient term 를 생략할 필요가 사라짐. [참고](#off-policy-actor-critic)\n    - Remind: 큐러닝(Q-learning) update rule\n        - $ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma \\max\\limits_{a}Q(s_{t+1},a) - Q(s_{t},a_{t})) $\n    - algorithm: OPDAC (Off-Policy Deterministic Actor-Critic)\n        - 살사를 이용한 on-policy deterministic actor-critic 과 아래 부분을 제외하고는 같음\n            - target policy 는 $ \\beta(a|s) $ 에 의해 생성된 trajectories 를 통해 학습함\n            - 업데이트 목표 부분에 실제 행동 값 $ a_{t+1} $ 이 아니라 정책으로부터 나온 행동 값 $ \\mu_{\\theta}(s_{t+1}) $ 사용함\n                - $ \\mu_{\\theta}(s_{t+1}) $ 는 가장 높은 Q 값을 가지는 행동. 즉, Q-learning.\n        - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $\n        - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $\n        - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $\n    - Stochastic off-policy actor-critic 은 대개 actor 와 critic 모두 importance sampling을 필요로 하지만, deterministic policy gradient 에선 importance sampling이 필요없음\n        - Actor 는 deterministic 이기에 sampling 자체가 필요없음\n            - Stochastic policy 인 경우, Actor 에서 importance sampling이 필요한 이유는 상태 $ s $ 에서의 가치 함수 값 $ V^{\\pi}(s) $ 을 estimate 하기 위해 $ \\pi $ 가 아니라 $ \\beta $ 에 따라 sampling을 한 후, 평균을 내기 때문임.\n            - 하지만 Deterministic policy 인 경우, 상태 $ s $ 에서의 가치 함수 값 $ V^{\\pi}(s) = Q^{\\pi}(s,\\mu_{\\theta}) $ 즉, action 이 상태 s 에 대해 deterministic 이기에 sampling 을 통해 estimate 할 필요가 없고, 따라서 importance sampling도 필요없어짐.\n            - stochastic vs. deterministic performance objective\n                - stochastic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads $\n                - deterministic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds $\n        - Critic 이 사용하는 Q-learning 은 importance sampling이 필요없는 off policy 알고리즘임.\n            - Q-learning 도 업데이트 목표를 특정 분포에서 샘플링을 통해 estimate 하는 것이 아니라 Q 함수를 최대화하는 action을 선택하는 것이기에 위 actor 에서의 deterministic 경우와 비슷하게 볼 수 있음\n3. compatible function approximation 및 gradient temporal-difference learning 을 이용한 actor-critic\n    - 위 살사/Q-learning 기반 on/off-policy 는 아래와 같은 문제가 존재\n        - function approximator 에 의한 bias\n            - 일반적으로 $ Q^{\\mu}(s,a) $ 를 $ Q^{w}(s,a) $ 로 대체하여 deterministic policy gradient 를 구하면, 그 gradient 는 ascent 하는 방향이 아닐 수도 있음\n        - off-policy learning 에 의한 instabilities\n    - 그래서 stochastic 처럼 $ \\nabla_{a}Q^{\\mu}(s,a) $ 를 $ \\nabla_{a}Q^{w}(s,a) $ 로 대체해도 deterministic policy gradient 에 영향을 미치지 않을 compatible function approximator $ Q^{w}(s,a) $ 를 찾아야 함\n    - Theorem 3. 아래 두 조건을 만족하면, $ Q^{w}(s,a) $ 는 deterministic policy $ \\mu_{\\theta}(s) $ 와 compatible 함. 즉, $ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $\n        - $ \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $\n        - $ w $ 는 $ MSE(\\theta, w) = E[\\epsilon(s;\\theta,w)^{\\top}\\epsilon(s;\\theta,w)] $ 를 최소화함\n            - $ \\epsilon(s;\\theta,w) = \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} - \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}  $\n    - Theorem 3 은 on-policy 뿐만 아니라 off-policy 에도 적용 가능함\n    - $ Q^{w}(s,a) = (a-\\mu_{\\theta}(s))^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top} w + V^{v}(s) $\n        - 어떠한 deterministic policy 에 대해서도 위 형태와 같은 compatible function approximator 가 존재함.\n        - 앞의 term 은 advantage 를, 뒤의 term 은 value 로 볼 수 있음\n    - $ Q^{w}(s,a) = \\phi(s,a)^{\\top} w + v^{\\top}\\phi(s) $\n        - 정의 : $ \\phi(s,a) \\overset{\\underset{\\mathrm{def}}{}}{=} \\nabla_{\\theta}\\mu_{\\theta}(s)(a-\\mu_{\\theta}(s)) $\n        - 일례 : $ V^{v}(s) = v^{\\top}\\phi(s) $\n        - Theorem 3 만족 여부\n            - 첫 번째 조건 만족.\n            - 두 번째 조건은 대강 만족.\n                - $ \\nabla_{a}Q^{\\mu}(s,a) $ 에 대한 unbiased sample 을 획득하기는 매우 어렵기에, 일반적인 정책 평가 방법들로 $ w $ 학습.\n                - 이 정책 평가 방법들을 이용하면 $ Q^{w}(s,a) \\approx Q^{\\mu}(s,a) $ 인 reasonable solution을 찾을 수 있기에 대강 $ \\nabla_{a}Q^{w}(s,a) \\approx \\nabla_{a}Q^{\\mu}(s,a) $ 이 될 것.\n        - action-value function 에 대한 linear function approximator 는 큰 값을 가지는 actions 에 대해선 diverge할 수 있어 global 하게 action-values 예측하기에는 좋지 않지만, local critic 에 사용할 때는 매우 유용하다.\n            - 즉, 절대값이 아니라 작은 변화량을 다루는 gradient method 경우엔 $ A^{w}(s,\\mu_{\\theta}(s)+\\delta) = \\delta^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $ 로, diverge 하지 않고, 값을 얻을 수 있음\n    - COPDAC-Q algorithm (Compatible Off-Policy Deterministic Actor-Critic Q-learning critic)\n        - Critic: 실제 action-value function 에 대한 linear function approximator 인 $ Q^{w}(s,a) = \\phi(s,a)^{\\top}w $ 를 estimate\n            - $ \\phi(s,a) = a^{\\top}\\nabla_{\\theta}\\mu_{\\theta} $\n            - Behavior policy $ \\beta(a|s) $ 로부터 얻은 samples를 이용하여 Q-learning 이나 gradient Q-learning 과 같은 off-policy algorithm 으로 학습 가능함\n        - Actor: estimated action-value function 에 대한 gradient 를 $ \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $ 로 치환 후, 정책을 업데이트함\n        - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $\n        - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) $\n        - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $\n    - off-policy Q-learning은 linear function approximation을 이용하면 diverge 할 수도 있음\n        - $ \\mu_{\\theta}(s_{t+1}) $ 이 diverge 할 수도 있기 때문으로 판단됨.\n        - 그렇기에 simple Q-learning 대신 다른 기법이 필요함.\n    - 그렇기에 critic 에 gradient Q-learning 사용한 COPDAC-GQ (Gradient Q-learning critic) algorithm 제안\n        - gradient temporal-difference learning 에 기반한 기법들은 true gradient descent algorithm 이며, converge가 보장됨. (Sutton, 2009)\n            - 기본 아이디어는 stochastic gradient descent 로 Mean-squared projected Bellman error (MSPBE) 를 최소화하는 것\n            - critic 이 actor 보다 빠른 time-scale 로 update 되도록 step size 들을 잘 조절하면, critic 은 MSPBE 를 최소화하는 parameters 로 converge 하게 됨\n            - critic 에 gradient temporal-difference learning 의 일종인 gradient Q-learning 사용한 논문 (Maei, 2010)\n    - COPDAC-GQ algorithm\n        - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $\n        - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $\n        - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) - \\alpha_{w}\\gamma\\phi(s_{t+1}, \\mu_{\\theta}(s_{t+1}))(\\phi(s_{t},a_{t})^{\\top} u_{t}) $\n        - $ v_{t+1} = v_{t} + \\alpha_{v}\\delta_{t}\\phi(s_{t}) - \\alpha_{v}\\gamma\\phi(s_{t+1})(\\phi(s_{t},a_{t})^{\\top} u_{t}) $\n        - $ u_{t+1} = u_{t} + \\alpha_{u}(\\delta_{t}-\\phi(s_{t}, a_{t})^{\\top} u_{t})\\phi(s_{t}, a_{t}) $\n    - stochastic actor-critic 과 같이 매 time-step 마다 update 시 필요한 계산의 복잡도는 $ O(mn) $\n        - m 은 action dimensions, n 은 number of policy parameters\n    - Natural policy gradient 를 이용해 deterministic policies 를 찾을 수 있음\n        - $ M(\\theta)^{-1}\\nabla_{\\theta}J(\\mu_{\\theta}) $ 는 any metric $ M(\\theta) $ 에 대한 our performance objective (식(14)) 의 steepest ascent direction 임 (Toussaint, 2012)\n        - Natural gradient 는 Fisher information metric $ M_{\\pi}(\\theta) $ 에 대한 steepest ascent direction 임\n            -  $ M_{\\pi}(\\theta) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)^{\\top}] $\n            - Fisher information metric 은 policy reparameterization 에 대해 불변임 (Bagnell, 2003)\n        - deterministic policies 에 대해 metric 으로 $ M_{\\mu}(\\theta) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}] $ 을 사용.\n        \t- 이는 variance 가 0 인 policy 에 대한 Fisher information metric 으로 볼 수 있음\n        \t- $ \\frac{\\nabla_{\\theta}\\pi_{\\theta}(a\\vert s)}{\\pi_{\\theta}(a\\vert s)}$ 에서 policy variance 가 0 이면, 특정 s 에 대한 $ \\pi_{\\theta}(a|s)$ 만 1 이 되고, 나머지는 0 임\n        - deterministic policy gradient theorem 과 compatible function approximation 을 결합하면 $ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] $ 이 됨\n            - $ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $\n            - $ \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)} \\approx \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $\n        - 그렇기에 steepest ascent direction 은 $ M_{\\mu}(\\theta)^{-1}\\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = w $ 이 됨\n            - $ E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}]^{-1}E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] = w $\n        - 이 알고리즘은 COPDAC-Q 혹은 COPDAC-GQ 에서 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $ 식을 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta}w_{t} $ 로 바꿔주기만 하면 됨\n\n## Experiments\n### Continuous Bandit\n- Stochastic Actor-Critic (SAC)과 COPDAC 간 성능 비교 수행\n    - Action dimension이 커질수록 성능 차이가 심함\n    - 빠르게 수렴하는 것을 통해 DPG의 data efficiency가 SPG에 비해 좋다는 것을 확인할 수 있지만, 반면, time-step이 증가할수록 SAC와 COPDAC 간 성능 차이가 줄어드는 것을 통해 성능 차이가 심하다는 것은 일정 time step 내에서만 해당하는 것이라고 유추해볼 수 있음\n    - <img src=\"https://www.dropbox.com/s/hrkyq0s2f24z66r/Screenshot%202018-06-16%2017.47.38.png?dl=1\">\n\n### Continuous Reinforcement Learning\n- COPDAC-Q가 SAC와 off-policy stochastic actor-critic(OffPAC-TD) 간 성능 비교 수행\n    - COPDAC-Q의 성능이 약간 더 좋음\n    - COPDAC-Q의 학습이 더 빨리 이뤄짐\n    - <img src=\"https://www.dropbox.com/s/qdca4augapmzsxi/Screenshot%202018-06-16%2017.47.07.png?dl=1\">\n### Octopus Arm\n- 목표: 6 segments octopus arm (20 action dimensions & 50 state dimensions)을 control하여 target을 맞추는 것\n    - COPDAC-Q 사용 시, action space dimension이 큰 octopus arm을 잘 control하여 target을 맞춤\n    - <img src=\"https://www.dropbox.com/s/xrxb0a52wntekld/Screenshot%202018-06-16%2017.46.28.png?dl=1\" width=600px>\n    - 기존 기법들은 action spaces 혹은 action 과 state spaces 둘 다 작은 경우들에 대해서만 실험했다고 하며, 비교하고 있지 않음.\n        - 기존 기법들이 6 segments octopus arm에서 동작을 잘 안 했을 것 같긴한데, 그래도 실험해서 결과를 보여주지...왜 안 했을까?\n    - 8 segment arm 동영상이 저자 홈페이지에 있다고 하는데, 안 보임.\n- [참고] Octopus Arm 이란?\n    - [OctopusArm Youtube Link](https://www.youtube.com/watch?v=AxeeHif0euY)\n    - <img src=\"https://www.dropbox.com/s/950ycj06sudakjx/Screenshot%202018-06-16%2017.45.52.png?dl=1\">\n","source":"_posts/dpg.md","raw":"---\ntitle: Deterministic Policy Gradient Algorithms\ndate: 2018-06-16 17:21:48\ntags: [\"프로젝트\", \"피지여행\"]\ncategories: 프로젝트\nauthor: 이웅원\nsubtitle: 피지여행 2번째 논문\n---\n\nAuthors: David Silver<sup>1)</sup>, Guy Lever<sup>2)</sup>, Nicloas Heess<sup>1)</sup>, Thomas Degris<sup>1)</sup>, Daan Wierstra<sup>1)</sup>, Martin Riedmiller<sup>1)</sup>\nAffiliation: 1) Google Deepmind, 2) University College London (UCL)\nProceeding: International Conference on Machine Learning (ICML) 2014\n\n* [Deterministic Policy Gradient Algorithms](#deterministic-policy-gradient-algorithms)\n\t* [Summary](#summary)\n\t* [Background](#background)\n\t\t* [Performance objective function](#performance-objective-function)\n\t\t* [SPG Theorem](#spg-theorem)\n\t\t* [Stochastic Actor-Critic Algorithms](#stochastic-actor-critic-algorithms)\n\t\t* [Off-policy Actor-Critic](#off-policy-actor-critic)\n\t* [Gradient of Deterministic Policies](#gradient-of-deterministic-policies)\n\t\t* [Regulariy Conditions](#regulariy-conditions)\n\t\t* [Deterministic Policy Gradient Theorem](#deterministic-policy-gradient-theorem)\n\t\t* [DPG 형태에 대한 informal intuition](#dpg-형태에-대한-informal-intuition)\n\t\t* [DPG 는 SPG 의 limiting case 임](#dpg-는-spg-의-limiting-case-임)\n\t* [Deterministic Actor-Critic Algorithms](#deterministic-actor-critic-algorithms)\n\t* [Experiments](#experiments)\n\t\t* [Continuous Bandit](#continuous-bandit)\n\t\t* [Continuous Reinforcement Learning](#continuous-reinforcement-learning)\n\t\t* [Octopus Arm](#octopus-arm)\n\n<!-- /code_chunk_output -->\n\n<br>\n\n---\n## 1. Summary\n- Deterministic Policy Gradient (DPG) Theorem 제안함 [[Theorem 1](#deterministic-policy-gradient-theorem)]\n    1) DPG는 존재하며,\n    2) DPG는 Expected gradient of the action-value function의 형태를 띈다.\n- Policy variance가 0에 수렴할 경우, DPG는 Stochastic Policy Gradient(SPG)와 동일해짐을 보임 [[Theorem 2](#dpg-는-spg-의-limiting-case-임)]\n    - Theorem 2 로 인해 기존 Policy Gradient (PG) 와 관련된 기법들을 DPG 에 적용할 수 있게 됨\n        - 예. Sutton PG, natural gradients, actor-critic, episodic/batch methods\n- 적절한 exploration 을 위해 model-free, off-policy Actor-Critic algorithm 을 제안함\n    - Action-value function approximator 사용으로 인해 policy gradient가 bias되는 것을 방지하기 위해 compatibility conditions을 제공 [[Theorem 3](##deterministic-actor-critic-algorithms)]\n- DPG 는 SPG 보다 성능이 좋음\n    - 특히 high dimensional action spaces 을 가지는 tasks에서의 성능 향상이 큼\n        - SPG의 policy gradient는 state와 action spaces 모두에 대해서, DPG의 policy gradient는  state spaces에 대해서만 평균을 취함\n        - 결과적으로, action spaces의 dimension이 커질수록 data efficiency가 높은 DPG의 학습이 더 잘 이뤄지게 됨\n        - 무한정 학습을 시키게 되면, SPG도 최적으로 수렴할 것으로 예상되기에 위 성능 비교는 일정 iteration 내로 한정함\n    - 기존 기법들에 비해 computation 양이 많지 않음\n        - Computation 은 action dimensionality 와 policy parameters 수에 비례함\n<br>\n\n---\n## 2. Background\n### 2.1 Performance objective function\n\n$$\n\\begin{align}\nJ(\\pi_{\\theta}) &= \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\pi_{\\theta}(s,a)r(s,a)dads = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[r(s,a)]\n\\end{align}\n$$\n\n### 2.2 SPG Theorem\n- State distribution $ \\rho^{\\pi}(s) $ 은 policy parameters에 영향을 받지만, policy gradient 를 계산할 때는 state distribution 의 gradient 를 고려할 필요가 없다.\n- $$\\begin{eqnarray}\\nabla_{\\theta}J(\\pi_{\\theta}) &=& \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\\\ &=& E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]\n\\end{eqnarray}$$\n\n### 2.3 Stochastic Actor-Critic Algorithms\n- Actor 와 Critic 이 번갈아가면서 동작하며 stochastic policy 를 최적화하는 기법\n- Actor: $ Q^{\\pi}(s,a) $ 를 근사한 $ Q^w(s,a) $ 를 이용해 stochastic policy gradient 를 ascent 하는 방향으로 policy parameter $ \\theta $ 를 업데이트함으로써 stochastic policy 를 발전시킴\n    - $ \\nabla_{\\theta}J(\\pi_{\\theta}) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}log\\pi_{\\theta}(a|s)Q^{w}(s,a)] $\n- Critic: SARSA 나 Q-learning 같은 Temporal-difference (TD) learning 을 이용해 action-value function의 parameter, $ w $ 를 업데이트함으로써 $ Q^w(s,a) $ 가 $ Q^{\\pi}(s,a) $ 과 유사해지도록 함\n- 실제 값인 $ Q^{\\pi}(s,a) $ 대신 이를 근사한 $ Q^w(s,a) $ 를 사용하게 되면, 일반적으로 bias 가 발생하게 된다. 하지만, compatible condition 에 부합하는 $ Q^w(s,a) $ 를 사용하게 되면, bias 가 발생하지 않는다.\n\n### 2.4 Off-policy Actor-Critic\n- Distinct behavior policy $ \\beta(a|s) ( \\neq \\pi_{\\theta}(a|s) ) $ 로부터 샘플링된 trajectories 를 이용한 Actor-Critic\n- Performance objective function\n    - $\\begin{eqnarray}\n        J_{\\beta}(\\pi_{\\theta}) \n        &=& \\int_{S}\\rho^{\\beta}(s)V^{\\pi}(s)ds \\nonumber \\\\\\\\\n        &=& \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads\n        \\end{eqnarray} $\n- off-policy policy gradient\n    - $ \\begin{eqnarray}\n        \\nabla_{\\theta}J_{\\beta}(\\pi_{\\theta}) &\\approx& \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\end{eqnarray} $\n      $=E_{s \\sim \\rho^{\\beta}, a \\sim \\beta}[\\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)}\\nabla_{\\theta}log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]$\n    - off-policy policy gradient 식에서의 물결 표시는 [Degris, 2012b] 논문에 근거함\n        - [Degris, 2012b] \"Linear off-policy actor-critic,\" ICML 2012\n        - Exact off-policy policy gradient 와 이를 approximate 한 policy gradient 는 아래와 같음. (빨간색 상자에 있는 항목을 삭제함으로써 근사함)\n            - <img src=\"https://www.dropbox.com/s/xzpv3okc139c1fs/Screenshot%202018-06-16%2017.48.51.png?dl=1\" width=500px>\n        - [Degris, 2012b] Theorem 1 에 의해 policy parameter 가 approximated policy gradient ( $\\nabla_{u}𝑄^{\\pi,\\gamma}(𝑠,𝑎)$ term 제거)에 따라 업데이트되어도 policy 는 improve 가 됨이 보장되기에 exact off-policy policy gradient 대신 approximated off-policy policy gradient 를 사용해도 괜찮음.\n            - <img src=\"https://www.dropbox.com/s/mk13931r4scjngo/Screenshot%202018-06-16%2017.49.24.png?dl=1\" width=500px>\n    - off-policy policy gradient 식에서 $ \\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)} $ 는 importance sampling ratio 임\n        - off-policy actor-critic 에서는 $ \\beta $ 에 의해 샘플링된 trajectory 를 이용해서 stochastic policy $ \\pi $ 를 예측하는 것이기 때문에 imnportance sampling 이 필요함\n<br>\n\n---\n## 3. Gradient of Deterministic Policies\n### 3.1 Regulariy Conditions\n- 어떠한 이론이 성립하기 위한 전제 조건\n- Regularity conditions A.1\n    - $ p(s'|s,a), \\nabla_{a}p(s'|s,a), \\mu_{\\theta}(s), \\nabla_{\\theta}\\mu_{\\theta}(s), r(s,a), \\nabla_{a}r(s,a), p_{1}(s) $ are continuous in all parameters and variables $ s, a, s' $ and $ x $.\n- regularity conditions A.2\n    - There exists a $ b $ and $ L $ such that $ \\sup_{s}p_{1}(s) < b $, $ \\sup_{a,s,s'}p(s'|s,a) < b $, $ \\sup_{a,s}r(s,a) < b $, $ \\sup_{a,s,s'}\\|\\nabla_{a}p(s'|s,a)\\| < L $, and $ \\sup_{a,s}\\|\\nabla_{a}r(s,a)\\| < L $.\n\n### 3.2 Deterministic Policy Gradient Theorem\n- Deterministic policy\n    - $ \\mu_{\\theta} : S \\to A $ with parameter vector $ \\theta \\in \\rm I \\! R^n $\n- Probability distribution\n    - $ p(s \\to s', t, \\mu) $\n- Discounted state distribution\n    - $ \\rho^{\\mu}(s) $\n- Performance objective\n\n$$\nJ(\\mu_{\\theta}) = E[r^{\\gamma}_{1} | \\mu] \n$$\n\n$$\n= \\int_{S}\\rho^{\\mu}(s)r(s,\\mu_{\\theta}(s))ds \n= E_{s \\sim \\rho^{\\mu}}[r(s,\\mu_{\\theta}(s))]\n$$\n\n- DPG Theorem\n    - MDP 가 A.1 만족한다면, 아래 식(9)이 성립함\n    $\\nabla_{\\theta}J(\\mu_{\\theta}) = \\int_{S}\\rho^{\\mu}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}ds \\nonumber$\n    $= E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]   \\nonumber $ (9)\n    \n\t- DPG는 State space 에 대해서만 평균을 취하면 되기에, State와 Action space 모두에 대해 평균을 취해야 하는 [SPG](#spg-theorem)에 비해 data efficiency가 좋다. 즉, 더 적은 양의 데이터로도 학습이 잘 이뤄지게 된다.\n\n    \n### 3.3 DPG 형태에 대한 informal intuition\n- Generalized policy iteration\n    - 정책 평가와 정책 발전을 한 번 씩 번갈아 가면서 실행하는 정책 iteration\n        - 위와 같이 해도 정책 평가에서 예측한 가치함수가 최적 가치함수에 수렴함\n- 정책 평가\n    - action-value function $ Q^{\\pi}(s,a) $ or $ Q^{\\mu}(s,a) $ 을 estimate 하는 것\n- 정책 발전\n    - 위 estimated action-value function 에 따라 정책을 update 하는 것\n    - 주로 action-value function 에 대한 greedy maximisation 사용함\n        - $ \\mu^{k+1}(s) = \\arg\\max\\limits_{a}Q^{\\mu^{k}}(s,a) $\n        - greedy 정책 발전은 매 단계마다 global maximization을 해야하는데, 이는 continuous action spaces 에서 계산량이 급격히 늘어나게 됨.\n    - 그렇기에 policy gradient 방법이 나옴\n        - policy 를 $ \\theta $ 에 대해서 parameterize 함\n        - 매 단계마다 global maximisation 수행하는 대신, 방문하는 state $ s $ 마다 policy parameter 를 action-value function $ Q $ 의 $ \\theta $ 에 대한 gradient $ \\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s)) $ 방향으로 proportional 하게 update 함\n        - 하지만 각 state 는 다른 방향을 제시할 수 있기에, state distribution $ \\rho^{\\mu}(s) $ 에 대한 기대값을 취해 policy parameter 를 update 할 수도 있음\n            - $ \\theta^{k+1} = \\theta^{k} + \\alpha \\rm I\\!E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s))] $\n        - 이는 chain-rule 에 따라 아래와 같이 분리될 수 있음\n            - $ \\theta^{k+1} = \\theta^{k} + \\alpha \\rm I\\!E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu^{k}}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $ (7)\n            - chain rule: $ \\frac{\\partial Q}{\\partial \\theta} = \\frac{\\partial a}{\\partial \\theta} \\frac{\\partial Q}{\\partial a} $\n        - 하지만 state distribution $ \\rho^{\\mu} $ 은 정책에 dependent 함\n            - 정책이 바꾸게 되면, 바뀐 정책에 따라 방문하게 되는 state 가 변하게 되기 때문에 state distribution이 변하게 됨\n        - 그렇기에 정책 update 시 state distribution에 대한 gradient 를 고려하지 않는데 정책 발전이 이뤄진다는 것은 직관적으로 와닿지 않을 수 있음\n        - deterministic policy gradient theorem 은 state distribution에 대한 gradient 계산없이 위 식(7) 대로만 update 해도 performance objective 의 gradient 를 정확하게 따름을 의미한다.\n\n\n### 3.4 DPG 는 SPG 의 limiting case 임\n- stochastic policy parameterization\n    - $ \\pi_{\\mu_{\\theta},\\sigma} $ by a deterministic policy $ \\mu_{\\theta} : S \\to A $ and a variance parameter $ \\sigma $\n    - $ \\sigma = 0 $ 이면, $ \\pi_{\\mu_{\\theta},\\sigma} \\equiv \\mu_{\\theta} $\n- Theorem 2. Policy 의 variance 가 0 에 수렴하면, 즉, $ \\sigma \\to 0 $, stochastic policy gradient 와 deterministic policy gradient 는 동일해짐\n    - 조건 : stochastic policy $ \\pi_{\\mu_{\\theta},\\sigma} = \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $\n        - $ \\sigma $ 는 variance\n        - $ \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $ 는 conditions B.1 만족\n        - MDP 는 conditions A.1 및 A.2 만족\n    - 결과 :\n        - $ \\lim\\limits_{\\sigma\\downarrow0}\\nabla_{\\theta}J(\\pi_{\\mu_{\\theta},\\sigma}) = \\nabla_{\\theta}J(\\mu_{\\theta})  $\n            - 좌변은 standard stochastic gradient 이며, 우변은 deterministic gradient.\n    - 의미 :\n        - deterministic policy gradient 는 stochastic policy gradient 의 특수 case 임\n        - 기존 유명한 policy gradients 기법들에 deterministic policy gradients 를 적용할 수 있음\n            - 기존 기법들 예: compatible function approximation (Sutton, 1999), natural gradients (Kakade, 2001), actor-critic (Bhatnagar, 2007) or episodic/batch methods (Peters, 2005)\n<br>\n \n---\n## 4. Deterministic Actor-Critic Algorithms\n1. 살사 critic 을 이용한 on-policy actor-critic\n    - 단점\n        - deterministic policy 에 의해 행동하면 exploration 이 잘 되지 않기에, sub-optimal 에 빠지기 쉬움\n    - 목적\n        - 교훈/정보제공\n        - 환경에서 충분한 noise 를 제공하여 exploration을 시킬 수 있다면, deterministic policy 를 사용한다고 하여도 좋은 학습 결과를 얻을 수도 있음$\n            - 예. 바람이 agent의 행동에 영향(noise)을 줌\n    - Remind: 살사(SARSA) update rule\n        - $ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma Q(s_{t+1},a_{t+1}) - Q(s_{t},a_{t})) $\n    - Algorithm\n        - Critic 은 MSE 를 $ \\bf minimize $ 하는 방향, 즉, action-value function 을 stochastic gradient $ \\bf descent $ 방법으로 update 함\n            - $ MSE = [Q^{\\mu}(s,a) - Q^{w}(s,a)]^2 $\n                - critic 은 실제 $ Q^{\\mu}(s,a) $ 대신 미분 가능한 $ Q^{w}(s,a) $ 로 대체하여 action-value function 을 estimate 하며, 이 둘 간 Mean Square Error 를 최소화하는 것이 목표\n            - $ \\nabla_{w}MSE \\approx -2 * [r + \\gamma Q^{w}(s',a') - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $\n                - $ \\nabla_{w}MSE = -2 * [Q^{\\mu}(s,a) - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $\n                - $ Q^{\\mu}(s,a) $ 를 $ r + \\gamma Q^{w}(s',a') $ 로 대체\n                    - $ Q^{\\mu}(s,a) = r + \\gamma Q^{\\mu}(s',a') $\n            - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $\n                - $w_{t+1} = w_{t} - \\alpha_{w}\\nabla_{w}MSE  \\nonumber$\n                $ \\approx w_{t} - \\alpha_{w} * (-2 * [r + \\gamma Q^{w}(s',a') - Q^{w}(s,a)] \\nabla_{w}Q^{w}(s,a)$\n                - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},a_{t+1}) - Q^{w}(s_{t},a_{t}) $\n        - Actor 는 식(9)에 따라 보상이 $ \\bf maximize $ 되는 방향, 즉, deterministic policy 를 stochastic gradient $ \\bf ascent $ 방법으로 update함\n            - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $\n2. Q-learning 을 이용한 off-policy actor-critic\n    - stochastic behavior policy $ \\beta(a|s) $ 에 의해 생성된 trajectories 로부터 deterministic target policy $ \\mu_{\\theta}(s) $ 를 학습하는 off-policy actor-critic\n    - performance objective\n        - $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)V^{\\mu}(s)ds \\nonumber \\\\\\\\$\n          $= \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds \\nonumber \\\\\\\\$\n          $= E_{s \\sim \\rho^{\\beta}}[Q^{\\mu}(s,\\mu_{\\theta}(s))]$\n    - off-policy deterministic policy gradient\n        - $ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $\n            - 논문에는 아래와 같이 나와있는데, 물결 표시 부분은 오류로 판단됨.\n            - $ \\begin{eqnarray}\n                \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) &\\approx& \\int_{S}\\rho^{\\beta}(s)\\nabla_{\\theta}\\mu_{\\theta}(a|s)Q^{\\mu}(s,a)ds \\nonumber \\\\\n                &=& E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]\n                \\end{eqnarray} $\n            - 근거: Action이 deterministic 하기에 stochastic 경우와는 다르게 performance objective 에서 action 에 대해 평균을 구할 필요가 없음. 그렇기에, 곱의 미분이 있을 필요가 없고, [Degris, 2012b] 에서처럼 곱의 미분을 통해 생기는 action-value function 에 대한 gradient term 를 생략할 필요가 사라짐. [참고](#off-policy-actor-critic)\n    - Remind: 큐러닝(Q-learning) update rule\n        - $ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma \\max\\limits_{a}Q(s_{t+1},a) - Q(s_{t},a_{t})) $\n    - algorithm: OPDAC (Off-Policy Deterministic Actor-Critic)\n        - 살사를 이용한 on-policy deterministic actor-critic 과 아래 부분을 제외하고는 같음\n            - target policy 는 $ \\beta(a|s) $ 에 의해 생성된 trajectories 를 통해 학습함\n            - 업데이트 목표 부분에 실제 행동 값 $ a_{t+1} $ 이 아니라 정책으로부터 나온 행동 값 $ \\mu_{\\theta}(s_{t+1}) $ 사용함\n                - $ \\mu_{\\theta}(s_{t+1}) $ 는 가장 높은 Q 값을 가지는 행동. 즉, Q-learning.\n        - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $\n        - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $\n        - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $\n    - Stochastic off-policy actor-critic 은 대개 actor 와 critic 모두 importance sampling을 필요로 하지만, deterministic policy gradient 에선 importance sampling이 필요없음\n        - Actor 는 deterministic 이기에 sampling 자체가 필요없음\n            - Stochastic policy 인 경우, Actor 에서 importance sampling이 필요한 이유는 상태 $ s $ 에서의 가치 함수 값 $ V^{\\pi}(s) $ 을 estimate 하기 위해 $ \\pi $ 가 아니라 $ \\beta $ 에 따라 sampling을 한 후, 평균을 내기 때문임.\n            - 하지만 Deterministic policy 인 경우, 상태 $ s $ 에서의 가치 함수 값 $ V^{\\pi}(s) = Q^{\\pi}(s,\\mu_{\\theta}) $ 즉, action 이 상태 s 에 대해 deterministic 이기에 sampling 을 통해 estimate 할 필요가 없고, 따라서 importance sampling도 필요없어짐.\n            - stochastic vs. deterministic performance objective\n                - stochastic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads $\n                - deterministic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds $\n        - Critic 이 사용하는 Q-learning 은 importance sampling이 필요없는 off policy 알고리즘임.\n            - Q-learning 도 업데이트 목표를 특정 분포에서 샘플링을 통해 estimate 하는 것이 아니라 Q 함수를 최대화하는 action을 선택하는 것이기에 위 actor 에서의 deterministic 경우와 비슷하게 볼 수 있음\n3. compatible function approximation 및 gradient temporal-difference learning 을 이용한 actor-critic\n    - 위 살사/Q-learning 기반 on/off-policy 는 아래와 같은 문제가 존재\n        - function approximator 에 의한 bias\n            - 일반적으로 $ Q^{\\mu}(s,a) $ 를 $ Q^{w}(s,a) $ 로 대체하여 deterministic policy gradient 를 구하면, 그 gradient 는 ascent 하는 방향이 아닐 수도 있음\n        - off-policy learning 에 의한 instabilities\n    - 그래서 stochastic 처럼 $ \\nabla_{a}Q^{\\mu}(s,a) $ 를 $ \\nabla_{a}Q^{w}(s,a) $ 로 대체해도 deterministic policy gradient 에 영향을 미치지 않을 compatible function approximator $ Q^{w}(s,a) $ 를 찾아야 함\n    - Theorem 3. 아래 두 조건을 만족하면, $ Q^{w}(s,a) $ 는 deterministic policy $ \\mu_{\\theta}(s) $ 와 compatible 함. 즉, $ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $\n        - $ \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $\n        - $ w $ 는 $ MSE(\\theta, w) = E[\\epsilon(s;\\theta,w)^{\\top}\\epsilon(s;\\theta,w)] $ 를 최소화함\n            - $ \\epsilon(s;\\theta,w) = \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} - \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}  $\n    - Theorem 3 은 on-policy 뿐만 아니라 off-policy 에도 적용 가능함\n    - $ Q^{w}(s,a) = (a-\\mu_{\\theta}(s))^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top} w + V^{v}(s) $\n        - 어떠한 deterministic policy 에 대해서도 위 형태와 같은 compatible function approximator 가 존재함.\n        - 앞의 term 은 advantage 를, 뒤의 term 은 value 로 볼 수 있음\n    - $ Q^{w}(s,a) = \\phi(s,a)^{\\top} w + v^{\\top}\\phi(s) $\n        - 정의 : $ \\phi(s,a) \\overset{\\underset{\\mathrm{def}}{}}{=} \\nabla_{\\theta}\\mu_{\\theta}(s)(a-\\mu_{\\theta}(s)) $\n        - 일례 : $ V^{v}(s) = v^{\\top}\\phi(s) $\n        - Theorem 3 만족 여부\n            - 첫 번째 조건 만족.\n            - 두 번째 조건은 대강 만족.\n                - $ \\nabla_{a}Q^{\\mu}(s,a) $ 에 대한 unbiased sample 을 획득하기는 매우 어렵기에, 일반적인 정책 평가 방법들로 $ w $ 학습.\n                - 이 정책 평가 방법들을 이용하면 $ Q^{w}(s,a) \\approx Q^{\\mu}(s,a) $ 인 reasonable solution을 찾을 수 있기에 대강 $ \\nabla_{a}Q^{w}(s,a) \\approx \\nabla_{a}Q^{\\mu}(s,a) $ 이 될 것.\n        - action-value function 에 대한 linear function approximator 는 큰 값을 가지는 actions 에 대해선 diverge할 수 있어 global 하게 action-values 예측하기에는 좋지 않지만, local critic 에 사용할 때는 매우 유용하다.\n            - 즉, 절대값이 아니라 작은 변화량을 다루는 gradient method 경우엔 $ A^{w}(s,\\mu_{\\theta}(s)+\\delta) = \\delta^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $ 로, diverge 하지 않고, 값을 얻을 수 있음\n    - COPDAC-Q algorithm (Compatible Off-Policy Deterministic Actor-Critic Q-learning critic)\n        - Critic: 실제 action-value function 에 대한 linear function approximator 인 $ Q^{w}(s,a) = \\phi(s,a)^{\\top}w $ 를 estimate\n            - $ \\phi(s,a) = a^{\\top}\\nabla_{\\theta}\\mu_{\\theta} $\n            - Behavior policy $ \\beta(a|s) $ 로부터 얻은 samples를 이용하여 Q-learning 이나 gradient Q-learning 과 같은 off-policy algorithm 으로 학습 가능함\n        - Actor: estimated action-value function 에 대한 gradient 를 $ \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $ 로 치환 후, 정책을 업데이트함\n        - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $\n        - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) $\n        - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $\n    - off-policy Q-learning은 linear function approximation을 이용하면 diverge 할 수도 있음\n        - $ \\mu_{\\theta}(s_{t+1}) $ 이 diverge 할 수도 있기 때문으로 판단됨.\n        - 그렇기에 simple Q-learning 대신 다른 기법이 필요함.\n    - 그렇기에 critic 에 gradient Q-learning 사용한 COPDAC-GQ (Gradient Q-learning critic) algorithm 제안\n        - gradient temporal-difference learning 에 기반한 기법들은 true gradient descent algorithm 이며, converge가 보장됨. (Sutton, 2009)\n            - 기본 아이디어는 stochastic gradient descent 로 Mean-squared projected Bellman error (MSPBE) 를 최소화하는 것\n            - critic 이 actor 보다 빠른 time-scale 로 update 되도록 step size 들을 잘 조절하면, critic 은 MSPBE 를 최소화하는 parameters 로 converge 하게 됨\n            - critic 에 gradient temporal-difference learning 의 일종인 gradient Q-learning 사용한 논문 (Maei, 2010)\n    - COPDAC-GQ algorithm\n        - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $\n        - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $\n        - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) - \\alpha_{w}\\gamma\\phi(s_{t+1}, \\mu_{\\theta}(s_{t+1}))(\\phi(s_{t},a_{t})^{\\top} u_{t}) $\n        - $ v_{t+1} = v_{t} + \\alpha_{v}\\delta_{t}\\phi(s_{t}) - \\alpha_{v}\\gamma\\phi(s_{t+1})(\\phi(s_{t},a_{t})^{\\top} u_{t}) $\n        - $ u_{t+1} = u_{t} + \\alpha_{u}(\\delta_{t}-\\phi(s_{t}, a_{t})^{\\top} u_{t})\\phi(s_{t}, a_{t}) $\n    - stochastic actor-critic 과 같이 매 time-step 마다 update 시 필요한 계산의 복잡도는 $ O(mn) $\n        - m 은 action dimensions, n 은 number of policy parameters\n    - Natural policy gradient 를 이용해 deterministic policies 를 찾을 수 있음\n        - $ M(\\theta)^{-1}\\nabla_{\\theta}J(\\mu_{\\theta}) $ 는 any metric $ M(\\theta) $ 에 대한 our performance objective (식(14)) 의 steepest ascent direction 임 (Toussaint, 2012)\n        - Natural gradient 는 Fisher information metric $ M_{\\pi}(\\theta) $ 에 대한 steepest ascent direction 임\n            -  $ M_{\\pi}(\\theta) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)^{\\top}] $\n            - Fisher information metric 은 policy reparameterization 에 대해 불변임 (Bagnell, 2003)\n        - deterministic policies 에 대해 metric 으로 $ M_{\\mu}(\\theta) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}] $ 을 사용.\n        \t- 이는 variance 가 0 인 policy 에 대한 Fisher information metric 으로 볼 수 있음\n        \t- $ \\frac{\\nabla_{\\theta}\\pi_{\\theta}(a\\vert s)}{\\pi_{\\theta}(a\\vert s)}$ 에서 policy variance 가 0 이면, 특정 s 에 대한 $ \\pi_{\\theta}(a|s)$ 만 1 이 되고, 나머지는 0 임\n        - deterministic policy gradient theorem 과 compatible function approximation 을 결합하면 $ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] $ 이 됨\n            - $ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $\n            - $ \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)} \\approx \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $\n        - 그렇기에 steepest ascent direction 은 $ M_{\\mu}(\\theta)^{-1}\\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = w $ 이 됨\n            - $ E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}]^{-1}E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] = w $\n        - 이 알고리즘은 COPDAC-Q 혹은 COPDAC-GQ 에서 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $ 식을 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta}w_{t} $ 로 바꿔주기만 하면 됨\n\n## Experiments\n### Continuous Bandit\n- Stochastic Actor-Critic (SAC)과 COPDAC 간 성능 비교 수행\n    - Action dimension이 커질수록 성능 차이가 심함\n    - 빠르게 수렴하는 것을 통해 DPG의 data efficiency가 SPG에 비해 좋다는 것을 확인할 수 있지만, 반면, time-step이 증가할수록 SAC와 COPDAC 간 성능 차이가 줄어드는 것을 통해 성능 차이가 심하다는 것은 일정 time step 내에서만 해당하는 것이라고 유추해볼 수 있음\n    - <img src=\"https://www.dropbox.com/s/hrkyq0s2f24z66r/Screenshot%202018-06-16%2017.47.38.png?dl=1\">\n\n### Continuous Reinforcement Learning\n- COPDAC-Q가 SAC와 off-policy stochastic actor-critic(OffPAC-TD) 간 성능 비교 수행\n    - COPDAC-Q의 성능이 약간 더 좋음\n    - COPDAC-Q의 학습이 더 빨리 이뤄짐\n    - <img src=\"https://www.dropbox.com/s/qdca4augapmzsxi/Screenshot%202018-06-16%2017.47.07.png?dl=1\">\n### Octopus Arm\n- 목표: 6 segments octopus arm (20 action dimensions & 50 state dimensions)을 control하여 target을 맞추는 것\n    - COPDAC-Q 사용 시, action space dimension이 큰 octopus arm을 잘 control하여 target을 맞춤\n    - <img src=\"https://www.dropbox.com/s/xrxb0a52wntekld/Screenshot%202018-06-16%2017.46.28.png?dl=1\" width=600px>\n    - 기존 기법들은 action spaces 혹은 action 과 state spaces 둘 다 작은 경우들에 대해서만 실험했다고 하며, 비교하고 있지 않음.\n        - 기존 기법들이 6 segments octopus arm에서 동작을 잘 안 했을 것 같긴한데, 그래도 실험해서 결과를 보여주지...왜 안 했을까?\n    - 8 segment arm 동영상이 저자 홈페이지에 있다고 하는데, 안 보임.\n- [참고] Octopus Arm 이란?\n    - [OctopusArm Youtube Link](https://www.youtube.com/watch?v=AxeeHif0euY)\n    - <img src=\"https://www.dropbox.com/s/950ycj06sudakjx/Screenshot%202018-06-16%2017.45.52.png?dl=1\">\n","slug":"dpg","published":1,"updated":"2018-06-16T14:09:54.593Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjiinnj6q0005j28a4kpaxi6x","content":"<p>Authors: David Silver<sup>1)</sup>, Guy Lever<sup>2)</sup>, Nicloas Heess<sup>1)</sup>, Thomas Degris<sup>1)</sup>, Daan Wierstra<sup>1)</sup>, Martin Riedmiller<sup>1)</sup><br>Affiliation: 1) Google Deepmind, 2) University College London (UCL)<br>Proceeding: International Conference on Machine Learning (ICML) 2014</p>\n<ul>\n<li><a href=\"#deterministic-policy-gradient-algorithms\">Deterministic Policy Gradient Algorithms</a><ul>\n<li><a href=\"#summary\">Summary</a></li>\n<li><a href=\"#background\">Background</a><ul>\n<li><a href=\"#performance-objective-function\">Performance objective function</a></li>\n<li><a href=\"#spg-theorem\">SPG Theorem</a></li>\n<li><a href=\"#stochastic-actor-critic-algorithms\">Stochastic Actor-Critic Algorithms</a></li>\n<li><a href=\"#off-policy-actor-critic\">Off-policy Actor-Critic</a></li>\n</ul>\n</li>\n<li><a href=\"#gradient-of-deterministic-policies\">Gradient of Deterministic Policies</a><ul>\n<li><a href=\"#regulariy-conditions\">Regulariy Conditions</a></li>\n<li><a href=\"#deterministic-policy-gradient-theorem\">Deterministic Policy Gradient Theorem</a></li>\n<li><a href=\"#dpg-형태에-대한-informal-intuition\">DPG 형태에 대한 informal intuition</a></li>\n<li><a href=\"#dpg-는-spg-의-limiting-case-임\">DPG 는 SPG 의 limiting case 임</a></li>\n</ul>\n</li>\n<li><a href=\"#deterministic-actor-critic-algorithms\">Deterministic Actor-Critic Algorithms</a></li>\n<li><a href=\"#experiments\">Experiments</a><ul>\n<li><a href=\"#continuous-bandit\">Continuous Bandit</a></li>\n<li><a href=\"#continuous-reinforcement-learning\">Continuous Reinforcement Learning</a></li>\n<li><a href=\"#octopus-arm\">Octopus Arm</a></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<!-- /code_chunk_output -->\n<p><br></p>\n<hr>\n<h2 id=\"1-Summary\"><a href=\"#1-Summary\" class=\"headerlink\" title=\"1. Summary\"></a>1. Summary</h2><ul>\n<li>Deterministic Policy Gradient (DPG) Theorem 제안함 [<a href=\"#deterministic-policy-gradient-theorem\">Theorem 1</a>]<br>  1) DPG는 존재하며,<br>  2) DPG는 Expected gradient of the action-value function의 형태를 띈다.</li>\n<li>Policy variance가 0에 수렴할 경우, DPG는 Stochastic Policy Gradient(SPG)와 동일해짐을 보임 [<a href=\"#dpg-는-spg-의-limiting-case-임\">Theorem 2</a>]<ul>\n<li>Theorem 2 로 인해 기존 Policy Gradient (PG) 와 관련된 기법들을 DPG 에 적용할 수 있게 됨<ul>\n<li>예. Sutton PG, natural gradients, actor-critic, episodic/batch methods</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>적절한 exploration 을 위해 model-free, off-policy Actor-Critic algorithm 을 제안함<ul>\n<li>Action-value function approximator 사용으로 인해 policy gradient가 bias되는 것을 방지하기 위해 compatibility conditions을 제공 [<a href=\"##deterministic-actor-critic-algorithms\">Theorem 3</a>]</li>\n</ul>\n</li>\n<li>DPG 는 SPG 보다 성능이 좋음<ul>\n<li>특히 high dimensional action spaces 을 가지는 tasks에서의 성능 향상이 큼<ul>\n<li>SPG의 policy gradient는 state와 action spaces 모두에 대해서, DPG의 policy gradient는  state spaces에 대해서만 평균을 취함</li>\n<li>결과적으로, action spaces의 dimension이 커질수록 data efficiency가 높은 DPG의 학습이 더 잘 이뤄지게 됨</li>\n<li>무한정 학습을 시키게 되면, SPG도 최적으로 수렴할 것으로 예상되기에 위 성능 비교는 일정 iteration 내로 한정함</li>\n</ul>\n</li>\n<li>기존 기법들에 비해 computation 양이 많지 않음<ul>\n<li>Computation 은 action dimensionality 와 policy parameters 수에 비례함<br><br></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2 id=\"2-Background\"><a href=\"#2-Background\" class=\"headerlink\" title=\"2. Background\"></a>2. Background</h2><h3 id=\"2-1-Performance-objective-function\"><a href=\"#2-1-Performance-objective-function\" class=\"headerlink\" title=\"2.1 Performance objective function\"></a>2.1 Performance objective function</h3><p>$$<br>\\begin{align}<br>J(\\pi_{\\theta}) &amp;= \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\pi_{\\theta}(s,a)r(s,a)dads = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[r(s,a)]<br>\\end{align}<br>$$</p>\n<h3 id=\"2-2-SPG-Theorem\"><a href=\"#2-2-SPG-Theorem\" class=\"headerlink\" title=\"2.2 SPG Theorem\"></a>2.2 SPG Theorem</h3><ul>\n<li>State distribution $ \\rho^{\\pi}(s) $ 은 policy parameters에 영향을 받지만, policy gradient 를 계산할 때는 state distribution 의 gradient 를 고려할 필요가 없다.</li>\n<li>$$\\begin{eqnarray}\\nabla_{\\theta}J(\\pi_{\\theta}) &amp;=&amp; \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\ &amp;=&amp; E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]<br>\\end{eqnarray}$$</li>\n</ul>\n<h3 id=\"2-3-Stochastic-Actor-Critic-Algorithms\"><a href=\"#2-3-Stochastic-Actor-Critic-Algorithms\" class=\"headerlink\" title=\"2.3 Stochastic Actor-Critic Algorithms\"></a>2.3 Stochastic Actor-Critic Algorithms</h3><ul>\n<li>Actor 와 Critic 이 번갈아가면서 동작하며 stochastic policy 를 최적화하는 기법</li>\n<li>Actor: $ Q^{\\pi}(s,a) $ 를 근사한 $ Q^w(s,a) $ 를 이용해 stochastic policy gradient 를 ascent 하는 방향으로 policy parameter $ \\theta $ 를 업데이트함으로써 stochastic policy 를 발전시킴<ul>\n<li>$ \\nabla_{\\theta}J(\\pi_{\\theta}) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}log\\pi_{\\theta}(a|s)Q^{w}(s,a)] $</li>\n</ul>\n</li>\n<li>Critic: SARSA 나 Q-learning 같은 Temporal-difference (TD) learning 을 이용해 action-value function의 parameter, $ w $ 를 업데이트함으로써 $ Q^w(s,a) $ 가 $ Q^{\\pi}(s,a) $ 과 유사해지도록 함</li>\n<li>실제 값인 $ Q^{\\pi}(s,a) $ 대신 이를 근사한 $ Q^w(s,a) $ 를 사용하게 되면, 일반적으로 bias 가 발생하게 된다. 하지만, compatible condition 에 부합하는 $ Q^w(s,a) $ 를 사용하게 되면, bias 가 발생하지 않는다.</li>\n</ul>\n<h3 id=\"2-4-Off-policy-Actor-Critic\"><a href=\"#2-4-Off-policy-Actor-Critic\" class=\"headerlink\" title=\"2.4 Off-policy Actor-Critic\"></a>2.4 Off-policy Actor-Critic</h3><ul>\n<li>Distinct behavior policy $ \\beta(a|s) ( \\neq \\pi_{\\theta}(a|s) ) $ 로부터 샘플링된 trajectories 를 이용한 Actor-Critic</li>\n<li>Performance objective function<ul>\n<li>$\\begin{eqnarray}<br>  J_{\\beta}(\\pi_{\\theta})<br>  &amp;=&amp; \\int_{S}\\rho^{\\beta}(s)V^{\\pi}(s)ds \\nonumber \\\\<br>  &amp;=&amp; \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads<br>  \\end{eqnarray} $</li>\n</ul>\n</li>\n<li>off-policy policy gradient<ul>\n<li>$ \\begin{eqnarray}<br>  \\nabla_{\\theta}J_{\\beta}(\\pi_{\\theta}) &amp;\\approx&amp; \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\end{eqnarray} $<br>$=E_{s \\sim \\rho^{\\beta}, a \\sim \\beta}[\\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)}\\nabla_{\\theta}log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]$</li>\n<li>off-policy policy gradient 식에서의 물결 표시는 [Degris, 2012b] 논문에 근거함<ul>\n<li>[Degris, 2012b] “Linear off-policy actor-critic,” ICML 2012</li>\n<li>Exact off-policy policy gradient 와 이를 approximate 한 policy gradient 는 아래와 같음. (빨간색 상자에 있는 항목을 삭제함으로써 근사함)<ul>\n<li><img src=\"https://www.dropbox.com/s/xzpv3okc139c1fs/Screenshot%202018-06-16%2017.48.51.png?dl=1\" width=\"500px\"></li>\n</ul>\n</li>\n<li>[Degris, 2012b] Theorem 1 에 의해 policy parameter 가 approximated policy gradient ( $\\nabla_{u}𝑄^{\\pi,\\gamma}(𝑠,𝑎)$ term 제거)에 따라 업데이트되어도 policy 는 improve 가 됨이 보장되기에 exact off-policy policy gradient 대신 approximated off-policy policy gradient 를 사용해도 괜찮음.<ul>\n<li><img src=\"https://www.dropbox.com/s/mk13931r4scjngo/Screenshot%202018-06-16%2017.49.24.png?dl=1\" width=\"500px\"></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>off-policy policy gradient 식에서 $ \\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)} $ 는 importance sampling ratio 임<ul>\n<li>off-policy actor-critic 에서는 $ \\beta $ 에 의해 샘플링된 trajectory 를 이용해서 stochastic policy $ \\pi $ 를 예측하는 것이기 때문에 imnportance sampling 이 필요함<br><br></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2 id=\"3-Gradient-of-Deterministic-Policies\"><a href=\"#3-Gradient-of-Deterministic-Policies\" class=\"headerlink\" title=\"3. Gradient of Deterministic Policies\"></a>3. Gradient of Deterministic Policies</h2><h3 id=\"3-1-Regulariy-Conditions\"><a href=\"#3-1-Regulariy-Conditions\" class=\"headerlink\" title=\"3.1 Regulariy Conditions\"></a>3.1 Regulariy Conditions</h3><ul>\n<li>어떠한 이론이 성립하기 위한 전제 조건</li>\n<li>Regularity conditions A.1<ul>\n<li>$ p(s’|s,a), \\nabla_{a}p(s’|s,a), \\mu_{\\theta}(s), \\nabla_{\\theta}\\mu_{\\theta}(s), r(s,a), \\nabla_{a}r(s,a), p_{1}(s) $ are continuous in all parameters and variables $ s, a, s’ $ and $ x $.</li>\n</ul>\n</li>\n<li>regularity conditions A.2<ul>\n<li>There exists a $ b $ and $ L $ such that $ \\sup_{s}p_{1}(s) &lt; b $, $ \\sup_{a,s,s’}p(s’|s,a) &lt; b $, $ \\sup_{a,s}r(s,a) &lt; b $, $ \\sup_{a,s,s’}|\\nabla_{a}p(s’|s,a)| &lt; L $, and $ \\sup_{a,s}|\\nabla_{a}r(s,a)| &lt; L $.</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"3-2-Deterministic-Policy-Gradient-Theorem\"><a href=\"#3-2-Deterministic-Policy-Gradient-Theorem\" class=\"headerlink\" title=\"3.2 Deterministic Policy Gradient Theorem\"></a>3.2 Deterministic Policy Gradient Theorem</h3><ul>\n<li>Deterministic policy<ul>\n<li>$ \\mu_{\\theta} : S \\to A $ with parameter vector $ \\theta \\in \\rm I ! R^n $</li>\n</ul>\n</li>\n<li>Probability distribution<ul>\n<li>$ p(s \\to s’, t, \\mu) $</li>\n</ul>\n</li>\n<li>Discounted state distribution<ul>\n<li>$ \\rho^{\\mu}(s) $</li>\n</ul>\n</li>\n<li>Performance objective</li>\n</ul>\n<p>$$<br>J(\\mu_{\\theta}) = E[r^{\\gamma}_{1} | \\mu]<br>$$</p>\n<p>$$<br>= \\int_{S}\\rho^{\\mu}(s)r(s,\\mu_{\\theta}(s))ds<br>= E_{s \\sim \\rho^{\\mu}}[r(s,\\mu_{\\theta}(s))]<br>$$</p>\n<ul>\n<li><p>DPG Theorem</p>\n<ul>\n<li><p>MDP 가 A.1 만족한다면, 아래 식(9)이 성립함<br>$\\nabla_{\\theta}J(\\mu_{\\theta}) = \\int_{S}\\rho^{\\mu}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}ds \\nonumber$<br>$= E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]   \\nonumber $ (9)</p>\n</li>\n<li><p>DPG는 State space 에 대해서만 평균을 취하면 되기에, State와 Action space 모두에 대해 평균을 취해야 하는 <a href=\"#spg-theorem\">SPG</a>에 비해 data efficiency가 좋다. 즉, 더 적은 양의 데이터로도 학습이 잘 이뤄지게 된다.</p>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"3-3-DPG-형태에-대한-informal-intuition\"><a href=\"#3-3-DPG-형태에-대한-informal-intuition\" class=\"headerlink\" title=\"3.3 DPG 형태에 대한 informal intuition\"></a>3.3 DPG 형태에 대한 informal intuition</h3><ul>\n<li>Generalized policy iteration<ul>\n<li>정책 평가와 정책 발전을 한 번 씩 번갈아 가면서 실행하는 정책 iteration<ul>\n<li>위와 같이 해도 정책 평가에서 예측한 가치함수가 최적 가치함수에 수렴함</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>정책 평가<ul>\n<li>action-value function $ Q^{\\pi}(s,a) $ or $ Q^{\\mu}(s,a) $ 을 estimate 하는 것</li>\n</ul>\n</li>\n<li>정책 발전<ul>\n<li>위 estimated action-value function 에 따라 정책을 update 하는 것</li>\n<li>주로 action-value function 에 대한 greedy maximisation 사용함<ul>\n<li>$ \\mu^{k+1}(s) = \\arg\\max\\limits_{a}Q^{\\mu^{k}}(s,a) $</li>\n<li>greedy 정책 발전은 매 단계마다 global maximization을 해야하는데, 이는 continuous action spaces 에서 계산량이 급격히 늘어나게 됨.</li>\n</ul>\n</li>\n<li>그렇기에 policy gradient 방법이 나옴<ul>\n<li>policy 를 $ \\theta $ 에 대해서 parameterize 함</li>\n<li>매 단계마다 global maximisation 수행하는 대신, 방문하는 state $ s $ 마다 policy parameter 를 action-value function $ Q $ 의 $ \\theta $ 에 대한 gradient $ \\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s)) $ 방향으로 proportional 하게 update 함</li>\n<li>하지만 각 state 는 다른 방향을 제시할 수 있기에, state distribution $ \\rho^{\\mu}(s) $ 에 대한 기대값을 취해 policy parameter 를 update 할 수도 있음<ul>\n<li>$ \\theta^{k+1} = \\theta^{k} + \\alpha \\rm I!E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s))] $</li>\n</ul>\n</li>\n<li>이는 chain-rule 에 따라 아래와 같이 분리될 수 있음<ul>\n<li>$ \\theta^{k+1} = \\theta^{k} + \\alpha \\rm I!E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu^{k}}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $ (7)</li>\n<li>chain rule: $ \\frac{\\partial Q}{\\partial \\theta} = \\frac{\\partial a}{\\partial \\theta} \\frac{\\partial Q}{\\partial a} $</li>\n</ul>\n</li>\n<li>하지만 state distribution $ \\rho^{\\mu} $ 은 정책에 dependent 함<ul>\n<li>정책이 바꾸게 되면, 바뀐 정책에 따라 방문하게 되는 state 가 변하게 되기 때문에 state distribution이 변하게 됨</li>\n</ul>\n</li>\n<li>그렇기에 정책 update 시 state distribution에 대한 gradient 를 고려하지 않는데 정책 발전이 이뤄진다는 것은 직관적으로 와닿지 않을 수 있음</li>\n<li>deterministic policy gradient theorem 은 state distribution에 대한 gradient 계산없이 위 식(7) 대로만 update 해도 performance objective 의 gradient 를 정확하게 따름을 의미한다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"3-4-DPG-는-SPG-의-limiting-case-임\"><a href=\"#3-4-DPG-는-SPG-의-limiting-case-임\" class=\"headerlink\" title=\"3.4 DPG 는 SPG 의 limiting case 임\"></a>3.4 DPG 는 SPG 의 limiting case 임</h3><ul>\n<li>stochastic policy parameterization<ul>\n<li>$ \\pi_{\\mu_{\\theta},\\sigma} $ by a deterministic policy $ \\mu_{\\theta} : S \\to A $ and a variance parameter $ \\sigma $</li>\n<li>$ \\sigma = 0 $ 이면, $ \\pi_{\\mu_{\\theta},\\sigma} \\equiv \\mu_{\\theta} $</li>\n</ul>\n</li>\n<li>Theorem 2. Policy 의 variance 가 0 에 수렴하면, 즉, $ \\sigma \\to 0 $, stochastic policy gradient 와 deterministic policy gradient 는 동일해짐<ul>\n<li>조건 : stochastic policy $ \\pi_{\\mu_{\\theta},\\sigma} = \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $<ul>\n<li>$ \\sigma $ 는 variance</li>\n<li>$ \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $ 는 conditions B.1 만족</li>\n<li>MDP 는 conditions A.1 및 A.2 만족</li>\n</ul>\n</li>\n<li>결과 :<ul>\n<li>$ \\lim\\limits_{\\sigma\\downarrow0}\\nabla_{\\theta}J(\\pi_{\\mu_{\\theta},\\sigma}) = \\nabla_{\\theta}J(\\mu_{\\theta})  $<ul>\n<li>좌변은 standard stochastic gradient 이며, 우변은 deterministic gradient.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>의미 :<ul>\n<li>deterministic policy gradient 는 stochastic policy gradient 의 특수 case 임</li>\n<li>기존 유명한 policy gradients 기법들에 deterministic policy gradients 를 적용할 수 있음<ul>\n<li>기존 기법들 예: compatible function approximation (Sutton, 1999), natural gradients (Kakade, 2001), actor-critic (Bhatnagar, 2007) or episodic/batch methods (Peters, 2005)<br><br></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2 id=\"4-Deterministic-Actor-Critic-Algorithms\"><a href=\"#4-Deterministic-Actor-Critic-Algorithms\" class=\"headerlink\" title=\"4. Deterministic Actor-Critic Algorithms\"></a>4. Deterministic Actor-Critic Algorithms</h2><ol>\n<li>살사 critic 을 이용한 on-policy actor-critic<ul>\n<li>단점<ul>\n<li>deterministic policy 에 의해 행동하면 exploration 이 잘 되지 않기에, sub-optimal 에 빠지기 쉬움</li>\n</ul>\n</li>\n<li>목적<ul>\n<li>교훈/정보제공</li>\n<li>환경에서 충분한 noise 를 제공하여 exploration을 시킬 수 있다면, deterministic policy 를 사용한다고 하여도 좋은 학습 결과를 얻을 수도 있음$<ul>\n<li>예. 바람이 agent의 행동에 영향(noise)을 줌</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Remind: 살사(SARSA) update rule<ul>\n<li>$ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma Q(s_{t+1},a_{t+1}) - Q(s_{t},a_{t})) $</li>\n</ul>\n</li>\n<li>Algorithm<ul>\n<li>Critic 은 MSE 를 $ \\bf minimize $ 하는 방향, 즉, action-value function 을 stochastic gradient $ \\bf descent $ 방법으로 update 함<ul>\n<li>$ MSE = [Q^{\\mu}(s,a) - Q^{w}(s,a)]^2 $<ul>\n<li>critic 은 실제 $ Q^{\\mu}(s,a) $ 대신 미분 가능한 $ Q^{w}(s,a) $ 로 대체하여 action-value function 을 estimate 하며, 이 둘 간 Mean Square Error 를 최소화하는 것이 목표</li>\n</ul>\n</li>\n<li>$ \\nabla_{w}MSE \\approx -2 * [r + \\gamma Q^{w}(s’,a’) - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $<ul>\n<li>$ \\nabla_{w}MSE = -2 * [Q^{\\mu}(s,a) - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $</li>\n<li>$ Q^{\\mu}(s,a) $ 를 $ r + \\gamma Q^{w}(s’,a’) $ 로 대체<ul>\n<li>$ Q^{\\mu}(s,a) = r + \\gamma Q^{\\mu}(s’,a’) $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $<ul>\n<li>$w_{t+1} = w_{t} - \\alpha_{w}\\nabla_{w}MSE  \\nonumber$<br>$ \\approx w_{t} - \\alpha_{w} <em> (-2 </em> [r + \\gamma Q^{w}(s’,a’) - Q^{w}(s,a)] \\nabla_{w}Q^{w}(s,a)$</li>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},a_{t+1}) - Q^{w}(s_{t},a_{t}) $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Actor 는 식(9)에 따라 보상이 $ \\bf maximize $ 되는 방향, 즉, deterministic policy 를 stochastic gradient $ \\bf ascent $ 방법으로 update함<ul>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Q-learning 을 이용한 off-policy actor-critic<ul>\n<li>stochastic behavior policy $ \\beta(a|s) $ 에 의해 생성된 trajectories 로부터 deterministic target policy $ \\mu_{\\theta}(s) $ 를 학습하는 off-policy actor-critic</li>\n<li>performance objective<ul>\n<li>$ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)V^{\\mu}(s)ds \\nonumber \\\\$<br>$= \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds \\nonumber \\\\$<br>$= E_{s \\sim \\rho^{\\beta}}[Q^{\\mu}(s,\\mu_{\\theta}(s))]$</li>\n</ul>\n</li>\n<li>off-policy deterministic policy gradient<ul>\n<li>$ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $<ul>\n<li>논문에는 아래와 같이 나와있는데, 물결 표시 부분은 오류로 판단됨.</li>\n<li>$ \\begin{eqnarray}<br>  \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) &amp;\\approx&amp; \\int_{S}\\rho^{\\beta}(s)\\nabla_{\\theta}\\mu_{\\theta}(a|s)Q^{\\mu}(s,a)ds \\nonumber \\<br>  &amp;=&amp; E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]<br>  \\end{eqnarray} $</li>\n<li>근거: Action이 deterministic 하기에 stochastic 경우와는 다르게 performance objective 에서 action 에 대해 평균을 구할 필요가 없음. 그렇기에, 곱의 미분이 있을 필요가 없고, [Degris, 2012b] 에서처럼 곱의 미분을 통해 생기는 action-value function 에 대한 gradient term 를 생략할 필요가 사라짐. <a href=\"#off-policy-actor-critic\">참고</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Remind: 큐러닝(Q-learning) update rule<ul>\n<li>$ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma \\max\\limits_{a}Q(s_{t+1},a) - Q(s_{t},a_{t})) $</li>\n</ul>\n</li>\n<li>algorithm: OPDAC (Off-Policy Deterministic Actor-Critic)<ul>\n<li>살사를 이용한 on-policy deterministic actor-critic 과 아래 부분을 제외하고는 같음<ul>\n<li>target policy 는 $ \\beta(a|s) $ 에 의해 생성된 trajectories 를 통해 학습함</li>\n<li>업데이트 목표 부분에 실제 행동 값 $ a_{t+1} $ 이 아니라 정책으로부터 나온 행동 값 $ \\mu_{\\theta}(s_{t+1}) $ 사용함<ul>\n<li>$ \\mu_{\\theta}(s_{t+1}) $ 는 가장 높은 Q 값을 가지는 행동. 즉, Q-learning.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $</li>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $</li>\n</ul>\n</li>\n<li>Stochastic off-policy actor-critic 은 대개 actor 와 critic 모두 importance sampling을 필요로 하지만, deterministic policy gradient 에선 importance sampling이 필요없음<ul>\n<li>Actor 는 deterministic 이기에 sampling 자체가 필요없음<ul>\n<li>Stochastic policy 인 경우, Actor 에서 importance sampling이 필요한 이유는 상태 $ s $ 에서의 가치 함수 값 $ V^{\\pi}(s) $ 을 estimate 하기 위해 $ \\pi $ 가 아니라 $ \\beta $ 에 따라 sampling을 한 후, 평균을 내기 때문임.</li>\n<li>하지만 Deterministic policy 인 경우, 상태 $ s $ 에서의 가치 함수 값 $ V^{\\pi}(s) = Q^{\\pi}(s,\\mu_{\\theta}) $ 즉, action 이 상태 s 에 대해 deterministic 이기에 sampling 을 통해 estimate 할 필요가 없고, 따라서 importance sampling도 필요없어짐.</li>\n<li>stochastic vs. deterministic performance objective<ul>\n<li>stochastic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads $</li>\n<li>deterministic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Critic 이 사용하는 Q-learning 은 importance sampling이 필요없는 off policy 알고리즘임.<ul>\n<li>Q-learning 도 업데이트 목표를 특정 분포에서 샘플링을 통해 estimate 하는 것이 아니라 Q 함수를 최대화하는 action을 선택하는 것이기에 위 actor 에서의 deterministic 경우와 비슷하게 볼 수 있음</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>compatible function approximation 및 gradient temporal-difference learning 을 이용한 actor-critic<ul>\n<li>위 살사/Q-learning 기반 on/off-policy 는 아래와 같은 문제가 존재<ul>\n<li>function approximator 에 의한 bias<ul>\n<li>일반적으로 $ Q^{\\mu}(s,a) $ 를 $ Q^{w}(s,a) $ 로 대체하여 deterministic policy gradient 를 구하면, 그 gradient 는 ascent 하는 방향이 아닐 수도 있음</li>\n</ul>\n</li>\n<li>off-policy learning 에 의한 instabilities</li>\n</ul>\n</li>\n<li>그래서 stochastic 처럼 $ \\nabla_{a}Q^{\\mu}(s,a) $ 를 $ \\nabla_{a}Q^{w}(s,a) $ 로 대체해도 deterministic policy gradient 에 영향을 미치지 않을 compatible function approximator $ Q^{w}(s,a) $ 를 찾아야 함</li>\n<li>Theorem 3. 아래 두 조건을 만족하면, $ Q^{w}(s,a) $ 는 deterministic policy $ \\mu_{\\theta}(s) $ 와 compatible 함. 즉, $ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $<ul>\n<li>$ \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $</li>\n<li>$ w $ 는 $ MSE(\\theta, w) = E[\\epsilon(s;\\theta,w)^{\\top}\\epsilon(s;\\theta,w)] $ 를 최소화함<ul>\n<li>$ \\epsilon(s;\\theta,w) = \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} - \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}  $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Theorem 3 은 on-policy 뿐만 아니라 off-policy 에도 적용 가능함</li>\n<li>$ Q^{w}(s,a) = (a-\\mu_{\\theta}(s))^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top} w + V^{v}(s) $<ul>\n<li>어떠한 deterministic policy 에 대해서도 위 형태와 같은 compatible function approximator 가 존재함.</li>\n<li>앞의 term 은 advantage 를, 뒤의 term 은 value 로 볼 수 있음</li>\n</ul>\n</li>\n<li>$ Q^{w}(s,a) = \\phi(s,a)^{\\top} w + v^{\\top}\\phi(s) $<ul>\n<li>정의 : $ \\phi(s,a) \\overset{\\underset{\\mathrm{def}}{}}{=} \\nabla_{\\theta}\\mu_{\\theta}(s)(a-\\mu_{\\theta}(s)) $</li>\n<li>일례 : $ V^{v}(s) = v^{\\top}\\phi(s) $</li>\n<li>Theorem 3 만족 여부<ul>\n<li>첫 번째 조건 만족.</li>\n<li>두 번째 조건은 대강 만족.<ul>\n<li>$ \\nabla_{a}Q^{\\mu}(s,a) $ 에 대한 unbiased sample 을 획득하기는 매우 어렵기에, 일반적인 정책 평가 방법들로 $ w $ 학습.</li>\n<li>이 정책 평가 방법들을 이용하면 $ Q^{w}(s,a) \\approx Q^{\\mu}(s,a) $ 인 reasonable solution을 찾을 수 있기에 대강 $ \\nabla_{a}Q^{w}(s,a) \\approx \\nabla_{a}Q^{\\mu}(s,a) $ 이 될 것.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>action-value function 에 대한 linear function approximator 는 큰 값을 가지는 actions 에 대해선 diverge할 수 있어 global 하게 action-values 예측하기에는 좋지 않지만, local critic 에 사용할 때는 매우 유용하다.<ul>\n<li>즉, 절대값이 아니라 작은 변화량을 다루는 gradient method 경우엔 $ A^{w}(s,\\mu_{\\theta}(s)+\\delta) = \\delta^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $ 로, diverge 하지 않고, 값을 얻을 수 있음</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>COPDAC-Q algorithm (Compatible Off-Policy Deterministic Actor-Critic Q-learning critic)<ul>\n<li>Critic: 실제 action-value function 에 대한 linear function approximator 인 $ Q^{w}(s,a) = \\phi(s,a)^{\\top}w $ 를 estimate<ul>\n<li>$ \\phi(s,a) = a^{\\top}\\nabla_{\\theta}\\mu_{\\theta} $</li>\n<li>Behavior policy $ \\beta(a|s) $ 로부터 얻은 samples를 이용하여 Q-learning 이나 gradient Q-learning 과 같은 off-policy algorithm 으로 학습 가능함</li>\n</ul>\n</li>\n<li>Actor: estimated action-value function 에 대한 gradient 를 $ \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $ 로 치환 후, 정책을 업데이트함</li>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) $</li>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $</li>\n</ul>\n</li>\n<li>off-policy Q-learning은 linear function approximation을 이용하면 diverge 할 수도 있음<ul>\n<li>$ \\mu_{\\theta}(s_{t+1}) $ 이 diverge 할 수도 있기 때문으로 판단됨.</li>\n<li>그렇기에 simple Q-learning 대신 다른 기법이 필요함.</li>\n</ul>\n</li>\n<li>그렇기에 critic 에 gradient Q-learning 사용한 COPDAC-GQ (Gradient Q-learning critic) algorithm 제안<ul>\n<li>gradient temporal-difference learning 에 기반한 기법들은 true gradient descent algorithm 이며, converge가 보장됨. (Sutton, 2009)<ul>\n<li>기본 아이디어는 stochastic gradient descent 로 Mean-squared projected Bellman error (MSPBE) 를 최소화하는 것</li>\n<li>critic 이 actor 보다 빠른 time-scale 로 update 되도록 step size 들을 잘 조절하면, critic 은 MSPBE 를 최소화하는 parameters 로 converge 하게 됨</li>\n<li>critic 에 gradient temporal-difference learning 의 일종인 gradient Q-learning 사용한 논문 (Maei, 2010)</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>COPDAC-GQ algorithm<ul>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $</li>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) - \\alpha_{w}\\gamma\\phi(s_{t+1}, \\mu_{\\theta}(s_{t+1}))(\\phi(s_{t},a_{t})^{\\top} u_{t}) $</li>\n<li>$ v_{t+1} = v_{t} + \\alpha_{v}\\delta_{t}\\phi(s_{t}) - \\alpha_{v}\\gamma\\phi(s_{t+1})(\\phi(s_{t},a_{t})^{\\top} u_{t}) $</li>\n<li>$ u_{t+1} = u_{t} + \\alpha_{u}(\\delta_{t}-\\phi(s_{t}, a_{t})^{\\top} u_{t})\\phi(s_{t}, a_{t}) $</li>\n</ul>\n</li>\n<li>stochastic actor-critic 과 같이 매 time-step 마다 update 시 필요한 계산의 복잡도는 $ O(mn) $<ul>\n<li>m 은 action dimensions, n 은 number of policy parameters</li>\n</ul>\n</li>\n<li>Natural policy gradient 를 이용해 deterministic policies 를 찾을 수 있음<ul>\n<li>$ M(\\theta)^{-1}\\nabla_{\\theta}J(\\mu_{\\theta}) $ 는 any metric $ M(\\theta) $ 에 대한 our performance objective (식(14)) 의 steepest ascent direction 임 (Toussaint, 2012)</li>\n<li>Natural gradient 는 Fisher information metric $ M_{\\pi}(\\theta) $ 에 대한 steepest ascent direction 임<ul>\n<li>$ M_{\\pi}(\\theta) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)^{\\top}] $</li>\n<li>Fisher information metric 은 policy reparameterization 에 대해 불변임 (Bagnell, 2003)</li>\n</ul>\n</li>\n<li>deterministic policies 에 대해 metric 으로 $ M_{\\mu}(\\theta) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}] $ 을 사용.<ul>\n<li>이는 variance 가 0 인 policy 에 대한 Fisher information metric 으로 볼 수 있음</li>\n<li>$ \\frac{\\nabla_{\\theta}\\pi_{\\theta}(a\\vert s)}{\\pi_{\\theta}(a\\vert s)}$ 에서 policy variance 가 0 이면, 특정 s 에 대한 $ \\pi_{\\theta}(a|s)$ 만 1 이 되고, 나머지는 0 임</li>\n</ul>\n</li>\n<li>deterministic policy gradient theorem 과 compatible function approximation 을 결합하면 $ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] $ 이 됨<ul>\n<li>$ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $</li>\n<li>$ \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)} \\approx \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $</li>\n</ul>\n</li>\n<li>그렇기에 steepest ascent direction 은 $ M_{\\mu}(\\theta)^{-1}\\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = w $ 이 됨<ul>\n<li>$ E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}]^{-1}E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] = w $</li>\n</ul>\n</li>\n<li>이 알고리즘은 COPDAC-Q 혹은 COPDAC-GQ 에서 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $ 식을 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta}w_{t} $ 로 바꿔주기만 하면 됨</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"Experiments\"><a href=\"#Experiments\" class=\"headerlink\" title=\"Experiments\"></a>Experiments</h2><h3 id=\"Continuous-Bandit\"><a href=\"#Continuous-Bandit\" class=\"headerlink\" title=\"Continuous Bandit\"></a>Continuous Bandit</h3><ul>\n<li>Stochastic Actor-Critic (SAC)과 COPDAC 간 성능 비교 수행<ul>\n<li>Action dimension이 커질수록 성능 차이가 심함</li>\n<li>빠르게 수렴하는 것을 통해 DPG의 data efficiency가 SPG에 비해 좋다는 것을 확인할 수 있지만, 반면, time-step이 증가할수록 SAC와 COPDAC 간 성능 차이가 줄어드는 것을 통해 성능 차이가 심하다는 것은 일정 time step 내에서만 해당하는 것이라고 유추해볼 수 있음</li>\n<li><img src=\"https://www.dropbox.com/s/hrkyq0s2f24z66r/Screenshot%202018-06-16%2017.47.38.png?dl=1\"></li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Continuous-Reinforcement-Learning\"><a href=\"#Continuous-Reinforcement-Learning\" class=\"headerlink\" title=\"Continuous Reinforcement Learning\"></a>Continuous Reinforcement Learning</h3><ul>\n<li>COPDAC-Q가 SAC와 off-policy stochastic actor-critic(OffPAC-TD) 간 성능 비교 수행<ul>\n<li>COPDAC-Q의 성능이 약간 더 좋음</li>\n<li>COPDAC-Q의 학습이 더 빨리 이뤄짐</li>\n<li><img src=\"https://www.dropbox.com/s/qdca4augapmzsxi/Screenshot%202018-06-16%2017.47.07.png?dl=1\"><h3 id=\"Octopus-Arm\"><a href=\"#Octopus-Arm\" class=\"headerlink\" title=\"Octopus Arm\"></a>Octopus Arm</h3></li>\n</ul>\n</li>\n<li>목표: 6 segments octopus arm (20 action dimensions &amp; 50 state dimensions)을 control하여 target을 맞추는 것<ul>\n<li>COPDAC-Q 사용 시, action space dimension이 큰 octopus arm을 잘 control하여 target을 맞춤</li>\n<li><img src=\"https://www.dropbox.com/s/xrxb0a52wntekld/Screenshot%202018-06-16%2017.46.28.png?dl=1\" width=\"600px\"></li>\n<li>기존 기법들은 action spaces 혹은 action 과 state spaces 둘 다 작은 경우들에 대해서만 실험했다고 하며, 비교하고 있지 않음.<ul>\n<li>기존 기법들이 6 segments octopus arm에서 동작을 잘 안 했을 것 같긴한데, 그래도 실험해서 결과를 보여주지…왜 안 했을까?</li>\n</ul>\n</li>\n<li>8 segment arm 동영상이 저자 홈페이지에 있다고 하는데, 안 보임.</li>\n</ul>\n</li>\n<li>[참고] Octopus Arm 이란?<ul>\n<li><a href=\"https://www.youtube.com/watch?v=AxeeHif0euY\" target=\"_blank\" rel=\"noopener\">OctopusArm Youtube Link</a></li>\n<li><img src=\"https://www.dropbox.com/s/950ycj06sudakjx/Screenshot%202018-06-16%2017.45.52.png?dl=1\"></li>\n</ul>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>Authors: David Silver<sup>1)</sup>, Guy Lever<sup>2)</sup>, Nicloas Heess<sup>1)</sup>, Thomas Degris<sup>1)</sup>, Daan Wierstra<sup>1)</sup>, Martin Riedmiller<sup>1)</sup><br>Affiliation: 1) Google Deepmind, 2) University College London (UCL)<br>Proceeding: International Conference on Machine Learning (ICML) 2014</p>\n<ul>\n<li><a href=\"#deterministic-policy-gradient-algorithms\">Deterministic Policy Gradient Algorithms</a><ul>\n<li><a href=\"#summary\">Summary</a></li>\n<li><a href=\"#background\">Background</a><ul>\n<li><a href=\"#performance-objective-function\">Performance objective function</a></li>\n<li><a href=\"#spg-theorem\">SPG Theorem</a></li>\n<li><a href=\"#stochastic-actor-critic-algorithms\">Stochastic Actor-Critic Algorithms</a></li>\n<li><a href=\"#off-policy-actor-critic\">Off-policy Actor-Critic</a></li>\n</ul>\n</li>\n<li><a href=\"#gradient-of-deterministic-policies\">Gradient of Deterministic Policies</a><ul>\n<li><a href=\"#regulariy-conditions\">Regulariy Conditions</a></li>\n<li><a href=\"#deterministic-policy-gradient-theorem\">Deterministic Policy Gradient Theorem</a></li>\n<li><a href=\"#dpg-형태에-대한-informal-intuition\">DPG 형태에 대한 informal intuition</a></li>\n<li><a href=\"#dpg-는-spg-의-limiting-case-임\">DPG 는 SPG 의 limiting case 임</a></li>\n</ul>\n</li>\n<li><a href=\"#deterministic-actor-critic-algorithms\">Deterministic Actor-Critic Algorithms</a></li>\n<li><a href=\"#experiments\">Experiments</a><ul>\n<li><a href=\"#continuous-bandit\">Continuous Bandit</a></li>\n<li><a href=\"#continuous-reinforcement-learning\">Continuous Reinforcement Learning</a></li>\n<li><a href=\"#octopus-arm\">Octopus Arm</a></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<!-- /code_chunk_output -->\n<p><br></p>\n<hr>\n<h2 id=\"1-Summary\"><a href=\"#1-Summary\" class=\"headerlink\" title=\"1. Summary\"></a>1. Summary</h2><ul>\n<li>Deterministic Policy Gradient (DPG) Theorem 제안함 [<a href=\"#deterministic-policy-gradient-theorem\">Theorem 1</a>]<br>  1) DPG는 존재하며,<br>  2) DPG는 Expected gradient of the action-value function의 형태를 띈다.</li>\n<li>Policy variance가 0에 수렴할 경우, DPG는 Stochastic Policy Gradient(SPG)와 동일해짐을 보임 [<a href=\"#dpg-는-spg-의-limiting-case-임\">Theorem 2</a>]<ul>\n<li>Theorem 2 로 인해 기존 Policy Gradient (PG) 와 관련된 기법들을 DPG 에 적용할 수 있게 됨<ul>\n<li>예. Sutton PG, natural gradients, actor-critic, episodic/batch methods</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>적절한 exploration 을 위해 model-free, off-policy Actor-Critic algorithm 을 제안함<ul>\n<li>Action-value function approximator 사용으로 인해 policy gradient가 bias되는 것을 방지하기 위해 compatibility conditions을 제공 [<a href=\"##deterministic-actor-critic-algorithms\">Theorem 3</a>]</li>\n</ul>\n</li>\n<li>DPG 는 SPG 보다 성능이 좋음<ul>\n<li>특히 high dimensional action spaces 을 가지는 tasks에서의 성능 향상이 큼<ul>\n<li>SPG의 policy gradient는 state와 action spaces 모두에 대해서, DPG의 policy gradient는  state spaces에 대해서만 평균을 취함</li>\n<li>결과적으로, action spaces의 dimension이 커질수록 data efficiency가 높은 DPG의 학습이 더 잘 이뤄지게 됨</li>\n<li>무한정 학습을 시키게 되면, SPG도 최적으로 수렴할 것으로 예상되기에 위 성능 비교는 일정 iteration 내로 한정함</li>\n</ul>\n</li>\n<li>기존 기법들에 비해 computation 양이 많지 않음<ul>\n<li>Computation 은 action dimensionality 와 policy parameters 수에 비례함<br><br></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2 id=\"2-Background\"><a href=\"#2-Background\" class=\"headerlink\" title=\"2. Background\"></a>2. Background</h2><h3 id=\"2-1-Performance-objective-function\"><a href=\"#2-1-Performance-objective-function\" class=\"headerlink\" title=\"2.1 Performance objective function\"></a>2.1 Performance objective function</h3><p>$$<br>\\begin{align}<br>J(\\pi_{\\theta}) &amp;= \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\pi_{\\theta}(s,a)r(s,a)dads = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[r(s,a)]<br>\\end{align}<br>$$</p>\n<h3 id=\"2-2-SPG-Theorem\"><a href=\"#2-2-SPG-Theorem\" class=\"headerlink\" title=\"2.2 SPG Theorem\"></a>2.2 SPG Theorem</h3><ul>\n<li>State distribution $ \\rho^{\\pi}(s) $ 은 policy parameters에 영향을 받지만, policy gradient 를 계산할 때는 state distribution 의 gradient 를 고려할 필요가 없다.</li>\n<li>$$\\begin{eqnarray}\\nabla_{\\theta}J(\\pi_{\\theta}) &amp;=&amp; \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\ &amp;=&amp; E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]<br>\\end{eqnarray}$$</li>\n</ul>\n<h3 id=\"2-3-Stochastic-Actor-Critic-Algorithms\"><a href=\"#2-3-Stochastic-Actor-Critic-Algorithms\" class=\"headerlink\" title=\"2.3 Stochastic Actor-Critic Algorithms\"></a>2.3 Stochastic Actor-Critic Algorithms</h3><ul>\n<li>Actor 와 Critic 이 번갈아가면서 동작하며 stochastic policy 를 최적화하는 기법</li>\n<li>Actor: $ Q^{\\pi}(s,a) $ 를 근사한 $ Q^w(s,a) $ 를 이용해 stochastic policy gradient 를 ascent 하는 방향으로 policy parameter $ \\theta $ 를 업데이트함으로써 stochastic policy 를 발전시킴<ul>\n<li>$ \\nabla_{\\theta}J(\\pi_{\\theta}) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}log\\pi_{\\theta}(a|s)Q^{w}(s,a)] $</li>\n</ul>\n</li>\n<li>Critic: SARSA 나 Q-learning 같은 Temporal-difference (TD) learning 을 이용해 action-value function의 parameter, $ w $ 를 업데이트함으로써 $ Q^w(s,a) $ 가 $ Q^{\\pi}(s,a) $ 과 유사해지도록 함</li>\n<li>실제 값인 $ Q^{\\pi}(s,a) $ 대신 이를 근사한 $ Q^w(s,a) $ 를 사용하게 되면, 일반적으로 bias 가 발생하게 된다. 하지만, compatible condition 에 부합하는 $ Q^w(s,a) $ 를 사용하게 되면, bias 가 발생하지 않는다.</li>\n</ul>\n<h3 id=\"2-4-Off-policy-Actor-Critic\"><a href=\"#2-4-Off-policy-Actor-Critic\" class=\"headerlink\" title=\"2.4 Off-policy Actor-Critic\"></a>2.4 Off-policy Actor-Critic</h3><ul>\n<li>Distinct behavior policy $ \\beta(a|s) ( \\neq \\pi_{\\theta}(a|s) ) $ 로부터 샘플링된 trajectories 를 이용한 Actor-Critic</li>\n<li>Performance objective function<ul>\n<li>$\\begin{eqnarray}<br>  J_{\\beta}(\\pi_{\\theta})<br>  &amp;=&amp; \\int_{S}\\rho^{\\beta}(s)V^{\\pi}(s)ds \\nonumber \\\\<br>  &amp;=&amp; \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads<br>  \\end{eqnarray} $</li>\n</ul>\n</li>\n<li>off-policy policy gradient<ul>\n<li>$ \\begin{eqnarray}<br>  \\nabla_{\\theta}J_{\\beta}(\\pi_{\\theta}) &amp;\\approx&amp; \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\end{eqnarray} $<br>$=E_{s \\sim \\rho^{\\beta}, a \\sim \\beta}[\\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)}\\nabla_{\\theta}log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]$</li>\n<li>off-policy policy gradient 식에서의 물결 표시는 [Degris, 2012b] 논문에 근거함<ul>\n<li>[Degris, 2012b] “Linear off-policy actor-critic,” ICML 2012</li>\n<li>Exact off-policy policy gradient 와 이를 approximate 한 policy gradient 는 아래와 같음. (빨간색 상자에 있는 항목을 삭제함으로써 근사함)<ul>\n<li><img src=\"https://www.dropbox.com/s/xzpv3okc139c1fs/Screenshot%202018-06-16%2017.48.51.png?dl=1\" width=\"500px\"></li>\n</ul>\n</li>\n<li>[Degris, 2012b] Theorem 1 에 의해 policy parameter 가 approximated policy gradient ( $\\nabla_{u}𝑄^{\\pi,\\gamma}(𝑠,𝑎)$ term 제거)에 따라 업데이트되어도 policy 는 improve 가 됨이 보장되기에 exact off-policy policy gradient 대신 approximated off-policy policy gradient 를 사용해도 괜찮음.<ul>\n<li><img src=\"https://www.dropbox.com/s/mk13931r4scjngo/Screenshot%202018-06-16%2017.49.24.png?dl=1\" width=\"500px\"></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>off-policy policy gradient 식에서 $ \\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)} $ 는 importance sampling ratio 임<ul>\n<li>off-policy actor-critic 에서는 $ \\beta $ 에 의해 샘플링된 trajectory 를 이용해서 stochastic policy $ \\pi $ 를 예측하는 것이기 때문에 imnportance sampling 이 필요함<br><br></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2 id=\"3-Gradient-of-Deterministic-Policies\"><a href=\"#3-Gradient-of-Deterministic-Policies\" class=\"headerlink\" title=\"3. Gradient of Deterministic Policies\"></a>3. Gradient of Deterministic Policies</h2><h3 id=\"3-1-Regulariy-Conditions\"><a href=\"#3-1-Regulariy-Conditions\" class=\"headerlink\" title=\"3.1 Regulariy Conditions\"></a>3.1 Regulariy Conditions</h3><ul>\n<li>어떠한 이론이 성립하기 위한 전제 조건</li>\n<li>Regularity conditions A.1<ul>\n<li>$ p(s’|s,a), \\nabla_{a}p(s’|s,a), \\mu_{\\theta}(s), \\nabla_{\\theta}\\mu_{\\theta}(s), r(s,a), \\nabla_{a}r(s,a), p_{1}(s) $ are continuous in all parameters and variables $ s, a, s’ $ and $ x $.</li>\n</ul>\n</li>\n<li>regularity conditions A.2<ul>\n<li>There exists a $ b $ and $ L $ such that $ \\sup_{s}p_{1}(s) &lt; b $, $ \\sup_{a,s,s’}p(s’|s,a) &lt; b $, $ \\sup_{a,s}r(s,a) &lt; b $, $ \\sup_{a,s,s’}|\\nabla_{a}p(s’|s,a)| &lt; L $, and $ \\sup_{a,s}|\\nabla_{a}r(s,a)| &lt; L $.</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"3-2-Deterministic-Policy-Gradient-Theorem\"><a href=\"#3-2-Deterministic-Policy-Gradient-Theorem\" class=\"headerlink\" title=\"3.2 Deterministic Policy Gradient Theorem\"></a>3.2 Deterministic Policy Gradient Theorem</h3><ul>\n<li>Deterministic policy<ul>\n<li>$ \\mu_{\\theta} : S \\to A $ with parameter vector $ \\theta \\in \\rm I ! R^n $</li>\n</ul>\n</li>\n<li>Probability distribution<ul>\n<li>$ p(s \\to s’, t, \\mu) $</li>\n</ul>\n</li>\n<li>Discounted state distribution<ul>\n<li>$ \\rho^{\\mu}(s) $</li>\n</ul>\n</li>\n<li>Performance objective</li>\n</ul>\n<p>$$<br>J(\\mu_{\\theta}) = E[r^{\\gamma}_{1} | \\mu]<br>$$</p>\n<p>$$<br>= \\int_{S}\\rho^{\\mu}(s)r(s,\\mu_{\\theta}(s))ds<br>= E_{s \\sim \\rho^{\\mu}}[r(s,\\mu_{\\theta}(s))]<br>$$</p>\n<ul>\n<li><p>DPG Theorem</p>\n<ul>\n<li><p>MDP 가 A.1 만족한다면, 아래 식(9)이 성립함<br>$\\nabla_{\\theta}J(\\mu_{\\theta}) = \\int_{S}\\rho^{\\mu}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}ds \\nonumber$<br>$= E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]   \\nonumber $ (9)</p>\n</li>\n<li><p>DPG는 State space 에 대해서만 평균을 취하면 되기에, State와 Action space 모두에 대해 평균을 취해야 하는 <a href=\"#spg-theorem\">SPG</a>에 비해 data efficiency가 좋다. 즉, 더 적은 양의 데이터로도 학습이 잘 이뤄지게 된다.</p>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"3-3-DPG-형태에-대한-informal-intuition\"><a href=\"#3-3-DPG-형태에-대한-informal-intuition\" class=\"headerlink\" title=\"3.3 DPG 형태에 대한 informal intuition\"></a>3.3 DPG 형태에 대한 informal intuition</h3><ul>\n<li>Generalized policy iteration<ul>\n<li>정책 평가와 정책 발전을 한 번 씩 번갈아 가면서 실행하는 정책 iteration<ul>\n<li>위와 같이 해도 정책 평가에서 예측한 가치함수가 최적 가치함수에 수렴함</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>정책 평가<ul>\n<li>action-value function $ Q^{\\pi}(s,a) $ or $ Q^{\\mu}(s,a) $ 을 estimate 하는 것</li>\n</ul>\n</li>\n<li>정책 발전<ul>\n<li>위 estimated action-value function 에 따라 정책을 update 하는 것</li>\n<li>주로 action-value function 에 대한 greedy maximisation 사용함<ul>\n<li>$ \\mu^{k+1}(s) = \\arg\\max\\limits_{a}Q^{\\mu^{k}}(s,a) $</li>\n<li>greedy 정책 발전은 매 단계마다 global maximization을 해야하는데, 이는 continuous action spaces 에서 계산량이 급격히 늘어나게 됨.</li>\n</ul>\n</li>\n<li>그렇기에 policy gradient 방법이 나옴<ul>\n<li>policy 를 $ \\theta $ 에 대해서 parameterize 함</li>\n<li>매 단계마다 global maximisation 수행하는 대신, 방문하는 state $ s $ 마다 policy parameter 를 action-value function $ Q $ 의 $ \\theta $ 에 대한 gradient $ \\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s)) $ 방향으로 proportional 하게 update 함</li>\n<li>하지만 각 state 는 다른 방향을 제시할 수 있기에, state distribution $ \\rho^{\\mu}(s) $ 에 대한 기대값을 취해 policy parameter 를 update 할 수도 있음<ul>\n<li>$ \\theta^{k+1} = \\theta^{k} + \\alpha \\rm I!E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s))] $</li>\n</ul>\n</li>\n<li>이는 chain-rule 에 따라 아래와 같이 분리될 수 있음<ul>\n<li>$ \\theta^{k+1} = \\theta^{k} + \\alpha \\rm I!E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu^{k}}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $ (7)</li>\n<li>chain rule: $ \\frac{\\partial Q}{\\partial \\theta} = \\frac{\\partial a}{\\partial \\theta} \\frac{\\partial Q}{\\partial a} $</li>\n</ul>\n</li>\n<li>하지만 state distribution $ \\rho^{\\mu} $ 은 정책에 dependent 함<ul>\n<li>정책이 바꾸게 되면, 바뀐 정책에 따라 방문하게 되는 state 가 변하게 되기 때문에 state distribution이 변하게 됨</li>\n</ul>\n</li>\n<li>그렇기에 정책 update 시 state distribution에 대한 gradient 를 고려하지 않는데 정책 발전이 이뤄진다는 것은 직관적으로 와닿지 않을 수 있음</li>\n<li>deterministic policy gradient theorem 은 state distribution에 대한 gradient 계산없이 위 식(7) 대로만 update 해도 performance objective 의 gradient 를 정확하게 따름을 의미한다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"3-4-DPG-는-SPG-의-limiting-case-임\"><a href=\"#3-4-DPG-는-SPG-의-limiting-case-임\" class=\"headerlink\" title=\"3.4 DPG 는 SPG 의 limiting case 임\"></a>3.4 DPG 는 SPG 의 limiting case 임</h3><ul>\n<li>stochastic policy parameterization<ul>\n<li>$ \\pi_{\\mu_{\\theta},\\sigma} $ by a deterministic policy $ \\mu_{\\theta} : S \\to A $ and a variance parameter $ \\sigma $</li>\n<li>$ \\sigma = 0 $ 이면, $ \\pi_{\\mu_{\\theta},\\sigma} \\equiv \\mu_{\\theta} $</li>\n</ul>\n</li>\n<li>Theorem 2. Policy 의 variance 가 0 에 수렴하면, 즉, $ \\sigma \\to 0 $, stochastic policy gradient 와 deterministic policy gradient 는 동일해짐<ul>\n<li>조건 : stochastic policy $ \\pi_{\\mu_{\\theta},\\sigma} = \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $<ul>\n<li>$ \\sigma $ 는 variance</li>\n<li>$ \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $ 는 conditions B.1 만족</li>\n<li>MDP 는 conditions A.1 및 A.2 만족</li>\n</ul>\n</li>\n<li>결과 :<ul>\n<li>$ \\lim\\limits_{\\sigma\\downarrow0}\\nabla_{\\theta}J(\\pi_{\\mu_{\\theta},\\sigma}) = \\nabla_{\\theta}J(\\mu_{\\theta})  $<ul>\n<li>좌변은 standard stochastic gradient 이며, 우변은 deterministic gradient.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>의미 :<ul>\n<li>deterministic policy gradient 는 stochastic policy gradient 의 특수 case 임</li>\n<li>기존 유명한 policy gradients 기법들에 deterministic policy gradients 를 적용할 수 있음<ul>\n<li>기존 기법들 예: compatible function approximation (Sutton, 1999), natural gradients (Kakade, 2001), actor-critic (Bhatnagar, 2007) or episodic/batch methods (Peters, 2005)<br><br></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2 id=\"4-Deterministic-Actor-Critic-Algorithms\"><a href=\"#4-Deterministic-Actor-Critic-Algorithms\" class=\"headerlink\" title=\"4. Deterministic Actor-Critic Algorithms\"></a>4. Deterministic Actor-Critic Algorithms</h2><ol>\n<li>살사 critic 을 이용한 on-policy actor-critic<ul>\n<li>단점<ul>\n<li>deterministic policy 에 의해 행동하면 exploration 이 잘 되지 않기에, sub-optimal 에 빠지기 쉬움</li>\n</ul>\n</li>\n<li>목적<ul>\n<li>교훈/정보제공</li>\n<li>환경에서 충분한 noise 를 제공하여 exploration을 시킬 수 있다면, deterministic policy 를 사용한다고 하여도 좋은 학습 결과를 얻을 수도 있음$<ul>\n<li>예. 바람이 agent의 행동에 영향(noise)을 줌</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Remind: 살사(SARSA) update rule<ul>\n<li>$ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma Q(s_{t+1},a_{t+1}) - Q(s_{t},a_{t})) $</li>\n</ul>\n</li>\n<li>Algorithm<ul>\n<li>Critic 은 MSE 를 $ \\bf minimize $ 하는 방향, 즉, action-value function 을 stochastic gradient $ \\bf descent $ 방법으로 update 함<ul>\n<li>$ MSE = [Q^{\\mu}(s,a) - Q^{w}(s,a)]^2 $<ul>\n<li>critic 은 실제 $ Q^{\\mu}(s,a) $ 대신 미분 가능한 $ Q^{w}(s,a) $ 로 대체하여 action-value function 을 estimate 하며, 이 둘 간 Mean Square Error 를 최소화하는 것이 목표</li>\n</ul>\n</li>\n<li>$ \\nabla_{w}MSE \\approx -2 * [r + \\gamma Q^{w}(s’,a’) - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $<ul>\n<li>$ \\nabla_{w}MSE = -2 * [Q^{\\mu}(s,a) - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $</li>\n<li>$ Q^{\\mu}(s,a) $ 를 $ r + \\gamma Q^{w}(s’,a’) $ 로 대체<ul>\n<li>$ Q^{\\mu}(s,a) = r + \\gamma Q^{\\mu}(s’,a’) $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $<ul>\n<li>$w_{t+1} = w_{t} - \\alpha_{w}\\nabla_{w}MSE  \\nonumber$<br>$ \\approx w_{t} - \\alpha_{w} <em> (-2 </em> [r + \\gamma Q^{w}(s’,a’) - Q^{w}(s,a)] \\nabla_{w}Q^{w}(s,a)$</li>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},a_{t+1}) - Q^{w}(s_{t},a_{t}) $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Actor 는 식(9)에 따라 보상이 $ \\bf maximize $ 되는 방향, 즉, deterministic policy 를 stochastic gradient $ \\bf ascent $ 방법으로 update함<ul>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Q-learning 을 이용한 off-policy actor-critic<ul>\n<li>stochastic behavior policy $ \\beta(a|s) $ 에 의해 생성된 trajectories 로부터 deterministic target policy $ \\mu_{\\theta}(s) $ 를 학습하는 off-policy actor-critic</li>\n<li>performance objective<ul>\n<li>$ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)V^{\\mu}(s)ds \\nonumber \\\\$<br>$= \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds \\nonumber \\\\$<br>$= E_{s \\sim \\rho^{\\beta}}[Q^{\\mu}(s,\\mu_{\\theta}(s))]$</li>\n</ul>\n</li>\n<li>off-policy deterministic policy gradient<ul>\n<li>$ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $<ul>\n<li>논문에는 아래와 같이 나와있는데, 물결 표시 부분은 오류로 판단됨.</li>\n<li>$ \\begin{eqnarray}<br>  \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) &amp;\\approx&amp; \\int_{S}\\rho^{\\beta}(s)\\nabla_{\\theta}\\mu_{\\theta}(a|s)Q^{\\mu}(s,a)ds \\nonumber \\<br>  &amp;=&amp; E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]<br>  \\end{eqnarray} $</li>\n<li>근거: Action이 deterministic 하기에 stochastic 경우와는 다르게 performance objective 에서 action 에 대해 평균을 구할 필요가 없음. 그렇기에, 곱의 미분이 있을 필요가 없고, [Degris, 2012b] 에서처럼 곱의 미분을 통해 생기는 action-value function 에 대한 gradient term 를 생략할 필요가 사라짐. <a href=\"#off-policy-actor-critic\">참고</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Remind: 큐러닝(Q-learning) update rule<ul>\n<li>$ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma \\max\\limits_{a}Q(s_{t+1},a) - Q(s_{t},a_{t})) $</li>\n</ul>\n</li>\n<li>algorithm: OPDAC (Off-Policy Deterministic Actor-Critic)<ul>\n<li>살사를 이용한 on-policy deterministic actor-critic 과 아래 부분을 제외하고는 같음<ul>\n<li>target policy 는 $ \\beta(a|s) $ 에 의해 생성된 trajectories 를 통해 학습함</li>\n<li>업데이트 목표 부분에 실제 행동 값 $ a_{t+1} $ 이 아니라 정책으로부터 나온 행동 값 $ \\mu_{\\theta}(s_{t+1}) $ 사용함<ul>\n<li>$ \\mu_{\\theta}(s_{t+1}) $ 는 가장 높은 Q 값을 가지는 행동. 즉, Q-learning.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $</li>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $</li>\n</ul>\n</li>\n<li>Stochastic off-policy actor-critic 은 대개 actor 와 critic 모두 importance sampling을 필요로 하지만, deterministic policy gradient 에선 importance sampling이 필요없음<ul>\n<li>Actor 는 deterministic 이기에 sampling 자체가 필요없음<ul>\n<li>Stochastic policy 인 경우, Actor 에서 importance sampling이 필요한 이유는 상태 $ s $ 에서의 가치 함수 값 $ V^{\\pi}(s) $ 을 estimate 하기 위해 $ \\pi $ 가 아니라 $ \\beta $ 에 따라 sampling을 한 후, 평균을 내기 때문임.</li>\n<li>하지만 Deterministic policy 인 경우, 상태 $ s $ 에서의 가치 함수 값 $ V^{\\pi}(s) = Q^{\\pi}(s,\\mu_{\\theta}) $ 즉, action 이 상태 s 에 대해 deterministic 이기에 sampling 을 통해 estimate 할 필요가 없고, 따라서 importance sampling도 필요없어짐.</li>\n<li>stochastic vs. deterministic performance objective<ul>\n<li>stochastic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads $</li>\n<li>deterministic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Critic 이 사용하는 Q-learning 은 importance sampling이 필요없는 off policy 알고리즘임.<ul>\n<li>Q-learning 도 업데이트 목표를 특정 분포에서 샘플링을 통해 estimate 하는 것이 아니라 Q 함수를 최대화하는 action을 선택하는 것이기에 위 actor 에서의 deterministic 경우와 비슷하게 볼 수 있음</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>compatible function approximation 및 gradient temporal-difference learning 을 이용한 actor-critic<ul>\n<li>위 살사/Q-learning 기반 on/off-policy 는 아래와 같은 문제가 존재<ul>\n<li>function approximator 에 의한 bias<ul>\n<li>일반적으로 $ Q^{\\mu}(s,a) $ 를 $ Q^{w}(s,a) $ 로 대체하여 deterministic policy gradient 를 구하면, 그 gradient 는 ascent 하는 방향이 아닐 수도 있음</li>\n</ul>\n</li>\n<li>off-policy learning 에 의한 instabilities</li>\n</ul>\n</li>\n<li>그래서 stochastic 처럼 $ \\nabla_{a}Q^{\\mu}(s,a) $ 를 $ \\nabla_{a}Q^{w}(s,a) $ 로 대체해도 deterministic policy gradient 에 영향을 미치지 않을 compatible function approximator $ Q^{w}(s,a) $ 를 찾아야 함</li>\n<li>Theorem 3. 아래 두 조건을 만족하면, $ Q^{w}(s,a) $ 는 deterministic policy $ \\mu_{\\theta}(s) $ 와 compatible 함. 즉, $ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $<ul>\n<li>$ \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $</li>\n<li>$ w $ 는 $ MSE(\\theta, w) = E[\\epsilon(s;\\theta,w)^{\\top}\\epsilon(s;\\theta,w)] $ 를 최소화함<ul>\n<li>$ \\epsilon(s;\\theta,w) = \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} - \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}  $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Theorem 3 은 on-policy 뿐만 아니라 off-policy 에도 적용 가능함</li>\n<li>$ Q^{w}(s,a) = (a-\\mu_{\\theta}(s))^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top} w + V^{v}(s) $<ul>\n<li>어떠한 deterministic policy 에 대해서도 위 형태와 같은 compatible function approximator 가 존재함.</li>\n<li>앞의 term 은 advantage 를, 뒤의 term 은 value 로 볼 수 있음</li>\n</ul>\n</li>\n<li>$ Q^{w}(s,a) = \\phi(s,a)^{\\top} w + v^{\\top}\\phi(s) $<ul>\n<li>정의 : $ \\phi(s,a) \\overset{\\underset{\\mathrm{def}}{}}{=} \\nabla_{\\theta}\\mu_{\\theta}(s)(a-\\mu_{\\theta}(s)) $</li>\n<li>일례 : $ V^{v}(s) = v^{\\top}\\phi(s) $</li>\n<li>Theorem 3 만족 여부<ul>\n<li>첫 번째 조건 만족.</li>\n<li>두 번째 조건은 대강 만족.<ul>\n<li>$ \\nabla_{a}Q^{\\mu}(s,a) $ 에 대한 unbiased sample 을 획득하기는 매우 어렵기에, 일반적인 정책 평가 방법들로 $ w $ 학습.</li>\n<li>이 정책 평가 방법들을 이용하면 $ Q^{w}(s,a) \\approx Q^{\\mu}(s,a) $ 인 reasonable solution을 찾을 수 있기에 대강 $ \\nabla_{a}Q^{w}(s,a) \\approx \\nabla_{a}Q^{\\mu}(s,a) $ 이 될 것.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>action-value function 에 대한 linear function approximator 는 큰 값을 가지는 actions 에 대해선 diverge할 수 있어 global 하게 action-values 예측하기에는 좋지 않지만, local critic 에 사용할 때는 매우 유용하다.<ul>\n<li>즉, 절대값이 아니라 작은 변화량을 다루는 gradient method 경우엔 $ A^{w}(s,\\mu_{\\theta}(s)+\\delta) = \\delta^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $ 로, diverge 하지 않고, 값을 얻을 수 있음</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>COPDAC-Q algorithm (Compatible Off-Policy Deterministic Actor-Critic Q-learning critic)<ul>\n<li>Critic: 실제 action-value function 에 대한 linear function approximator 인 $ Q^{w}(s,a) = \\phi(s,a)^{\\top}w $ 를 estimate<ul>\n<li>$ \\phi(s,a) = a^{\\top}\\nabla_{\\theta}\\mu_{\\theta} $</li>\n<li>Behavior policy $ \\beta(a|s) $ 로부터 얻은 samples를 이용하여 Q-learning 이나 gradient Q-learning 과 같은 off-policy algorithm 으로 학습 가능함</li>\n</ul>\n</li>\n<li>Actor: estimated action-value function 에 대한 gradient 를 $ \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $ 로 치환 후, 정책을 업데이트함</li>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) $</li>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $</li>\n</ul>\n</li>\n<li>off-policy Q-learning은 linear function approximation을 이용하면 diverge 할 수도 있음<ul>\n<li>$ \\mu_{\\theta}(s_{t+1}) $ 이 diverge 할 수도 있기 때문으로 판단됨.</li>\n<li>그렇기에 simple Q-learning 대신 다른 기법이 필요함.</li>\n</ul>\n</li>\n<li>그렇기에 critic 에 gradient Q-learning 사용한 COPDAC-GQ (Gradient Q-learning critic) algorithm 제안<ul>\n<li>gradient temporal-difference learning 에 기반한 기법들은 true gradient descent algorithm 이며, converge가 보장됨. (Sutton, 2009)<ul>\n<li>기본 아이디어는 stochastic gradient descent 로 Mean-squared projected Bellman error (MSPBE) 를 최소화하는 것</li>\n<li>critic 이 actor 보다 빠른 time-scale 로 update 되도록 step size 들을 잘 조절하면, critic 은 MSPBE 를 최소화하는 parameters 로 converge 하게 됨</li>\n<li>critic 에 gradient temporal-difference learning 의 일종인 gradient Q-learning 사용한 논문 (Maei, 2010)</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>COPDAC-GQ algorithm<ul>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $</li>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) - \\alpha_{w}\\gamma\\phi(s_{t+1}, \\mu_{\\theta}(s_{t+1}))(\\phi(s_{t},a_{t})^{\\top} u_{t}) $</li>\n<li>$ v_{t+1} = v_{t} + \\alpha_{v}\\delta_{t}\\phi(s_{t}) - \\alpha_{v}\\gamma\\phi(s_{t+1})(\\phi(s_{t},a_{t})^{\\top} u_{t}) $</li>\n<li>$ u_{t+1} = u_{t} + \\alpha_{u}(\\delta_{t}-\\phi(s_{t}, a_{t})^{\\top} u_{t})\\phi(s_{t}, a_{t}) $</li>\n</ul>\n</li>\n<li>stochastic actor-critic 과 같이 매 time-step 마다 update 시 필요한 계산의 복잡도는 $ O(mn) $<ul>\n<li>m 은 action dimensions, n 은 number of policy parameters</li>\n</ul>\n</li>\n<li>Natural policy gradient 를 이용해 deterministic policies 를 찾을 수 있음<ul>\n<li>$ M(\\theta)^{-1}\\nabla_{\\theta}J(\\mu_{\\theta}) $ 는 any metric $ M(\\theta) $ 에 대한 our performance objective (식(14)) 의 steepest ascent direction 임 (Toussaint, 2012)</li>\n<li>Natural gradient 는 Fisher information metric $ M_{\\pi}(\\theta) $ 에 대한 steepest ascent direction 임<ul>\n<li>$ M_{\\pi}(\\theta) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)^{\\top}] $</li>\n<li>Fisher information metric 은 policy reparameterization 에 대해 불변임 (Bagnell, 2003)</li>\n</ul>\n</li>\n<li>deterministic policies 에 대해 metric 으로 $ M_{\\mu}(\\theta) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}] $ 을 사용.<ul>\n<li>이는 variance 가 0 인 policy 에 대한 Fisher information metric 으로 볼 수 있음</li>\n<li>$ \\frac{\\nabla_{\\theta}\\pi_{\\theta}(a\\vert s)}{\\pi_{\\theta}(a\\vert s)}$ 에서 policy variance 가 0 이면, 특정 s 에 대한 $ \\pi_{\\theta}(a|s)$ 만 1 이 되고, 나머지는 0 임</li>\n</ul>\n</li>\n<li>deterministic policy gradient theorem 과 compatible function approximation 을 결합하면 $ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] $ 이 됨<ul>\n<li>$ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $</li>\n<li>$ \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)} \\approx \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $</li>\n</ul>\n</li>\n<li>그렇기에 steepest ascent direction 은 $ M_{\\mu}(\\theta)^{-1}\\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = w $ 이 됨<ul>\n<li>$ E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}]^{-1}E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] = w $</li>\n</ul>\n</li>\n<li>이 알고리즘은 COPDAC-Q 혹은 COPDAC-GQ 에서 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $ 식을 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta}w_{t} $ 로 바꿔주기만 하면 됨</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"Experiments\"><a href=\"#Experiments\" class=\"headerlink\" title=\"Experiments\"></a>Experiments</h2><h3 id=\"Continuous-Bandit\"><a href=\"#Continuous-Bandit\" class=\"headerlink\" title=\"Continuous Bandit\"></a>Continuous Bandit</h3><ul>\n<li>Stochastic Actor-Critic (SAC)과 COPDAC 간 성능 비교 수행<ul>\n<li>Action dimension이 커질수록 성능 차이가 심함</li>\n<li>빠르게 수렴하는 것을 통해 DPG의 data efficiency가 SPG에 비해 좋다는 것을 확인할 수 있지만, 반면, time-step이 증가할수록 SAC와 COPDAC 간 성능 차이가 줄어드는 것을 통해 성능 차이가 심하다는 것은 일정 time step 내에서만 해당하는 것이라고 유추해볼 수 있음</li>\n<li><img src=\"https://www.dropbox.com/s/hrkyq0s2f24z66r/Screenshot%202018-06-16%2017.47.38.png?dl=1\"></li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Continuous-Reinforcement-Learning\"><a href=\"#Continuous-Reinforcement-Learning\" class=\"headerlink\" title=\"Continuous Reinforcement Learning\"></a>Continuous Reinforcement Learning</h3><ul>\n<li>COPDAC-Q가 SAC와 off-policy stochastic actor-critic(OffPAC-TD) 간 성능 비교 수행<ul>\n<li>COPDAC-Q의 성능이 약간 더 좋음</li>\n<li>COPDAC-Q의 학습이 더 빨리 이뤄짐</li>\n<li><img src=\"https://www.dropbox.com/s/qdca4augapmzsxi/Screenshot%202018-06-16%2017.47.07.png?dl=1\"><h3 id=\"Octopus-Arm\"><a href=\"#Octopus-Arm\" class=\"headerlink\" title=\"Octopus Arm\"></a>Octopus Arm</h3></li>\n</ul>\n</li>\n<li>목표: 6 segments octopus arm (20 action dimensions &amp; 50 state dimensions)을 control하여 target을 맞추는 것<ul>\n<li>COPDAC-Q 사용 시, action space dimension이 큰 octopus arm을 잘 control하여 target을 맞춤</li>\n<li><img src=\"https://www.dropbox.com/s/xrxb0a52wntekld/Screenshot%202018-06-16%2017.46.28.png?dl=1\" width=\"600px\"></li>\n<li>기존 기법들은 action spaces 혹은 action 과 state spaces 둘 다 작은 경우들에 대해서만 실험했다고 하며, 비교하고 있지 않음.<ul>\n<li>기존 기법들이 6 segments octopus arm에서 동작을 잘 안 했을 것 같긴한데, 그래도 실험해서 결과를 보여주지…왜 안 했을까?</li>\n</ul>\n</li>\n<li>8 segment arm 동영상이 저자 홈페이지에 있다고 하는데, 안 보임.</li>\n</ul>\n</li>\n<li>[참고] Octopus Arm 이란?<ul>\n<li><a href=\"https://www.youtube.com/watch?v=AxeeHif0euY\" target=\"_blank\" rel=\"noopener\">OctopusArm Youtube Link</a></li>\n<li><img src=\"https://www.dropbox.com/s/950ycj06sudakjx/Screenshot%202018-06-16%2017.45.52.png?dl=1\"></li>\n</ul>\n</li>\n</ul>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"cjiinnj6m0001j28ap6bd88y6","category_id":"cjiinnj6o0003j28aoi038xjq","_id":"cjiinnj6t0009j28ar5qxxlpm"},{"post_id":"cjiinnj6n0002j28aigqpmv52","category_id":"cjiinnj6o0003j28aoi038xjq","_id":"cjiinnj6v000cj28a4b9xquku"},{"post_id":"cjiinnj6q0005j28a4kpaxi6x","category_id":"cjiinnj6o0003j28aoi038xjq","_id":"cjiinnj6x000fj28ahxrt77uz"}],"PostTag":[{"post_id":"cjiinnj6m0001j28ap6bd88y6","tag_id":"cjiinnj6q0004j28ajv2qofj6","_id":"cjiinnj6u000bj28a2flbzci1"},{"post_id":"cjiinnj6m0001j28ap6bd88y6","tag_id":"cjiinnj6r0007j28a9ifjaq6c","_id":"cjiinnj6v000dj28avnv7w7re"},{"post_id":"cjiinnj6n0002j28aigqpmv52","tag_id":"cjiinnj6q0004j28ajv2qofj6","_id":"cjiinnj6x000hj28a6oe113d1"},{"post_id":"cjiinnj6n0002j28aigqpmv52","tag_id":"cjiinnj6r0007j28a9ifjaq6c","_id":"cjiinnj6x000ij28ae2s24zk6"},{"post_id":"cjiinnj6q0005j28a4kpaxi6x","tag_id":"cjiinnj6q0004j28ajv2qofj6","_id":"cjiinnj6y000kj28a6c947dl2"},{"post_id":"cjiinnj6q0005j28a4kpaxi6x","tag_id":"cjiinnj6r0007j28a9ifjaq6c","_id":"cjiinnj6y000lj28adzfrq45d"}],"Tag":[{"name":"프로젝트","_id":"cjiinnj6q0004j28ajv2qofj6"},{"name":"피지여행","_id":"cjiinnj6r0007j28a9ifjaq6c"}]}}